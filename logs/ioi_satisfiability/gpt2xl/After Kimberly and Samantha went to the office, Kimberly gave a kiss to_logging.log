[2024-07-24 10:30:32,292][explain_satisfiability.py][line:287][INFO] ############ CASE TEXT isAfter Kimberly and Samantha went to the office, Kimberly gave a kiss to
[2024-07-24 10:30:32,292][explain_satisfiability.py][line:288][INFO] ############ CASE Prediction is  Samantha
[2024-07-24 10:30:32,292][explain_satisfiability.py][line:289][INFO] ############ Refined Forward Graph
[2024-07-24 10:30:32,292][explain_satisfiability.py][line:290][INFO] ****** Layer 1
[2024-07-24 10:30:32,292][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 0
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 1
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 2
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit21', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,293][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 3
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 4
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 5
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,294][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 6
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 7
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 8
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 9
[2024-07-24 10:30:32,295][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 10
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 11
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 12
[2024-07-24 10:30:32,296][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 13
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 14
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit28']
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 15
[2024-07-24 10:30:32,297][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 16
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 17
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit10']
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 18
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,298][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 19
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit26']
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 20
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit27']
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 21
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 22
[2024-07-24 10:30:32,299][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 23
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 24
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 25
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit23', 'circuit26']
[2024-07-24 10:30:32,300][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 26
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 27
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 28
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,301][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 0
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit5', 'circuit6', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 1
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,302][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 2
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit26']
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 3
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 4
[2024-07-24 10:30:32,303][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 5
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 6
[2024-07-24 10:30:32,304][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 7
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit23']
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 8
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,305][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 9
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 10
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,306][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 11
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 12
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit17']
[2024-07-24 10:30:32,307][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 13
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 14
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:30:32,308][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 15
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit19', 'circuit21', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 16
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,309][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 17
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 18
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 19
[2024-07-24 10:30:32,310][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 20
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit19', 'circuit20']
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 21
[2024-07-24 10:30:32,311][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit15', 'circuit17', 'circuit21', 'circuit22']
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 22
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit6', 'circuit27']
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 23
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,312][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 24
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 25
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,313][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 26
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 27
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,314][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 28
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 0
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,315][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 1
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 2
[2024-07-24 10:30:32,316][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 3
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit27']
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit18', 'circuit19']
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,317][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 4
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 5
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,318][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit4', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit17', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 6
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 7
[2024-07-24 10:30:32,319][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 8
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,320][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 9
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 10
[2024-07-24 10:30:32,321][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 11
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16']
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,322][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 12
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 13
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,323][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit6', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 14
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 15
[2024-07-24 10:30:32,324][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 16
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,325][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 17
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 18
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,326][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 19
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,327][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 20
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 21
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,328][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 22
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 23
[2024-07-24 10:30:32,329][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 24
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,330][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 25
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 26
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,331][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 27
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,332][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 28
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 0
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,333][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 1
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit19']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit18', 'circuit19', 'circuit22']
[2024-07-24 10:30:32,334][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 2
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit24']
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19']
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18']
[2024-07-24 10:30:32,335][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 3
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit28']
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit27']
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 4
[2024-07-24 10:30:32,336][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 5
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,337][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17']
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 6
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,338][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 7
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit8', 'circuit11', 'circuit14', 'circuit26']
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 8
[2024-07-24 10:30:32,339][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 9
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,340][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit20', 'circuit22']
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit11', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 10
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,341][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 11
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,342][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 12
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 13
[2024-07-24 10:30:32,343][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit6', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 14
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,344][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 15
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,345][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 16
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,346][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 17
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 18
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,347][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 19
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,348][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 20
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,349][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 21
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 22
[2024-07-24 10:30:32,350][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 23
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,351][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 24
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,352][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 25
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,353][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 26
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 27
[2024-07-24 10:30:32,354][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 28
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,355][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 0
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,356][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 1
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,357][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 2
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,358][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit20', 'circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 3
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,359][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 4
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24']
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,360][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 5
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit18', 'circuit21']
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,361][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 6
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,362][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 7
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 8
[2024-07-24 10:30:32,363][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 9
[2024-07-24 10:30:32,364][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16']
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 10
[2024-07-24 10:30:32,365][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit21', 'circuit23']
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 11
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,366][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 12
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,367][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 13
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,368][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 14
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,369][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 15
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,370][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 16
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,371][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 17
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,372][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 18
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,373][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 19
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,374][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 20
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,375][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 21
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,376][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 22
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,377][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 23
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 24
[2024-07-24 10:30:32,378][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 25
[2024-07-24 10:30:32,379][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 26
[2024-07-24 10:30:32,380][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 27
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,381][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 28
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,382][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 0
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,383][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit7', 'circuit8', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 1
[2024-07-24 10:30:32,384][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit14']
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17']
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit18']
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 2
[2024-07-24 10:30:32,385][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit26']
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,386][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 3
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit21', 'circuit22', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit25']
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,387][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit16', 'circuit19', 'circuit22']
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 4
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,388][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 5
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,389][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 6
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,390][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 7
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit26']
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,391][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 8
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit28']
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit14', 'circuit26']
[2024-07-24 10:30:32,392][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 9
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,393][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 10
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,394][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit18']
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit26']
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 11
[2024-07-24 10:30:32,395][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,396][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 12
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit15']
[2024-07-24 10:30:32,397][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 13
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,398][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 14
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,399][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 15
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,400][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 16
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17']
[2024-07-24 10:30:32,401][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 17
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,402][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 18
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,403][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 19
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,404][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 20
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,405][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 21
[2024-07-24 10:30:32,406][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,407][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 22
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,408][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 23
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,409][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 24
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,410][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 25
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,411][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 26
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,412][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 27
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,413][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 28
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,414][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 0
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,415][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit11', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,416][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 1
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit17', 'circuit18']
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15']
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16']
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,417][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 2
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit26']
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,418][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 3
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,419][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit19']
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 4
[2024-07-24 10:30:32,420][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit13', 'circuit14']
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15']
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,421][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 5
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,422][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 6
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,423][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 7
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:30:32,424][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 8
[2024-07-24 10:30:32,425][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,426][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 9
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,427][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 10
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,428][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 11
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,429][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 12
[2024-07-24 10:30:32,430][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,431][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit11', 'circuit13']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 13
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,432][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit11', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 14
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,433][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 15
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,434][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 16
[2024-07-24 10:30:32,435][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit15', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,436][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 17
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,437][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 18
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,438][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 19
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,439][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 20
[2024-07-24 10:30:32,440][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,441][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 21
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,442][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 22
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,443][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 23
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,444][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 24
[2024-07-24 10:30:32,445][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,446][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 25
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,447][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 26
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,448][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 27
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,449][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,450][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 28
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,451][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 0
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,452][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 1
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,453][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,454][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 2
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,455][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit21']
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 3
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit27']
[2024-07-24 10:30:32,456][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,457][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 4
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,458][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 5
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,459][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 6
[2024-07-24 10:30:32,460][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,461][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 7
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,462][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 8
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,463][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,464][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 9
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,465][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 10
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit27']
[2024-07-24 10:30:32,466][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 11
[2024-07-24 10:30:32,467][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,468][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 12
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,469][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 13
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,470][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit26']
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,471][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 14
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,472][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 15
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,473][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,474][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 16
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,475][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 17
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,476][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 18
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,477][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,478][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 19
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,479][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 20
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,480][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,481][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 21
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,482][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 22
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,483][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 23
[2024-07-24 10:30:32,484][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,485][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 24
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,486][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 25
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,487][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,488][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 26
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,489][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 27
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,490][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,491][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 28
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,492][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 0
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,493][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit6', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,494][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 1
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit6', 'circuit13', 'circuit16', 'circuit20', 'circuit26']
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:30:32,495][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 2
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,496][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit20']
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit23']
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,497][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 3
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit6', 'circuit7', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit23']
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:30:32,498][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit18', 'circuit23']
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 4
[2024-07-24 10:30:32,499][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit21']
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,500][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 5
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18']
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15']
[2024-07-24 10:30:32,501][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,502][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 6
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15']
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,503][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 7
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,504][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,505][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 8
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,506][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 9
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit27']
[2024-07-24 10:30:32,507][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit5', 'circuit6', 'circuit16']
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit21', 'circuit23']
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit25']
[2024-07-24 10:30:32,508][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19']
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit19', 'circuit20', 'circuit21', 'circuit23']
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 10
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,509][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit24']
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit25']
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit14']
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 11
[2024-07-24 10:30:32,510][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit23']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit25']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,511][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 12
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20']
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit17']
[2024-07-24 10:30:32,512][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit18']
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,513][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 13
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit8', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,514][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 14
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit24']
[2024-07-24 10:30:32,515][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,516][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 15
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,517][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit22']
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit14']
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 16
[2024-07-24 10:30:32,518][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,519][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 17
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit28']
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:30:32,520][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit15']
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,521][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 18
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit23', 'circuit26']
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19']
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit20']
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,522][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit22']
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 19
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,523][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,524][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 20
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,525][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 21
[2024-07-24 10:30:32,526][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,527][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 22
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,528][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,529][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 23
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,530][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 24
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,531][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,532][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 25
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,533][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 26
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,534][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,535][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 27
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,536][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,537][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 28
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,538][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 0
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,539][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,540][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 1
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,541][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit22', 'circuit27']
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit14']
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20', 'circuit23']
[2024-07-24 10:30:32,542][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit6', 'circuit7', 'circuit9', 'circuit13']
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 2
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit4', 'circuit5', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:30:32,543][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit19', 'circuit24']
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit15', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 3
[2024-07-24 10:30:32,544][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit22', 'circuit23']
[2024-07-24 10:30:32,545][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit25']
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 4
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,546][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit8', 'circuit10', 'circuit12', 'circuit14', 'circuit16', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit20', 'circuit26']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit26']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit23']
[2024-07-24 10:30:32,547][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit13', 'circuit18', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 5
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,548][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,549][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 6
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit17', 'circuit18', 'circuit26']
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16']
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-24 10:30:32,550][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit17', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 7
[2024-07-24 10:30:32,551][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,552][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20']
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit10', 'circuit12', 'circuit26']
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 8
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,553][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15']
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,554][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 9
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit27']
[2024-07-24 10:30:32,555][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit16']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit19', 'circuit20']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,556][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit24']
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 10
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,557][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 11
[2024-07-24 10:30:32,558][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit4', 'circuit6', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,559][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit10', 'circuit13', 'circuit27']
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit14', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit27']
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit17', 'circuit21', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 12
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,560][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit17']
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,561][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit17']
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 13
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit6', 'circuit9', 'circuit10', 'circuit13', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit18', 'circuit20', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,562][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,563][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 14
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,564][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,565][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 15
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,566][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 16
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,567][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,568][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 17
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,569][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,570][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 18
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,571][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 19
[2024-07-24 10:30:32,572][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,573][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 20
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,574][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,575][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 21
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,576][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,577][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 22
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,578][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 23
[2024-07-24 10:30:32,579][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,580][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 24
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,581][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,582][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 25
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,583][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,584][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 26
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,585][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 27
[2024-07-24 10:30:32,586][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,587][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 28
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,588][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,589][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 0
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit28']
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit17', 'circuit20', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,590][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24', 'circuit26']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit17', 'circuit19', 'circuit21']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit5', 'circuit7', 'circuit8', 'circuit10', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit25']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit18', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,591][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit21', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit11', 'circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 1
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,592][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,593][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 2
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,594][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,595][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 3
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14']
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,596][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit26']
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit19']
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit18']
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,597][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 4
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22', 'circuit24']
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit23', 'circuit24']
[2024-07-24 10:30:32,598][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit7', 'circuit10']
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,599][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 5
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,600][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 6
[2024-07-24 10:30:32,601][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit20', 'circuit23', 'circuit25']
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,602][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 7
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-24 10:30:32,603][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,604][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 8
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,605][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,606][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 9
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,607][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,608][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 10
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit16', 'circuit18']
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,609][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,610][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 11
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,611][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,612][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 12
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,613][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,614][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 13
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16']
[2024-07-24 10:30:32,615][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,616][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 14
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,617][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,618][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 15
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,619][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 16
[2024-07-24 10:30:32,620][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,621][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 17
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,622][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,623][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 18
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,624][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,625][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 19
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,626][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,627][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 20
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,628][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,629][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 21
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,630][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,631][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 22
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,632][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,633][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 23
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,634][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 24
[2024-07-24 10:30:32,635][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,636][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 25
[2024-07-24 10:30:32,637][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:30:32,638][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 26
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,639][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,640][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 27
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,641][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,642][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 28
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,643][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,644][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,645][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,645][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:32,645][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:30:34,387][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:34,389][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,390][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,391][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,392][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,394][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,397][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,400][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,404][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,405][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,406][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,407][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,408][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,411][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.9137, 0.0863], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,413][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([5.7728e-04, 9.9942e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,417][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.9696, 0.0304], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,418][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.1250, 0.8750], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,419][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0969, 0.9031], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,420][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,420][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.9108, 0.0892], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,423][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.9821, 0.0179], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,426][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.8998, 0.1002], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,429][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.9358, 0.0642], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,431][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.7165, 0.2835], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,432][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.7902, 0.2098], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,433][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.7560, 0.1799, 0.0641], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,433][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ and] are: tensor([4.4094e-03, 5.7634e-04, 9.9501e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,435][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4158, 0.0471, 0.5370], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,438][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2427, 0.0460, 0.7113], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,442][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.5967, 0.2286, 0.1747], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,444][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1843, 0.0052, 0.8105], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,445][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5740, 0.4052, 0.0208], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,446][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3820, 0.4216, 0.1963], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,446][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2571, 0.0272, 0.7157], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,448][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6279, 0.1133, 0.2589], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,451][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.6830, 0.0782, 0.2388], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,455][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.5265, 0.1430, 0.3304], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,457][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.4269, 0.1186, 0.3577, 0.0968], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,458][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([6.6725e-05, 2.4227e-04, 1.5137e-04, 9.9954e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,459][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.6210, 0.0304, 0.1753, 0.1733], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,460][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([3.6485e-03, 1.0442e-03, 8.0327e-05, 9.9523e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,462][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0484, 0.1255, 0.0035, 0.8225], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,464][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([3.3452e-03, 1.4990e-04, 2.0140e-07, 9.9650e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,468][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.4606, 0.2465, 0.1555, 0.1374], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,470][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.3680, 0.0580, 0.4564, 0.1176], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,471][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.5632, 0.0989, 0.2345, 0.1035], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,472][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.5651, 0.1649, 0.2500, 0.0200], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,473][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.4287, 0.1001, 0.1896, 0.2816], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,473][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.2015, 0.2044, 0.3767, 0.2174], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,475][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.4937, 0.0778, 0.0873, 0.0689, 0.2723], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,478][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0012, 0.0043, 0.0018, 0.0017, 0.9911], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,482][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.6280, 0.0482, 0.1987, 0.0351, 0.0901], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,484][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0306, 0.0060, 0.0020, 0.0024, 0.9590], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,485][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.4348, 0.0283, 0.0637, 0.0397, 0.4335], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,486][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ went] are: tensor([2.6716e-02, 1.5940e-04, 6.5624e-05, 1.9082e-05, 9.7304e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,486][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.2102, 0.3673, 0.0319, 0.3594, 0.0312], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,489][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.2440, 0.1522, 0.2595, 0.1282, 0.2161], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,492][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.4064, 0.0503, 0.3958, 0.0813, 0.0662], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,495][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.4393, 0.1147, 0.2147, 0.1133, 0.1180], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,497][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.4191, 0.0867, 0.1887, 0.0418, 0.2637], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,498][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.4399, 0.1233, 0.2190, 0.1070, 0.1108], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,499][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.4080, 0.1123, 0.0549, 0.1748, 0.2207, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,500][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([5.9612e-03, 6.9306e-04, 6.7567e-02, 1.6005e-04, 8.4831e-04, 9.2477e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,502][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.4016, 0.0453, 0.1809, 0.0323, 0.0815, 0.2583], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,504][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0324, 0.0044, 0.0211, 0.0133, 0.4595, 0.4693], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,508][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.1523, 0.0417, 0.0257, 0.0433, 0.6083, 0.1288], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,511][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0845, 0.0039, 0.1471, 0.0056, 0.0157, 0.7431], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,512][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.2160, 0.2349, 0.0118, 0.2168, 0.0563, 0.2642], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,512][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1284, 0.0872, 0.1093, 0.1677, 0.2790, 0.2283], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,513][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0611, 0.0070, 0.3532, 0.0128, 0.0661, 0.4998], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,515][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3714, 0.0749, 0.1988, 0.0671, 0.0933, 0.1944], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,518][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.3526, 0.0717, 0.2432, 0.0437, 0.0911, 0.1977], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,522][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.3004, 0.1363, 0.1596, 0.1296, 0.1103, 0.1637], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,524][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.4514, 0.1418, 0.0397, 0.1933, 0.1210, 0.0209, 0.0319],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,525][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.1075e-03, 1.6505e-03, 6.6737e-02, 2.4336e-04, 1.9121e-04, 8.7297e-02,
        8.3477e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,526][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3287, 0.0634, 0.1401, 0.0507, 0.1081, 0.2695, 0.0394],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,526][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0505, 0.0131, 0.0233, 0.0095, 0.0852, 0.1823, 0.6361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,528][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.3028, 0.0624, 0.0421, 0.0310, 0.2922, 0.1253, 0.1441],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,531][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1801, 0.0200, 0.1526, 0.0049, 0.0413, 0.1358, 0.4652],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,535][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1829, 0.3299, 0.0042, 0.4226, 0.0512, 0.0068, 0.0025],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,537][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1171, 0.0480, 0.0815, 0.0823, 0.1447, 0.2263, 0.3002],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,538][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0307, 0.0044, 0.1972, 0.0113, 0.0528, 0.2112, 0.4924],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,539][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.3256, 0.0869, 0.1587, 0.0673, 0.0796, 0.1543, 0.1278],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,540][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3565, 0.0794, 0.1712, 0.0466, 0.0453, 0.1163, 0.1847],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,542][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2389, 0.1301, 0.1251, 0.1574, 0.0910, 0.1214, 0.1361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,545][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.2053, 0.1619, 0.0809, 0.2650, 0.0833, 0.0679, 0.0995, 0.0362],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,548][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ office] are: tensor([2.0420e-04, 6.5123e-04, 5.5531e-04, 1.3894e-04, 3.1875e-04, 1.8401e-04,
        8.5779e-05, 9.9786e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,550][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.4260, 0.0785, 0.1116, 0.0588, 0.0668, 0.1043, 0.0847, 0.0693],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,551][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ office] are: tensor([1.7305e-02, 1.9678e-03, 5.0555e-04, 5.5320e-04, 6.2434e-03, 5.1402e-03,
        5.0596e-03, 9.6323e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,552][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.2158, 0.0556, 0.0071, 0.0089, 0.0364, 0.0187, 0.0199, 0.6377],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,553][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ office] are: tensor([4.5810e-03, 1.7134e-04, 2.9906e-06, 2.8348e-05, 1.7529e-05, 6.2133e-08,
        5.0048e-07, 9.9520e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,555][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.1748, 0.3585, 0.0251, 0.2690, 0.0297, 0.0238, 0.0165, 0.1026],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,558][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0680, 0.0047, 0.0571, 0.0061, 0.1084, 0.2255, 0.3426, 0.1875],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,562][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.2337, 0.0527, 0.1726, 0.0407, 0.0978, 0.1349, 0.2170, 0.0505],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,564][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.2912, 0.1058, 0.1328, 0.1093, 0.0944, 0.1104, 0.1109, 0.0452],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,565][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.2007, 0.0786, 0.1291, 0.0454, 0.0596, 0.0938, 0.0833, 0.3093],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,566][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.3736, 0.1328, 0.1263, 0.0893, 0.0542, 0.1064, 0.0495, 0.0679],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,567][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.4488, 0.1383, 0.0207, 0.1387, 0.1213, 0.0189, 0.0600, 0.0396, 0.0139],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,568][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [,] are: tensor([3.4270e-03, 1.6960e-03, 2.5544e-02, 2.1320e-04, 5.2634e-04, 9.1183e-03,
        1.2503e-03, 1.8613e-04, 9.5804e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,571][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2751, 0.0282, 0.1203, 0.0260, 0.0475, 0.0608, 0.0138, 0.0127, 0.4156],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,575][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0525, 0.0039, 0.0082, 0.0020, 0.0750, 0.0453, 0.1197, 0.0458, 0.6475],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,577][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.5137, 0.0712, 0.0152, 0.0302, 0.0713, 0.0514, 0.0281, 0.0448, 0.1739],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,578][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1373, 0.0221, 0.1975, 0.0082, 0.0593, 0.0981, 0.1465, 0.0410, 0.2899],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,579][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2122, 0.3613, 0.0074, 0.2406, 0.0456, 0.0146, 0.0048, 0.1088, 0.0045],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,580][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0536, 0.0219, 0.0340, 0.0567, 0.0692, 0.0911, 0.1794, 0.1447, 0.3493],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,582][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0342, 0.0038, 0.1780, 0.0098, 0.0216, 0.1580, 0.2313, 0.0127, 0.3506],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,584][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.2711, 0.0688, 0.1375, 0.0558, 0.0735, 0.1275, 0.1010, 0.0610, 0.1038],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,588][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.2288, 0.0781, 0.1645, 0.0596, 0.0699, 0.1364, 0.0993, 0.0524, 0.1110],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,591][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2235, 0.1212, 0.1099, 0.1078, 0.0827, 0.1005, 0.0572, 0.0748, 0.1225],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,591][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.2113, 0.0507, 0.1245, 0.0699, 0.0422, 0.1261, 0.0870, 0.1315, 0.1166,
        0.0401], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,592][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([1.7560e-04, 5.7683e-01, 2.0540e-04, 2.7027e-04, 3.2199e-04, 7.5310e-05,
        9.0057e-05, 3.9828e-05, 4.0615e-05, 4.2195e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,593][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.4141, 0.0643, 0.0868, 0.0308, 0.0578, 0.0734, 0.1257, 0.0588, 0.0504,
        0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,594][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([2.8049e-03, 4.9941e-03, 1.7635e-06, 3.0849e-04, 2.3571e-05, 2.1698e-05,
        2.3701e-05, 6.8353e-04, 7.6876e-05, 9.9106e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,597][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([2.1997e-03, 6.3906e-02, 1.0620e-04, 1.4711e-03, 1.0653e-04, 1.8748e-04,
        2.8328e-04, 6.2340e-04, 4.7328e-04, 9.3064e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,599][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([1.6272e-03, 7.1723e-01, 3.5309e-08, 2.4885e-05, 3.9304e-06, 7.3167e-09,
        8.5088e-09, 4.5148e-06, 4.4120e-09, 2.8111e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,603][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.1471, 0.0551, 0.1451, 0.1216, 0.0358, 0.0239, 0.1810, 0.0645, 0.1816,
        0.0442], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,604][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.1086, 0.0051, 0.0470, 0.0123, 0.0616, 0.1115, 0.2054, 0.0544, 0.3393,
        0.0548], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,605][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.3246, 0.0673, 0.1502, 0.0471, 0.0493, 0.0810, 0.1189, 0.0250, 0.0889,
        0.0478], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,606][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.2749, 0.0268, 0.1409, 0.0755, 0.0577, 0.1134, 0.1194, 0.0532, 0.1201,
        0.0183], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,608][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.1391, 0.2727, 0.0864, 0.0506, 0.0361, 0.0655, 0.0664, 0.0261, 0.0559,
        0.2013], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,611][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.1235, 0.0863, 0.1189, 0.0905, 0.1188, 0.1003, 0.0867, 0.1064, 0.0924,
        0.0762], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,615][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.2380, 0.0553, 0.0287, 0.1295, 0.1926, 0.0187, 0.0177, 0.0606, 0.0205,
        0.0641, 0.1742], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,617][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([9.4495e-04, 1.7525e-03, 2.3541e-03, 9.2108e-04, 8.8272e-03, 1.1956e-03,
        3.2153e-04, 2.5071e-04, 3.6698e-04, 7.2396e-04, 9.8234e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,618][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.3168, 0.0437, 0.0768, 0.0278, 0.0887, 0.1171, 0.0758, 0.0833, 0.0476,
        0.0337, 0.0888], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,619][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([2.3719e-03, 7.9066e-05, 2.9622e-05, 2.8585e-05, 1.0887e-03, 1.2949e-04,
        1.9758e-04, 5.4657e-04, 1.7714e-03, 6.5732e-03, 9.8718e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,620][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.1453, 0.0088, 0.0151, 0.0187, 0.0416, 0.0342, 0.0226, 0.0413, 0.0489,
        0.0308, 0.5929], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,622][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([4.7311e-02, 1.5741e-04, 3.9515e-05, 4.5281e-05, 1.0254e-02, 4.1308e-06,
        1.4030e-05, 1.7029e-05, 1.9015e-06, 1.5360e-05, 9.4214e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,625][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.1098, 0.1880, 0.0152, 0.2146, 0.0242, 0.0110, 0.0146, 0.1289, 0.0122,
        0.2555, 0.0260], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,628][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0507, 0.0116, 0.0226, 0.0163, 0.0256, 0.0513, 0.0859, 0.0589, 0.2109,
        0.1989, 0.2672], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,630][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1257, 0.0346, 0.1367, 0.0353, 0.0458, 0.1300, 0.1779, 0.0712, 0.1593,
        0.0391, 0.0444], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,631][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.1969, 0.0731, 0.1044, 0.0434, 0.0854, 0.1017, 0.0833, 0.0704, 0.0922,
        0.0740, 0.0751], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,632][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.1655, 0.0633, 0.0979, 0.0472, 0.1044, 0.0933, 0.0658, 0.0316, 0.0712,
        0.0459, 0.2138], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,633][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.2842, 0.0667, 0.1197, 0.0563, 0.0480, 0.1015, 0.0450, 0.0368, 0.0987,
        0.0703, 0.0729], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,635][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2529, 0.1188, 0.0238, 0.1142, 0.0621, 0.0154, 0.0284, 0.0582, 0.0245,
        0.1562, 0.1227, 0.0229], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,637][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([3.0344e-03, 1.5721e-03, 3.7297e-03, 4.4447e-04, 1.0823e-04, 9.8377e-03,
        3.0974e-02, 2.7116e-04, 1.0092e-03, 6.5362e-04, 2.1885e-04, 9.4815e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,641][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1840, 0.0541, 0.1199, 0.0370, 0.0829, 0.1943, 0.0274, 0.0378, 0.0791,
        0.0523, 0.1058, 0.0254], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,643][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([1.2275e-02, 3.8843e-04, 9.6358e-04, 4.3734e-04, 2.0242e-03, 4.1789e-03,
        1.6459e-02, 3.3595e-03, 5.0385e-02, 1.5837e-02, 1.3697e-01, 7.5672e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,644][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0597, 0.0103, 0.0060, 0.0062, 0.0612, 0.0159, 0.0171, 0.0315, 0.0308,
        0.0656, 0.5190, 0.1766], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,645][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0653, 0.0033, 0.1042, 0.0018, 0.0110, 0.0923, 0.2241, 0.0113, 0.0313,
        0.0006, 0.0075, 0.4474], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,647][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1094, 0.1798, 0.0031, 0.2185, 0.0453, 0.0058, 0.0024, 0.0887, 0.0022,
        0.2827, 0.0582, 0.0039], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,650][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0338, 0.0080, 0.0109, 0.0133, 0.0229, 0.0220, 0.0385, 0.0305, 0.1134,
        0.1516, 0.2796, 0.2756], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,654][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0159, 0.0023, 0.0802, 0.0037, 0.0221, 0.0970, 0.2275, 0.0138, 0.1537,
        0.0038, 0.0280, 0.3522], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,656][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1881, 0.0549, 0.1073, 0.0523, 0.0538, 0.1026, 0.0874, 0.0621, 0.0844,
        0.0490, 0.0700, 0.0880], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,657][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1766, 0.0530, 0.1130, 0.0443, 0.0391, 0.1051, 0.1125, 0.0425, 0.0739,
        0.0356, 0.0409, 0.1636], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,658][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1700, 0.0928, 0.0744, 0.0776, 0.0586, 0.0824, 0.0448, 0.0531, 0.0850,
        0.1180, 0.0818, 0.0614], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,659][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.1868, 0.0712, 0.0695, 0.1237, 0.0511, 0.0549, 0.0422, 0.0285, 0.0639,
        0.0557, 0.0658, 0.0348, 0.1519], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,661][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([4.9559e-05, 3.6111e-04, 5.9882e-05, 1.7654e-03, 1.3717e-04, 1.5519e-05,
        1.9007e-06, 3.0179e-05, 7.7887e-06, 1.0305e-04, 2.7624e-05, 2.5335e-06,
        9.9744e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,663][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.2029, 0.1692, 0.0603, 0.0381, 0.0471, 0.0462, 0.0816, 0.0787, 0.0245,
        0.1436, 0.0292, 0.0713, 0.0074], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,666][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([8.4526e-04, 1.0044e-05, 7.7830e-07, 1.3037e-04, 4.5988e-05, 6.2157e-06,
        3.7900e-06, 1.1595e-04, 2.9065e-05, 5.6849e-04, 2.3979e-03, 2.0917e-04,
        9.9564e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,668][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([1.8952e-02, 2.5689e-03, 3.5571e-04, 2.3308e-03, 1.1843e-03, 5.2210e-04,
        5.6876e-04, 1.1817e-03, 9.0799e-04, 6.7035e-03, 3.8290e-03, 2.0768e-03,
        9.5882e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,670][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([1.5195e-03, 1.4228e-05, 3.5548e-08, 3.9900e-05, 8.4897e-07, 2.2790e-09,
        2.4531e-09, 2.6204e-07, 1.1703e-09, 1.3789e-06, 2.8744e-07, 2.8264e-09,
        9.9842e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,671][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0985, 0.1312, 0.0500, 0.2252, 0.0106, 0.0199, 0.0461, 0.0480, 0.0414,
        0.1116, 0.0091, 0.0416, 0.1667], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,672][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0518, 0.0041, 0.0193, 0.0164, 0.0206, 0.0364, 0.0546, 0.0149, 0.1012,
        0.0406, 0.2309, 0.2893, 0.1200], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,673][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.2955, 0.0598, 0.0867, 0.0554, 0.0475, 0.0752, 0.0742, 0.0401, 0.0575,
        0.0383, 0.0672, 0.0714, 0.0312], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,676][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.1707, 0.0951, 0.0854, 0.0595, 0.0627, 0.0705, 0.0619, 0.0698, 0.0731,
        0.0861, 0.0781, 0.0641, 0.0227], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,680][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.1467, 0.0501, 0.0791, 0.0864, 0.0554, 0.0730, 0.0487, 0.0253, 0.0576,
        0.0322, 0.0458, 0.0499, 0.2498], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,683][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.1312, 0.0816, 0.0735, 0.0901, 0.0694, 0.0740, 0.0394, 0.0387, 0.0890,
        0.0881, 0.1176, 0.0439, 0.0636], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,684][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1914, 0.0540, 0.0201, 0.0932, 0.1126, 0.0109, 0.0244, 0.0259, 0.0213,
        0.0693, 0.1208, 0.0245, 0.2170, 0.0144], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,685][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.8451e-03, 1.6725e-04, 2.1798e-02, 5.0568e-05, 2.6964e-04, 4.6070e-01,
        1.1717e-03, 7.7429e-05, 4.8516e-03, 6.7732e-05, 3.8286e-04, 7.0091e-04,
        4.7462e-05, 5.0687e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,685][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2000, 0.0297, 0.0988, 0.0218, 0.0552, 0.1500, 0.0293, 0.0275, 0.0590,
        0.0244, 0.0680, 0.0263, 0.0376, 0.1724], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,687][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([3.8710e-03, 6.0273e-05, 1.8537e-04, 1.5176e-04, 2.6962e-03, 1.9014e-03,
        2.3687e-03, 4.2860e-04, 6.3094e-03, 1.2834e-03, 2.3872e-01, 6.6497e-02,
        1.3024e-01, 5.4529e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,690][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0342, 0.0040, 0.0030, 0.0033, 0.0569, 0.0111, 0.0067, 0.0259, 0.0098,
        0.0245, 0.3153, 0.0707, 0.2173, 0.2173], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,692][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.2171e-02, 1.5051e-03, 6.2463e-02, 2.3409e-03, 8.8713e-03, 3.8590e-01,
        1.4322e-01, 1.0633e-03, 1.3949e-02, 2.9430e-04, 4.4201e-03, 9.7312e-02,
        2.6858e-04, 2.5622e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,696][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0816, 0.1126, 0.0036, 0.1137, 0.0242, 0.1082, 0.0023, 0.0385, 0.0028,
        0.1667, 0.0370, 0.0039, 0.1071, 0.1978], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,697][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0251, 0.0055, 0.0059, 0.0093, 0.0119, 0.0083, 0.0185, 0.0248, 0.0473,
        0.0769, 0.1148, 0.1561, 0.2198, 0.2759], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,698][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0133, 0.0017, 0.0876, 0.0036, 0.0194, 0.1195, 0.1615, 0.0088, 0.1379,
        0.0024, 0.0187, 0.1894, 0.0175, 0.2189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,700][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1594, 0.0383, 0.0912, 0.0374, 0.0486, 0.0871, 0.0779, 0.0510, 0.0729,
        0.0343, 0.0643, 0.0827, 0.0519, 0.1030], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,703][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1240, 0.0367, 0.1067, 0.0349, 0.0659, 0.1194, 0.0831, 0.0408, 0.0762,
        0.0272, 0.0586, 0.0817, 0.0339, 0.1109], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,707][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1540, 0.0841, 0.0701, 0.0759, 0.0576, 0.0694, 0.0338, 0.0391, 0.0731,
        0.1014, 0.0774, 0.0452, 0.0472, 0.0717], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:34,719][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:34,720][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,721][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,722][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,722][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,723][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,724][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,724][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,725][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,726][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,726][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,727][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,728][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:34,731][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.9137, 0.0863], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,733][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([5.7728e-04, 9.9942e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,736][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.9696, 0.0304], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,738][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.1250, 0.8750], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,739][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.0969, 0.9031], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,739][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,740][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.9108, 0.0892], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,742][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.9821, 0.0179], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,746][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.8998, 0.1002], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,749][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.9358, 0.0642], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,751][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.7165, 0.2835], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,751][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.7902, 0.2098], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:34,752][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7560, 0.1799, 0.0641], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,753][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([4.4094e-03, 5.7634e-04, 9.9501e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,754][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.4158, 0.0471, 0.5370], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,756][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2427, 0.0460, 0.7113], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,759][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.5967, 0.2286, 0.1747], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,762][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1843, 0.0052, 0.8105], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,764][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5740, 0.4052, 0.0208], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,765][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.3820, 0.4216, 0.1963], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,766][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.2571, 0.0272, 0.7157], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,766][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6279, 0.1133, 0.2589], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,768][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.6830, 0.0782, 0.2388], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,771][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.5265, 0.1430, 0.3304], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:34,775][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.4269, 0.1186, 0.3577, 0.0968], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,777][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([6.6725e-05, 2.4227e-04, 1.5137e-04, 9.9954e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,778][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.6210, 0.0304, 0.1753, 0.1733], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,779][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([3.6485e-03, 1.0442e-03, 8.0327e-05, 9.9523e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,779][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.0484, 0.1255, 0.0035, 0.8225], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,780][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([3.3452e-03, 1.4990e-04, 2.0140e-07, 9.9650e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,782][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.4606, 0.2465, 0.1555, 0.1374], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,786][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.3680, 0.0580, 0.4564, 0.1176], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,789][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.5632, 0.0989, 0.2345, 0.1035], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,791][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.5651, 0.1649, 0.2500, 0.0200], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,791][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.4287, 0.1001, 0.1896, 0.2816], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,792][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.2015, 0.2044, 0.3767, 0.2174], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:34,793][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.4937, 0.0778, 0.0873, 0.0689, 0.2723], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,795][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0012, 0.0043, 0.0018, 0.0017, 0.9911], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,799][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.6280, 0.0482, 0.1987, 0.0351, 0.0901], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,803][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.0306, 0.0060, 0.0020, 0.0024, 0.9590], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,806][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.4348, 0.0283, 0.0637, 0.0397, 0.4335], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,806][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([2.6716e-02, 1.5940e-04, 6.5624e-05, 1.9082e-05, 9.7304e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,807][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.2102, 0.3673, 0.0319, 0.3594, 0.0312], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,808][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.2440, 0.1522, 0.2595, 0.1282, 0.2161], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,809][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.4064, 0.0503, 0.3958, 0.0813, 0.0662], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,812][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.4393, 0.1147, 0.2147, 0.1133, 0.1180], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,817][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.4191, 0.0867, 0.1887, 0.0418, 0.2637], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,819][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.4399, 0.1233, 0.2190, 0.1070, 0.1108], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:34,820][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.4080, 0.1123, 0.0549, 0.1748, 0.2207, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,820][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([5.9612e-03, 6.9306e-04, 6.7567e-02, 1.6005e-04, 8.4831e-04, 9.2477e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,821][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.4016, 0.0453, 0.1809, 0.0323, 0.0815, 0.2583], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,823][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0324, 0.0044, 0.0211, 0.0133, 0.4595, 0.4693], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,826][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.1523, 0.0417, 0.0257, 0.0433, 0.6083, 0.1288], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,830][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0845, 0.0039, 0.1471, 0.0056, 0.0157, 0.7431], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,832][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.2160, 0.2349, 0.0118, 0.2168, 0.0563, 0.2642], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,833][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1284, 0.0872, 0.1093, 0.1677, 0.2790, 0.2283], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,834][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0611, 0.0070, 0.3532, 0.0128, 0.0661, 0.4998], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,834][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.3714, 0.0749, 0.1988, 0.0671, 0.0933, 0.1944], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,837][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.3526, 0.0717, 0.2432, 0.0437, 0.0911, 0.1977], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,839][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.3004, 0.1363, 0.1596, 0.1296, 0.1103, 0.1637], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:34,843][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.4514, 0.1418, 0.0397, 0.1933, 0.1210, 0.0209, 0.0319],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,845][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([9.1075e-03, 1.6505e-03, 6.6737e-02, 2.4336e-04, 1.9121e-04, 8.7297e-02,
        8.3477e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,846][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3287, 0.0634, 0.1401, 0.0507, 0.1081, 0.2695, 0.0394],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,847][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0505, 0.0131, 0.0233, 0.0095, 0.0852, 0.1823, 0.6361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,848][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.3028, 0.0624, 0.0421, 0.0310, 0.2922, 0.1253, 0.1441],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,850][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1801, 0.0200, 0.1526, 0.0049, 0.0413, 0.1358, 0.4652],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,853][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1829, 0.3299, 0.0042, 0.4226, 0.0512, 0.0068, 0.0025],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,857][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1171, 0.0480, 0.0815, 0.0823, 0.1447, 0.2263, 0.3002],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,859][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0307, 0.0044, 0.1972, 0.0113, 0.0528, 0.2112, 0.4924],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,860][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.3256, 0.0869, 0.1587, 0.0673, 0.0796, 0.1543, 0.1278],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,860][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3565, 0.0794, 0.1712, 0.0466, 0.0453, 0.1163, 0.1847],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,861][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2389, 0.1301, 0.1251, 0.1574, 0.0910, 0.1214, 0.1361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:34,863][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.2053, 0.1619, 0.0809, 0.2650, 0.0833, 0.0679, 0.0995, 0.0362],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,865][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([2.0420e-04, 6.5123e-04, 5.5531e-04, 1.3894e-04, 3.1875e-04, 1.8401e-04,
        8.5779e-05, 9.9786e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,868][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.4260, 0.0785, 0.1116, 0.0588, 0.0668, 0.1043, 0.0847, 0.0693],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,871][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([1.7305e-02, 1.9678e-03, 5.0555e-04, 5.5320e-04, 6.2434e-03, 5.1402e-03,
        5.0596e-03, 9.6323e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,873][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.2158, 0.0556, 0.0071, 0.0089, 0.0364, 0.0187, 0.0199, 0.6377],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,873][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([4.5810e-03, 1.7134e-04, 2.9906e-06, 2.8348e-05, 1.7529e-05, 6.2133e-08,
        5.0048e-07, 9.9520e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,874][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.1748, 0.3585, 0.0251, 0.2690, 0.0297, 0.0238, 0.0165, 0.1026],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,875][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.0680, 0.0047, 0.0571, 0.0061, 0.1084, 0.2255, 0.3426, 0.1875],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,877][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.2337, 0.0527, 0.1726, 0.0407, 0.0978, 0.1349, 0.2170, 0.0505],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,880][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.2912, 0.1058, 0.1328, 0.1093, 0.0944, 0.1104, 0.1109, 0.0452],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,884][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.2007, 0.0786, 0.1291, 0.0454, 0.0596, 0.0938, 0.0833, 0.3093],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,886][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.3736, 0.1328, 0.1263, 0.0893, 0.0542, 0.1064, 0.0495, 0.0679],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:34,887][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.4488, 0.1383, 0.0207, 0.1387, 0.1213, 0.0189, 0.0600, 0.0396, 0.0139],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,888][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([3.4270e-03, 1.6960e-03, 2.5544e-02, 2.1320e-04, 5.2634e-04, 9.1183e-03,
        1.2503e-03, 1.8613e-04, 9.5804e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,889][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2751, 0.0282, 0.1203, 0.0260, 0.0475, 0.0608, 0.0138, 0.0127, 0.4156],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,891][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0525, 0.0039, 0.0082, 0.0020, 0.0750, 0.0453, 0.1197, 0.0458, 0.6475],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,893][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.5137, 0.0712, 0.0152, 0.0302, 0.0713, 0.0514, 0.0281, 0.0448, 0.1739],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,897][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1373, 0.0221, 0.1975, 0.0082, 0.0593, 0.0981, 0.1465, 0.0410, 0.2899],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,900][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.2122, 0.3613, 0.0074, 0.2406, 0.0456, 0.0146, 0.0048, 0.1088, 0.0045],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,900][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0536, 0.0219, 0.0340, 0.0567, 0.0692, 0.0911, 0.1794, 0.1447, 0.3493],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,901][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0342, 0.0038, 0.1780, 0.0098, 0.0216, 0.1580, 0.2313, 0.0127, 0.3506],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,902][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.2711, 0.0688, 0.1375, 0.0558, 0.0735, 0.1275, 0.1010, 0.0610, 0.1038],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,904][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.2288, 0.0781, 0.1645, 0.0596, 0.0699, 0.1364, 0.0993, 0.0524, 0.1110],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,907][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.2235, 0.1212, 0.1099, 0.1078, 0.0827, 0.1005, 0.0572, 0.0748, 0.1225],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:34,911][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.2113, 0.0507, 0.1245, 0.0699, 0.0422, 0.1261, 0.0870, 0.1315, 0.1166,
        0.0401], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,913][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([1.7560e-04, 5.7683e-01, 2.0540e-04, 2.7027e-04, 3.2199e-04, 7.5310e-05,
        9.0057e-05, 3.9828e-05, 4.0615e-05, 4.2195e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,914][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.4141, 0.0643, 0.0868, 0.0308, 0.0578, 0.0734, 0.1257, 0.0588, 0.0504,
        0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,915][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([2.8049e-03, 4.9941e-03, 1.7635e-06, 3.0849e-04, 2.3571e-05, 2.1698e-05,
        2.3701e-05, 6.8353e-04, 7.6876e-05, 9.9106e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,916][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([2.1997e-03, 6.3906e-02, 1.0620e-04, 1.4711e-03, 1.0653e-04, 1.8748e-04,
        2.8328e-04, 6.2340e-04, 4.7328e-04, 9.3064e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,917][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([1.6272e-03, 7.1723e-01, 3.5309e-08, 2.4885e-05, 3.9304e-06, 7.3167e-09,
        8.5088e-09, 4.5148e-06, 4.4120e-09, 2.8111e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,920][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.1471, 0.0551, 0.1451, 0.1216, 0.0358, 0.0239, 0.1810, 0.0645, 0.1816,
        0.0442], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,924][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.1086, 0.0051, 0.0470, 0.0123, 0.0616, 0.1115, 0.2054, 0.0544, 0.3393,
        0.0548], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,927][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.3246, 0.0673, 0.1502, 0.0471, 0.0493, 0.0810, 0.1189, 0.0250, 0.0889,
        0.0478], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,927][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.2749, 0.0268, 0.1409, 0.0755, 0.0577, 0.1134, 0.1194, 0.0532, 0.1201,
        0.0183], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,928][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.1391, 0.2727, 0.0864, 0.0506, 0.0361, 0.0655, 0.0664, 0.0261, 0.0559,
        0.2013], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,929][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.1235, 0.0863, 0.1189, 0.0905, 0.1188, 0.1003, 0.0867, 0.1064, 0.0924,
        0.0762], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:34,931][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.2380, 0.0553, 0.0287, 0.1295, 0.1926, 0.0187, 0.0177, 0.0606, 0.0205,
        0.0641, 0.1742], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,933][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([9.4495e-04, 1.7525e-03, 2.3541e-03, 9.2108e-04, 8.8272e-03, 1.1956e-03,
        3.2153e-04, 2.5071e-04, 3.6698e-04, 7.2396e-04, 9.8234e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,938][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.3168, 0.0437, 0.0768, 0.0278, 0.0887, 0.1171, 0.0758, 0.0833, 0.0476,
        0.0337, 0.0888], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,940][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([2.3719e-03, 7.9066e-05, 2.9622e-05, 2.8585e-05, 1.0887e-03, 1.2949e-04,
        1.9758e-04, 5.4657e-04, 1.7714e-03, 6.5732e-03, 9.8718e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,941][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.1453, 0.0088, 0.0151, 0.0187, 0.0416, 0.0342, 0.0226, 0.0413, 0.0489,
        0.0308, 0.5929], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,942][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([4.7311e-02, 1.5741e-04, 3.9515e-05, 4.5281e-05, 1.0254e-02, 4.1308e-06,
        1.4030e-05, 1.7029e-05, 1.9015e-06, 1.5360e-05, 9.4214e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,942][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.1098, 0.1880, 0.0152, 0.2146, 0.0242, 0.0110, 0.0146, 0.1289, 0.0122,
        0.2555, 0.0260], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,944][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0507, 0.0116, 0.0226, 0.0163, 0.0256, 0.0513, 0.0859, 0.0589, 0.2109,
        0.1989, 0.2672], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,947][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.1257, 0.0346, 0.1367, 0.0353, 0.0458, 0.1300, 0.1779, 0.0712, 0.1593,
        0.0391, 0.0444], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,951][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.1969, 0.0731, 0.1044, 0.0434, 0.0854, 0.1017, 0.0833, 0.0704, 0.0922,
        0.0740, 0.0751], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,953][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.1655, 0.0633, 0.0979, 0.0472, 0.1044, 0.0933, 0.0658, 0.0316, 0.0712,
        0.0459, 0.2138], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,954][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.2842, 0.0667, 0.1197, 0.0563, 0.0480, 0.1015, 0.0450, 0.0368, 0.0987,
        0.0703, 0.0729], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:34,955][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2529, 0.1188, 0.0238, 0.1142, 0.0621, 0.0154, 0.0284, 0.0582, 0.0245,
        0.1562, 0.1227, 0.0229], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,956][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([3.0344e-03, 1.5721e-03, 3.7297e-03, 4.4447e-04, 1.0823e-04, 9.8377e-03,
        3.0974e-02, 2.7116e-04, 1.0092e-03, 6.5362e-04, 2.1885e-04, 9.4815e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,958][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1840, 0.0541, 0.1199, 0.0370, 0.0829, 0.1943, 0.0274, 0.0378, 0.0791,
        0.0523, 0.1058, 0.0254], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,960][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([1.2275e-02, 3.8843e-04, 9.6358e-04, 4.3734e-04, 2.0242e-03, 4.1789e-03,
        1.6459e-02, 3.3595e-03, 5.0385e-02, 1.5837e-02, 1.3697e-01, 7.5672e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,965][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0597, 0.0103, 0.0060, 0.0062, 0.0612, 0.0159, 0.0171, 0.0315, 0.0308,
        0.0656, 0.5190, 0.1766], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,967][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0653, 0.0033, 0.1042, 0.0018, 0.0110, 0.0923, 0.2241, 0.0113, 0.0313,
        0.0006, 0.0075, 0.4474], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,968][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1094, 0.1798, 0.0031, 0.2185, 0.0453, 0.0058, 0.0024, 0.0887, 0.0022,
        0.2827, 0.0582, 0.0039], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,969][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0338, 0.0080, 0.0109, 0.0133, 0.0229, 0.0220, 0.0385, 0.0305, 0.1134,
        0.1516, 0.2796, 0.2756], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,969][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0159, 0.0023, 0.0802, 0.0037, 0.0221, 0.0970, 0.2275, 0.0138, 0.1537,
        0.0038, 0.0280, 0.3522], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,971][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1881, 0.0549, 0.1073, 0.0523, 0.0538, 0.1026, 0.0874, 0.0621, 0.0844,
        0.0490, 0.0700, 0.0880], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,974][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.1766, 0.0530, 0.1130, 0.0443, 0.0391, 0.1051, 0.1125, 0.0425, 0.0739,
        0.0356, 0.0409, 0.1636], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,978][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1700, 0.0928, 0.0744, 0.0776, 0.0586, 0.0824, 0.0448, 0.0531, 0.0850,
        0.1180, 0.0818, 0.0614], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:34,980][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.1868, 0.0712, 0.0695, 0.1237, 0.0511, 0.0549, 0.0422, 0.0285, 0.0639,
        0.0557, 0.0658, 0.0348, 0.1519], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,981][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([4.9559e-05, 3.6111e-04, 5.9882e-05, 1.7654e-03, 1.3717e-04, 1.5519e-05,
        1.9007e-06, 3.0179e-05, 7.7887e-06, 1.0305e-04, 2.7624e-05, 2.5335e-06,
        9.9744e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,982][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.2029, 0.1692, 0.0603, 0.0381, 0.0471, 0.0462, 0.0816, 0.0787, 0.0245,
        0.1436, 0.0292, 0.0713, 0.0074], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,983][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([8.4526e-04, 1.0044e-05, 7.7830e-07, 1.3037e-04, 4.5988e-05, 6.2157e-06,
        3.7900e-06, 1.1595e-04, 2.9065e-05, 5.6849e-04, 2.3979e-03, 2.0917e-04,
        9.9564e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,984][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([1.8952e-02, 2.5689e-03, 3.5571e-04, 2.3308e-03, 1.1843e-03, 5.2210e-04,
        5.6876e-04, 1.1817e-03, 9.0799e-04, 6.7035e-03, 3.8290e-03, 2.0768e-03,
        9.5882e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,987][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([1.5195e-03, 1.4228e-05, 3.5548e-08, 3.9900e-05, 8.4897e-07, 2.2790e-09,
        2.4531e-09, 2.6204e-07, 1.1703e-09, 1.3789e-06, 2.8744e-07, 2.8264e-09,
        9.9842e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,990][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.0985, 0.1312, 0.0500, 0.2252, 0.0106, 0.0199, 0.0461, 0.0480, 0.0414,
        0.1116, 0.0091, 0.0416, 0.1667], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,994][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0518, 0.0041, 0.0193, 0.0164, 0.0206, 0.0364, 0.0546, 0.0149, 0.1012,
        0.0406, 0.2309, 0.2893, 0.1200], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,995][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.2955, 0.0598, 0.0867, 0.0554, 0.0475, 0.0752, 0.0742, 0.0401, 0.0575,
        0.0383, 0.0672, 0.0714, 0.0312], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,996][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([0.1707, 0.0951, 0.0854, 0.0595, 0.0627, 0.0705, 0.0619, 0.0698, 0.0731,
        0.0861, 0.0781, 0.0641, 0.0227], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,996][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.1467, 0.0501, 0.0791, 0.0864, 0.0554, 0.0730, 0.0487, 0.0253, 0.0576,
        0.0322, 0.0458, 0.0499, 0.2498], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:34,998][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.1312, 0.0816, 0.0735, 0.0901, 0.0694, 0.0740, 0.0394, 0.0387, 0.0890,
        0.0881, 0.1176, 0.0439, 0.0636], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,001][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1914, 0.0540, 0.0201, 0.0932, 0.1126, 0.0109, 0.0244, 0.0259, 0.0213,
        0.0693, 0.1208, 0.0245, 0.2170, 0.0144], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,003][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([2.8451e-03, 1.6725e-04, 2.1798e-02, 5.0568e-05, 2.6964e-04, 4.6070e-01,
        1.1717e-03, 7.7429e-05, 4.8516e-03, 6.7732e-05, 3.8286e-04, 7.0091e-04,
        4.7462e-05, 5.0687e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,008][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.2000, 0.0297, 0.0988, 0.0218, 0.0552, 0.1500, 0.0293, 0.0275, 0.0590,
        0.0244, 0.0680, 0.0263, 0.0376, 0.1724], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,008][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([3.8710e-03, 6.0273e-05, 1.8537e-04, 1.5176e-04, 2.6962e-03, 1.9014e-03,
        2.3687e-03, 4.2860e-04, 6.3094e-03, 1.2834e-03, 2.3872e-01, 6.6497e-02,
        1.3024e-01, 5.4529e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,009][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0342, 0.0040, 0.0030, 0.0033, 0.0569, 0.0111, 0.0067, 0.0259, 0.0098,
        0.0245, 0.3153, 0.0707, 0.2173, 0.2173], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,010][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.2171e-02, 1.5051e-03, 6.2463e-02, 2.3409e-03, 8.8713e-03, 3.8590e-01,
        1.4322e-01, 1.0633e-03, 1.3949e-02, 2.9430e-04, 4.4201e-03, 9.7312e-02,
        2.6858e-04, 2.5622e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,012][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0816, 0.1126, 0.0036, 0.1137, 0.0242, 0.1082, 0.0023, 0.0385, 0.0028,
        0.1667, 0.0370, 0.0039, 0.1071, 0.1978], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,015][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0251, 0.0055, 0.0059, 0.0093, 0.0119, 0.0083, 0.0185, 0.0248, 0.0473,
        0.0769, 0.1148, 0.1561, 0.2198, 0.2759], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,019][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0133, 0.0017, 0.0876, 0.0036, 0.0194, 0.1195, 0.1615, 0.0088, 0.1379,
        0.0024, 0.0187, 0.1894, 0.0175, 0.2189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,021][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1594, 0.0383, 0.0912, 0.0374, 0.0486, 0.0871, 0.0779, 0.0510, 0.0729,
        0.0343, 0.0643, 0.0827, 0.0519, 0.1030], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,022][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1240, 0.0367, 0.1067, 0.0349, 0.0659, 0.1194, 0.0831, 0.0408, 0.0762,
        0.0272, 0.0586, 0.0817, 0.0339, 0.1109], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,023][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1540, 0.0841, 0.0701, 0.0759, 0.0576, 0.0694, 0.0338, 0.0391, 0.0731,
        0.1014, 0.0774, 0.0452, 0.0472, 0.0717], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,026][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:35,029][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[33263],
        [ 6740],
        [ 9627],
        [    2],
        [17397],
        [ 9734],
        [26404],
        [19692],
        [ 4638],
        [10406],
        [10724],
        [18783],
        [ 8078],
        [11001]], device='cuda:0')
[2024-07-24 10:30:35,032][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[34221],
        [  176],
        [28439],
        [    1],
        [30106],
        [36236],
        [30786],
        [41504],
        [39398],
        [  355],
        [38198],
        [32949],
        [25297],
        [39320]], device='cuda:0')
[2024-07-24 10:30:35,034][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[2817],
        [3348],
        [3229],
        [1554],
        [2087],
        [1528],
        [1044],
        [2475],
        [1548],
        [5532],
        [4643],
        [5305],
        [3303],
        [5979]], device='cuda:0')
[2024-07-24 10:30:35,037][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[45663],
        [23008],
        [14377],
        [ 4537],
        [31169],
        [25304],
        [48397],
        [15529],
        [ 5714],
        [26693],
        [25417],
        [48696],
        [21200],
        [27194]], device='cuda:0')
[2024-07-24 10:30:35,038][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 8501],
        [ 8493],
        [11833],
        [ 6636],
        [10662],
        [18340],
        [18636],
        [11988],
        [16543],
        [10775],
        [16114],
        [19899],
        [11433],
        [22980]], device='cuda:0')
[2024-07-24 10:30:35,040][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[13225],
        [35109],
        [32503],
        [ 1093],
        [26583],
        [36741],
        [25369],
        [34411],
        [46178],
        [33634],
        [16827],
        [24507],
        [20933],
        [33025]], device='cuda:0')
[2024-07-24 10:30:35,041][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 1682],
        [38816],
        [ 2969],
        [  253],
        [ 3192],
        [ 7544],
        [ 2411],
        [17305],
        [  740],
        [41330],
        [ 7515],
        [ 7037],
        [ 7845],
        [ 3490]], device='cuda:0')
[2024-07-24 10:30:35,044][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[27688],
        [15866],
        [ 9280],
        [20241],
        [21548],
        [ 6857],
        [12273],
        [29359],
        [11272],
        [18721],
        [31210],
        [12149],
        [14554],
        [ 7950]], device='cuda:0')
[2024-07-24 10:30:35,047][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[49881],
        [49427],
        [36669],
        [35029],
        [ 4627],
        [13178],
        [ 2860],
        [ 8067],
        [10663],
        [36696],
        [ 7451],
        [ 6961],
        [ 6988],
        [12331]], device='cuda:0')
[2024-07-24 10:30:35,049][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[36383],
        [36117],
        [34943],
        [45691],
        [40449],
        [37972],
        [45105],
        [46101],
        [41507],
        [43355],
        [29983],
        [34094],
        [36252],
        [36212]], device='cuda:0')
[2024-07-24 10:30:35,052][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[24370],
        [23881],
        [15295],
        [16226],
        [15375],
        [11390],
        [ 8999],
        [ 9922],
        [11530],
        [11245],
        [ 8643],
        [13630],
        [13728],
        [11554]], device='cuda:0')
[2024-07-24 10:30:35,053][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[30153],
        [28765],
        [29592],
        [27743],
        [27350],
        [30059],
        [25661],
        [22198],
        [26019],
        [25604],
        [25962],
        [24357],
        [21552],
        [26284]], device='cuda:0')
[2024-07-24 10:30:35,055][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[13395],
        [31723],
        [18161],
        [35906],
        [21303],
        [21753],
        [44775],
        [43112],
        [40085],
        [48527],
        [23997],
        [48380],
        [40232],
        [40536]], device='cuda:0')
[2024-07-24 10:30:35,056][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[35019],
        [23724],
        [22157],
        [14126],
        [22899],
        [18196],
        [23361],
        [21786],
        [21276],
        [20851],
        [21760],
        [18349],
        [20392],
        [19080]], device='cuda:0')
[2024-07-24 10:30:35,059][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[13110],
        [ 5800],
        [12360],
        [    1],
        [29348],
        [ 6518],
        [46137],
        [33275],
        [ 5444],
        [ 7389],
        [14489],
        [28338],
        [12845],
        [ 6012]], device='cuda:0')
[2024-07-24 10:30:35,062][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[14321],
        [13400],
        [11121],
        [ 7746],
        [ 3544],
        [ 2906],
        [ 3216],
        [ 4786],
        [ 3817],
        [10756],
        [ 2833],
        [ 4002],
        [ 8033],
        [ 8185]], device='cuda:0')
[2024-07-24 10:30:35,064][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[22717],
        [12243],
        [12352],
        [37266],
        [13926],
        [12518],
        [ 7999],
        [32324],
        [30949],
        [16630],
        [23102],
        [ 6996],
        [26716],
        [13439]], device='cuda:0')
[2024-07-24 10:30:35,067][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[26564],
        [26740],
        [29957],
        [24740],
        [27422],
        [21750],
        [22003],
        [25214],
        [18808],
        [25505],
        [28553],
        [25215],
        [29307],
        [20544]], device='cuda:0')
[2024-07-24 10:30:35,068][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[16185],
        [17274],
        [31354],
        [31451],
        [17069],
        [20710],
        [27479],
        [16898],
        [26282],
        [14011],
        [19016],
        [27395],
        [22178],
        [24973]], device='cuda:0')
[2024-07-24 10:30:35,070][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[18763],
        [18992],
        [28905],
        [31665],
        [31291],
        [33632],
        [33403],
        [32295],
        [35479],
        [16679],
        [35671],
        [36637],
        [26820],
        [36911]], device='cuda:0')
[2024-07-24 10:30:35,071][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 9223],
        [26476],
        [30070],
        [20045],
        [29565],
        [28307],
        [34309],
        [19258],
        [28759],
        [25052],
        [23627],
        [30852],
        [22505],
        [28873]], device='cuda:0')
[2024-07-24 10:30:35,074][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[35965],
        [28702],
        [ 8671],
        [ 4599],
        [  767],
        [ 5916],
        [  604],
        [  656],
        [  806],
        [24561],
        [  927],
        [ 1461],
        [  848],
        [ 9025]], device='cuda:0')
[2024-07-24 10:30:35,077][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[7992],
        [8175],
        [7220],
        [1349],
        [ 881],
        [ 256],
        [ 472],
        [ 201],
        [ 192],
        [ 593],
        [1606],
        [ 979],
        [ 613],
        [ 762]], device='cuda:0')
[2024-07-24 10:30:35,079][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[11404],
        [11566],
        [ 6904],
        [ 7236],
        [ 6383],
        [ 2750],
        [ 2077],
        [ 3917],
        [ 1810],
        [ 4764],
        [ 3111],
        [ 2311],
        [ 4761],
        [ 1670]], device='cuda:0')
[2024-07-24 10:30:35,082][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[23244],
        [24157],
        [29391],
        [30004],
        [29751],
        [30859],
        [29314],
        [28155],
        [27320],
        [26929],
        [28113],
        [27504],
        [26585],
        [26481]], device='cuda:0')
[2024-07-24 10:30:35,083][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[43993],
        [37596],
        [49205],
        [44698],
        [49156],
        [49925],
        [49546],
        [46817],
        [49810],
        [39471],
        [49776],
        [49566],
        [47138],
        [49916]], device='cuda:0')
[2024-07-24 10:30:35,084][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[30999],
        [34237],
        [30927],
        [24237],
        [32623],
        [32605],
        [17978],
        [30711],
        [29510],
        [23742],
        [27039],
        [15496],
        [17665],
        [18891]], device='cuda:0')
[2024-07-24 10:30:35,086][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[27990],
        [30320],
        [23340],
        [30794],
        [31536],
        [28673],
        [34685],
        [31494],
        [30168],
        [35467],
        [28861],
        [32113],
        [33025],
        [26776]], device='cuda:0')
[2024-07-24 10:30:35,089][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[39542],
        [44522],
        [39447],
        [50257],
        [24108],
        [44185],
        [ 5379],
        [19410],
        [44816],
        [43200],
        [36726],
        [23693],
        [38386],
        [44689]], device='cuda:0')
[2024-07-24 10:30:35,091][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602],
        [13602]], device='cuda:0')
[2024-07-24 10:30:35,111][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:35,113][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,113][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,114][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,115][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,115][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,116][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,117][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,118][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,118][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,119][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,120][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,121][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,121][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.2053, 0.7947], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,122][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.6983, 0.3017], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,126][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.6555, 0.3445], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,129][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.6816, 0.3184], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,130][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([5.1847e-06, 9.9999e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,130][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.8895, 0.1105], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,131][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.9987, 0.0013], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,132][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.6271, 0.3729], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,134][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.9942, 0.0058], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,136][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([8.6343e-04, 9.9914e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,138][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([3.4467e-04, 9.9966e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,142][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.9781, 0.0219], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,143][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0630, 0.5964, 0.3406], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,144][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0963, 0.8845, 0.0193], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,144][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4627, 0.2554, 0.2819], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,145][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.3530, 0.1784, 0.4685], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,146][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ and] are: tensor([3.1879e-09, 1.3022e-03, 9.9870e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,149][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.3699, 0.0591, 0.5710], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,153][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9235, 0.0049, 0.0716], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,156][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.1878, 0.7257, 0.0865], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,156][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6670, 0.0083, 0.3247], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,157][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ and] are: tensor([5.5787e-04, 9.0165e-01, 9.7791e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,158][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0014, 0.5748, 0.4238], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,159][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2470, 0.0098, 0.7432], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,162][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.0420, 0.3630, 0.3322, 0.2628], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,166][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.1230, 0.2525, 0.5300, 0.0945], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,169][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.3623, 0.1984, 0.2240, 0.2153], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,170][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.3476, 0.1170, 0.4336, 0.1017], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,171][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0285, 0.1438, 0.7991, 0.0286], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,171][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.3841, 0.0388, 0.5556, 0.0214], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,172][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.8366, 0.0036, 0.1231, 0.0367], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,174][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.8358, 0.0749, 0.0517, 0.0376], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,177][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.5864, 0.0062, 0.3776, 0.0298], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,179][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([5.7418e-04, 8.6428e-01, 1.2190e-01, 1.3244e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,183][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0010, 0.3458, 0.3103, 0.3429], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,184][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.1504, 0.0226, 0.6346, 0.1924], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,184][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0392, 0.1455, 0.1705, 0.1301, 0.5148], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,185][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.1112, 0.3803, 0.2672, 0.2254, 0.0158], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,187][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.3041, 0.1596, 0.1780, 0.1807, 0.1776], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,190][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.2599, 0.1169, 0.3306, 0.0984, 0.1943], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,192][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ went] are: tensor([4.8242e-06, 1.2463e-02, 9.1402e-01, 1.9012e-04, 7.3318e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,196][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.3343, 0.0368, 0.4744, 0.0214, 0.1331], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,197][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.9421, 0.0011, 0.0406, 0.0115, 0.0047], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,198][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.1048, 0.2865, 0.0888, 0.3177, 0.2022], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,198][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.5041, 0.0128, 0.2756, 0.0364, 0.1712], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,199][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ went] are: tensor([5.2441e-04, 9.0629e-01, 7.8598e-02, 1.2442e-02, 2.1463e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,201][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.0012, 0.2350, 0.1888, 0.2760, 0.2990], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,205][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.2541, 0.0044, 0.3931, 0.0183, 0.3302], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,208][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0201, 0.1012, 0.0887, 0.0802, 0.6172, 0.0926], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,210][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0921, 0.3305, 0.3142, 0.1403, 0.1129, 0.0100], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,211][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2443, 0.1418, 0.1479, 0.1606, 0.1555, 0.1500], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,211][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1896, 0.0858, 0.2217, 0.0777, 0.1492, 0.2760], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,212][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([7.8576e-05, 5.2692e-02, 8.0305e-01, 8.1165e-04, 1.2006e-01, 2.3302e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,214][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.1475, 0.0266, 0.2356, 0.0181, 0.0851, 0.4872], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,218][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.8265, 0.0025, 0.0587, 0.0189, 0.0210, 0.0723], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,221][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0929, 0.2371, 0.0542, 0.3464, 0.1985, 0.0710], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,225][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.4818, 0.0249, 0.2160, 0.0487, 0.1327, 0.0958], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,226][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([5.9485e-04, 8.8239e-01, 9.7011e-02, 1.6221e-02, 2.9543e-03, 8.2393e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,226][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0016, 0.1658, 0.1452, 0.2149, 0.2387, 0.2338], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,227][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.3389, 0.0027, 0.1908, 0.0198, 0.0494, 0.3985], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,229][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0248, 0.0987, 0.1040, 0.0985, 0.2561, 0.2053, 0.2126],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,232][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1546, 0.2146, 0.1930, 0.1124, 0.1950, 0.1166, 0.0138],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,236][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2140, 0.1223, 0.1333, 0.1405, 0.1379, 0.1285, 0.1234],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,238][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1460, 0.0689, 0.1838, 0.0591, 0.1108, 0.1934, 0.2380],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,239][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([4.4920e-04, 6.7420e-02, 6.4548e-01, 2.6514e-03, 1.6446e-01, 5.0940e-02,
        6.8602e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,240][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1079, 0.0198, 0.1711, 0.0130, 0.0576, 0.3340, 0.2967],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,241][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5018, 0.0023, 0.0366, 0.0103, 0.0060, 0.0455, 0.3975],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,243][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0588, 0.3408, 0.0336, 0.2643, 0.2212, 0.0337, 0.0477],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,245][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2157, 0.0138, 0.1282, 0.0224, 0.0564, 0.0599, 0.5036],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,248][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([5.2870e-04, 8.8370e-01, 7.9418e-02, 1.3643e-02, 2.1019e-03, 5.2694e-04,
        2.0081e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,252][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0015, 0.1315, 0.1190, 0.1694, 0.1928, 0.1876, 0.1981],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,252][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2233, 0.0047, 0.1710, 0.0291, 0.0582, 0.3499, 0.1638],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,253][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0059, 0.0463, 0.0781, 0.0420, 0.4057, 0.1601, 0.1944, 0.0674],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,254][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0723, 0.3453, 0.0741, 0.1873, 0.1930, 0.0272, 0.0889, 0.0119],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,256][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.2006, 0.1073, 0.1163, 0.1199, 0.1164, 0.1116, 0.1019, 0.1259],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,259][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.1509, 0.0502, 0.1933, 0.0444, 0.0970, 0.2083, 0.1866, 0.0693],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,261][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ office] are: tensor([2.8109e-05, 2.7740e-02, 8.3343e-01, 4.9965e-04, 8.8341e-02, 1.7491e-02,
        1.4279e-02, 1.8193e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,265][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.1210, 0.0109, 0.1662, 0.0063, 0.0406, 0.3474, 0.2756, 0.0320],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,266][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.6690, 0.0017, 0.0529, 0.0091, 0.0046, 0.0474, 0.2111, 0.0043],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,267][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.1305, 0.2581, 0.0671, 0.1185, 0.1654, 0.0906, 0.0840, 0.0858],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,267][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.1579, 0.0054, 0.1266, 0.0164, 0.0991, 0.0441, 0.5432, 0.0072],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,269][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ office] are: tensor([6.1489e-04, 8.8280e-01, 7.8762e-02, 1.3400e-02, 2.1324e-03, 4.3083e-04,
        1.9144e-02, 2.7180e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,272][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.0017, 0.1025, 0.0949, 0.1388, 0.1479, 0.1485, 0.1494, 0.2163],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,274][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ office] are: tensor([7.6130e-02, 2.3542e-04, 9.8424e-02, 4.7899e-03, 2.0909e-02, 2.3438e-01,
        3.0970e-02, 5.3416e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,278][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0190, 0.0943, 0.0715, 0.0931, 0.1779, 0.1259, 0.1836, 0.1106, 0.1242],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,279][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0395, 0.2451, 0.1346, 0.2944, 0.1233, 0.0299, 0.0407, 0.0606, 0.0319],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,280][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1682, 0.0953, 0.1009, 0.1096, 0.1069, 0.0987, 0.0921, 0.1147, 0.1136],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,281][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1277, 0.0528, 0.1546, 0.0440, 0.0916, 0.1709, 0.1646, 0.0756, 0.1181],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,283][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0036, 0.1106, 0.4526, 0.0053, 0.1646, 0.0581, 0.0926, 0.0919, 0.0207],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,287][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0817, 0.0148, 0.1155, 0.0095, 0.0385, 0.2179, 0.1984, 0.0343, 0.2894],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,290][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.3660, 0.0030, 0.0335, 0.0177, 0.0101, 0.0536, 0.3967, 0.0045, 0.1149],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,292][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0950, 0.2539, 0.0349, 0.1718, 0.2501, 0.0274, 0.0446, 0.0687, 0.0536],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,293][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.1712, 0.0087, 0.2038, 0.0168, 0.0595, 0.0376, 0.4043, 0.0085, 0.0896],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,293][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [,] are: tensor([5.3856e-04, 8.6170e-01, 8.5798e-02, 1.4070e-02, 2.2372e-03, 6.4835e-04,
        2.0489e-02, 3.4773e-03, 1.1046e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,294][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0015, 0.0788, 0.0739, 0.1111, 0.1304, 0.1218, 0.1210, 0.1682, 0.1933],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,296][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0704, 0.0064, 0.1246, 0.0219, 0.0644, 0.2471, 0.0865, 0.3440, 0.0347],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,299][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0105, 0.0675, 0.0675, 0.1033, 0.1988, 0.1016, 0.1732, 0.0679, 0.1443,
        0.0653], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,303][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0711, 0.0326, 0.2377, 0.0518, 0.1342, 0.0410, 0.0615, 0.0572, 0.2592,
        0.0537], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,305][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.1579, 0.0848, 0.0961, 0.0941, 0.0928, 0.0901, 0.0835, 0.1020, 0.1056,
        0.0931], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,306][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.1204, 0.0441, 0.1523, 0.0403, 0.0842, 0.1661, 0.1657, 0.0641, 0.1183,
        0.0446], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,307][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.1250, 0.0748, 0.0998, 0.0250, 0.0971, 0.1006, 0.1948, 0.1072, 0.1291,
        0.0466], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,308][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0895, 0.0084, 0.1223, 0.0044, 0.0261, 0.2659, 0.1947, 0.0185, 0.2674,
        0.0029], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,311][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.4262, 0.0020, 0.0498, 0.0058, 0.0024, 0.0471, 0.2937, 0.0017, 0.1662,
        0.0051], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,315][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.1108, 0.0719, 0.0451, 0.1649, 0.1027, 0.0892, 0.1147, 0.1582, 0.0832,
        0.0593], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,318][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([1.5378e-01, 4.3749e-04, 6.0326e-02, 4.1681e-03, 4.6176e-02, 2.6334e-02,
        4.6845e-01, 3.6835e-03, 2.3524e-01, 1.3992e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,319][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([4.7094e-04, 8.2693e-01, 1.1243e-01, 1.1197e-02, 2.0339e-03, 8.5371e-04,
        2.0987e-02, 3.3936e-03, 1.1728e-02, 9.9752e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,320][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0013, 0.0616, 0.0648, 0.0907, 0.1074, 0.1045, 0.1051, 0.1451, 0.1636,
        0.1559], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,321][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0399, 0.0051, 0.0712, 0.0062, 0.0629, 0.3478, 0.0945, 0.2866, 0.0781,
        0.0076], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,323][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0074, 0.0449, 0.0528, 0.0503, 0.2343, 0.0950, 0.1253, 0.0592, 0.1280,
        0.0439, 0.1589], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,325][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0174, 0.2104, 0.0479, 0.0868, 0.0120, 0.0141, 0.0699, 0.0785, 0.0615,
        0.3878, 0.0137], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,329][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1419, 0.0784, 0.0837, 0.0884, 0.0857, 0.0811, 0.0728, 0.0927, 0.0935,
        0.0840, 0.0978], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,332][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.1170, 0.0422, 0.1378, 0.0377, 0.0798, 0.1632, 0.1425, 0.0581, 0.1165,
        0.0467, 0.0585], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,333][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0016, 0.1025, 0.6139, 0.0026, 0.1250, 0.0353, 0.0481, 0.0548, 0.0098,
        0.0012, 0.0052], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,333][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0803, 0.0099, 0.1107, 0.0059, 0.0330, 0.2287, 0.1803, 0.0245, 0.2480,
        0.0040, 0.0747], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,334][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.5630, 0.0028, 0.0625, 0.0091, 0.0098, 0.0479, 0.1727, 0.0030, 0.1026,
        0.0083, 0.0183], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,336][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0494, 0.0772, 0.0663, 0.1831, 0.0902, 0.0326, 0.0585, 0.0817, 0.1570,
        0.1431, 0.0608], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,340][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1505, 0.0037, 0.1181, 0.0107, 0.0651, 0.0339, 0.4382, 0.0056, 0.1267,
        0.0072, 0.0401], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,342][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([5.1934e-04, 8.5393e-01, 8.9344e-02, 1.1720e-02, 2.0086e-03, 5.6820e-04,
        1.7515e-02, 3.2847e-03, 9.0943e-03, 1.0235e-02, 1.7850e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,345][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0010, 0.0538, 0.0525, 0.0780, 0.0931, 0.0867, 0.0831, 0.1150, 0.1344,
        0.1352, 0.1671], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,346][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([3.3311e-02, 2.0662e-03, 7.7592e-02, 7.5041e-03, 7.0280e-02, 1.9906e-01,
        2.9363e-02, 4.7321e-01, 7.9616e-03, 3.7813e-04, 9.9270e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,347][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0074, 0.0513, 0.0448, 0.0317, 0.1420, 0.0734, 0.1222, 0.0499, 0.1146,
        0.0607, 0.1608, 0.1411], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,348][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0662, 0.1223, 0.0330, 0.0534, 0.0373, 0.0370, 0.0814, 0.0211, 0.0796,
        0.2846, 0.1802, 0.0041], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,350][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1257, 0.0710, 0.0776, 0.0805, 0.0776, 0.0749, 0.0702, 0.0884, 0.0866,
        0.0786, 0.0896, 0.0792], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,352][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0942, 0.0387, 0.1101, 0.0334, 0.0658, 0.1235, 0.1441, 0.0590, 0.0944,
        0.0433, 0.0509, 0.1426], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,356][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0567, 0.0869, 0.1799, 0.0152, 0.1263, 0.0859, 0.1630, 0.1254, 0.0740,
        0.0225, 0.0334, 0.0310], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,359][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0593, 0.0108, 0.0855, 0.0070, 0.0297, 0.1637, 0.1425, 0.0254, 0.1983,
        0.0057, 0.0651, 0.2069], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,359][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3002, 0.0028, 0.0256, 0.0108, 0.0051, 0.0354, 0.2778, 0.0018, 0.0750,
        0.0098, 0.0078, 0.2480], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,360][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0652, 0.1370, 0.0382, 0.1682, 0.0920, 0.0336, 0.0402, 0.0672, 0.0717,
        0.1677, 0.0809, 0.0382], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,362][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.1434, 0.0073, 0.1012, 0.0147, 0.0419, 0.0359, 0.3959, 0.0077, 0.0702,
        0.0119, 0.0270, 0.1430], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,365][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([4.4602e-04, 8.5138e-01, 8.1506e-02, 1.2023e-02, 1.8725e-03, 5.4903e-04,
        1.8049e-02, 3.2756e-03, 9.7648e-03, 1.1611e-02, 1.8992e-03, 7.6262e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,369][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0015, 0.0410, 0.0436, 0.0655, 0.0746, 0.0753, 0.0762, 0.0985, 0.1234,
        0.1086, 0.1591, 0.1327], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,371][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0924, 0.0059, 0.0890, 0.0141, 0.0744, 0.2517, 0.0799, 0.3037, 0.0222,
        0.0021, 0.0402, 0.0243], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,372][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0045, 0.0311, 0.0269, 0.0222, 0.1653, 0.0573, 0.0702, 0.0347, 0.0752,
        0.0301, 0.2293, 0.1880, 0.0653], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,373][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0396, 0.1439, 0.1144, 0.0572, 0.0756, 0.0248, 0.1056, 0.0267, 0.1237,
        0.0997, 0.1211, 0.0564, 0.0113], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,374][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.1278, 0.0677, 0.0708, 0.0734, 0.0712, 0.0670, 0.0627, 0.0789, 0.0786,
        0.0718, 0.0809, 0.0685, 0.0806], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,376][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.0921, 0.0378, 0.1179, 0.0340, 0.0661, 0.1243, 0.1236, 0.0566, 0.0946,
        0.0384, 0.0469, 0.1229, 0.0449], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,379][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.2009, 0.0766, 0.0622, 0.0224, 0.0718, 0.0638, 0.1352, 0.0909, 0.0954,
        0.0487, 0.0400, 0.0527, 0.0392], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,383][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.0672, 0.0054, 0.0879, 0.0030, 0.0197, 0.1887, 0.1478, 0.0120, 0.2032,
        0.0016, 0.0477, 0.2129, 0.0029], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,385][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.3151, 0.0022, 0.0364, 0.0151, 0.0053, 0.0359, 0.2310, 0.0022, 0.0977,
        0.0064, 0.0089, 0.2211, 0.0227], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,386][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0659, 0.0724, 0.0282, 0.2671, 0.0719, 0.0417, 0.0405, 0.1054, 0.0733,
        0.1447, 0.0421, 0.0428, 0.0041], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,387][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.1659, 0.0026, 0.0876, 0.0087, 0.0684, 0.0295, 0.3541, 0.0055, 0.1243,
        0.0077, 0.0539, 0.0876, 0.0042], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,388][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([7.7116e-04, 8.3272e-01, 9.7833e-02, 1.2761e-02, 2.3082e-03, 6.8567e-04,
        1.7289e-02, 4.1834e-03, 9.7841e-03, 9.5014e-03, 1.9730e-03, 6.8534e-03,
        3.3402e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,391][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0014, 0.0389, 0.0412, 0.0564, 0.0632, 0.0643, 0.0599, 0.0879, 0.1032,
        0.0945, 0.1252, 0.0971, 0.1667], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,394][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([2.8868e-02, 2.8009e-03, 5.8221e-02, 9.0531e-03, 6.0547e-02, 1.1851e-01,
        2.0246e-02, 3.3743e-01, 4.8265e-03, 3.2254e-04, 4.5805e-02, 9.5725e-03,
        3.0380e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,398][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0063, 0.0334, 0.0307, 0.0253, 0.1897, 0.0311, 0.0742, 0.0418, 0.0767,
        0.0359, 0.1925, 0.1397, 0.0965, 0.0262], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,399][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0385, 0.1143, 0.1078, 0.0534, 0.0396, 0.0036, 0.0696, 0.0429, 0.0977,
        0.1606, 0.1140, 0.0508, 0.1046, 0.0026], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,400][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1051, 0.0624, 0.0634, 0.0702, 0.0676, 0.0642, 0.0593, 0.0744, 0.0726,
        0.0696, 0.0797, 0.0665, 0.0784, 0.0666], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,400][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0804, 0.0336, 0.0892, 0.0310, 0.0611, 0.1122, 0.1075, 0.0493, 0.0779,
        0.0375, 0.0449, 0.1068, 0.0443, 0.1240], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,402][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0797, 0.0996, 0.1414, 0.0164, 0.1091, 0.0758, 0.1376, 0.1074, 0.0744,
        0.0258, 0.0347, 0.0328, 0.0194, 0.0461], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,405][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0513, 0.0087, 0.0746, 0.0055, 0.0247, 0.1476, 0.1254, 0.0209, 0.1762,
        0.0044, 0.0551, 0.1836, 0.0065, 0.1154], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,409][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.3583, 0.0013, 0.0262, 0.0084, 0.0116, 0.0331, 0.2023, 0.0022, 0.0569,
        0.0044, 0.0198, 0.2285, 0.0091, 0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,411][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0665, 0.0776, 0.0446, 0.1093, 0.1126, 0.0399, 0.0431, 0.0395, 0.0841,
        0.0740, 0.0907, 0.0412, 0.1257, 0.0510], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,412][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.1366, 0.0088, 0.1005, 0.0162, 0.0433, 0.0390, 0.3674, 0.0077, 0.0661,
        0.0129, 0.0266, 0.1451, 0.0094, 0.0203], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,413][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([5.3723e-04, 8.2049e-01, 9.4855e-02, 1.3029e-02, 2.3868e-03, 7.9865e-04,
        2.1464e-02, 3.9595e-03, 1.2165e-02, 1.3614e-02, 2.3558e-03, 8.9447e-03,
        4.3642e-03, 1.0343e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,415][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0022, 0.0255, 0.0285, 0.0461, 0.0520, 0.0524, 0.0507, 0.0669, 0.0852,
        0.0746, 0.1163, 0.0882, 0.1550, 0.1565], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,418][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0954, 0.0039, 0.0754, 0.0143, 0.0577, 0.2118, 0.0429, 0.2992, 0.0140,
        0.0011, 0.0367, 0.0128, 0.0936, 0.0413], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,438][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:35,440][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,443][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,445][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,446][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,446][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,446][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,447][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,447][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,447][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,447][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,448][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,448][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,448][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.9703, 0.0297], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,449][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.6908, 0.3092], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,449][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.3393, 0.6607], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,449][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.7266, 0.2734], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,450][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,450][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.8902, 0.1098], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,451][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0016, 0.9984], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,451][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.9290, 0.0710], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,451][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.8993, 0.1007], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,452][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0014, 0.9986], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,452][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0024, 0.9976], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,452][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([1.9993e-08, 1.0000e+00], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,453][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.9534, 0.0454, 0.0012], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,453][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0699, 0.9076, 0.0224], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,453][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.2123, 0.5105, 0.2771], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,454][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5371, 0.2037, 0.2592], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,454][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.3335, 0.3335, 0.3330], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,454][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3597, 0.0601, 0.5802], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,455][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([1.4750e-06, 1.0000e+00, 2.3059e-06], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,457][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.5972, 0.2007, 0.2021], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,457][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.3317, 0.0484, 0.6199], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,458][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([5.6575e-05, 6.7544e-01, 3.2450e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,458][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0017, 0.2959, 0.7024], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,458][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([1.6981e-01, 4.8197e-04, 8.2971e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,459][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.9307, 0.0618, 0.0062, 0.0013], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,459][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.1069, 0.2738, 0.5001, 0.1192], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,459][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.1460, 0.4148, 0.2239, 0.2152], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,461][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.4266, 0.1491, 0.2001, 0.2243], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,464][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.2502, 0.2502, 0.2498, 0.2498], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,468][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.3594, 0.0356, 0.5820, 0.0231], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,470][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([3.9161e-05, 1.7805e-01, 2.6199e-04, 8.2165e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,470][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.2993, 0.1423, 0.4182, 0.1402], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,471][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.1964, 0.0216, 0.4695, 0.3125], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,471][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([6.4059e-05, 8.1345e-01, 1.5907e-01, 2.7413e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,471][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.0021, 0.2395, 0.5935, 0.1649], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,472][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([5.6937e-10, 1.9999e-08, 2.2303e-10, 1.0000e+00], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,472][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([9.4633e-01, 5.0902e-02, 2.1067e-03, 8.3207e-05, 5.8063e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,472][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0773, 0.4319, 0.1748, 0.3003, 0.0157], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,473][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.1213, 0.2978, 0.1641, 0.1879, 0.2289], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,474][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.3356, 0.1048, 0.1558, 0.1750, 0.2287], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,477][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.2001, 0.2001, 0.1998, 0.1998, 0.2002], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,480][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.3247, 0.0370, 0.5023, 0.0244, 0.1116], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,484][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0134, 0.0605, 0.0046, 0.2877, 0.6338], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,484][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.3132, 0.2501, 0.1746, 0.2079, 0.0542], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,485][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.1118, 0.0125, 0.1897, 0.1351, 0.5509], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,485][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([5.2079e-05, 6.4701e-01, 2.9736e-01, 3.3048e-02, 2.2538e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,485][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.0014, 0.1756, 0.4380, 0.1340, 0.2511], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,486][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([8.1046e-11, 8.5270e-09, 1.2155e-09, 3.0332e-09, 1.0000e+00],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,486][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.3142e-01, 6.0633e-02, 4.7485e-03, 6.3627e-04, 2.0549e-03, 5.0508e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,486][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0623, 0.4193, 0.2350, 0.1534, 0.1253, 0.0046], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,487][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0888, 0.2929, 0.1119, 0.1702, 0.1944, 0.1419], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,488][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.2689, 0.0900, 0.1216, 0.1328, 0.1438, 0.2428], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,488][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.1667, 0.1667, 0.1665, 0.1665, 0.1668, 0.1668], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,489][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.1230, 0.0258, 0.2145, 0.0208, 0.0652, 0.5507], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,491][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([6.9001e-07, 1.0054e-03, 1.3827e-07, 3.0098e-03, 9.9598e-01, 3.0802e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,494][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.2227, 0.1188, 0.1992, 0.1521, 0.0673, 0.2398], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,498][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0886, 0.0158, 0.1358, 0.1136, 0.3866, 0.2595], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,499][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([3.3255e-05, 8.2055e-01, 1.5977e-01, 1.2995e-02, 6.4211e-03, 2.3789e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,499][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0011, 0.1265, 0.3137, 0.0823, 0.2123, 0.2642], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,499][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.1129e-04, 1.8082e-05, 2.5269e-04, 1.3015e-04, 7.3773e-05, 9.9941e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,500][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.9210, 0.0656, 0.0062, 0.0014, 0.0035, 0.0012, 0.0011],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,500][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.1065, 0.2184, 0.2408, 0.1024, 0.2273, 0.0914, 0.0133],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,501][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0733, 0.2394, 0.1035, 0.1446, 0.1764, 0.1055, 0.1573],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,501][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2305, 0.0753, 0.0979, 0.1092, 0.1216, 0.1972, 0.1683],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,501][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1429, 0.1429, 0.1427, 0.1427, 0.1430, 0.1429, 0.1429],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,503][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0891, 0.0175, 0.1465, 0.0132, 0.0430, 0.3643, 0.3263],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,505][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([1.4861e-05, 8.7401e-01, 3.0530e-06, 2.7903e-02, 9.8020e-02, 4.4261e-05,
        1.9963e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,508][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1250, 0.1004, 0.1033, 0.1647, 0.0807, 0.1289, 0.2970],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,512][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0548, 0.0106, 0.0996, 0.0810, 0.2763, 0.1963, 0.2813],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,512][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([3.3406e-05, 6.8834e-01, 1.4313e-01, 7.9917e-03, 4.4453e-03, 2.2891e-04,
        1.5583e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,513][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0010, 0.0895, 0.2319, 0.0552, 0.1740, 0.2151, 0.2333],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,513][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([6.5908e-09, 8.8986e-07, 4.6393e-08, 2.1963e-05, 4.0624e-07, 9.2333e-09,
        9.9998e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,514][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([9.3822e-01, 5.6363e-02, 3.1591e-03, 2.4812e-04, 1.1504e-03, 2.0228e-04,
        1.3759e-04, 5.1648e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,514][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.0641, 0.3614, 0.0735, 0.2308, 0.1846, 0.0121, 0.0639, 0.0096],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,514][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.0739, 0.2012, 0.1076, 0.1190, 0.1393, 0.1041, 0.1083, 0.1465],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,515][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.2121, 0.0511, 0.0772, 0.0844, 0.0980, 0.1903, 0.1470, 0.1399],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,517][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1250, 0.1251],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,520][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.0831, 0.0083, 0.1315, 0.0058, 0.0246, 0.4138, 0.2979, 0.0352],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,523][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([1.6771e-06, 8.5289e-04, 2.1635e-05, 9.2725e-05, 9.9857e-04, 8.2496e-05,
        5.1239e-08, 9.9795e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,525][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.1824, 0.0564, 0.1209, 0.0664, 0.1876, 0.1257, 0.1735, 0.0870],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,526][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.0466, 0.0070, 0.0738, 0.0639, 0.2231, 0.1506, 0.2543, 0.1807],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,526][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([7.7347e-05, 5.0503e-01, 1.1979e-01, 2.2642e-02, 2.3457e-02, 5.0173e-04,
        3.2710e-01, 1.4033e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,526][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.0008, 0.0893, 0.2423, 0.0619, 0.1387, 0.1826, 0.2080, 0.0763],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,527][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([5.4143e-10, 2.3248e-06, 2.0029e-08, 9.9636e-07, 2.3172e-05, 5.1882e-09,
        6.4915e-08, 9.9997e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,527][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.8983, 0.0732, 0.0094, 0.0027, 0.0053, 0.0021, 0.0019, 0.0038, 0.0033],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,528][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0261, 0.2237, 0.1500, 0.2891, 0.1398, 0.0159, 0.0346, 0.1101, 0.0108],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,528][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0596, 0.1952, 0.0762, 0.1166, 0.1467, 0.0842, 0.1035, 0.1569, 0.0610],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,530][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1750, 0.0505, 0.0633, 0.0731, 0.0675, 0.1270, 0.1097, 0.0898, 0.2442],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,533][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.1111, 0.1111, 0.1110, 0.1110, 0.1112, 0.1112, 0.1111, 0.1112, 0.1112],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,537][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0663, 0.0137, 0.0982, 0.0102, 0.0299, 0.2300, 0.2120, 0.0403, 0.2993],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,539][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([8.6722e-06, 1.1350e-01, 2.3650e-06, 2.4181e-02, 4.7225e-01, 9.2498e-05,
        3.8526e-06, 3.8985e-01, 1.0941e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,539][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1601, 0.1067, 0.0904, 0.1859, 0.0398, 0.1053, 0.1510, 0.0625, 0.0983],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,540][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0500, 0.0092, 0.0740, 0.0582, 0.1960, 0.1387, 0.2009, 0.1438, 0.1291],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,540][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([3.1626e-05, 8.1016e-01, 9.3915e-02, 6.5417e-03, 2.3340e-03, 1.0290e-04,
        4.4526e-02, 3.6784e-04, 4.2022e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,540][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0010, 0.0613, 0.1723, 0.0350, 0.1385, 0.1721, 0.1771, 0.0637, 0.1790],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,541][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0462, 0.3491, 0.0030, 0.3696, 0.0673, 0.0145, 0.0133, 0.1214, 0.0156],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,541][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.8232, 0.0800, 0.0159, 0.0094, 0.0118, 0.0074, 0.0080, 0.0109, 0.0145,
        0.0188], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,543][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.0978, 0.0440, 0.3041, 0.0739, 0.1656, 0.0342, 0.0538, 0.0580, 0.1381,
        0.0307], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,546][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.0573, 0.1591, 0.0921, 0.0926, 0.1106, 0.0824, 0.0957, 0.1355, 0.0614,
        0.1133], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,550][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.1515, 0.0379, 0.0490, 0.0531, 0.0531, 0.0993, 0.0832, 0.0734, 0.2021,
        0.1973], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,552][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.1000, 0.1000, 0.0999, 0.0999, 0.1001, 0.1000, 0.1000, 0.1000, 0.1001,
        0.1000], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,552][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.0628, 0.0062, 0.0952, 0.0038, 0.0167, 0.3075, 0.1972, 0.0177, 0.2913,
        0.0016], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,553][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0012, 0.1195, 0.0081, 0.0299, 0.0043, 0.0155, 0.0029, 0.0620, 0.0184,
        0.7382], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,553][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.0367, 0.0060, 0.0565, 0.0559, 0.0625, 0.0891, 0.2702, 0.0652, 0.3562,
        0.0017], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,554][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.0257, 0.0025, 0.0620, 0.0357, 0.1630, 0.1415, 0.2745, 0.1480, 0.1369,
        0.0104], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,554][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([5.2651e-05, 9.1092e-01, 4.0931e-02, 8.1528e-03, 4.5987e-04, 3.9720e-05,
        7.6086e-03, 1.1478e-04, 1.4326e-02, 1.7394e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,554][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0010, 0.0579, 0.1683, 0.0428, 0.1137, 0.1511, 0.1666, 0.0662, 0.1801,
        0.0522], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,555][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([1.9102e-10, 2.5863e-01, 4.5360e-09, 1.1174e-06, 6.7464e-08, 1.5501e-08,
        8.2608e-11, 1.0307e-08, 1.7068e-08, 7.4136e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,556][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([9.2151e-01, 6.4461e-02, 5.7754e-03, 7.6467e-04, 2.2714e-03, 6.1171e-04,
        4.6900e-04, 1.3264e-03, 8.0495e-04, 6.3574e-04, 1.3723e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,559][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0147, 0.2546, 0.0478, 0.1488, 0.0171, 0.0085, 0.0661, 0.1419, 0.0235,
        0.2679, 0.0092], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,562][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0505, 0.1572, 0.0749, 0.0956, 0.1120, 0.0746, 0.0781, 0.1236, 0.0571,
        0.1066, 0.0698], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,566][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.1058, 0.0185, 0.0271, 0.0291, 0.0283, 0.0707, 0.0553, 0.0447, 0.1750,
        0.1591, 0.2864], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,566][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0909, 0.0909, 0.0908, 0.0908, 0.0910, 0.0909, 0.0909, 0.0909, 0.0910,
        0.0909, 0.0909], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,567][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0589, 0.0082, 0.0892, 0.0056, 0.0221, 0.2607, 0.1923, 0.0258, 0.2784,
        0.0026, 0.0561], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,567][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([7.1283e-05, 3.3061e-02, 2.0344e-04, 1.3476e-03, 6.0478e-02, 2.3017e-04,
        1.1311e-07, 3.8395e-01, 1.0316e-05, 2.3545e-01, 2.8519e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,567][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0518, 0.0464, 0.0811, 0.0800, 0.0246, 0.1148, 0.1851, 0.1885, 0.1899,
        0.0262, 0.0116], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,568][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0387, 0.0057, 0.0631, 0.0422, 0.1629, 0.1283, 0.1974, 0.1270, 0.1199,
        0.0163, 0.0985], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,568][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([2.6432e-05, 7.3588e-01, 1.0648e-01, 1.0748e-02, 5.2242e-03, 1.4861e-04,
        5.8595e-02, 9.2403e-04, 5.5539e-02, 2.5574e-02, 8.6761e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,570][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0007, 0.0544, 0.1573, 0.0381, 0.0990, 0.1329, 0.1487, 0.0527, 0.1664,
        0.0459, 0.1040], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,572][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([8.4678e-10, 2.6419e-09, 2.0755e-08, 9.0516e-08, 1.9718e-07, 3.2410e-08,
        6.7992e-13, 1.0929e-08, 4.2599e-08, 6.9265e-09, 1.0000e+00],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,575][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.8829, 0.0694, 0.0092, 0.0027, 0.0050, 0.0022, 0.0021, 0.0039, 0.0038,
        0.0043, 0.0067, 0.0077], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,579][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0731, 0.1642, 0.0526, 0.0735, 0.0575, 0.0373, 0.1166, 0.0379, 0.0480,
        0.2028, 0.1328, 0.0039], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,579][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0434, 0.1364, 0.0658, 0.0829, 0.0921, 0.0685, 0.0886, 0.1317, 0.0516,
        0.1025, 0.0600, 0.0768], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,580][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1046, 0.0208, 0.0270, 0.0288, 0.0268, 0.0603, 0.0517, 0.0398, 0.1316,
        0.1328, 0.2079, 0.1678], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,580][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0833, 0.0834, 0.0832, 0.0832, 0.0834, 0.0834, 0.0833, 0.0834, 0.0834,
        0.0833, 0.0833, 0.0833], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,581][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0468, 0.0100, 0.0738, 0.0078, 0.0237, 0.1742, 0.1507, 0.0322, 0.2130,
        0.0046, 0.0514, 0.2118], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,581][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([2.1345e-06, 1.4331e-01, 9.5183e-07, 1.1369e-02, 6.1316e-03, 1.5405e-05,
        8.9465e-07, 1.1752e-02, 4.7296e-06, 8.0838e-01, 1.9028e-02, 2.6513e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,581][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0646, 0.0410, 0.0863, 0.0840, 0.0265, 0.1177, 0.1647, 0.0661, 0.1675,
        0.0137, 0.0253, 0.1425], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,582][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0397, 0.0067, 0.0613, 0.0446, 0.1479, 0.1129, 0.1518, 0.1115, 0.1010,
        0.0183, 0.0822, 0.1222], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,583][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([2.5206e-05, 8.6807e-01, 4.9883e-02, 6.1895e-03, 7.2835e-04, 3.7953e-05,
        2.1539e-02, 1.4389e-04, 2.6852e-02, 2.4155e-02, 1.1851e-04, 2.2621e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,586][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0006, 0.0483, 0.1352, 0.0276, 0.0977, 0.1249, 0.1303, 0.0485, 0.1347,
        0.0350, 0.0936, 0.1235], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,588][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.8788e-09, 8.5136e-08, 3.4308e-08, 1.2292e-05, 2.3746e-07, 1.3285e-07,
        1.3991e-07, 4.1358e-08, 1.2829e-07, 4.4217e-07, 1.3040e-06, 9.9999e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,591][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.8377, 0.0694, 0.0106, 0.0040, 0.0063, 0.0033, 0.0034, 0.0054, 0.0063,
        0.0078, 0.0112, 0.0148, 0.0197], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,593][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.0512, 0.1816, 0.1698, 0.0989, 0.0886, 0.0154, 0.0911, 0.0402, 0.0528,
        0.0689, 0.0830, 0.0502, 0.0082], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,593][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.0442, 0.1422, 0.0697, 0.0775, 0.0926, 0.0628, 0.0750, 0.1131, 0.0488,
        0.0976, 0.0583, 0.0606, 0.0575], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,594][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0736, 0.0122, 0.0179, 0.0183, 0.0166, 0.0450, 0.0359, 0.0245, 0.1056,
        0.0970, 0.1574, 0.1266, 0.2694], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,594][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.0769, 0.0769, 0.0768, 0.0768, 0.0770, 0.0769, 0.0769, 0.0769, 0.0770,
        0.0769, 0.0769, 0.0769, 0.0770], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,595][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.0433, 0.0038, 0.0633, 0.0023, 0.0115, 0.2128, 0.1480, 0.0110, 0.2206,
        0.0008, 0.0337, 0.2462, 0.0027], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,595][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([4.6225e-06, 6.3708e-02, 7.7055e-06, 1.4126e-02, 2.9851e-02, 7.5616e-05,
        1.1563e-06, 5.6520e-02, 1.5991e-05, 4.8461e-01, 2.3443e-02, 2.1745e-05,
        3.2762e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,596][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([3.6238e-02, 2.6059e-02, 3.7764e-02, 6.4862e-02, 9.1425e-02, 1.3376e-01,
        1.3420e-01, 5.9732e-02, 1.0248e-01, 2.0649e-02, 4.2920e-02, 2.4972e-01,
        1.9171e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,597][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.0266, 0.0032, 0.0481, 0.0325, 0.1269, 0.1010, 0.1785, 0.1049, 0.0959,
        0.0101, 0.0838, 0.1529, 0.0354], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,599][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([5.0396e-05, 6.9666e-01, 3.8938e-02, 6.0895e-03, 7.0043e-04, 3.6651e-05,
        4.9902e-03, 3.1370e-04, 9.8335e-03, 1.1846e-02, 9.2171e-05, 7.2473e-04,
        2.2973e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,603][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.0006, 0.0479, 0.1347, 0.0318, 0.0857, 0.1141, 0.1206, 0.0502, 0.1332,
        0.0376, 0.0876, 0.1149, 0.0411], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,606][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([2.3203e-09, 1.1056e-05, 4.6772e-10, 2.2168e-04, 1.9396e-04, 2.5312e-08,
        1.3682e-09, 1.4840e-07, 7.7212e-09, 2.1982e-05, 1.2235e-05, 1.6443e-07,
        9.9954e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,606][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.8768, 0.0642, 0.0078, 0.0021, 0.0040, 0.0018, 0.0017, 0.0031, 0.0031,
        0.0035, 0.0058, 0.0068, 0.0087, 0.0105], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,607][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0326, 0.1511, 0.1181, 0.0693, 0.0520, 0.0029, 0.0760, 0.1013, 0.0313,
        0.1241, 0.0878, 0.0550, 0.0973, 0.0012], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,607][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0336, 0.1361, 0.0454, 0.0773, 0.0880, 0.0592, 0.0695, 0.1057, 0.0389,
        0.1029, 0.0608, 0.0591, 0.0611, 0.0624], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,608][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0645, 0.0115, 0.0150, 0.0157, 0.0124, 0.0338, 0.0291, 0.0185, 0.0771,
        0.0738, 0.1061, 0.0969, 0.1781, 0.2673], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,608][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0714, 0.0714, 0.0713, 0.0713, 0.0715, 0.0714, 0.0714, 0.0714, 0.0715,
        0.0714, 0.0714, 0.0714, 0.0715, 0.0715], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,609][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0358, 0.0082, 0.0569, 0.0063, 0.0189, 0.1425, 0.1249, 0.0268, 0.1714,
        0.0037, 0.0413, 0.1770, 0.0077, 0.1785], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,609][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([1.1113e-06, 2.5161e-04, 1.4304e-07, 1.2968e-03, 2.6705e-01, 1.4575e-06,
        1.2207e-07, 6.9737e-04, 1.1491e-06, 1.8285e-03, 7.2810e-01, 2.2846e-06,
        7.5677e-04, 7.6018e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,612][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0432, 0.0338, 0.0903, 0.0519, 0.0202, 0.1140, 0.1330, 0.0335, 0.1758,
        0.0097, 0.0196, 0.1304, 0.0501, 0.0945], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,615][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0415, 0.0067, 0.0578, 0.0398, 0.1338, 0.1020, 0.1360, 0.0935, 0.0908,
        0.0170, 0.0750, 0.1075, 0.0315, 0.0672], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,617][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.8613e-05, 6.8004e-01, 3.2129e-02, 2.3834e-03, 2.7453e-04, 1.6555e-05,
        8.3935e-03, 4.8089e-05, 1.0192e-02, 8.3990e-03, 3.4607e-05, 7.5605e-04,
        2.5708e-01, 2.4218e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,620][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0007, 0.0399, 0.1154, 0.0227, 0.0869, 0.1117, 0.1128, 0.0426, 0.1161,
        0.0290, 0.0832, 0.1079, 0.0363, 0.0948], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,620][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([2.4046e-05, 7.7181e-06, 1.2067e-04, 1.1855e-04, 7.4214e-05, 5.8501e-01,
        2.2733e-07, 1.4881e-05, 3.0731e-04, 1.7244e-05, 2.7533e-04, 8.6222e-06,
        6.2614e-04, 4.1340e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,621][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:35,622][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[34663],
        [24621],
        [ 6399],
        [ 2207],
        [12793],
        [ 7764],
        [10147],
        [21505],
        [13694],
        [27925],
        [12714],
        [ 9986],
        [10936],
        [ 9717]], device='cuda:0')
[2024-07-24 10:30:35,624][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[32421],
        [12064],
        [19118],
        [    2],
        [29809],
        [17704],
        [34405],
        [27444],
        [16568],
        [15513],
        [14477],
        [19803],
        [13555],
        [17472]], device='cuda:0')
[2024-07-24 10:30:35,625][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[14119],
        [16054],
        [13891],
        [11790],
        [ 5210],
        [ 5178],
        [ 6767],
        [ 4655],
        [ 6738],
        [ 7132],
        [ 5538],
        [ 6323],
        [ 5137],
        [ 4872]], device='cuda:0')
[2024-07-24 10:30:35,627][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[37173],
        [29362],
        [11997],
        [29840],
        [16903],
        [21212],
        [26489],
        [15618],
        [13265],
        [32950],
        [10717],
        [18900],
        [27399],
        [20742]], device='cuda:0')
[2024-07-24 10:30:35,630][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[29781],
        [32465],
        [33323],
        [35505],
        [35245],
        [35447],
        [35475],
        [35262],
        [34972],
        [34911],
        [34396],
        [34855],
        [34812],
        [34949]], device='cuda:0')
[2024-07-24 10:30:35,631][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[24865],
        [23146],
        [17870],
        [17629],
        [19066],
        [18441],
        [21097],
        [19776],
        [19169],
        [19165],
        [18153],
        [18762],
        [17641],
        [17418]], device='cuda:0')
[2024-07-24 10:30:35,633][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[14878],
        [39179],
        [14090],
        [16159],
        [13782],
        [14021],
        [13949],
        [13760],
        [13725],
        [10322],
        [14185],
        [11277],
        [ 9123],
        [10592]], device='cuda:0')
[2024-07-24 10:30:35,636][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[32529],
        [32473],
        [34480],
        [34325],
        [33858],
        [33690],
        [34129],
        [34042],
        [34448],
        [34545],
        [34428],
        [34874],
        [35026],
        [34720]], device='cuda:0')
[2024-07-24 10:30:35,638][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[31964],
        [31947],
        [32604],
        [30835],
        [31494],
        [31203],
        [29862],
        [31099],
        [30215],
        [32246],
        [31430],
        [28269],
        [27729],
        [28445]], device='cuda:0')
[2024-07-24 10:30:35,639][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[26726],
        [ 7941],
        [ 5231],
        [18181],
        [  504],
        [  299],
        [  694],
        [ 2827],
        [ 1937],
        [ 3352],
        [ 3086],
        [ 2180],
        [  845],
        [ 4971]], device='cuda:0')
[2024-07-24 10:30:35,640][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[35584],
        [35781],
        [41247],
        [42941],
        [38110],
        [40912],
        [41465],
        [40113],
        [41284],
        [39681],
        [39262],
        [39112],
        [37719],
        [39580]], device='cuda:0')
[2024-07-24 10:30:35,641][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[47526],
        [32475],
        [33649],
        [34189],
        [33655],
        [33979],
        [33697],
        [33704],
        [33901],
        [34270],
        [34019],
        [33991],
        [34208],
        [34276]], device='cuda:0')
[2024-07-24 10:30:35,642][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[13200],
        [16475],
        [16313],
        [20891],
        [25086],
        [24528],
        [26234],
        [25901],
        [24328],
        [24975],
        [27793],
        [26409],
        [25062],
        [24250]], device='cuda:0')
[2024-07-24 10:30:35,644][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[30565],
        [31180],
        [42190],
        [44498],
        [43164],
        [41919],
        [43193],
        [44603],
        [44828],
        [45074],
        [45043],
        [44666],
        [45256],
        [44717]], device='cuda:0')
[2024-07-24 10:30:35,646][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[28594],
        [12920],
        [ 4101],
        [14085],
        [36183],
        [ 4924],
        [ 5223],
        [42154],
        [12865],
        [29146],
        [ 9264],
        [ 5436],
        [22591],
        [ 8646]], device='cuda:0')
[2024-07-24 10:30:35,647][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[30987],
        [31049],
        [31089],
        [31184],
        [31112],
        [31186],
        [31248],
        [31160],
        [31389],
        [32063],
        [31259],
        [31589],
        [32027],
        [31711]], device='cuda:0')
[2024-07-24 10:30:35,649][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 7467],
        [14119],
        [42435],
        [22164],
        [34763],
        [24299],
        [11257],
        [20552],
        [18885],
        [13149],
        [33900],
        [14059],
        [12266],
        [11184]], device='cuda:0')
[2024-07-24 10:30:35,652][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[17824],
        [24836],
        [24042],
        [26058],
        [25981],
        [26245],
        [25704],
        [25609],
        [25506],
        [25427],
        [25459],
        [25458],
        [25565],
        [25842]], device='cuda:0')
[2024-07-24 10:30:35,654][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[19351],
        [19301],
        [19557],
        [19520],
        [19806],
        [20221],
        [20264],
        [20313],
        [20807],
        [21019],
        [21556],
        [21502],
        [22039],
        [22358]], device='cuda:0')
[2024-07-24 10:30:35,655][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[17507],
        [17519],
        [17492],
        [17484],
        [17489],
        [17491],
        [17498],
        [17493],
        [17493],
        [17492],
        [17495],
        [17499],
        [17498],
        [17495]], device='cuda:0')
[2024-07-24 10:30:35,656][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[26383],
        [26272],
        [26768],
        [26777],
        [26733],
        [27485],
        [27932],
        [28116],
        [28974],
        [29045],
        [29118],
        [29165],
        [29299],
        [29170]], device='cuda:0')
[2024-07-24 10:30:35,657][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[43977],
        [28251],
        [28331],
        [ 4528],
        [18982],
        [27282],
        [26012],
        [43868],
        [34286],
        [24119],
        [29212],
        [29438],
        [24280],
        [ 5948]], device='cuda:0')
[2024-07-24 10:30:35,658][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[43315],
        [43281],
        [45863],
        [46071],
        [47893],
        [46012],
        [48550],
        [49133],
        [48351],
        [45429],
        [46282],
        [47238],
        [49377],
        [46392]], device='cuda:0')
[2024-07-24 10:30:35,659][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[12339],
        [12208],
        [ 9831],
        [ 9544],
        [ 9973],
        [ 9634],
        [ 9635],
        [ 9699],
        [ 9620],
        [ 9728],
        [ 9361],
        [ 9389],
        [ 9374],
        [ 9405]], device='cuda:0')
[2024-07-24 10:30:35,661][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[25721],
        [13143],
        [17043],
        [14427],
        [17441],
        [14573],
        [17914],
        [21816],
        [14934],
        [13369],
        [15934],
        [13672],
        [12833],
        [12843]], device='cuda:0')
[2024-07-24 10:30:35,663][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[39660],
        [34654],
        [39040],
        [36123],
        [39219],
        [40663],
        [40028],
        [39864],
        [40447],
        [40076],
        [40722],
        [40919],
        [40878],
        [40765]], device='cuda:0')
[2024-07-24 10:30:35,664][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[21348],
        [46340],
        [11037],
        [44354],
        [45501],
        [11839],
        [19559],
        [44188],
        [47181],
        [48358],
        [38941],
        [ 7411],
        [45902],
        [11473]], device='cuda:0')
[2024-07-24 10:30:35,666][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[26345],
        [21021],
        [20196],
        [24727],
        [18001],
        [25956],
        [24850],
        [15465],
        [14774],
        [17333],
        [18668],
        [25532],
        [21552],
        [32452]], device='cuda:0')
[2024-07-24 10:30:35,669][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[33603],
        [49111],
        [48311],
        [50106],
        [22196],
        [48328],
        [48510],
        [12908],
        [45053],
        [45868],
        [46328],
        [49241],
        [35786],
        [46530]], device='cuda:0')
[2024-07-24 10:30:35,672][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609],
        [35609]], device='cuda:0')
[2024-07-24 10:30:35,704][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:35,708][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,709][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,709][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,709][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,710][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,710][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,710][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,710][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,711][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,711][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,711][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,713][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,715][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.2710, 0.7290], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,718][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([4.5247e-04, 9.9955e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,722][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.0034, 0.9966], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,722][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.0756, 0.9244], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,723][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0322, 0.9678], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,723][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0676, 0.9324], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,723][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0757, 0.9243], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,724][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.5029, 0.4971], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,724][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.1005, 0.8995], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,724][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.0185, 0.9815], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,725][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.3177, 0.6823], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,725][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.1879, 0.8121], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,726][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.1587, 0.4218, 0.4195], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,728][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ and] are: tensor([5.8691e-06, 9.9968e-01, 3.1499e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,729][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ and] are: tensor([3.7767e-05, 3.8044e-01, 6.1952e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,734][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0049, 0.4506, 0.5445], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,736][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0012, 0.8097, 0.1892], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,736][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0033, 0.2286, 0.7681], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,737][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0131, 0.2949, 0.6921], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,737][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3076, 0.4246, 0.2678], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,737][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0134, 0.4778, 0.5087], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,738][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ and] are: tensor([4.5620e-04, 2.3280e-01, 7.6675e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,738][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1127, 0.3356, 0.5517], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,738][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1382, 0.5460, 0.3158], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,739][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.0859, 0.1969, 0.2487, 0.4685], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,739][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([9.3307e-05, 4.2398e-01, 4.3511e-03, 5.7158e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,740][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([1.7803e-06, 6.2449e-03, 1.2663e-01, 8.6713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,742][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([5.7399e-04, 4.4797e-02, 1.7232e-01, 7.8231e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,743][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([6.5954e-04, 5.6851e-02, 2.2139e-01, 7.2110e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,745][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([3.9473e-04, 4.2997e-02, 2.9274e-01, 6.6387e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,750][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.0089, 0.1351, 0.3112, 0.5448], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,750][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.2791, 0.2767, 0.2321, 0.2121], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,750][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.0011, 0.0856, 0.2170, 0.6963], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,751][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([1.3356e-04, 4.2666e-02, 6.6600e-01, 2.9120e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,751][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0549, 0.1505, 0.3683, 0.4262], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,751][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.0894, 0.4400, 0.3011, 0.1695], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,752][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0556, 0.1326, 0.1475, 0.3059, 0.3585], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,752][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ went] are: tensor([5.0008e-05, 5.4658e-01, 1.9276e-03, 3.7298e-01, 7.8466e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,752][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ went] are: tensor([6.7121e-07, 8.7630e-04, 3.1036e-02, 8.1422e-01, 1.5387e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,753][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0006, 0.0252, 0.1132, 0.5234, 0.3376], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,754][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ went] are: tensor([2.4034e-04, 2.0153e-02, 1.5908e-01, 3.7516e-01, 4.4536e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,756][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ went] are: tensor([7.2478e-05, 1.0324e-02, 9.3749e-02, 5.9607e-01, 2.9979e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,759][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.0052, 0.0728, 0.1659, 0.2908, 0.4652], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,763][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.2281, 0.2233, 0.1951, 0.1839, 0.1695], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,764][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.0011, 0.0646, 0.0918, 0.6523, 0.1902], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,764][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ went] are: tensor([1.2776e-05, 1.0839e-02, 9.2293e-02, 5.2914e-01, 3.6771e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,764][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.0323, 0.0977, 0.2428, 0.2620, 0.3651], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,765][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.0716, 0.4246, 0.2614, 0.1373, 0.1051], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,765][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0484, 0.1182, 0.1215, 0.2551, 0.2913, 0.1656], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,766][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([6.4183e-05, 1.1536e-01, 3.1613e-03, 7.2487e-01, 1.4573e-01, 1.0821e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,766][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([1.5045e-07, 7.4152e-05, 2.3229e-03, 8.3031e-02, 3.9512e-01, 5.1945e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,766][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([2.4148e-04, 1.0114e-02, 3.3569e-02, 2.3801e-01, 1.4563e-01, 5.7243e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,767][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.5860e-05, 4.7270e-03, 4.5619e-03, 1.0665e-01, 7.8129e-01, 1.0276e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,768][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([7.3115e-05, 3.7226e-03, 3.1372e-02, 1.3271e-01, 1.0367e-01, 7.2845e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,770][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0039, 0.0493, 0.1090, 0.2007, 0.3104, 0.3267], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,773][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1296, 0.2160, 0.1414, 0.2111, 0.1750, 0.1269], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,775][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([1.7290e-04, 3.9477e-03, 1.2181e-02, 9.0669e-02, 6.5460e-01, 2.3843e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,777][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([4.9238e-06, 2.4907e-03, 1.4321e-02, 1.6325e-01, 2.0641e-01, 6.1352e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,778][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0274, 0.0681, 0.1443, 0.1599, 0.2192, 0.3810], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,778][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0792, 0.3502, 0.2235, 0.1128, 0.0895, 0.1449], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,778][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0375, 0.0940, 0.0998, 0.2145, 0.2376, 0.1394, 0.1774],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,779][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.0578e-06, 3.8372e-01, 6.2412e-04, 3.9256e-01, 2.2179e-01, 1.0317e-03,
        2.7852e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,779][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.3617e-08, 1.9961e-05, 4.1965e-04, 2.1084e-02, 3.3479e-02, 7.6943e-01,
        1.7557e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,780][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([8.6081e-05, 4.3327e-03, 1.1360e-02, 1.1174e-01, 7.5395e-02, 3.8568e-01,
        4.1141e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,780][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([3.7486e-05, 5.5832e-03, 8.9401e-03, 1.4084e-01, 2.3150e-01, 2.8932e-01,
        3.2378e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,780][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.2142e-05, 1.1793e-03, 8.3930e-03, 4.2568e-02, 4.9759e-02, 5.2542e-01,
        3.7266e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,782][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0034, 0.0339, 0.1046, 0.1769, 0.2417, 0.2845, 0.1550],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,785][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1318, 0.1612, 0.1296, 0.1586, 0.1458, 0.1222, 0.1509],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,787][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.3449e-04, 3.8115e-03, 9.3696e-03, 6.8213e-02, 1.8304e-01, 4.4867e-01,
        2.8676e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,789][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.7873e-06, 1.0886e-03, 4.3278e-03, 6.6842e-02, 8.0870e-02, 4.2518e-01,
        4.2169e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,791][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0193, 0.0535, 0.1028, 0.1212, 0.1696, 0.2588, 0.2748],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,791][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0812, 0.3186, 0.1906, 0.0987, 0.0780, 0.1245, 0.1084],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,792][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0260, 0.0617, 0.0739, 0.1455, 0.1726, 0.1093, 0.1337, 0.2772],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,792][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ office] are: tensor([7.7241e-05, 9.6870e-03, 5.1355e-03, 5.3043e-02, 4.8028e-02, 7.3209e-03,
        1.0788e-03, 8.7563e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,793][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ office] are: tensor([1.1681e-09, 1.2659e-06, 1.4617e-05, 3.2527e-03, 3.2955e-03, 7.3521e-02,
        2.4423e-01, 6.7569e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,793][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ office] are: tensor([9.6557e-06, 2.7383e-04, 1.0965e-03, 1.0716e-02, 1.7574e-02, 7.4736e-02,
        2.2540e-01, 6.7020e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,793][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ office] are: tensor([3.7888e-06, 5.1317e-04, 1.0020e-03, 1.0210e-02, 3.5242e-02, 5.3688e-02,
        1.7772e-01, 7.2162e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,794][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ office] are: tensor([1.7337e-06, 1.6707e-04, 8.1079e-04, 7.1889e-03, 6.1780e-03, 9.3301e-02,
        1.6308e-01, 7.2927e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,794][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.0024, 0.0240, 0.0810, 0.1299, 0.1971, 0.2333, 0.1307, 0.2016],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,796][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.1403, 0.1341, 0.1240, 0.1260, 0.1139, 0.0945, 0.1164, 0.1509],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,798][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ office] are: tensor([3.6279e-05, 5.0877e-04, 2.2736e-03, 2.0236e-02, 4.5165e-02, 1.5484e-01,
        2.4862e-01, 5.2832e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,800][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ office] are: tensor([3.2599e-07, 1.9410e-04, 7.1233e-04, 9.4379e-03, 1.0348e-02, 9.0313e-02,
        2.2268e-01, 6.6632e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,803][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.0110, 0.0229, 0.0668, 0.0649, 0.0955, 0.1930, 0.1942, 0.3516],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,805][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.0810, 0.2605, 0.1901, 0.0879, 0.0709, 0.1192, 0.1032, 0.0874],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:35,805][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0260, 0.0605, 0.0634, 0.1369, 0.1494, 0.0860, 0.1092, 0.2562, 0.1125],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,806][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [,] are: tensor([2.0227e-05, 1.2539e-01, 3.3736e-04, 1.8652e-01, 1.4987e-01, 1.1023e-03,
        1.2157e-03, 5.3266e-01, 2.8861e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,806][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [,] are: tensor([6.1815e-10, 4.8562e-07, 7.4048e-06, 4.9551e-04, 5.3950e-04, 6.1703e-03,
        2.7710e-02, 4.3131e-01, 5.3376e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,806][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [,] are: tensor([1.0808e-05, 3.5060e-04, 8.5323e-04, 1.0262e-02, 5.3342e-03, 4.2254e-02,
        1.0315e-01, 4.1573e-01, 4.2206e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,807][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [,] are: tensor([6.9724e-06, 1.8649e-03, 1.1913e-03, 2.6429e-02, 9.3706e-03, 3.1440e-02,
        1.2654e-01, 4.4439e-01, 3.5876e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,807][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [,] are: tensor([1.0417e-06, 4.5858e-05, 3.6973e-04, 1.6864e-03, 2.9771e-03, 2.6182e-02,
        6.4448e-02, 4.4183e-01, 4.6246e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,808][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0017, 0.0208, 0.0576, 0.1056, 0.1607, 0.2099, 0.1259, 0.1940, 0.1238],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,809][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0967, 0.1376, 0.1025, 0.1293, 0.1243, 0.0925, 0.1128, 0.1243, 0.0801],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,811][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [,] are: tensor([2.7736e-05, 4.2954e-04, 1.3779e-03, 4.2546e-03, 2.5949e-02, 6.1724e-02,
        9.8704e-02, 4.1745e-01, 3.9008e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,813][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [,] are: tensor([9.8604e-08, 4.3068e-05, 2.1742e-04, 3.1644e-03, 3.6635e-03, 2.9196e-02,
        6.3229e-02, 5.5185e-01, 3.4863e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,817][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0104, 0.0241, 0.0534, 0.0578, 0.0805, 0.1252, 0.1267, 0.2199, 0.3020],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,819][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0739, 0.2639, 0.1713, 0.0826, 0.0648, 0.1068, 0.0934, 0.0770, 0.0662],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:35,819][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0230, 0.0480, 0.0579, 0.1087, 0.1363, 0.0838, 0.1067, 0.2185, 0.0998,
        0.1173], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,819][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0005, 0.1084, 0.0185, 0.3072, 0.0899, 0.0138, 0.0019, 0.3525, 0.0193,
        0.0881], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,820][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([4.2124e-11, 2.7222e-09, 9.4101e-08, 6.7356e-06, 2.5788e-06, 6.9265e-05,
        6.2803e-04, 2.9082e-03, 2.9253e-02, 9.6713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,820][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([8.0558e-07, 1.2314e-05, 5.3152e-05, 4.4983e-04, 6.0083e-04, 3.6448e-03,
        1.0473e-02, 4.2815e-02, 1.2422e-01, 8.1773e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,821][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([1.7875e-06, 3.3718e-05, 1.5765e-04, 2.6271e-03, 1.2801e-03, 4.9262e-03,
        1.9216e-02, 2.3554e-02, 1.5460e-01, 7.9360e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,821][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([1.7790e-07, 1.6835e-06, 5.8798e-05, 3.5992e-04, 5.5334e-04, 4.0421e-03,
        1.1606e-02, 1.2378e-01, 3.7039e-01, 4.8921e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,821][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0012, 0.0145, 0.0417, 0.0802, 0.1258, 0.1588, 0.0994, 0.1450, 0.0999,
        0.2336], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,823][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.1127, 0.1198, 0.0983, 0.1049, 0.0900, 0.0752, 0.1040, 0.1218, 0.0803,
        0.0929], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,825][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([5.4240e-06, 2.7299e-05, 2.6623e-04, 2.1641e-03, 4.5672e-03, 1.1451e-02,
        2.8888e-02, 7.9366e-02, 2.6088e-01, 6.1238e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,827][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([4.3270e-08, 1.6683e-06, 3.7842e-05, 2.5308e-04, 7.6800e-04, 6.1258e-03,
        1.3513e-02, 9.3433e-02, 1.9168e-01, 6.9419e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,830][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0039, 0.0153, 0.0362, 0.0366, 0.0471, 0.0839, 0.0961, 0.1493, 0.2511,
        0.2804], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,832][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0479, 0.2005, 0.1581, 0.0816, 0.0653, 0.1074, 0.0976, 0.0797, 0.0788,
        0.0831], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:35,833][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0169, 0.0408, 0.0459, 0.0973, 0.1077, 0.0659, 0.0852, 0.1915, 0.0840,
        0.1216, 0.1431], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,833][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([5.9451e-07, 1.3586e-01, 1.8256e-04, 1.4080e-01, 5.4624e-02, 2.3070e-04,
        1.0594e-04, 4.6198e-01, 1.6695e-03, 1.4917e-01, 5.5385e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,833][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([6.5702e-12, 5.3043e-10, 1.1035e-08, 4.4028e-07, 2.3817e-07, 8.0820e-06,
        4.6010e-05, 8.2298e-04, 7.9122e-03, 9.1690e-01, 7.4312e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,834][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([5.8984e-07, 6.3974e-06, 2.8179e-05, 2.2020e-04, 1.0689e-04, 1.3955e-03,
        3.1045e-03, 1.8468e-02, 4.6104e-02, 8.3219e-01, 9.8379e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,834][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([9.4578e-07, 2.3524e-05, 1.1122e-04, 4.4675e-04, 4.2119e-04, 2.3159e-03,
        5.7899e-03, 1.9729e-02, 8.0231e-02, 5.8819e-01, 3.0274e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,835][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([3.7006e-08, 1.2040e-06, 8.1399e-06, 4.8080e-05, 4.4418e-05, 4.5798e-04,
        1.8083e-03, 1.2010e-02, 5.9604e-02, 8.5037e-01, 7.5651e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,835][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0007, 0.0086, 0.0269, 0.0560, 0.1079, 0.1163, 0.0717, 0.1218, 0.0800,
        0.1879, 0.2223], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,837][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0898, 0.1075, 0.0910, 0.0929, 0.0914, 0.0850, 0.0988, 0.1015, 0.0798,
        0.0846, 0.0775], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,839][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([2.4251e-06, 2.4842e-05, 6.7659e-05, 2.1924e-04, 2.7974e-04, 2.5394e-03,
        4.2447e-03, 1.2403e-02, 6.2223e-02, 8.5937e-01, 5.8624e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,841][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([5.4240e-09, 9.1008e-07, 7.1586e-06, 7.7306e-05, 5.9647e-05, 5.7559e-04,
        1.5883e-03, 1.4281e-02, 4.6040e-02, 8.5735e-01, 8.0024e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,844][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0031, 0.0097, 0.0306, 0.0240, 0.0375, 0.0843, 0.0740, 0.0957, 0.2147,
        0.1881, 0.2384], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,846][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0444, 0.2170, 0.1614, 0.0683, 0.0600, 0.1115, 0.0893, 0.0658, 0.0694,
        0.0656, 0.0474], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:35,846][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0151, 0.0401, 0.0424, 0.0928, 0.0961, 0.0586, 0.0726, 0.1837, 0.0774,
        0.1103, 0.1321, 0.0787], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,847][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([2.5917e-07, 3.1952e-01, 8.5632e-05, 1.5147e-01, 2.2892e-02, 1.1289e-04,
        4.0165e-05, 2.5096e-01, 4.9544e-04, 2.2848e-01, 2.5923e-02, 2.0514e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,847][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([1.0970e-11, 2.9018e-10, 5.7370e-09, 3.2574e-07, 5.2663e-07, 1.2358e-05,
        2.2296e-05, 6.6334e-04, 3.6828e-03, 2.1410e-01, 4.7109e-01, 3.1042e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,848][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([4.7667e-07, 6.9873e-06, 1.3911e-05, 1.3227e-04, 8.1218e-05, 5.0905e-04,
        1.6305e-03, 9.4282e-03, 1.8990e-02, 6.3603e-01, 1.3941e-01, 1.9376e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,848][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([6.3946e-07, 1.5977e-05, 2.6089e-05, 3.4914e-04, 3.9933e-04, 6.3660e-04,
        1.1843e-03, 9.0732e-03, 2.1540e-02, 2.3560e-01, 5.2984e-01, 2.0134e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,849][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([9.9855e-08, 1.1466e-06, 8.5076e-06, 4.3310e-05, 4.2362e-05, 5.2951e-04,
        1.2025e-03, 1.1428e-02, 4.8790e-02, 3.7890e-01, 2.1728e-01, 3.4178e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,849][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0005, 0.0069, 0.0255, 0.0551, 0.0999, 0.0995, 0.0635, 0.0969, 0.0714,
        0.1682, 0.2037, 0.1087], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,851][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0613, 0.1196, 0.0751, 0.0957, 0.0825, 0.0719, 0.0875, 0.1037, 0.0570,
        0.1021, 0.0748, 0.0689], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,853][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([1.0237e-06, 7.3201e-06, 2.3095e-05, 1.3262e-04, 2.9197e-04, 9.0419e-04,
        1.5495e-03, 1.4732e-02, 2.1614e-02, 1.7691e-01, 6.7218e-01, 1.1165e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,855][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([7.9256e-09, 6.8277e-07, 3.4656e-06, 4.2826e-05, 3.8937e-05, 3.3001e-04,
        3.5488e-04, 1.4072e-02, 2.5224e-02, 4.1886e-01, 2.1788e-01, 3.2319e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,858][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0037, 0.0101, 0.0236, 0.0220, 0.0311, 0.0600, 0.0576, 0.0922, 0.1300,
        0.1410, 0.1798, 0.2489], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,860][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0508, 0.2041, 0.1511, 0.0652, 0.0552, 0.1005, 0.0829, 0.0628, 0.0616,
        0.0596, 0.0450, 0.0613], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:35,860][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0134, 0.0292, 0.0375, 0.0707, 0.0898, 0.0567, 0.0722, 0.1449, 0.0691,
        0.0809, 0.1149, 0.0813, 0.1394], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,861][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0005, 0.0467, 0.0058, 0.2124, 0.0628, 0.0157, 0.0015, 0.4278, 0.0215,
        0.0811, 0.0273, 0.0046, 0.0923], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,861][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([1.6124e-12, 5.5494e-11, 1.8007e-09, 4.5897e-08, 6.8674e-08, 2.1837e-06,
        5.8362e-06, 8.4942e-05, 6.0841e-04, 3.5178e-02, 8.0455e-02, 3.9720e-01,
        4.8647e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,862][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([5.6750e-08, 1.6874e-06, 3.9797e-06, 4.5545e-05, 3.7163e-05, 9.6354e-05,
        7.0816e-04, 6.4140e-03, 8.5677e-03, 1.7460e-01, 8.5242e-02, 4.0299e-01,
        3.2129e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,862][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([2.1695e-07, 2.9499e-06, 8.4502e-06, 7.1205e-05, 8.1975e-05, 2.0483e-04,
        6.9235e-04, 2.3632e-03, 8.7272e-03, 5.5400e-02, 1.1599e-01, 2.2351e-01,
        5.9295e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,862][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([1.1412e-08, 2.3080e-07, 1.7471e-06, 1.5416e-05, 1.4034e-05, 1.3323e-04,
        3.1840e-04, 4.1721e-03, 1.1105e-02, 1.3357e-01, 5.5737e-02, 3.5074e-01,
        4.4419e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,863][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0006, 0.0049, 0.0198, 0.0372, 0.0687, 0.0763, 0.0493, 0.0745, 0.0617,
        0.1306, 0.1676, 0.0957, 0.2132], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,864][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0961, 0.0953, 0.0891, 0.0734, 0.0731, 0.0663, 0.0739, 0.0924, 0.0689,
        0.0686, 0.0632, 0.0626, 0.0772], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,867][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([4.9297e-07, 6.6830e-06, 1.2930e-05, 9.5753e-05, 8.8579e-05, 5.6469e-04,
        1.2512e-03, 3.1312e-03, 1.4925e-02, 1.7893e-01, 2.8355e-01, 1.5761e-01,
        3.5983e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,869][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([5.6073e-09, 1.9806e-07, 1.3625e-06, 1.6203e-05, 2.2018e-05, 1.7349e-04,
        4.0780e-04, 6.8371e-03, 1.2622e-02, 1.1423e-01, 1.0872e-01, 4.0788e-01,
        3.4909e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,872][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0023, 0.0059, 0.0181, 0.0142, 0.0195, 0.0459, 0.0426, 0.0611, 0.1162,
        0.0997, 0.1301, 0.2002, 0.2442], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,874][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0514, 0.1872, 0.1435, 0.0619, 0.0541, 0.0956, 0.0775, 0.0621, 0.0605,
        0.0571, 0.0440, 0.0601, 0.0451], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:35,874][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0139, 0.0333, 0.0350, 0.0726, 0.0796, 0.0464, 0.0598, 0.1387, 0.0624,
        0.0873, 0.1082, 0.0653, 0.1424, 0.0550], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,874][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.8834e-05, 4.8571e-02, 6.2783e-04, 1.9083e-01, 4.9445e-02, 2.3910e-03,
        1.6296e-03, 4.8004e-01, 1.0494e-02, 7.3312e-02, 7.4307e-02, 1.0965e-03,
        6.3713e-02, 3.5208e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,875][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([1.1991e-12, 1.5919e-12, 2.8130e-11, 1.8970e-09, 3.4252e-09, 4.0916e-09,
        2.1818e-07, 6.0972e-06, 1.0950e-05, 7.6412e-04, 2.9268e-03, 1.5501e-02,
        2.2937e-01, 7.5142e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,875][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([8.7802e-08, 5.9613e-07, 1.4427e-06, 1.5260e-05, 6.2872e-06, 2.1921e-05,
        2.3117e-04, 1.4474e-03, 2.5834e-03, 4.2820e-02, 2.0809e-02, 1.0556e-01,
        3.5128e-01, 4.7523e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,876][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.2603e-08, 2.0033e-07, 2.5347e-07, 5.1782e-06, 2.8777e-05, 4.4578e-06,
        1.8976e-05, 1.6217e-04, 1.9472e-04, 3.2158e-03, 2.0190e-02, 5.6875e-03,
        9.0441e-01, 6.6081e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,876][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.1557e-08, 5.4536e-08, 3.5132e-07, 1.7254e-06, 1.0353e-06, 6.0550e-06,
        5.0001e-05, 6.6661e-04, 1.9984e-03, 1.4808e-02, 1.2117e-02, 4.5414e-02,
        1.8668e-01, 7.3826e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,877][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0003, 0.0038, 0.0145, 0.0297, 0.0548, 0.0546, 0.0409, 0.0587, 0.0465,
        0.1116, 0.1352, 0.0748, 0.1786, 0.1960], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,878][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0485, 0.0861, 0.0654, 0.0992, 0.0760, 0.0650, 0.0702, 0.0748, 0.0539,
        0.0805, 0.0722, 0.0551, 0.0905, 0.0626], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,880][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([3.5572e-07, 9.1758e-07, 2.7953e-06, 1.9718e-05, 1.2364e-04, 3.9788e-05,
        3.0463e-04, 8.5265e-04, 3.6851e-03, 1.2529e-02, 1.3143e-01, 5.8073e-02,
        3.9162e-01, 4.0132e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,882][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([5.1851e-10, 2.1498e-08, 9.6875e-08, 1.6083e-06, 1.0849e-06, 4.4370e-06,
        2.4015e-05, 3.7132e-04, 8.2259e-04, 1.1699e-02, 6.0294e-03, 3.2715e-02,
        3.1470e-01, 6.3363e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,886][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0020, 0.0050, 0.0123, 0.0107, 0.0153, 0.0304, 0.0287, 0.0415, 0.0672,
        0.0676, 0.0940, 0.1179, 0.1635, 0.3439], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,888][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0574, 0.1815, 0.1346, 0.0562, 0.0507, 0.0860, 0.0751, 0.0542, 0.0537,
        0.0489, 0.0391, 0.0532, 0.0415, 0.0680], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:35,917][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:35,918][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,918][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,918][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,919][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,919][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,919][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,920][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,920][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,920][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,921][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,921][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,921][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:35,921][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,922][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.9115, 0.0885], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,922][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.0034, 0.9966], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,924][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([9.9974e-01, 2.5902e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,927][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.0322, 0.9678], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,930][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.3319, 0.6681], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,932][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.2891, 0.7109], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,932][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.4812, 0.5188], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,933][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.3131, 0.6869], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,933][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.4682, 0.5318], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,933][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.3177, 0.6823], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,934][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.0769, 0.9231], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:35,934][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3057, 0.3318, 0.3625], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,934][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.4019, 0.3886, 0.2095], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,935][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([3.7767e-05, 3.8044e-01, 6.1952e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,936][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.9965, 0.0011, 0.0024], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,939][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0012, 0.8097, 0.1892], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,942][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1510, 0.6893, 0.1597], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,946][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.3396, 0.4514, 0.2090], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,946][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.3242, 0.4903, 0.1855], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,946][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0609, 0.5120, 0.4272], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,947][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.2063, 0.2325, 0.5612], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,947][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.1127, 0.3356, 0.5517], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,947][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0444, 0.5354, 0.4202], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:35,948][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.1868, 0.2120, 0.3450, 0.2562], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,948][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.7659, 0.0519, 0.0976, 0.0847], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,948][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([1.7803e-06, 6.2449e-03, 1.2663e-01, 8.6713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,950][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.9804, 0.0025, 0.0043, 0.0127], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,951][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([6.5954e-04, 5.6851e-02, 2.2139e-01, 7.2110e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,954][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.0469, 0.4654, 0.3961, 0.0916], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,958][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.1050, 0.1673, 0.1104, 0.6174], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,959][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.3345, 0.3010, 0.1851, 0.1794], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,960][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.0020, 0.1867, 0.7340, 0.0773], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,960][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.1759, 0.1742, 0.4406, 0.2093], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,960][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.0549, 0.1505, 0.3683, 0.4262], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,961][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.0241, 0.2490, 0.2777, 0.4492], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:35,961][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.1317, 0.1464, 0.2483, 0.3562, 0.1174], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,962][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.4103, 0.1556, 0.1057, 0.2162, 0.1122], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,962][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([6.7121e-07, 8.7630e-04, 3.1036e-02, 8.1422e-01, 1.5387e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,962][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.9605, 0.0052, 0.0054, 0.0218, 0.0071], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,963][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([2.4034e-04, 2.0153e-02, 1.5908e-01, 3.7516e-01, 4.4536e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,966][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.0093, 0.2577, 0.2681, 0.4374, 0.0275], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,969][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0850, 0.1801, 0.0947, 0.4760, 0.1643], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,973][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.2590, 0.2949, 0.1485, 0.1407, 0.1569], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,973][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.0047, 0.2748, 0.5028, 0.0817, 0.1360], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,974][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.1236, 0.1219, 0.3099, 0.1609, 0.2837], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,974][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.0323, 0.0977, 0.2428, 0.2620, 0.3651], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,975][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.0205, 0.1926, 0.1680, 0.3286, 0.2904], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:35,975][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1444, 0.1242, 0.1922, 0.2193, 0.1191, 0.2008], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,975][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.1201, 0.1964, 0.1215, 0.2740, 0.1847, 0.1033], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,976][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.5045e-07, 7.4152e-05, 2.3229e-03, 8.3031e-02, 3.9512e-01, 5.1945e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,976][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.9806, 0.0011, 0.0025, 0.0085, 0.0022, 0.0051], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,977][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([1.5860e-05, 4.7270e-03, 4.5619e-03, 1.0665e-01, 7.8129e-01, 1.0276e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,980][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0269, 0.2723, 0.3906, 0.2490, 0.0429, 0.0184], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,983][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.1059, 0.1775, 0.0522, 0.4410, 0.1872, 0.0363], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,987][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1987, 0.2295, 0.1172, 0.1562, 0.1855, 0.1129], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,987][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0098, 0.0878, 0.1917, 0.0454, 0.3492, 0.3160], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,988][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0732, 0.0772, 0.1741, 0.1048, 0.1786, 0.3921], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,988][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0274, 0.0681, 0.1443, 0.1599, 0.2192, 0.3810], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,988][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0161, 0.1383, 0.1112, 0.2345, 0.2130, 0.2870], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:35,989][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1071, 0.1197, 0.1497, 0.2117, 0.1191, 0.1908, 0.1018],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,989][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2199, 0.1847, 0.1129, 0.2246, 0.1416, 0.0916, 0.0246],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,989][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([1.3617e-08, 1.9961e-05, 4.1965e-04, 2.1084e-02, 3.3479e-02, 7.6943e-01,
        1.7557e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,990][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([9.8617e-01, 5.5304e-04, 1.3041e-03, 5.1096e-03, 1.0855e-03, 2.8706e-03,
        2.9031e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,991][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([3.7486e-05, 5.5832e-03, 8.9401e-03, 1.4084e-01, 2.3150e-01, 2.8932e-01,
        3.2378e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,993][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0134, 0.2725, 0.3256, 0.2280, 0.0583, 0.0922, 0.0100],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:35,996][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0826, 0.1843, 0.0685, 0.3894, 0.1646, 0.0436, 0.0671],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,000][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1870, 0.2222, 0.1047, 0.1143, 0.1527, 0.0923, 0.1267],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,001][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0011, 0.0869, 0.1602, 0.0398, 0.1221, 0.5238, 0.0661],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,001][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0490, 0.0525, 0.1200, 0.0750, 0.1268, 0.2821, 0.2947],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,001][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0193, 0.0535, 0.1028, 0.1212, 0.1696, 0.2588, 0.2748],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,002][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0141, 0.1211, 0.0821, 0.1848, 0.1638, 0.2132, 0.2208],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,002][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.0720, 0.1263, 0.1216, 0.1856, 0.1060, 0.1486, 0.1014, 0.1384],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,003][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.2129, 0.0828, 0.0797, 0.1394, 0.0993, 0.0682, 0.0105, 0.3072],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,003][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([1.1681e-09, 1.2659e-06, 1.4617e-05, 3.2527e-03, 3.2955e-03, 7.3521e-02,
        2.4423e-01, 6.7569e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,003][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.9654, 0.0013, 0.0020, 0.0084, 0.0021, 0.0041, 0.0047, 0.0119],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,004][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([3.7888e-06, 5.1317e-04, 1.0020e-03, 1.0210e-02, 3.5242e-02, 5.3688e-02,
        1.7772e-01, 7.2162e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,007][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.0121, 0.2951, 0.1239, 0.3690, 0.0398, 0.1005, 0.0469, 0.0127],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,010][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.0518, 0.1087, 0.0563, 0.3302, 0.1112, 0.0298, 0.0549, 0.2572],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,014][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.1724, 0.1964, 0.0952, 0.0974, 0.1170, 0.0802, 0.0854, 0.1559],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,014][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([1.5411e-04, 3.7733e-02, 1.3187e-01, 2.4061e-02, 8.4237e-02, 6.6527e-01,
        5.3113e-02, 3.5621e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,015][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.0547, 0.0492, 0.1119, 0.0555, 0.0983, 0.2443, 0.2732, 0.1129],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,015][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.0110, 0.0229, 0.0668, 0.0649, 0.0955, 0.1930, 0.1942, 0.3516],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,016][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0117, 0.0538, 0.0606, 0.1002, 0.0907, 0.1430, 0.1466, 0.3934],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,016][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0749, 0.0929, 0.1029, 0.1650, 0.0980, 0.1344, 0.0882, 0.1432, 0.1005],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,016][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.4500, 0.0840, 0.0683, 0.0965, 0.0570, 0.0734, 0.0116, 0.1486, 0.0106],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,017][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([6.1815e-10, 4.8562e-07, 7.4048e-06, 4.9551e-04, 5.3950e-04, 6.1703e-03,
        2.7710e-02, 4.3131e-01, 5.3376e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,018][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.9588, 0.0011, 0.0024, 0.0081, 0.0021, 0.0048, 0.0045, 0.0095, 0.0088],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,020][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([6.9724e-06, 1.8649e-03, 1.1913e-03, 2.6429e-02, 9.3706e-03, 3.1440e-02,
        1.2654e-01, 4.4439e-01, 3.5876e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,023][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0157, 0.2111, 0.2488, 0.1700, 0.0761, 0.0720, 0.0424, 0.0781, 0.0858],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,028][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0784, 0.1366, 0.0501, 0.2693, 0.1185, 0.0322, 0.0460, 0.2276, 0.0413],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,028][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1362, 0.1886, 0.0810, 0.0980, 0.1271, 0.0721, 0.0956, 0.1382, 0.0632],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,028][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0010, 0.1109, 0.1150, 0.0768, 0.1005, 0.4264, 0.0925, 0.0391, 0.0378],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,029][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0320, 0.0324, 0.0739, 0.0476, 0.0790, 0.1706, 0.1819, 0.1122, 0.2705],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,029][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0104, 0.0241, 0.0534, 0.0578, 0.0805, 0.1252, 0.1267, 0.2199, 0.3020],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,030][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0092, 0.0676, 0.0445, 0.0971, 0.0775, 0.1011, 0.1087, 0.2940, 0.2002],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,030][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.0533, 0.0583, 0.0959, 0.1015, 0.0750, 0.1331, 0.0904, 0.1478, 0.1258,
        0.1189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,030][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.2316, 0.0428, 0.0862, 0.0789, 0.0768, 0.0849, 0.0123, 0.3055, 0.0102,
        0.0707], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,031][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([4.2124e-11, 2.7222e-09, 9.4101e-08, 6.7356e-06, 2.5788e-06, 6.9265e-05,
        6.2803e-04, 2.9082e-03, 2.9253e-02, 9.6713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,032][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([9.8732e-01, 1.2831e-04, 3.8780e-04, 1.5891e-03, 2.9963e-04, 7.7743e-04,
        7.4536e-04, 1.8712e-03, 1.8775e-03, 5.0084e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,033][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([1.7875e-06, 3.3718e-05, 1.5765e-04, 2.6271e-03, 1.2801e-03, 4.9262e-03,
        1.9216e-02, 2.3554e-02, 1.5460e-01, 7.9360e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,037][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.0159, 0.0269, 0.2359, 0.1406, 0.0394, 0.0909, 0.0814, 0.1407, 0.2102,
        0.0182], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,040][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0295, 0.0772, 0.0289, 0.2877, 0.0631, 0.0124, 0.0237, 0.1501, 0.0260,
        0.3015], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,042][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.1535, 0.1379, 0.0783, 0.0915, 0.0945, 0.0673, 0.0919, 0.1272, 0.0681,
        0.0898], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,042][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.0006, 0.0198, 0.1120, 0.0295, 0.1006, 0.5283, 0.1244, 0.0379, 0.0234,
        0.0234], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,043][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0283, 0.0242, 0.0648, 0.0316, 0.0634, 0.1586, 0.1850, 0.0827, 0.3040,
        0.0573], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,043][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0039, 0.0153, 0.0362, 0.0366, 0.0471, 0.0839, 0.0961, 0.1493, 0.2511,
        0.2804], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,043][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.0051, 0.0291, 0.0305, 0.0583, 0.0464, 0.0661, 0.0768, 0.1776, 0.1666,
        0.3434], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,044][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0450, 0.0625, 0.0871, 0.1235, 0.0916, 0.1237, 0.0754, 0.1406, 0.0931,
        0.1145, 0.0429], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,044][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.1684, 0.0585, 0.0800, 0.1093, 0.0826, 0.0734, 0.0161, 0.2629, 0.0149,
        0.0952, 0.0387], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,045][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([6.5702e-12, 5.3043e-10, 1.1035e-08, 4.4028e-07, 2.3817e-07, 8.0820e-06,
        4.6010e-05, 8.2298e-04, 7.9122e-03, 9.1690e-01, 7.4312e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,046][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.8817, 0.0021, 0.0028, 0.0112, 0.0030, 0.0050, 0.0057, 0.0155, 0.0112,
        0.0422, 0.0195], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,048][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([9.4578e-07, 2.3524e-05, 1.1122e-04, 4.4675e-04, 4.2119e-04, 2.3159e-03,
        5.7899e-03, 1.9729e-02, 8.0231e-02, 5.8819e-01, 3.0274e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,051][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0096, 0.1995, 0.1403, 0.1201, 0.0329, 0.0563, 0.0605, 0.1007, 0.1263,
        0.1480, 0.0058], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,055][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0320, 0.0561, 0.0242, 0.1704, 0.0493, 0.0114, 0.0209, 0.1233, 0.0227,
        0.3523, 0.1375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,058][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.1250, 0.1420, 0.0730, 0.0776, 0.0966, 0.0770, 0.0840, 0.1042, 0.0692,
        0.0809, 0.0706], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,058][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0007, 0.0542, 0.2013, 0.0157, 0.0788, 0.4449, 0.0886, 0.0164, 0.0271,
        0.0599, 0.0123], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,058][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0248, 0.0219, 0.0567, 0.0299, 0.0540, 0.1347, 0.1493, 0.0706, 0.2384,
        0.0648, 0.1548], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,059][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0031, 0.0097, 0.0306, 0.0240, 0.0375, 0.0843, 0.0740, 0.0957, 0.2147,
        0.1881, 0.2384], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,059][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0060, 0.0304, 0.0284, 0.0457, 0.0414, 0.0699, 0.0628, 0.1405, 0.1531,
        0.2950, 0.1268], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,060][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0566, 0.0551, 0.0932, 0.1042, 0.0605, 0.1114, 0.0695, 0.1213, 0.0949,
        0.1181, 0.0542, 0.0611], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,061][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1183, 0.1332, 0.0729, 0.1473, 0.1020, 0.0710, 0.0265, 0.1458, 0.0358,
        0.0808, 0.0492, 0.0174], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,062][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([1.0970e-11, 2.9018e-10, 5.7370e-09, 3.2574e-07, 5.2663e-07, 1.2358e-05,
        2.2296e-05, 6.6334e-04, 3.6828e-03, 2.1410e-01, 4.7109e-01, 3.1042e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,064][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([9.3473e-01, 4.6327e-04, 1.3165e-03, 4.4237e-03, 9.9885e-04, 2.7297e-03,
        2.7589e-03, 4.9736e-03, 6.0550e-03, 1.3237e-02, 7.9918e-03, 2.0321e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,066][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([6.3946e-07, 1.5977e-05, 2.6089e-05, 3.4914e-04, 3.9933e-04, 6.3660e-04,
        1.1843e-03, 9.0732e-03, 2.1540e-02, 2.3560e-01, 5.2984e-01, 2.0134e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,069][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0074, 0.1431, 0.1170, 0.1168, 0.0465, 0.0657, 0.0239, 0.1607, 0.1239,
        0.1338, 0.0575, 0.0036], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,071][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0316, 0.0635, 0.0209, 0.1424, 0.0507, 0.0138, 0.0187, 0.1158, 0.0181,
        0.3683, 0.1288, 0.0272], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,071][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0853, 0.1490, 0.0636, 0.0760, 0.0893, 0.0594, 0.0845, 0.1204, 0.0519,
        0.0945, 0.0697, 0.0564], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,072][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0005, 0.0793, 0.1251, 0.0440, 0.0622, 0.3274, 0.0778, 0.0293, 0.0311,
        0.1367, 0.0538, 0.0327], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,072][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0189, 0.0189, 0.0425, 0.0270, 0.0468, 0.0998, 0.1020, 0.0663, 0.1691,
        0.0633, 0.1354, 0.2099], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,072][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0037, 0.0101, 0.0236, 0.0220, 0.0311, 0.0600, 0.0576, 0.0922, 0.1300,
        0.1410, 0.1798, 0.2489], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,073][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0065, 0.0276, 0.0242, 0.0428, 0.0345, 0.0546, 0.0526, 0.1269, 0.1126,
        0.2407, 0.1137, 0.1633], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,073][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.0348, 0.0465, 0.0698, 0.0992, 0.0586, 0.0973, 0.0599, 0.1096, 0.0804,
        0.1056, 0.0555, 0.0582, 0.1245], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,075][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.1308, 0.0714, 0.0582, 0.1196, 0.0844, 0.0558, 0.0110, 0.2331, 0.0117,
        0.0851, 0.0325, 0.0069, 0.0995], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,076][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([1.6124e-12, 5.5494e-11, 1.8007e-09, 4.5897e-08, 6.8674e-08, 2.1837e-06,
        5.8362e-06, 8.4942e-05, 6.0841e-04, 3.5178e-02, 8.0455e-02, 3.9720e-01,
        4.8647e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,078][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([9.2449e-01, 3.9019e-04, 9.1108e-04, 3.5120e-03, 7.4928e-04, 1.7613e-03,
        1.8776e-03, 4.4536e-03, 4.0126e-03, 1.1096e-02, 6.9510e-03, 1.5377e-02,
        2.4417e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,080][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([2.1695e-07, 2.9499e-06, 8.4502e-06, 7.1205e-05, 8.1975e-05, 2.0483e-04,
        6.9235e-04, 2.3632e-03, 8.7272e-03, 5.5400e-02, 1.1599e-01, 2.2351e-01,
        5.9295e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,084][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.0093, 0.1237, 0.1461, 0.0886, 0.0585, 0.0909, 0.0648, 0.0864, 0.1208,
        0.0834, 0.0560, 0.0606, 0.0108], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,085][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.0187, 0.0311, 0.0207, 0.1000, 0.0457, 0.0148, 0.0234, 0.0932, 0.0230,
        0.2085, 0.1146, 0.0322, 0.2741], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,085][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.1166, 0.1300, 0.0705, 0.0685, 0.0810, 0.0605, 0.0646, 0.0935, 0.0577,
        0.0728, 0.0625, 0.0533, 0.0685], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,086][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([3.3476e-05, 3.5066e-02, 1.0096e-01, 1.5094e-02, 2.3590e-02, 5.9274e-01,
        4.3728e-02, 5.8840e-03, 1.1463e-02, 8.3121e-02, 6.6596e-02, 1.8459e-02,
        3.2641e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,086][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([0.0196, 0.0155, 0.0416, 0.0191, 0.0366, 0.0974, 0.1133, 0.0442, 0.1821,
        0.0374, 0.1033, 0.2392, 0.0506], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,086][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.0023, 0.0059, 0.0181, 0.0142, 0.0195, 0.0459, 0.0426, 0.0611, 0.1162,
        0.0997, 0.1301, 0.2002, 0.2442], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,087][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0078, 0.0263, 0.0266, 0.0386, 0.0323, 0.0521, 0.0478, 0.1173, 0.1078,
        0.1970, 0.0975, 0.1449, 0.1040], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,089][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0510, 0.0449, 0.0669, 0.0749, 0.0440, 0.0677, 0.0487, 0.0986, 0.0734,
        0.0956, 0.0440, 0.0570, 0.1513, 0.0821], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,092][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0629, 0.1071, 0.0633, 0.1402, 0.1039, 0.0595, 0.0358, 0.1180, 0.0491,
        0.0852, 0.0659, 0.0224, 0.0648, 0.0220], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,095][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.1991e-12, 1.5919e-12, 2.8130e-11, 1.8970e-09, 3.4252e-09, 4.0916e-09,
        2.1818e-07, 6.0972e-06, 1.0950e-05, 7.6412e-04, 2.9268e-03, 1.5501e-02,
        2.2937e-01, 7.5142e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,097][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([8.5493e-01, 6.8407e-04, 1.6738e-03, 5.4298e-03, 1.2200e-03, 3.0917e-03,
        3.1813e-03, 5.4614e-03, 6.5754e-03, 1.4295e-02, 8.6108e-03, 2.1773e-02,
        2.7549e-02, 4.5527e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,098][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([1.2603e-08, 2.0033e-07, 2.5347e-07, 5.1782e-06, 2.8777e-05, 4.4578e-06,
        1.8976e-05, 1.6217e-04, 1.9472e-04, 3.2158e-03, 2.0190e-02, 5.6875e-03,
        9.0441e-01, 6.6081e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,098][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0093, 0.1247, 0.1612, 0.1090, 0.0181, 0.0069, 0.0221, 0.1192, 0.0773,
        0.1018, 0.1211, 0.0162, 0.1087, 0.0045], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,099][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0320, 0.0456, 0.0117, 0.1000, 0.0437, 0.0094, 0.0143, 0.0875, 0.0135,
        0.2406, 0.1037, 0.0179, 0.2687, 0.0115], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,099][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0885, 0.1045, 0.0580, 0.0776, 0.0871, 0.0557, 0.0711, 0.0895, 0.0582,
        0.0760, 0.0712, 0.0491, 0.0711, 0.0425], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,100][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0006, 0.0888, 0.1019, 0.0482, 0.1187, 0.2276, 0.0735, 0.0194, 0.0273,
        0.1123, 0.0611, 0.0495, 0.0178, 0.0534], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,100][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0160, 0.0147, 0.0335, 0.0196, 0.0342, 0.0745, 0.0816, 0.0463, 0.1297,
        0.0425, 0.0949, 0.1687, 0.0618, 0.1820], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,102][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0020, 0.0050, 0.0123, 0.0107, 0.0153, 0.0304, 0.0287, 0.0415, 0.0672,
        0.0676, 0.0940, 0.1179, 0.1635, 0.3439], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,104][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0066, 0.0219, 0.0176, 0.0312, 0.0269, 0.0345, 0.0379, 0.0870, 0.0789,
        0.1560, 0.0786, 0.1095, 0.0932, 0.2202], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,106][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:36,108][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[32516],
        [12604],
        [ 1844],
        [ 4718],
        [ 3883],
        [ 1353],
        [ 2327],
        [16071],
        [12167],
        [34609],
        [12598],
        [ 8866],
        [16510],
        [14782]], device='cuda:0')
[2024-07-24 10:30:36,111][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[34300],
        [28978],
        [ 5857],
        [12248],
        [15033],
        [ 7102],
        [ 8817],
        [21257],
        [11362],
        [30301],
        [14123],
        [ 7960],
        [10768],
        [ 7805]], device='cuda:0')
[2024-07-24 10:30:36,113][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[13576],
        [15841],
        [16375],
        [20839],
        [20436],
        [20316],
        [20193],
        [20933],
        [21207],
        [20968],
        [20900],
        [20779],
        [21083],
        [21091]], device='cuda:0')
[2024-07-24 10:30:36,114][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[13446],
        [ 4254],
        [ 4256],
        [  307],
        [  767],
        [  149],
        [  711],
        [18224],
        [ 4338],
        [ 1636],
        [ 5118],
        [ 3078],
        [ 2945],
        [ 3972]], device='cuda:0')
[2024-07-24 10:30:36,115][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[11984],
        [10507],
        [17244],
        [37542],
        [39796],
        [34606],
        [20445],
        [37917],
        [41122],
        [ 8661],
        [ 8500],
        [10848],
        [20482],
        [29674]], device='cuda:0')
[2024-07-24 10:30:36,116][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[49020],
        [47888],
        [47931],
        [48007],
        [47813],
        [46352],
        [49043],
        [47901],
        [47973],
        [47568],
        [47033],
        [47815],
        [48763],
        [47229]], device='cuda:0')
[2024-07-24 10:30:36,117][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[15662],
        [31295],
        [31617],
        [37753],
        [39390],
        [35014],
        [24472],
        [31210],
        [28361],
        [32268],
        [28360],
        [18964],
        [37968],
        [38897]], device='cuda:0')
[2024-07-24 10:30:36,119][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[34983],
        [31892],
        [41423],
        [39183],
        [39928],
        [44270],
        [43122],
        [45374],
        [42304],
        [39044],
        [36339],
        [38838],
        [38360],
        [44553]], device='cuda:0')
[2024-07-24 10:30:36,121][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[23874],
        [20749],
        [24011],
        [28530],
        [27580],
        [27325],
        [27340],
        [27723],
        [29385],
        [31001],
        [29495],
        [29191],
        [27852],
        [27603]], device='cuda:0')
[2024-07-24 10:30:36,122][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[34471],
        [41444],
        [41595],
        [43559],
        [41915],
        [42160],
        [41527],
        [43140],
        [43030],
        [43350],
        [42749],
        [43020],
        [42788],
        [42664]], device='cuda:0')
[2024-07-24 10:30:36,124][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[25036],
        [ 9328],
        [10963],
        [ 5044],
        [ 3230],
        [ 6714],
        [ 6064],
        [ 7189],
        [ 7976],
        [ 9379],
        [12545],
        [21329],
        [18807],
        [20282]], device='cuda:0')
[2024-07-24 10:30:36,127][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[10810],
        [12248],
        [ 5790],
        [ 9758],
        [16898],
        [ 7871],
        [ 7900],
        [ 5494],
        [ 3977],
        [ 6031],
        [ 7197],
        [ 4449],
        [11819],
        [11658]], device='cuda:0')
[2024-07-24 10:30:36,129][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[ 6442],
        [22842],
        [21643],
        [22796],
        [25565],
        [22107],
        [20804],
        [21838],
        [26672],
        [26936],
        [30393],
        [29154],
        [26304],
        [22518]], device='cuda:0')
[2024-07-24 10:30:36,130][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[28476],
        [29855],
        [27953],
        [28153],
        [28608],
        [28247],
        [28084],
        [28269],
        [28315],
        [28336],
        [28330],
        [28248],
        [28300],
        [28137]], device='cuda:0')
[2024-07-24 10:30:36,131][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[32895],
        [15852],
        [13979],
        [11334],
        [15128],
        [16782],
        [27583],
        [ 5327],
        [36704],
        [41912],
        [35640],
        [28682],
        [33937],
        [35018]], device='cuda:0')
[2024-07-24 10:30:36,132][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[40614],
        [40461],
        [39639],
        [39011],
        [38865],
        [39194],
        [39963],
        [39995],
        [40247],
        [40295],
        [40167],
        [40331],
        [39936],
        [39943]], device='cuda:0')
[2024-07-24 10:30:36,133][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[28167],
        [29267],
        [27728],
        [27912],
        [27836],
        [24701],
        [26103],
        [27998],
        [28016],
        [28246],
        [28508],
        [28025],
        [29335],
        [28579]], device='cuda:0')
[2024-07-24 10:30:36,134][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[ 9112],
        [21324],
        [14522],
        [29038],
        [30664],
        [37134],
        [37568],
        [16320],
        [23504],
        [26410],
        [26272],
        [29142],
        [33987],
        [37088]], device='cuda:0')
[2024-07-24 10:30:36,136][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[23644],
        [23647],
        [23713],
        [23935],
        [24295],
        [23966],
        [23867],
        [24240],
        [24411],
        [23824],
        [25516],
        [24731],
        [24777],
        [25983]], device='cuda:0')
[2024-07-24 10:30:36,138][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[19540],
        [24693],
        [23710],
        [35077],
        [22841],
        [13442],
        [21919],
        [28963],
        [23326],
        [22758],
        [21520],
        [22261],
        [18282],
        [24999]], device='cuda:0')
[2024-07-24 10:30:36,139][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[25665],
        [27782],
        [27895],
        [27995],
        [27449],
        [28962],
        [30758],
        [29952],
        [30526],
        [29937],
        [29190],
        [28713],
        [33241],
        [27341]], device='cuda:0')
[2024-07-24 10:30:36,141][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[18885],
        [23343],
        [20913],
        [18764],
        [17580],
        [17024],
        [17373],
        [18553],
        [18364],
        [18019],
        [17657],
        [17786],
        [19083],
        [19075]], device='cuda:0')
[2024-07-24 10:30:36,144][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[6370],
        [2392],
        [2573],
        [3230],
        [2829],
        [2726],
        [2680],
        [2251],
        [2341],
        [2432],
        [2482],
        [2256],
        [2435],
        [2426]], device='cuda:0')
[2024-07-24 10:30:36,146][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[38684],
        [40538],
        [39757],
        [37850],
        [36822],
        [30602],
        [34134],
        [33121],
        [37019],
        [35293],
        [35492],
        [36953],
        [33667],
        [36193]], device='cuda:0')
[2024-07-24 10:30:36,147][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[16957],
        [17476],
        [14942],
        [15177],
        [15983],
        [13202],
        [13650],
        [14162],
        [14231],
        [14233],
        [14431],
        [14025],
        [13868],
        [12934]], device='cuda:0')
[2024-07-24 10:30:36,148][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[11239],
        [ 5086],
        [ 4372],
        [ 3176],
        [ 5252],
        [ 7778],
        [ 7770],
        [ 8248],
        [ 7716],
        [ 5034],
        [ 5583],
        [ 4511],
        [ 4607],
        [ 7219]], device='cuda:0')
[2024-07-24 10:30:36,149][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[17166],
        [23543],
        [29018],
        [28687],
        [27119],
        [28854],
        [30455],
        [25540],
        [27588],
        [26982],
        [28142],
        [27607],
        [26286],
        [25636]], device='cuda:0')
[2024-07-24 10:30:36,151][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[39379],
        [36038],
        [38289],
        [34802],
        [36791],
        [39339],
        [37008],
        [38403],
        [37192],
        [38782],
        [38371],
        [38281],
        [37727],
        [36132]], device='cuda:0')
[2024-07-24 10:30:36,152][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[29884],
        [22659],
        [33019],
        [31797],
        [32467],
        [32713],
        [27131],
        [36684],
        [28604],
        [25684],
        [24724],
        [30502],
        [26875],
        [29914]], device='cuda:0')
[2024-07-24 10:30:36,154][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214],
        [45214]], device='cuda:0')
[2024-07-24 10:30:36,187][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:36,187][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,188][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,188][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,188][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,189][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,189][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,189][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,190][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,191][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,193][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,196][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,198][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,206][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.5903, 0.4097], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,209][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0913, 0.9087], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,213][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.7837, 0.2163], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,213][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.2052, 0.7948], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,214][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.8207, 0.1793], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,214][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.7814, 0.2186], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,214][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.9419, 0.0581], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,215][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.7914, 0.2086], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,215][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.8220, 0.1780], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,215][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.7868, 0.2132], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,216][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.3362, 0.6638], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,216][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.9415, 0.0585], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,217][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4445, 0.3071, 0.2484], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,220][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0424, 0.5545, 0.4031], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,223][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.6031, 0.1717, 0.2252], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,227][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.1411, 0.3745, 0.4844], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,227][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.4988, 0.1874, 0.3139], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,228][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1012, 0.8453, 0.0535], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,228][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9149, 0.0098, 0.0753], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,228][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.6664, 0.0056, 0.3280], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,229][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.7317, 0.0066, 0.2617], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,229][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6856, 0.0188, 0.2956], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,229][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.3335, 0.1497, 0.5168], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,231][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.9084, 0.0055, 0.0861], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,234][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.3419, 0.2334, 0.1937, 0.2310], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,238][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.0223, 0.2028, 0.2124, 0.5625], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,240][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.5121, 0.1297, 0.1798, 0.1784], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,240][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.0939, 0.3063, 0.3678, 0.2320], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,241][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.4804, 0.1596, 0.2528, 0.1072], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,241][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.0934, 0.6082, 0.1667, 0.1317], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,241][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.6086, 0.0082, 0.1630, 0.2202], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,242][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.1163, 0.0036, 0.3672, 0.5129], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,242][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.3377, 0.0051, 0.3448, 0.3124], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,243][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.3333, 0.0184, 0.3431, 0.3053], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,243][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.1398, 0.2071, 0.2896, 0.3635], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,245][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.6901, 0.0099, 0.1379, 0.1622], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,247][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.2897, 0.1912, 0.1535, 0.1883, 0.1772], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,251][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0121, 0.1536, 0.1410, 0.3799, 0.3135], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,254][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.3801, 0.1168, 0.1734, 0.1721, 0.1576], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,254][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0844, 0.2333, 0.2806, 0.1824, 0.2193], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,254][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.3392, 0.1476, 0.1696, 0.1671, 0.1765], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,255][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.0668, 0.2113, 0.2640, 0.2983, 0.1596], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,255][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.4158, 0.0063, 0.1360, 0.3292, 0.1127], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,256][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ went] are: tensor([1.1980e-01, 5.2489e-04, 1.2672e-01, 6.0861e-01, 1.4434e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,256][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.3719, 0.0024, 0.3478, 0.2070, 0.0709], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,256][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.5874, 0.0044, 0.2103, 0.1014, 0.0965], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,258][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.1757, 0.2344, 0.2112, 0.3236, 0.0552], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,261][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.8392, 0.0015, 0.0853, 0.0451, 0.0289], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,265][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.2410, 0.1620, 0.1333, 0.1604, 0.1515, 0.1517], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,267][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0103, 0.0827, 0.0837, 0.2612, 0.2293, 0.3329], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,267][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.3180, 0.0965, 0.1405, 0.1374, 0.1282, 0.1794], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,268][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0782, 0.1868, 0.2296, 0.1495, 0.1840, 0.1720], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,268][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.2795, 0.1059, 0.1773, 0.1138, 0.1642, 0.1593], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,268][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0613, 0.4513, 0.0782, 0.2562, 0.0843, 0.0686], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,269][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.2511, 0.0069, 0.1204, 0.3902, 0.1553, 0.0761], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,269][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([2.8898e-02, 4.2835e-04, 9.8280e-02, 4.3154e-01, 2.5325e-01, 1.8760e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,270][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2090, 0.0021, 0.2967, 0.1694, 0.1265, 0.1963], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,270][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.2155, 0.0067, 0.1678, 0.1618, 0.3834, 0.0649], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,272][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1708, 0.0668, 0.2478, 0.0905, 0.0224, 0.4017], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,274][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.4663, 0.0030, 0.1455, 0.1180, 0.1625, 0.1048], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,278][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1974, 0.1350, 0.1126, 0.1352, 0.1290, 0.1273, 0.1634],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,280][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0083, 0.0672, 0.0609, 0.1949, 0.1722, 0.2569, 0.2396],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,281][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2732, 0.0750, 0.1059, 0.1080, 0.1015, 0.1384, 0.1981],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,281][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0679, 0.1623, 0.1905, 0.1284, 0.1564, 0.1477, 0.1469],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,282][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2424, 0.0939, 0.1556, 0.1190, 0.1200, 0.1327, 0.1363],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,282][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0265, 0.6065, 0.0330, 0.2522, 0.0327, 0.0476, 0.0014],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,282][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([8.6701e-01, 2.4043e-04, 6.5277e-03, 9.9540e-03, 4.4431e-03, 3.1170e-03,
        1.0871e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,283][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([2.8845e-01, 8.0642e-06, 2.1160e-03, 4.4919e-03, 2.0238e-03, 1.8189e-03,
        7.0109e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,284][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([7.2224e-01, 7.0848e-05, 8.4956e-03, 4.3558e-03, 2.6074e-03, 4.6625e-03,
        2.5757e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,286][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.7357, 0.0010, 0.0394, 0.0223, 0.0211, 0.0106, 0.1700],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,289][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0773, 0.1765, 0.1723, 0.1732, 0.0538, 0.0583, 0.2885],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,292][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.0966e-01, 1.6311e-04, 7.1857e-03, 7.4645e-03, 3.5827e-03, 4.0570e-03,
        6.7885e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,293][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.1884, 0.1206, 0.0983, 0.1203, 0.1124, 0.1141, 0.1505, 0.0953],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,294][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0048, 0.0512, 0.0542, 0.1267, 0.1260, 0.2109, 0.1871, 0.2392],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,294][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.2098, 0.0671, 0.0975, 0.0992, 0.0961, 0.1245, 0.1789, 0.1269],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,295][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.0550, 0.1427, 0.1768, 0.1122, 0.1375, 0.1306, 0.1313, 0.1139],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,295][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.2322, 0.0843, 0.1419, 0.1041, 0.1148, 0.1235, 0.1222, 0.0769],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,295][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.0146, 0.0785, 0.0514, 0.0494, 0.0650, 0.1329, 0.0293, 0.5790],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,296][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ office] are: tensor([6.4435e-01, 1.4397e-04, 3.3325e-03, 4.2689e-03, 2.5859e-03, 1.8980e-03,
        9.9468e-02, 2.4395e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,296][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ office] are: tensor([2.0473e-01, 2.9087e-06, 1.0075e-03, 1.0115e-03, 6.2562e-04, 6.5879e-04,
        3.6854e-01, 4.2341e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,297][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ office] are: tensor([5.4435e-01, 2.8658e-05, 5.1005e-03, 2.2899e-03, 1.2344e-03, 2.5080e-03,
        1.9564e-01, 2.4885e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,299][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.5137, 0.0006, 0.0213, 0.0148, 0.0124, 0.0058, 0.1539, 0.2775],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,302][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.1017, 0.1531, 0.1838, 0.0717, 0.0471, 0.1045, 0.1110, 0.2270],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,305][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ office] are: tensor([8.8027e-01, 1.0747e-04, 4.2686e-03, 5.0597e-03, 1.6510e-03, 2.3878e-03,
        4.8337e-02, 5.7918e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,307][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.1632, 0.1073, 0.0886, 0.1087, 0.1022, 0.1001, 0.1309, 0.0861, 0.1128],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,308][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0046, 0.0424, 0.0356, 0.1113, 0.0936, 0.1509, 0.1424, 0.2198, 0.1994],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,308][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1844, 0.0564, 0.0851, 0.0884, 0.0835, 0.1065, 0.1489, 0.1188, 0.1280],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,308][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0567, 0.1117, 0.1383, 0.0949, 0.1153, 0.1123, 0.1124, 0.1050, 0.1534],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,309][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1876, 0.0740, 0.1231, 0.0906, 0.1024, 0.1075, 0.1059, 0.0835, 0.1254],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,309][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0165, 0.3356, 0.0089, 0.1375, 0.0996, 0.0065, 0.0076, 0.3766, 0.0112],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,310][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [,] are: tensor([2.5680e-01, 3.1176e-04, 4.8117e-03, 1.2065e-02, 3.5339e-03, 3.9603e-03,
        1.6045e-01, 4.1916e-01, 1.3891e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,311][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [,] are: tensor([1.1127e-02, 1.4606e-06, 2.0440e-04, 8.7957e-04, 3.7481e-04, 4.2201e-04,
        1.3636e-01, 7.4790e-01, 1.0273e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,312][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [,] are: tensor([8.8005e-02, 2.2024e-05, 2.4424e-03, 1.6915e-03, 1.5127e-03, 2.8389e-03,
        1.8347e-01, 5.5439e-01, 1.6563e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,315][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.3376, 0.0011, 0.0225, 0.0326, 0.0123, 0.0096, 0.1435, 0.2344, 0.2064],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,318][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0661, 0.0615, 0.3308, 0.0493, 0.0068, 0.0582, 0.0885, 0.0283, 0.3105],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,320][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [,] are: tensor([3.7356e-01, 2.0019e-04, 9.5952e-03, 7.6973e-03, 5.5424e-03, 6.6646e-03,
        1.2552e-01, 2.8114e-01, 1.9008e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,321][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.1499, 0.1032, 0.0813, 0.0982, 0.0909, 0.0911, 0.1191, 0.0766, 0.1010,
        0.0885], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,321][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0038, 0.0288, 0.0380, 0.0910, 0.0773, 0.1257, 0.1302, 0.1825, 0.1819,
        0.1407], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,322][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.1656, 0.0486, 0.0663, 0.0735, 0.0698, 0.0864, 0.1276, 0.1009, 0.1074,
        0.1538], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,322][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.0459, 0.1107, 0.1304, 0.0903, 0.1048, 0.0989, 0.1002, 0.0891, 0.1364,
        0.0932], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,322][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.2127, 0.0493, 0.1215, 0.0787, 0.0874, 0.1137, 0.0915, 0.0780, 0.1164,
        0.0508], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,323][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0746, 0.0131, 0.0467, 0.0403, 0.0350, 0.1932, 0.0250, 0.2710, 0.2818,
        0.0193], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,323][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([5.0382e-01, 5.2087e-06, 2.3537e-04, 2.7630e-04, 7.2927e-05, 8.6681e-05,
        2.6817e-03, 1.2866e-02, 5.1760e-03, 4.7478e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,324][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([9.4255e-02, 4.0366e-08, 2.2859e-05, 3.2719e-05, 8.3689e-06, 9.7921e-06,
        3.3875e-03, 1.9283e-02, 4.3964e-03, 8.7860e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,325][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([5.5764e-01, 7.4568e-07, 1.7613e-04, 1.1800e-04, 3.8092e-05, 6.1016e-05,
        3.5992e-03, 1.1373e-02, 6.7886e-03, 4.2021e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,327][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([4.6090e-01, 5.9001e-05, 2.1903e-03, 1.5771e-03, 4.5117e-04, 4.6312e-04,
        8.4991e-03, 1.6562e-02, 1.4215e-02, 4.9509e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,331][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0451, 0.1106, 0.1504, 0.2167, 0.0154, 0.0458, 0.1543, 0.0630, 0.1031,
        0.0956], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,334][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([7.2674e-01, 1.2036e-05, 6.5724e-04, 8.1561e-04, 2.4929e-04, 2.9156e-04,
        6.4876e-03, 1.5163e-02, 1.2722e-02, 2.3686e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,334][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.1398, 0.0936, 0.0748, 0.0908, 0.0844, 0.0843, 0.1115, 0.0706, 0.0945,
        0.0799, 0.0758], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,335][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0028, 0.0322, 0.0293, 0.0797, 0.0667, 0.1139, 0.1049, 0.1574, 0.1514,
        0.1539, 0.1077], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,335][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1383, 0.0496, 0.0648, 0.0692, 0.0620, 0.0813, 0.1234, 0.0834, 0.0965,
        0.1336, 0.0979], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,335][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0374, 0.0964, 0.1228, 0.0769, 0.0964, 0.0929, 0.0922, 0.0829, 0.1289,
        0.0837, 0.0894], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,336][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.1715, 0.0720, 0.1022, 0.0722, 0.0901, 0.0954, 0.0924, 0.0774, 0.0989,
        0.0707, 0.0573], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,336][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0263, 0.1309, 0.0457, 0.0649, 0.0619, 0.0341, 0.0174, 0.3917, 0.0649,
        0.1347, 0.0276], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,337][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([1.0549e-02, 6.3354e-06, 1.6374e-04, 3.0752e-04, 1.1801e-04, 6.6458e-05,
        3.6858e-03, 1.0731e-02, 4.6733e-03, 9.6581e-01, 3.8843e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,339][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([4.4495e-04, 1.3770e-08, 5.1505e-06, 1.2786e-05, 4.3409e-06, 3.1610e-06,
        1.1769e-03, 7.2249e-03, 1.7668e-03, 9.8692e-01, 2.4375e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,340][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([1.0831e-02, 1.1833e-06, 2.1289e-04, 9.9853e-05, 5.8998e-05, 1.0048e-04,
        8.7727e-03, 3.0862e-02, 1.0992e-02, 9.2901e-01, 9.0552e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,342][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([1.5231e-01, 9.7424e-05, 6.8597e-03, 1.5820e-03, 1.6850e-03, 1.2666e-03,
        2.6248e-02, 6.8935e-02, 6.3620e-02, 5.6112e-01, 1.1627e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,347][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0764, 0.0735, 0.1377, 0.0427, 0.0436, 0.1192, 0.1008, 0.0581, 0.2195,
        0.0766, 0.0519], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,347][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([2.3782e-01, 2.9157e-05, 2.6637e-03, 8.7886e-04, 6.2445e-04, 8.0585e-04,
        1.6804e-02, 4.7470e-02, 3.6841e-02, 6.1519e-01, 4.0866e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,348][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1201, 0.0825, 0.0680, 0.0829, 0.0782, 0.0761, 0.0980, 0.0662, 0.0854,
        0.0738, 0.0701, 0.0986], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,348][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0021, 0.0275, 0.0222, 0.0698, 0.0592, 0.1025, 0.0917, 0.1327, 0.1386,
        0.1497, 0.1057, 0.0982], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,349][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1173, 0.0389, 0.0566, 0.0583, 0.0565, 0.0712, 0.0991, 0.0790, 0.0847,
        0.1129, 0.0900, 0.1355], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,349][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0347, 0.0921, 0.1129, 0.0713, 0.0909, 0.0860, 0.0838, 0.0815, 0.1203,
        0.0810, 0.0833, 0.0622], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,349][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1456, 0.0653, 0.0906, 0.0729, 0.0827, 0.0802, 0.0835, 0.0719, 0.0937,
        0.0699, 0.0637, 0.0800], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,350][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0121, 0.0925, 0.0206, 0.0930, 0.0280, 0.0198, 0.0088, 0.3167, 0.1700,
        0.2230, 0.0103, 0.0050], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,351][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([2.5462e-01, 6.2585e-06, 1.6492e-04, 1.9506e-04, 7.2647e-05, 5.2227e-05,
        1.6629e-03, 7.6574e-03, 3.1416e-03, 5.4698e-01, 4.0218e-03, 1.8142e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,352][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([1.3696e-02, 1.3836e-08, 3.1119e-06, 5.7687e-06, 2.7369e-06, 1.7714e-06,
        8.8417e-04, 6.0715e-03, 9.1048e-04, 7.6444e-01, 5.2795e-03, 2.0870e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,354][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([8.2445e-02, 8.0441e-07, 1.0619e-04, 4.4706e-05, 4.1843e-05, 4.7945e-05,
        2.7013e-03, 1.2358e-02, 4.1033e-03, 5.7476e-01, 1.1943e-02, 3.1144e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,356][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.7218e-01, 3.9053e-05, 2.2971e-03, 8.7245e-04, 1.0186e-03, 5.3405e-04,
        8.4883e-03, 2.1548e-02, 1.5357e-02, 4.0479e-01, 1.8903e-01, 1.8385e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,360][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0587, 0.1062, 0.1282, 0.0943, 0.0238, 0.0520, 0.1239, 0.0404, 0.1060,
        0.1068, 0.0375, 0.1222], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,361][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([6.4220e-01, 7.9335e-06, 7.3007e-04, 2.9617e-04, 2.2558e-04, 2.1698e-04,
        5.3507e-03, 1.0060e-02, 1.1084e-02, 1.3995e-01, 2.0822e-02, 1.6906e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,361][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.1114, 0.0760, 0.0629, 0.0740, 0.0690, 0.0708, 0.0902, 0.0587, 0.0763,
        0.0654, 0.0623, 0.0898, 0.0933], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,362][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0020, 0.0274, 0.0260, 0.0639, 0.0580, 0.0953, 0.0847, 0.1232, 0.1271,
        0.1197, 0.0849, 0.0935, 0.0943], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,362][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0998, 0.0365, 0.0523, 0.0547, 0.0536, 0.0657, 0.0923, 0.0738, 0.0766,
        0.0985, 0.0820, 0.1203, 0.0939], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,363][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.0296, 0.0891, 0.1130, 0.0665, 0.0864, 0.0804, 0.0805, 0.0700, 0.1138,
        0.0706, 0.0794, 0.0608, 0.0600], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,363][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.1589, 0.0636, 0.0866, 0.0634, 0.0849, 0.0827, 0.0698, 0.0649, 0.0863,
        0.0649, 0.0590, 0.0656, 0.0495], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,365][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.0101, 0.0405, 0.0513, 0.0608, 0.0108, 0.0855, 0.0121, 0.3291, 0.1963,
        0.1242, 0.0140, 0.0084, 0.0569], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,367][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([2.2580e-01, 2.1639e-06, 7.7408e-05, 1.0640e-04, 3.2085e-05, 2.7663e-05,
        1.0078e-03, 5.6413e-03, 2.2151e-03, 3.1522e-01, 2.2287e-03, 1.0775e-01,
        3.3989e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,369][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([4.4133e-02, 5.3394e-09, 2.1259e-06, 2.7172e-06, 6.3347e-07, 7.0034e-07,
        4.9173e-04, 1.6075e-03, 4.6968e-04, 2.6012e-01, 1.2477e-03, 1.0910e-01,
        5.8282e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,371][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([1.4833e-01, 2.9818e-07, 8.4258e-05, 1.9081e-05, 1.6289e-05, 2.0508e-05,
        1.2306e-03, 7.1884e-03, 2.2894e-03, 3.6284e-01, 4.0594e-03, 1.0485e-01,
        3.6907e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,374][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([3.1610e-01, 1.6769e-05, 1.2011e-03, 5.1943e-04, 3.8021e-04, 2.3560e-04,
        4.9563e-03, 9.4497e-03, 8.0999e-03, 1.9054e-01, 4.1713e-02, 9.2554e-02,
        3.3423e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,374][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0763, 0.0681, 0.1318, 0.0867, 0.0222, 0.0931, 0.0521, 0.0422, 0.1706,
        0.0617, 0.0899, 0.0588, 0.0464], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,375][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([5.0544e-01, 3.7166e-06, 3.6455e-04, 1.8203e-04, 8.7592e-05, 9.0601e-05,
        3.6784e-03, 6.3923e-03, 6.2943e-03, 1.0032e-01, 9.4609e-03, 1.5875e-01,
        2.0894e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,375][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0968, 0.0680, 0.0572, 0.0678, 0.0635, 0.0636, 0.0805, 0.0542, 0.0703,
        0.0601, 0.0576, 0.0799, 0.0848, 0.0955], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,375][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0028, 0.0181, 0.0169, 0.0539, 0.0455, 0.0698, 0.0720, 0.1007, 0.1012,
        0.1105, 0.0756, 0.0727, 0.0942, 0.1660], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,376][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1042, 0.0308, 0.0438, 0.0452, 0.0429, 0.0573, 0.0857, 0.0602, 0.0664,
        0.0909, 0.0692, 0.1120, 0.0868, 0.1046], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,376][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0353, 0.0813, 0.0980, 0.0635, 0.0800, 0.0745, 0.0742, 0.0700, 0.1035,
        0.0713, 0.0724, 0.0556, 0.0589, 0.0616], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,377][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.1352, 0.0505, 0.0818, 0.0540, 0.0752, 0.0758, 0.0698, 0.0577, 0.0812,
        0.0530, 0.0581, 0.0659, 0.0648, 0.0770], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,378][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0101, 0.0761, 0.0136, 0.0523, 0.0147, 0.0120, 0.0085, 0.1972, 0.1270,
        0.2813, 0.0120, 0.0083, 0.1655, 0.0215], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,380][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([6.7560e-02, 1.4763e-06, 4.8097e-05, 9.9338e-05, 1.7518e-05, 8.7901e-06,
        9.7915e-04, 3.5923e-03, 1.3851e-03, 4.2672e-01, 1.0883e-03, 1.3509e-01,
        2.7755e-01, 8.5859e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,383][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([2.0397e-03, 5.9942e-10, 3.1217e-07, 6.6032e-07, 1.5661e-07, 1.2082e-07,
        1.4655e-04, 8.2292e-04, 1.3657e-04, 1.4071e-01, 6.3451e-04, 6.6692e-02,
        6.5143e-01, 1.3739e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,385][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([2.6684e-02, 5.9766e-08, 1.7567e-05, 5.7452e-06, 2.9289e-06, 3.6519e-06,
        6.3623e-04, 2.0585e-03, 9.1465e-04, 1.3886e-01, 1.5642e-03, 6.6490e-02,
        5.2152e-01, 2.4124e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,387][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([1.2584e-01, 1.1696e-05, 6.7843e-04, 4.2126e-04, 5.9407e-04, 8.5685e-05,
        4.9338e-03, 8.3701e-03, 1.0090e-02, 2.1895e-01, 7.9279e-02, 1.1618e-01,
        3.4131e-01, 9.3260e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,388][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0735, 0.0264, 0.0961, 0.0457, 0.0095, 0.1787, 0.0509, 0.0171, 0.1585,
        0.0276, 0.0462, 0.0316, 0.0312, 0.2069], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,388][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.6712e-01, 1.3865e-06, 2.2422e-04, 8.7665e-05, 8.9330e-05, 5.2032e-05,
        2.7195e-03, 5.3362e-03, 5.7402e-03, 7.2401e-02, 9.8990e-03, 1.6899e-01,
        4.4539e-01, 1.2195e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,423][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:36,426][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,426][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,426][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,427][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,427][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,427][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,428][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,428][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,428][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,429][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,430][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,432][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,435][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.0664, 0.9336], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,447][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.8996, 0.1004], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,450][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.9260, 0.0740], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,454][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.5534, 0.4466], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,454][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.9909, 0.0091], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,454][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.9737, 0.0263], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,455][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.9419, 0.0581], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,455][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.7914, 0.2086], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,455][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.8220, 0.1780], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,456][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.7868, 0.2132], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,456][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.9746, 0.0254], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,456][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.9415, 0.0585], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,458][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0756, 0.0039, 0.9205], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,460][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.7843, 0.0125, 0.2032], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,463][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.7243, 0.0107, 0.2651], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,467][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2030, 0.0072, 0.7898], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,468][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.9214, 0.0150, 0.0636], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,468][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.8670, 0.0309, 0.1021], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,468][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.9149, 0.0098, 0.0753], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,469][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.6664, 0.0056, 0.3280], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,469][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7317, 0.0066, 0.2617], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,469][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6856, 0.0188, 0.2956], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,470][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9348, 0.0108, 0.0544], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,470][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.9084, 0.0055, 0.0861], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,470][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.1131, 0.0019, 0.2042, 0.6808], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,472][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.4643, 0.0115, 0.2692, 0.2549], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,475][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.3775, 0.0034, 0.2018, 0.4174], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,479][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.0361, 0.0016, 0.3783, 0.5840], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,481][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.8083, 0.0569, 0.1147, 0.0201], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,481][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.6646, 0.0648, 0.1634, 0.1072], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,482][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.6086, 0.0082, 0.1630, 0.2202], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,482][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.1163, 0.0036, 0.3672, 0.5129], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,482][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.3377, 0.0051, 0.3448, 0.3124], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,483][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.3333, 0.0184, 0.3431, 0.3053], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,483][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.8140, 0.0084, 0.0700, 0.1076], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,483][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.6901, 0.0099, 0.1379, 0.1622], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,484][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.1225, 0.0052, 0.4876, 0.0511, 0.3336], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,486][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.4797, 0.0047, 0.2620, 0.1547, 0.0989], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,488][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.2871, 0.0030, 0.1862, 0.4630, 0.0608], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,492][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.0509, 0.0005, 0.2050, 0.3209, 0.4228], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,495][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.6708, 0.0307, 0.1586, 0.0624, 0.0775], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,495][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.6692, 0.0196, 0.2010, 0.0383, 0.0719], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,495][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.4158, 0.0063, 0.1360, 0.3292, 0.1127], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,496][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([1.1980e-01, 5.2489e-04, 1.2672e-01, 6.0861e-01, 1.4434e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,496][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.3719, 0.0024, 0.3478, 0.2070, 0.0709], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,496][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.5874, 0.0044, 0.2103, 0.1014, 0.0965], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,497][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.7566, 0.0072, 0.0695, 0.0725, 0.0942], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,497][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.8392, 0.0015, 0.0853, 0.0451, 0.0289], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,497][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0431, 0.0009, 0.1368, 0.0126, 0.0113, 0.7954], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,499][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.3023, 0.0047, 0.2053, 0.1768, 0.1593, 0.1516], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,502][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0421, 0.0019, 0.1152, 0.5092, 0.2164, 0.1151], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,504][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.1954e-02, 2.6005e-04, 8.7466e-02, 1.1203e-01, 6.7615e-01, 1.1214e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,508][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.6201, 0.0309, 0.1449, 0.0378, 0.1103, 0.0560], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,508][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.4904, 0.0252, 0.1539, 0.0416, 0.1332, 0.1557], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,509][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.2511, 0.0069, 0.1204, 0.3902, 0.1553, 0.0761], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,509][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.8898e-02, 4.2835e-04, 9.8280e-02, 4.3154e-01, 2.5325e-01, 1.8760e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,510][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.2090, 0.0021, 0.2967, 0.1694, 0.1265, 0.1963], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,510][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.2155, 0.0067, 0.1678, 0.1618, 0.3834, 0.0649], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,510][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.6988, 0.0059, 0.0816, 0.0587, 0.0867, 0.0682], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,511][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.4663, 0.0030, 0.1455, 0.1180, 0.1625, 0.1048], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,511][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.2363e-02, 7.3742e-04, 9.4174e-02, 7.4514e-03, 5.0580e-03, 9.2557e-02,
        7.8766e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,512][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([5.1049e-01, 2.9629e-04, 1.7008e-02, 9.4696e-03, 8.0796e-03, 9.8193e-03,
        4.4484e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,514][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([4.8130e-01, 4.9141e-05, 5.9281e-03, 1.0171e-02, 3.1499e-03, 4.5652e-03,
        4.9483e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,515][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([4.9307e-02, 6.0676e-06, 2.9958e-03, 3.4386e-03, 6.0979e-03, 2.5827e-03,
        9.3557e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,519][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.7687, 0.0176, 0.0769, 0.0249, 0.0355, 0.0245, 0.0518],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,522][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.7305, 0.0081, 0.0618, 0.0161, 0.0300, 0.0373, 0.1163],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,522][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([8.6701e-01, 2.4043e-04, 6.5277e-03, 9.9540e-03, 4.4431e-03, 3.1170e-03,
        1.0871e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,522][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([2.8845e-01, 8.0642e-06, 2.1160e-03, 4.4919e-03, 2.0238e-03, 1.8189e-03,
        7.0109e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,523][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([7.2224e-01, 7.0848e-05, 8.4956e-03, 4.3558e-03, 2.6074e-03, 4.6625e-03,
        2.5757e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,523][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.7357, 0.0010, 0.0394, 0.0223, 0.0211, 0.0106, 0.1700],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,524][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.8327, 0.0015, 0.0138, 0.0105, 0.0147, 0.0080, 0.1187],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,524][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([9.0966e-01, 1.6311e-04, 7.1857e-03, 7.4645e-03, 3.5827e-03, 4.0570e-03,
        6.7885e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,524][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.0126, 0.0017, 0.1278, 0.0049, 0.0038, 0.2197, 0.5674, 0.0622],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,525][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([2.8960e-01, 2.1163e-04, 1.8069e-02, 4.4868e-03, 4.9741e-03, 8.7493e-03,
        3.9804e-01, 2.7587e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,526][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([2.2720e-01, 1.3931e-05, 1.8103e-03, 2.4138e-03, 7.9302e-04, 1.3788e-03,
        4.3313e-01, 3.3326e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,527][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([1.1299e-02, 4.6862e-07, 3.2267e-04, 2.3267e-04, 5.1408e-04, 2.7988e-04,
        1.6111e-01, 8.2624e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,530][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.8034, 0.0131, 0.0585, 0.0218, 0.0267, 0.0165, 0.0307, 0.0293],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,534][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.7291, 0.0077, 0.0584, 0.0170, 0.0185, 0.0269, 0.0812, 0.0612],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,535][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([6.4435e-01, 1.4397e-04, 3.3325e-03, 4.2689e-03, 2.5859e-03, 1.8980e-03,
        9.9468e-02, 2.4395e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,536][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([2.0473e-01, 2.9087e-06, 1.0075e-03, 1.0115e-03, 6.2562e-04, 6.5879e-04,
        3.6854e-01, 4.2341e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,536][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([5.4435e-01, 2.8658e-05, 5.1005e-03, 2.2899e-03, 1.2344e-03, 2.5080e-03,
        1.9564e-01, 2.4885e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,537][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.5137, 0.0006, 0.0213, 0.0148, 0.0124, 0.0058, 0.1539, 0.2775],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,537][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([8.2335e-01, 6.8490e-04, 9.1431e-03, 4.9142e-03, 8.6326e-03, 3.6021e-03,
        6.7491e-02, 8.2178e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,537][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([8.8027e-01, 1.0747e-04, 4.2686e-03, 5.0597e-03, 1.6510e-03, 2.3878e-03,
        4.8337e-02, 5.7918e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,538][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0194, 0.0008, 0.1271, 0.0043, 0.0070, 0.1475, 0.2534, 0.0047, 0.4358],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,538][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([9.5389e-02, 2.5201e-04, 9.8683e-03, 5.7061e-03, 4.1742e-03, 7.3832e-03,
        2.8752e-01, 3.2657e-01, 2.6313e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,539][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([4.0418e-02, 1.4876e-05, 1.3490e-03, 4.2299e-03, 1.1442e-03, 1.6820e-03,
        3.4558e-01, 4.8451e-01, 1.2107e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,540][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([1.8614e-03, 6.2614e-07, 1.8779e-04, 2.5142e-04, 3.3277e-04, 2.2970e-04,
        9.8441e-02, 8.0884e-01, 8.9850e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,543][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.7127, 0.0134, 0.0662, 0.0194, 0.0269, 0.0205, 0.0364, 0.0315, 0.0728],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,547][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.5950, 0.0062, 0.0536, 0.0122, 0.0265, 0.0369, 0.1094, 0.0605, 0.0996],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,549][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([2.5680e-01, 3.1176e-04, 4.8117e-03, 1.2065e-02, 3.5339e-03, 3.9603e-03,
        1.6045e-01, 4.1916e-01, 1.3891e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,549][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([1.1127e-02, 1.4606e-06, 2.0440e-04, 8.7957e-04, 3.7481e-04, 4.2201e-04,
        1.3636e-01, 7.4790e-01, 1.0273e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,550][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([8.8005e-02, 2.2024e-05, 2.4424e-03, 1.6915e-03, 1.5127e-03, 2.8389e-03,
        1.8347e-01, 5.5439e-01, 1.6563e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,550][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.3376, 0.0011, 0.0225, 0.0326, 0.0123, 0.0096, 0.1435, 0.2344, 0.2064],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,551][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.5724, 0.0015, 0.0152, 0.0126, 0.0146, 0.0096, 0.1310, 0.1043, 0.1388],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,551][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([3.7356e-01, 2.0019e-04, 9.5952e-03, 7.6973e-03, 5.5424e-03, 6.6646e-03,
        1.2552e-01, 2.8114e-01, 1.9008e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,551][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.0070, 0.0796, 0.0604, 0.0030, 0.0024, 0.0776, 0.1766, 0.0021, 0.5003,
        0.0910], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,552][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([4.2323e-01, 4.2681e-05, 3.6733e-03, 1.4483e-03, 9.7659e-04, 1.2688e-03,
        6.8780e-02, 8.1663e-02, 6.7720e-02, 3.5120e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,553][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([4.3605e-01, 2.3719e-07, 7.9360e-05, 9.8092e-05, 1.5546e-05, 2.4721e-05,
        3.6705e-03, 4.6556e-03, 3.0276e-03, 5.5238e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,554][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([1.6319e-02, 1.4725e-08, 1.0204e-05, 1.2217e-05, 5.1894e-06, 2.6999e-06,
        1.0698e-03, 1.7593e-02, 2.7966e-03, 9.6219e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,557][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.8472, 0.0048, 0.0365, 0.0215, 0.0129, 0.0115, 0.0158, 0.0130, 0.0295,
        0.0073], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,561][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.8471, 0.0025, 0.0190, 0.0075, 0.0096, 0.0086, 0.0262, 0.0212, 0.0273,
        0.0311], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,562][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([5.0382e-01, 5.2087e-06, 2.3537e-04, 2.7630e-04, 7.2927e-05, 8.6681e-05,
        2.6817e-03, 1.2866e-02, 5.1760e-03, 4.7478e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,563][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([9.4255e-02, 4.0366e-08, 2.2859e-05, 3.2719e-05, 8.3689e-06, 9.7921e-06,
        3.3875e-03, 1.9283e-02, 4.3964e-03, 8.7860e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,563][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([5.5764e-01, 7.4568e-07, 1.7613e-04, 1.1800e-04, 3.8092e-05, 6.1016e-05,
        3.5992e-03, 1.1373e-02, 6.7886e-03, 4.2021e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,564][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([4.6090e-01, 5.9001e-05, 2.1903e-03, 1.5771e-03, 4.5117e-04, 4.6312e-04,
        8.4991e-03, 1.6562e-02, 1.4215e-02, 4.9509e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,564][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([7.4735e-01, 3.1561e-04, 3.4279e-03, 3.8136e-03, 1.9575e-03, 1.5205e-03,
        2.0706e-02, 1.5870e-02, 2.6151e-02, 1.7888e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,564][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([7.2674e-01, 1.2036e-05, 6.5724e-04, 8.1561e-04, 2.4929e-04, 2.9156e-04,
        6.4876e-03, 1.5163e-02, 1.2722e-02, 2.3686e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,565][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([1.7757e-02, 2.3474e-04, 1.0085e-01, 8.4448e-03, 7.0931e-03, 2.3475e-01,
        1.2987e-01, 2.6279e-03, 4.3837e-01, 2.3228e-04, 5.9769e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,565][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([4.8334e-02, 4.0417e-05, 4.5225e-03, 1.1593e-03, 1.1158e-03, 1.4367e-03,
        9.6508e-02, 9.8407e-02, 1.1573e-01, 5.5713e-01, 7.5623e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,566][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([1.5992e-03, 2.3991e-07, 1.8296e-05, 3.4042e-05, 4.1399e-06, 8.9207e-06,
        3.0310e-03, 4.1827e-03, 1.3777e-03, 9.8816e-01, 1.5872e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,568][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([2.4392e-04, 9.7433e-09, 4.8624e-06, 3.5069e-06, 5.3928e-06, 2.9326e-06,
        1.1914e-03, 1.3449e-02, 1.9350e-03, 9.7869e-01, 4.4777e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,571][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.6800, 0.0176, 0.0535, 0.0182, 0.0252, 0.0189, 0.0347, 0.0404, 0.0578,
        0.0236, 0.0302], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,574][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.4487, 0.0064, 0.0448, 0.0147, 0.0174, 0.0318, 0.0836, 0.0850, 0.0903,
        0.0955, 0.0820], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,576][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([1.0549e-02, 6.3354e-06, 1.6374e-04, 3.0752e-04, 1.1801e-04, 6.6458e-05,
        3.6858e-03, 1.0731e-02, 4.6733e-03, 9.6581e-01, 3.8843e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,577][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([4.4495e-04, 1.3770e-08, 5.1505e-06, 1.2786e-05, 4.3409e-06, 3.1610e-06,
        1.1769e-03, 7.2249e-03, 1.7668e-03, 9.8692e-01, 2.4375e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,577][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([1.0831e-02, 1.1833e-06, 2.1289e-04, 9.9853e-05, 5.8998e-05, 1.0048e-04,
        8.7727e-03, 3.0862e-02, 1.0992e-02, 9.2901e-01, 9.0552e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,577][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([1.5231e-01, 9.7424e-05, 6.8597e-03, 1.5820e-03, 1.6850e-03, 1.2666e-03,
        2.6248e-02, 6.8935e-02, 6.3620e-02, 5.6112e-01, 1.1627e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,578][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.3454, 0.0007, 0.0085, 0.0051, 0.0095, 0.0044, 0.0516, 0.0444, 0.0550,
        0.4200, 0.0555], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,578][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([2.3782e-01, 2.9157e-05, 2.6637e-03, 8.7886e-04, 6.2445e-04, 8.0585e-04,
        1.6804e-02, 4.7470e-02, 3.6841e-02, 6.1519e-01, 4.0866e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,579][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([7.9415e-03, 2.7797e-04, 3.9249e-02, 9.7769e-03, 3.6132e-03, 5.8698e-02,
        1.5457e-01, 1.9509e-03, 1.6397e-01, 2.6612e-04, 2.6813e-03, 5.5700e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,579][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([1.9172e-01, 1.9766e-05, 2.1202e-03, 4.9433e-04, 4.0790e-04, 6.0972e-04,
        3.3176e-02, 4.2494e-02, 4.9244e-02, 2.2649e-01, 3.4930e-02, 4.1830e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,580][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([4.3408e-02, 1.9161e-07, 2.3259e-05, 3.1287e-05, 8.4191e-06, 8.6559e-06,
        1.9683e-03, 5.0533e-03, 1.0996e-03, 7.9707e-01, 3.4125e-03, 1.4791e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,581][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([4.2379e-03, 6.3623e-09, 3.8357e-06, 2.4330e-06, 6.7313e-06, 2.0790e-06,
        7.7164e-04, 8.3225e-03, 1.0394e-03, 6.7703e-01, 1.5179e-02, 2.9341e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,585][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.6869, 0.0118, 0.0468, 0.0197, 0.0237, 0.0151, 0.0281, 0.0337, 0.0505,
        0.0184, 0.0292, 0.0360], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,588][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.6232, 0.0023, 0.0208, 0.0058, 0.0082, 0.0120, 0.0433, 0.0305, 0.0417,
        0.0354, 0.0418, 0.1351], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,590][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([2.5462e-01, 6.2585e-06, 1.6492e-04, 1.9506e-04, 7.2647e-05, 5.2227e-05,
        1.6629e-03, 7.6574e-03, 3.1416e-03, 5.4698e-01, 4.0218e-03, 1.8142e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,590][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([1.3696e-02, 1.3836e-08, 3.1119e-06, 5.7687e-06, 2.7369e-06, 1.7714e-06,
        8.8417e-04, 6.0715e-03, 9.1048e-04, 7.6444e-01, 5.2795e-03, 2.0870e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,591][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([8.2445e-02, 8.0441e-07, 1.0619e-04, 4.4706e-05, 4.1843e-05, 4.7945e-05,
        2.7013e-03, 1.2358e-02, 4.1033e-03, 5.7476e-01, 1.1943e-02, 3.1144e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,591][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([1.7218e-01, 3.9053e-05, 2.2971e-03, 8.7245e-04, 1.0186e-03, 5.3405e-04,
        8.4883e-03, 2.1548e-02, 1.5357e-02, 4.0479e-01, 1.8903e-01, 1.8385e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,591][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([5.3099e-01, 3.5688e-04, 3.6739e-03, 2.3191e-03, 3.4058e-03, 1.7978e-03,
        2.4486e-02, 2.5813e-02, 2.6088e-02, 1.8481e-01, 2.3682e-02, 1.7259e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,592][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([6.4220e-01, 7.9335e-06, 7.3007e-04, 2.9617e-04, 2.2558e-04, 2.1698e-04,
        5.3507e-03, 1.0060e-02, 1.1084e-02, 1.3995e-01, 2.0822e-02, 1.6906e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,592][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.0368, 0.0009, 0.0645, 0.0078, 0.0035, 0.1417, 0.1393, 0.0020, 0.4211,
        0.0008, 0.0020, 0.1157, 0.0638], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,593][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([1.7582e-01, 1.1939e-05, 1.3332e-03, 3.0134e-04, 2.2404e-04, 3.1609e-04,
        1.8758e-02, 2.5518e-02, 2.5283e-02, 1.7161e-01, 1.5049e-02, 2.5330e-01,
        3.1248e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,594][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([8.3523e-02, 3.7095e-08, 1.4652e-05, 1.0306e-05, 3.0091e-06, 4.6094e-06,
        1.2624e-03, 2.5711e-03, 7.2434e-04, 2.4864e-01, 2.1991e-03, 1.7680e-01,
        4.8425e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,595][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([2.4490e-03, 6.5184e-10, 8.1468e-07, 2.5239e-07, 5.7799e-07, 2.2166e-07,
        1.4169e-04, 1.6979e-03, 1.6219e-04, 6.2039e-02, 1.6162e-03, 5.8157e-02,
        8.7374e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,598][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.8528, 0.0087, 0.0230, 0.0088, 0.0091, 0.0078, 0.0110, 0.0162, 0.0172,
        0.0098, 0.0165, 0.0155, 0.0036], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,602][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.7449, 0.0020, 0.0192, 0.0077, 0.0043, 0.0072, 0.0214, 0.0193, 0.0227,
        0.0340, 0.0214, 0.0733, 0.0227], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,603][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([2.2580e-01, 2.1639e-06, 7.7408e-05, 1.0640e-04, 3.2085e-05, 2.7663e-05,
        1.0078e-03, 5.6413e-03, 2.2151e-03, 3.1522e-01, 2.2287e-03, 1.0775e-01,
        3.3989e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,604][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([4.4133e-02, 5.3394e-09, 2.1259e-06, 2.7172e-06, 6.3347e-07, 7.0034e-07,
        4.9173e-04, 1.6075e-03, 4.6968e-04, 2.6012e-01, 1.2477e-03, 1.0910e-01,
        5.8282e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,604][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([1.4833e-01, 2.9818e-07, 8.4258e-05, 1.9081e-05, 1.6289e-05, 2.0508e-05,
        1.2306e-03, 7.1884e-03, 2.2894e-03, 3.6284e-01, 4.0594e-03, 1.0485e-01,
        3.6907e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,605][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([3.1610e-01, 1.6769e-05, 1.2011e-03, 5.1943e-04, 3.8021e-04, 2.3560e-04,
        4.9563e-03, 9.4497e-03, 8.0999e-03, 1.9054e-01, 4.1713e-02, 9.2554e-02,
        3.3423e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,605][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([6.4598e-01, 1.7175e-04, 2.5454e-03, 1.7170e-03, 1.5422e-03, 1.0899e-03,
        1.3087e-02, 1.2641e-02, 1.6059e-02, 9.4745e-02, 1.1851e-02, 6.1970e-02,
        1.3660e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,606][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([5.0544e-01, 3.7166e-06, 3.6455e-04, 1.8203e-04, 8.7592e-05, 9.0601e-05,
        3.6784e-03, 6.3923e-03, 6.2943e-03, 1.0032e-01, 9.4609e-03, 1.5875e-01,
        2.0894e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,606][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.1735e-02, 2.1991e-04, 4.3349e-02, 2.9814e-03, 3.3092e-03, 2.2171e-01,
        1.1555e-01, 1.7002e-03, 2.2110e-01, 3.0406e-04, 4.6560e-03, 8.8861e-02,
        2.6684e-03, 2.8185e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,606][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([9.2476e-02, 4.4941e-06, 5.6219e-04, 1.8775e-04, 1.7184e-04, 1.6537e-04,
        1.3542e-02, 1.3018e-02, 2.0155e-02, 9.6691e-02, 1.5165e-02, 2.2196e-01,
        3.2370e-01, 2.0220e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,608][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([6.4922e-03, 1.3329e-08, 2.2257e-06, 4.6485e-06, 1.1925e-06, 4.4323e-07,
        7.2659e-04, 1.1340e-03, 3.0405e-04, 2.3001e-01, 8.7925e-04, 1.2872e-01,
        5.4608e-01, 8.5645e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,609][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([3.3726e-04, 7.9941e-11, 7.0040e-08, 3.6526e-08, 8.9553e-08, 1.8324e-08,
        2.2779e-05, 1.7842e-04, 2.6143e-05, 2.4923e-02, 2.5305e-04, 1.9711e-02,
        8.6557e-01, 8.8976e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,612][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.6820, 0.0085, 0.0468, 0.0097, 0.0318, 0.0158, 0.0284, 0.0231, 0.0505,
        0.0103, 0.0369, 0.0321, 0.0083, 0.0159], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,615][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.5225, 0.0016, 0.0157, 0.0034, 0.0075, 0.0089, 0.0379, 0.0227, 0.0344,
        0.0324, 0.0436, 0.1294, 0.0413, 0.0986], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,617][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([6.7560e-02, 1.4763e-06, 4.8097e-05, 9.9338e-05, 1.7518e-05, 8.7901e-06,
        9.7915e-04, 3.5923e-03, 1.3851e-03, 4.2672e-01, 1.0883e-03, 1.3509e-01,
        2.7755e-01, 8.5859e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,618][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.0397e-03, 5.9942e-10, 3.1217e-07, 6.6032e-07, 1.5661e-07, 1.2082e-07,
        1.4655e-04, 8.2292e-04, 1.3657e-04, 1.4071e-01, 6.3451e-04, 6.6692e-02,
        6.5143e-01, 1.3739e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,618][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([2.6684e-02, 5.9766e-08, 1.7567e-05, 5.7452e-06, 2.9289e-06, 3.6519e-06,
        6.3623e-04, 2.0585e-03, 9.1465e-04, 1.3886e-01, 1.5642e-03, 6.6490e-02,
        5.2152e-01, 2.4124e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,619][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.2584e-01, 1.1696e-05, 6.7843e-04, 4.2126e-04, 5.9407e-04, 8.5685e-05,
        4.9338e-03, 8.3701e-03, 1.0090e-02, 2.1895e-01, 7.9279e-02, 1.1618e-01,
        3.4131e-01, 9.3260e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,619][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([5.0102e-01, 1.3181e-04, 2.2262e-03, 1.1409e-03, 1.3540e-03, 1.2325e-03,
        1.2475e-02, 9.4725e-03, 1.6789e-02, 9.9340e-02, 1.4566e-02, 6.8771e-02,
        1.3460e-01, 1.3689e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,619][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.6712e-01, 1.3865e-06, 2.2422e-04, 8.7665e-05, 8.9330e-05, 5.2032e-05,
        2.7195e-03, 5.3362e-03, 5.7402e-03, 7.2401e-02, 9.8990e-03, 1.6899e-01,
        4.4539e-01, 1.2195e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,621][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:36,622][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[32831],
        [ 6104],
        [ 2104],
        [ 2078],
        [ 2860],
        [  808],
        [ 2355],
        [11765],
        [ 6635],
        [31398],
        [ 8037],
        [ 8499],
        [18952],
        [ 7822]], device='cuda:0')
[2024-07-24 10:30:36,623][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[33318],
        [ 8304],
        [ 2370],
        [ 3666],
        [ 4393],
        [ 1496],
        [ 2144],
        [15137],
        [12163],
        [33779],
        [13699],
        [ 8647],
        [19712],
        [12834]], device='cuda:0')
[2024-07-24 10:30:36,626][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[46344],
        [45653],
        [44873],
        [45077],
        [44679],
        [44220],
        [43777],
        [43431],
        [43187],
        [43221],
        [43084],
        [42808],
        [42849],
        [42752]], device='cuda:0')
[2024-07-24 10:30:36,627][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[21048],
        [23500],
        [23513],
        [23567],
        [23931],
        [21410],
        [22180],
        [19771],
        [20031],
        [19566],
        [19758],
        [20122],
        [19758],
        [18617]], device='cuda:0')
[2024-07-24 10:30:36,629][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[39471],
        [36739],
        [33022],
        [31007],
        [27915],
        [26455],
        [25625],
        [25031],
        [25052],
        [24764],
        [23351],
        [24017],
        [24427],
        [23958]], device='cuda:0')
[2024-07-24 10:30:36,632][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[26351],
        [20131],
        [24202],
        [24829],
        [26050],
        [26971],
        [27738],
        [28712],
        [29065],
        [28466],
        [28332],
        [28433],
        [28702],
        [29037]], device='cuda:0')
[2024-07-24 10:30:36,634][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[13958],
        [15758],
        [11899],
        [13979],
        [16651],
        [13173],
        [11923],
        [11778],
        [11539],
        [11713],
        [11936],
        [12085],
        [12983],
        [12353]], device='cuda:0')
[2024-07-24 10:30:36,635][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[38045],
        [39450],
        [35318],
        [28003],
        [14879],
        [26570],
        [28753],
        [30024],
        [29844],
        [36303],
        [29857],
        [31485],
        [28950],
        [19630]], device='cuda:0')
[2024-07-24 10:30:36,636][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 7137],
        [ 7054],
        [ 7125],
        [10636],
        [11027],
        [11749],
        [ 6816],
        [ 8603],
        [10020],
        [ 7873],
        [11181],
        [10213],
        [10696],
        [11418]], device='cuda:0')
[2024-07-24 10:30:36,637][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 9545],
        [ 9159],
        [ 9865],
        [18359],
        [21491],
        [17168],
        [ 5585],
        [ 6932],
        [ 8932],
        [12489],
        [13157],
        [11859],
        [20002],
        [19986]], device='cuda:0')
[2024-07-24 10:30:36,638][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[19608],
        [21030],
        [24700],
        [25940],
        [28098],
        [31324],
        [18599],
        [29749],
        [37895],
        [25087],
        [29517],
        [29190],
        [31958],
        [33236]], device='cuda:0')
[2024-07-24 10:30:36,639][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[10949],
        [13948],
        [10513],
        [ 8344],
        [ 6609],
        [ 4664],
        [ 8303],
        [ 6232],
        [ 5229],
        [16022],
        [13420],
        [10052],
        [16458],
        [14395]], device='cuda:0')
[2024-07-24 10:30:36,640][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[ 8891],
        [34734],
        [18763],
        [32859],
        [33078],
        [16529],
        [25155],
        [22890],
        [11519],
        [25308],
        [14818],
        [21146],
        [17907],
        [12594]], device='cuda:0')
[2024-07-24 10:30:36,643][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[39819],
        [40853],
        [40884],
        [43083],
        [41454],
        [43442],
        [41744],
        [42606],
        [46668],
        [45830],
        [48794],
        [41796],
        [43835],
        [45587]], device='cuda:0')
[2024-07-24 10:30:36,644][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[40498],
        [14117],
        [28284],
        [17910],
        [25278],
        [24806],
        [39675],
        [29392],
        [20198],
        [28745],
        [19172],
        [31674],
        [28005],
        [24726]], device='cuda:0')
[2024-07-24 10:30:36,646][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 9090],
        [ 4702],
        [14015],
        [ 9572],
        [12621],
        [13128],
        [18019],
        [16717],
        [16352],
        [14633],
        [15796],
        [17524],
        [15967],
        [15637]], device='cuda:0')
[2024-07-24 10:30:36,649][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[40783],
        [42395],
        [38669],
        [41561],
        [41418],
        [42066],
        [41624],
        [41625],
        [40283],
        [43131],
        [41357],
        [42593],
        [39150],
        [37111]], device='cuda:0')
[2024-07-24 10:30:36,651][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[11605],
        [12171],
        [10578],
        [18035],
        [19141],
        [19174],
        [ 8449],
        [ 9958],
        [11678],
        [15770],
        [15518],
        [13372],
        [ 6648],
        [ 5949]], device='cuda:0')
[2024-07-24 10:30:36,652][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[23858],
        [11412],
        [15777],
        [ 9622],
        [10168],
        [17430],
        [28803],
        [38934],
        [39249],
        [14140],
        [14173],
        [13930],
        [27217],
        [28195]], device='cuda:0')
[2024-07-24 10:30:36,653][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[13458],
        [13225],
        [12327],
        [11352],
        [12280],
        [13289],
        [11369],
        [11163],
        [12183],
        [11615],
        [11889],
        [11552],
        [11595],
        [12141]], device='cuda:0')
[2024-07-24 10:30:36,654][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[34313],
        [32082],
        [28672],
        [27045],
        [31849],
        [31181],
        [28515],
        [26369],
        [26231],
        [25448],
        [26309],
        [24973],
        [23907],
        [26280]], device='cuda:0')
[2024-07-24 10:30:36,655][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[40296],
        [40498],
        [40413],
        [39605],
        [38193],
        [36513],
        [40772],
        [37639],
        [32246],
        [26305],
        [14167],
        [22980],
        [27183],
        [22728]], device='cuda:0')
[2024-07-24 10:30:36,656][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[39562],
        [38762],
        [39159],
        [38798],
        [40421],
        [39911],
        [41363],
        [44424],
        [45728],
        [39281],
        [39311],
        [39463],
        [31692],
        [31113]], device='cuda:0')
[2024-07-24 10:30:36,659][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[28169],
        [20886],
        [19758],
        [ 9261],
        [11681],
        [12962],
        [21211],
        [14461],
        [15622],
        [15352],
        [11162],
        [12566],
        [11461],
        [14273]], device='cuda:0')
[2024-07-24 10:30:36,660][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[20626],
        [15029],
        [16522],
        [17259],
        [17817],
        [18389],
        [19168],
        [12979],
        [15532],
        [16917],
        [16839],
        [17098],
        [20256],
        [18271]], device='cuda:0')
[2024-07-24 10:30:36,662][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[37555],
        [39418],
        [38411],
        [38807],
        [38012],
        [37076],
        [36325],
        [33463],
        [34831],
        [41097],
        [40592],
        [38605],
        [39896],
        [38416]], device='cuda:0')
[2024-07-24 10:30:36,665][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[1926],
        [1876],
        [2043],
        [2467],
        [2134],
        [2911],
        [2050],
        [1900],
        [2132],
        [1881],
        [2953],
        [1791],
        [2838],
        [5991]], device='cuda:0')
[2024-07-24 10:30:36,667][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[33551],
        [36316],
        [36753],
        [37548],
        [37359],
        [35736],
        [34462],
        [36382],
        [36006],
        [39244],
        [39928],
        [40895],
        [41324],
        [40026]], device='cuda:0')
[2024-07-24 10:30:36,668][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[42038],
        [47797],
        [42800],
        [44896],
        [41051],
        [38238],
        [31984],
        [31081],
        [30711],
        [40456],
        [38358],
        [35799],
        [37538],
        [36844]], device='cuda:0')
[2024-07-24 10:30:36,669][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682],
        [32682]], device='cuda:0')
[2024-07-24 10:30:36,706][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:36,709][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,711][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,714][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,716][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,716][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,717][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,717][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,717][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,718][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,718][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,718][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,719][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,719][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0290, 0.9710], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,721][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.2163, 0.7837], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,723][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.8999, 0.1001], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,727][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.3595, 0.6405], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,730][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.2115, 0.7885], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,730][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.3287, 0.6713], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,730][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.6726, 0.3274], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,731][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.3792, 0.6208], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,731][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.9357, 0.0643], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,731][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.7799, 0.2201], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,732][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.9988, 0.0012], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,732][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([9.9976e-01, 2.4251e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,732][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0195, 0.5442, 0.4363], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,733][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0701, 0.3797, 0.5502], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,735][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.5460, 0.0758, 0.3782], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,737][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0542, 0.0201, 0.9256], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,741][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1462, 0.2007, 0.6532], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,743][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0865, 0.0247, 0.8888], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,744][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5059, 0.2511, 0.2430], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,744][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.1910, 0.4244, 0.3847], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,744][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.8322, 0.0798, 0.0880], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,745][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.3694, 0.0374, 0.5932], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,745][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ and] are: tensor([9.8305e-01, 7.9997e-04, 1.6155e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,745][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ and] are: tensor([2.5302e-03, 3.7773e-06, 9.9747e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,746][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.0199, 0.3628, 0.3030, 0.3143], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,746][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.0785, 0.2520, 0.4105, 0.2590], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,748][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.3853, 0.0952, 0.3260, 0.1935], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,750][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.0486, 0.0234, 0.6511, 0.2770], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,754][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0444, 0.1393, 0.1271, 0.6892], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,757][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.0562, 0.0311, 0.5126, 0.4001], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,757][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.3835, 0.1995, 0.1934, 0.2236], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,757][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.1345, 0.3139, 0.2662, 0.2855], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,758][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.9381, 0.0298, 0.0236, 0.0084], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,758][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.2171, 0.0469, 0.4928, 0.2433], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,758][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.9345, 0.0039, 0.0381, 0.0235], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,759][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([1.1394e-03, 7.6254e-07, 8.6673e-01, 1.3213e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,759][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0144, 0.2719, 0.2257, 0.2324, 0.2557], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,759][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0522, 0.2116, 0.3217, 0.1991, 0.2154], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,761][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.5050, 0.0392, 0.2463, 0.1073, 0.1022], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,764][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0120, 0.0045, 0.6503, 0.1205, 0.2127], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,768][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.0684, 0.1035, 0.2079, 0.2757, 0.3445], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,770][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.0375, 0.0059, 0.5887, 0.1568, 0.2111], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,770][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.3329, 0.1628, 0.1572, 0.1835, 0.1636], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,771][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.0905, 0.2294, 0.2412, 0.2238, 0.2150], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,771][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.7684, 0.0748, 0.0544, 0.0258, 0.0766], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,771][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.3659, 0.0159, 0.3016, 0.1487, 0.1679], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,772][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ went] are: tensor([9.7036e-01, 6.5157e-04, 1.7624e-02, 7.6848e-03, 3.6786e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,772][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ went] are: tensor([9.6906e-04, 8.8314e-08, 3.9265e-01, 5.9759e-01, 8.7876e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,772][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0062, 0.2146, 0.1700, 0.1732, 0.2026, 0.2334], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,773][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0208, 0.1557, 0.2322, 0.1625, 0.1802, 0.2487], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,775][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2515, 0.0367, 0.1980, 0.1175, 0.1030, 0.2933], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,777][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0037, 0.0048, 0.4409, 0.0943, 0.2363, 0.2199], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,781][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0525, 0.0914, 0.2694, 0.2060, 0.1966, 0.1841], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,783][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0083, 0.0070, 0.4511, 0.1120, 0.3304, 0.0913], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,784][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.3035, 0.1412, 0.1336, 0.1587, 0.1392, 0.1238], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,784][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0703, 0.2175, 0.2031, 0.1871, 0.1650, 0.1570], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,784][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.7164, 0.0618, 0.0520, 0.0280, 0.0647, 0.0770], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,785][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0895, 0.0157, 0.2534, 0.1442, 0.2822, 0.2150], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,785][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.9442, 0.0012, 0.0246, 0.0104, 0.0051, 0.0145], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,786][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([7.9866e-08, 2.2657e-07, 2.2942e-01, 2.1642e-01, 5.5408e-01, 8.6927e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:36,786][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0109, 0.1657, 0.1376, 0.1441, 0.1576, 0.1706, 0.2135],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,787][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0285, 0.1229, 0.1734, 0.1374, 0.1535, 0.1938, 0.1906],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,788][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1921, 0.0228, 0.1060, 0.0639, 0.0564, 0.1393, 0.4195],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,790][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([2.5286e-02, 5.7930e-04, 1.0501e-01, 2.8465e-02, 2.4400e-02, 3.8146e-02,
        7.7811e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,794][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0377, 0.0842, 0.1783, 0.2008, 0.1219, 0.1233, 0.2540],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,796][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([5.1305e-02, 5.6204e-04, 5.2281e-02, 2.6611e-02, 2.7707e-02, 1.2316e-02,
        8.2922e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,797][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2587, 0.1218, 0.1184, 0.1389, 0.1246, 0.1123, 0.1254],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,797][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0994, 0.1567, 0.1504, 0.1549, 0.1490, 0.1354, 0.1541],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,798][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7540, 0.0456, 0.0348, 0.0176, 0.0336, 0.0404, 0.0740],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,798][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2376, 0.0032, 0.0782, 0.0464, 0.0529, 0.0626, 0.5191],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,798][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([9.4891e-01, 8.9915e-04, 1.7892e-02, 8.3313e-03, 3.1442e-03, 9.3340e-03,
        1.1485e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,799][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.1048e-03, 1.3188e-11, 1.7370e-04, 9.9529e-04, 3.6541e-05, 1.2098e-07,
        9.9769e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:36,799][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0057, 0.1554, 0.1238, 0.1205, 0.1379, 0.1700, 0.1934, 0.0933],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,799][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0297, 0.1068, 0.1601, 0.1019, 0.1227, 0.1842, 0.1754, 0.1190],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,801][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.1708, 0.0150, 0.1284, 0.0614, 0.0754, 0.1173, 0.3543, 0.0773],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,803][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ office] are: tensor([1.4715e-02, 2.7248e-04, 1.7528e-02, 7.7948e-03, 7.0306e-03, 8.1034e-03,
        3.6167e-01, 5.8288e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,806][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.0374, 0.0627, 0.1786, 0.1493, 0.0989, 0.1239, 0.2015, 0.1478],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,808][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ office] are: tensor([5.4908e-02, 3.4764e-04, 2.6589e-02, 1.2843e-02, 1.0586e-02, 6.0902e-03,
        4.1098e-01, 4.7766e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,810][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.2396, 0.1085, 0.1059, 0.1227, 0.1084, 0.0978, 0.1117, 0.1053],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,810][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0983, 0.1439, 0.1395, 0.1228, 0.1304, 0.1160, 0.1319, 0.1172],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,811][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.7271, 0.0391, 0.0286, 0.0125, 0.0297, 0.0286, 0.0602, 0.0741],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,811][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.2056, 0.0017, 0.0460, 0.0244, 0.0225, 0.0347, 0.2887, 0.3766],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,812][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ office] are: tensor([9.0610e-01, 8.7384e-04, 2.1835e-02, 1.1729e-02, 3.2345e-03, 1.1365e-02,
        1.8356e-02, 2.6502e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,812][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ office] are: tensor([6.2289e-03, 5.1030e-15, 6.0941e-08, 6.5450e-07, 4.3213e-09, 9.7146e-11,
        2.8665e-03, 9.9090e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:36,812][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0055, 0.1373, 0.1071, 0.1073, 0.1240, 0.1424, 0.1706, 0.0765, 0.1294],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,813][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0157, 0.0887, 0.1353, 0.1107, 0.1124, 0.1476, 0.1587, 0.1110, 0.1199],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,815][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1733, 0.0267, 0.1008, 0.0689, 0.0570, 0.1272, 0.2294, 0.0799, 0.1368],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,817][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [,] are: tensor([2.2553e-03, 2.4144e-04, 2.0460e-02, 7.0659e-03, 9.5986e-03, 1.4018e-02,
        2.5675e-01, 6.2017e-01, 6.9442e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,819][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0331, 0.0511, 0.1487, 0.1392, 0.0905, 0.0889, 0.1707, 0.1714, 0.1064],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,822][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [,] are: tensor([5.6877e-03, 4.2969e-04, 2.3269e-02, 9.5671e-03, 1.2501e-02, 6.7017e-03,
        3.4291e-01, 4.8628e-01, 1.1266e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,824][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2146, 0.1001, 0.0969, 0.1128, 0.0997, 0.0899, 0.1029, 0.0971, 0.0860],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,824][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0817, 0.1312, 0.1166, 0.1158, 0.1043, 0.0958, 0.1226, 0.1155, 0.1164],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,824][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.5716, 0.0415, 0.0316, 0.0186, 0.0331, 0.0392, 0.0682, 0.0740, 0.1223],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,825][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0626, 0.0024, 0.0372, 0.0285, 0.0337, 0.0496, 0.3046, 0.3629, 0.1185],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,825][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.8708, 0.0022, 0.0256, 0.0128, 0.0064, 0.0135, 0.0207, 0.0243, 0.0237],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,826][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [,] are: tensor([1.0695e-10, 1.0916e-14, 1.5321e-09, 3.2749e-08, 1.5899e-09, 8.9159e-12,
        6.8252e-05, 9.9993e-01, 9.2582e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:36,826][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0060, 0.1111, 0.0935, 0.0913, 0.1029, 0.1274, 0.1472, 0.0732, 0.1166,
        0.1308], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,826][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0427, 0.0812, 0.1308, 0.0901, 0.1045, 0.1433, 0.1329, 0.0943, 0.1163,
        0.0639], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,828][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.2398, 0.0063, 0.0596, 0.0375, 0.0272, 0.0872, 0.2341, 0.0630, 0.1360,
        0.1092], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,830][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([5.0548e-02, 2.4404e-05, 5.7762e-03, 3.2136e-03, 1.6101e-03, 2.9108e-03,
        4.9111e-02, 2.0006e-01, 2.5746e-02, 6.6101e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,833][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0244, 0.0920, 0.1108, 0.1679, 0.0973, 0.0779, 0.1283, 0.1214, 0.0876,
        0.0925], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,835][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([6.6780e-02, 2.4196e-05, 3.3146e-03, 3.2714e-03, 1.2046e-03, 7.2013e-04,
        5.2717e-02, 8.0488e-02, 2.8104e-02, 7.6338e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,837][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.1926, 0.0858, 0.0852, 0.0986, 0.0881, 0.0791, 0.0907, 0.0859, 0.0769,
        0.1170], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,837][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.0878, 0.1086, 0.0824, 0.1117, 0.0915, 0.0827, 0.1036, 0.0936, 0.1070,
        0.1310], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,838][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.7125, 0.0210, 0.0162, 0.0068, 0.0206, 0.0224, 0.0428, 0.0537, 0.0899,
        0.0142], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,838][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([2.4381e-01, 2.6366e-04, 1.2002e-02, 9.1693e-03, 4.9193e-03, 8.3453e-03,
        7.4684e-02, 1.0435e-01, 3.6124e-02, 5.0633e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,839][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.8763, 0.0017, 0.0258, 0.0100, 0.0069, 0.0112, 0.0169, 0.0195, 0.0199,
        0.0118], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,839][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([8.6962e-06, 1.1654e-22, 2.9503e-13, 2.3223e-12, 3.2135e-15, 1.0441e-16,
        3.1163e-09, 6.2579e-06, 7.5824e-11, 9.9999e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:36,839][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0051, 0.1035, 0.0847, 0.0844, 0.0955, 0.1199, 0.1353, 0.0639, 0.1064,
        0.1118, 0.0895], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,840][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0226, 0.0862, 0.1231, 0.0790, 0.0813, 0.1357, 0.1269, 0.0995, 0.1030,
        0.0695, 0.0733], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,841][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1854, 0.0124, 0.0881, 0.0509, 0.0301, 0.0696, 0.2235, 0.0678, 0.1106,
        0.1132, 0.0484], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,843][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([9.5758e-04, 4.5565e-05, 5.4485e-03, 1.5790e-03, 1.6906e-03, 2.2300e-03,
        5.2532e-02, 1.6883e-01, 1.4957e-02, 7.2530e-01, 2.6426e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,846][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0239, 0.0413, 0.1015, 0.1277, 0.1119, 0.0692, 0.1437, 0.0840, 0.0822,
        0.0460, 0.1685], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,849][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([2.2255e-03, 3.6725e-05, 4.6983e-03, 1.4848e-03, 7.7333e-04, 5.5455e-04,
        5.7463e-02, 7.0865e-02, 1.6726e-02, 8.2418e-01, 2.0992e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,850][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.1689, 0.0810, 0.0781, 0.0904, 0.0804, 0.0719, 0.0834, 0.0790, 0.0703,
        0.1083, 0.0885], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,851][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0460, 0.1054, 0.1036, 0.0916, 0.1001, 0.0749, 0.0995, 0.1007, 0.0845,
        0.0960, 0.0979], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,851][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.5525, 0.0341, 0.0216, 0.0135, 0.0343, 0.0311, 0.0542, 0.0814, 0.1165,
        0.0276, 0.0332], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,852][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([4.1949e-02, 3.6909e-04, 9.0772e-03, 6.1947e-03, 6.0186e-03, 8.1480e-03,
        9.4116e-02, 1.4268e-01, 4.1269e-02, 5.5745e-01, 9.2727e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,852][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([9.3979e-01, 7.1865e-04, 9.7852e-03, 5.4482e-03, 2.6470e-03, 4.8306e-03,
        7.9357e-03, 1.1358e-02, 8.6652e-03, 3.6285e-03, 5.1884e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,852][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([5.8217e-14, 2.7581e-21, 6.7123e-14, 8.6380e-14, 6.0806e-15, 1.8719e-17,
        5.6773e-10, 4.8824e-06, 9.1947e-13, 1.0000e+00, 2.3043e-10],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:36,853][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0053, 0.0946, 0.0781, 0.0790, 0.0897, 0.1030, 0.1181, 0.0600, 0.0936,
        0.1020, 0.0836, 0.0929], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,853][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0206, 0.0726, 0.1007, 0.0749, 0.0886, 0.1149, 0.1062, 0.0834, 0.1017,
        0.0671, 0.0817, 0.0876], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,855][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1018, 0.0199, 0.0669, 0.0436, 0.0283, 0.0649, 0.1666, 0.0623, 0.0882,
        0.1037, 0.0613, 0.1925], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,857][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([5.0745e-03, 8.2447e-05, 9.4794e-03, 2.8012e-03, 2.6111e-03, 3.3757e-03,
        5.3667e-02, 1.5855e-01, 2.0797e-02, 4.7125e-01, 4.6627e-02, 2.2569e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,860][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0221, 0.0484, 0.0922, 0.1683, 0.0819, 0.0558, 0.1219, 0.0808, 0.0731,
        0.0450, 0.0867, 0.1235], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,862][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([7.9750e-03, 9.4811e-05, 4.9641e-03, 3.3435e-03, 1.6384e-03, 1.0418e-03,
        5.1357e-02, 7.9263e-02, 2.2548e-02, 6.4347e-01, 3.2312e-02, 1.5199e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,866][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1519, 0.0731, 0.0709, 0.0824, 0.0735, 0.0662, 0.0755, 0.0721, 0.0643,
        0.0981, 0.0815, 0.0906], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,867][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0601, 0.0974, 0.0834, 0.0805, 0.0783, 0.0650, 0.0882, 0.0824, 0.0806,
        0.1017, 0.0859, 0.0966], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,867][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.6373, 0.0253, 0.0180, 0.0102, 0.0220, 0.0243, 0.0440, 0.0568, 0.0892,
        0.0188, 0.0186, 0.0354], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,868][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0499, 0.0005, 0.0121, 0.0078, 0.0083, 0.0100, 0.0688, 0.1010, 0.0335,
        0.3479, 0.1159, 0.2443], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,868][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.8044, 0.0028, 0.0252, 0.0128, 0.0064, 0.0135, 0.0175, 0.0239, 0.0236,
        0.0119, 0.0112, 0.0468], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,868][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([5.3860e-12, 1.8013e-19, 2.9219e-13, 1.7064e-12, 7.7665e-14, 1.6496e-16,
        1.5568e-09, 1.0335e-05, 4.4898e-12, 9.9997e-01, 3.0410e-09, 1.6565e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:36,869][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0067, 0.0838, 0.0717, 0.0728, 0.0780, 0.0913, 0.1077, 0.0585, 0.0857,
        0.1001, 0.0757, 0.0868, 0.0813], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,871][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0173, 0.0623, 0.0974, 0.0683, 0.0741, 0.1082, 0.0996, 0.0749, 0.0918,
        0.0560, 0.0756, 0.0849, 0.0896], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,874][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0931, 0.0124, 0.0788, 0.0392, 0.0271, 0.0789, 0.1833, 0.0411, 0.0851,
        0.0837, 0.0435, 0.2071, 0.0267], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,877][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([4.3856e-03, 1.4580e-05, 1.9964e-03, 9.5002e-04, 3.3035e-04, 6.7578e-04,
        1.6771e-02, 6.0445e-02, 6.6873e-03, 3.1784e-01, 1.2116e-02, 1.3284e-01,
        4.4494e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,879][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.0200, 0.0448, 0.0715, 0.1906, 0.0669, 0.0491, 0.0861, 0.0473, 0.0643,
        0.0506, 0.0835, 0.0572, 0.1682], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,880][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([5.5812e-03, 1.2568e-05, 1.5438e-03, 1.1047e-03, 3.5403e-04, 2.1489e-04,
        1.9340e-02, 3.0097e-02, 9.0926e-03, 4.8883e-01, 1.0318e-02, 1.1452e-01,
        3.1899e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,880][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.1375, 0.0657, 0.0648, 0.0747, 0.0673, 0.0611, 0.0695, 0.0669, 0.0599,
        0.0898, 0.0761, 0.0840, 0.0827], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,881][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0522, 0.0871, 0.0780, 0.0718, 0.0729, 0.0573, 0.0814, 0.0784, 0.0696,
        0.0869, 0.0707, 0.0811, 0.1126], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,881][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.6025, 0.0231, 0.0170, 0.0085, 0.0208, 0.0231, 0.0436, 0.0574, 0.0877,
        0.0180, 0.0186, 0.0354, 0.0443], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,881][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([4.3747e-02, 1.3472e-04, 5.2367e-03, 3.6207e-03, 2.6420e-03, 3.5999e-03,
        3.7564e-02, 4.1904e-02, 1.3328e-02, 2.2447e-01, 4.5303e-02, 1.8133e-01,
        3.9713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,882][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.8157, 0.0023, 0.0201, 0.0081, 0.0050, 0.0073, 0.0134, 0.0179, 0.0186,
        0.0135, 0.0129, 0.0427, 0.0225], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,882][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([1.7403e-11, 4.6298e-23, 6.7853e-16, 1.1132e-14, 1.6665e-17, 3.9851e-20,
        8.7151e-12, 1.3540e-07, 2.9524e-14, 1.5440e-01, 1.1619e-11, 6.2847e-06,
        8.4559e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:36,884][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0054, 0.0734, 0.0637, 0.0640, 0.0722, 0.0808, 0.1029, 0.0471, 0.0781,
        0.0833, 0.0675, 0.0798, 0.0731, 0.1089], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,887][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0117, 0.0617, 0.0839, 0.0657, 0.0705, 0.0837, 0.0950, 0.0696, 0.0816,
        0.0551, 0.0612, 0.0795, 0.0874, 0.0935], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,891][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0666, 0.0109, 0.0513, 0.0335, 0.0223, 0.0625, 0.1779, 0.0445, 0.0782,
        0.0875, 0.0469, 0.1944, 0.0201, 0.1035], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,893][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([2.1921e-03, 4.6157e-06, 9.9645e-04, 4.5114e-04, 3.2074e-04, 3.1622e-04,
        1.2659e-02, 4.5649e-02, 5.0772e-03, 2.1542e-01, 8.3710e-03, 9.8368e-02,
        3.8253e-01, 2.2765e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,893][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0211, 0.0388, 0.0971, 0.0938, 0.0744, 0.0717, 0.1196, 0.0868, 0.0742,
        0.0363, 0.0710, 0.0865, 0.0670, 0.0616], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,894][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.1543e-03, 3.8313e-06, 8.0304e-04, 3.4188e-04, 1.9425e-04, 7.2992e-05,
        1.7211e-02, 2.0344e-02, 6.5648e-03, 3.0895e-01, 6.7487e-03, 1.0475e-01,
        4.5764e-01, 7.4217e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,894][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1291, 0.0617, 0.0606, 0.0700, 0.0622, 0.0562, 0.0645, 0.0612, 0.0547,
        0.0835, 0.0701, 0.0778, 0.0773, 0.0712], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,895][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0597, 0.0735, 0.0656, 0.0689, 0.0644, 0.0562, 0.0731, 0.0676, 0.0653,
        0.0801, 0.0669, 0.0759, 0.1115, 0.0712], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,895][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.4283, 0.0287, 0.0266, 0.0141, 0.0314, 0.0365, 0.0566, 0.0700, 0.1103,
        0.0259, 0.0298, 0.0454, 0.0515, 0.0450], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,896][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([4.2580e-02, 7.3112e-05, 3.4021e-03, 2.6888e-03, 2.0105e-03, 2.1278e-03,
        2.9402e-02, 3.0741e-02, 1.3062e-02, 2.1300e-01, 3.6588e-02, 1.4796e-01,
        3.0488e-01, 1.7149e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,897][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([8.9829e-01, 7.7865e-04, 1.1890e-02, 5.0910e-03, 3.3754e-03, 5.2450e-03,
        7.8990e-03, 8.5333e-03, 1.2602e-02, 3.9582e-03, 5.6116e-03, 2.6370e-02,
        4.3406e-03, 6.0196e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,899][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([5.8292e-15, 1.5337e-25, 2.5302e-18, 8.4799e-17, 8.8461e-19, 9.5289e-23,
        2.8450e-13, 1.0652e-08, 2.3864e-16, 1.4578e-02, 1.3034e-13, 9.5683e-08,
        9.8542e-01, 1.4386e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:36,935][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:36,936][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,937][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,937][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,937][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,938][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,938][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,938][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,939][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,939][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,940][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,942][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,945][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:36,948][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.2117, 0.7883], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,950][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.1609, 0.8391], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,950][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.9334, 0.0666], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,951][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.3595, 0.6405], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,951][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([9.9985e-01, 1.4716e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,951][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.3287, 0.6713], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,952][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.3596, 0.6404], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,952][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.0226, 0.9774], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,952][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.9924e-01, 7.5873e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,953][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.7799, 0.2201], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,954][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.9988, 0.0012], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,954][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([9.9976e-01, 2.4251e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:36,955][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.1298, 0.0075, 0.8627], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,958][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.2836, 0.0269, 0.6895], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,962][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.7586, 0.0257, 0.2158], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,964][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0542, 0.0201, 0.9256], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,965][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([9.9797e-01, 1.5418e-04, 1.8784e-03], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,965][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0865, 0.0247, 0.8888], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,965][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.1550, 0.0517, 0.7932], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,966][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0015, 0.0508, 0.9477], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,966][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.9472, 0.0022, 0.0506], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,966][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.3694, 0.0374, 0.5932], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,967][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([9.8305e-01, 7.9997e-04, 1.6155e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,967][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([2.5302e-03, 3.7773e-06, 9.9747e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:36,968][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.0296, 0.0131, 0.6365, 0.3207], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,971][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.0205, 0.0249, 0.5395, 0.4150], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,974][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.5869, 0.0598, 0.1911, 0.1622], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,978][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.0486, 0.0234, 0.6511, 0.2770], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,978][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([9.9288e-01, 4.0248e-04, 2.8359e-03, 3.8851e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,979][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.0562, 0.0311, 0.5126, 0.4001], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,979][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.0606, 0.0543, 0.6431, 0.2419], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,979][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.0017, 0.0847, 0.5787, 0.3349], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,980][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.8934, 0.0130, 0.0673, 0.0264], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,980][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.2171, 0.0469, 0.4928, 0.2433], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,980][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.9345, 0.0039, 0.0381, 0.0235], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,981][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([1.1394e-03, 7.6254e-07, 8.6673e-01, 1.3213e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:36,984][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.0440, 0.0015, 0.3893, 0.1669, 0.3983], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,987][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0540, 0.0090, 0.4666, 0.2678, 0.2027], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,991][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.7185, 0.0173, 0.1330, 0.0811, 0.0501], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,992][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.0120, 0.0045, 0.6503, 0.1205, 0.2127], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,992][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([9.9655e-01, 1.6466e-04, 1.6679e-03, 1.3299e-03, 2.8295e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,992][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.0375, 0.0059, 0.5887, 0.1568, 0.2111], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,993][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0380, 0.0140, 0.4119, 0.1681, 0.3680], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,993][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([3.0833e-04, 8.6659e-03, 3.1919e-01, 6.2820e-02, 6.0901e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,993][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.8749, 0.0036, 0.0908, 0.0162, 0.0145], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,994][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.3659, 0.0159, 0.3016, 0.1487, 0.1679], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,994][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([9.7036e-01, 6.5157e-04, 1.7624e-02, 7.6848e-03, 3.6786e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,995][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([9.6906e-04, 8.8314e-08, 3.9265e-01, 5.9759e-01, 8.7876e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:36,998][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0045, 0.0017, 0.1992, 0.0834, 0.6264, 0.0848], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,001][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0133, 0.0085, 0.3285, 0.2092, 0.3067, 0.1339], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,005][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.4581, 0.0327, 0.1927, 0.1390, 0.0898, 0.0877], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,005][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0037, 0.0048, 0.4409, 0.0943, 0.2363, 0.2199], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,006][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.9577e-01, 1.6311e-04, 1.7917e-03, 1.7173e-03, 2.2104e-04, 3.3649e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,006][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0083, 0.0070, 0.4511, 0.1120, 0.3304, 0.0913], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,006][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0161, 0.0146, 0.3381, 0.1274, 0.3537, 0.1502], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,007][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([1.3607e-04, 5.2498e-03, 2.2968e-01, 3.8075e-02, 4.2241e-01, 3.0445e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,007][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.7041, 0.0058, 0.1550, 0.0239, 0.0444, 0.0668], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,008][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0895, 0.0157, 0.2534, 0.1442, 0.2822, 0.2150], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,010][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.9442, 0.0012, 0.0246, 0.0104, 0.0051, 0.0145], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,011][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([7.9866e-08, 2.2657e-07, 2.2942e-01, 2.1642e-01, 5.5408e-01, 8.6927e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,014][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.4626e-02, 3.8214e-05, 1.0121e-02, 5.6550e-03, 1.4113e-02, 4.9502e-03,
        9.5050e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,016][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([7.2429e-02, 6.7598e-04, 5.6677e-02, 5.2472e-02, 3.1853e-02, 2.7781e-02,
        7.5811e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,018][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.6700, 0.0099, 0.0718, 0.0598, 0.0299, 0.0310, 0.1276],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,018][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([2.5286e-02, 5.7930e-04, 1.0501e-01, 2.8465e-02, 2.4400e-02, 3.8146e-02,
        7.7811e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,019][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.9698e-01, 8.8706e-05, 1.0267e-03, 1.1462e-03, 1.1888e-04, 1.6118e-04,
        4.8240e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,019][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([5.1305e-02, 5.6204e-04, 5.2281e-02, 2.6611e-02, 2.7707e-02, 1.2316e-02,
        8.2922e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,019][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0426, 0.0025, 0.0828, 0.0448, 0.0716, 0.0350, 0.7206],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,020][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([2.9906e-04, 2.5293e-03, 8.7676e-02, 2.3721e-02, 1.3531e-01, 9.9146e-02,
        6.5132e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,020][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.8490, 0.0026, 0.0618, 0.0136, 0.0131, 0.0209, 0.0390],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,021][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2376, 0.0032, 0.0782, 0.0464, 0.0529, 0.0626, 0.5191],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,021][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([9.4891e-01, 8.9915e-04, 1.7892e-02, 8.3313e-03, 3.1442e-03, 9.3340e-03,
        1.1485e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,022][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([1.1048e-03, 1.3188e-11, 1.7370e-04, 9.9529e-04, 3.6541e-05, 1.2098e-07,
        9.9769e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,023][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([5.0320e-03, 2.4147e-05, 3.6766e-03, 1.6745e-03, 3.8604e-03, 1.2884e-03,
        2.9247e-01, 6.9198e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,025][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([5.9803e-02, 3.4870e-04, 1.8526e-02, 1.7995e-02, 8.1044e-03, 6.2534e-03,
        2.8431e-01, 6.0466e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,029][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.6295, 0.0059, 0.0636, 0.0489, 0.0294, 0.0336, 0.1087, 0.0804],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,032][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([1.4715e-02, 2.7248e-04, 1.7528e-02, 7.7948e-03, 7.0306e-03, 8.1034e-03,
        3.6167e-01, 5.8288e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,032][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([9.9704e-01, 4.3462e-05, 6.6788e-04, 7.2507e-04, 8.0261e-05, 1.2666e-04,
        3.1276e-04, 9.9885e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,032][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([5.4908e-02, 3.4764e-04, 2.6589e-02, 1.2843e-02, 1.0586e-02, 6.0902e-03,
        4.1098e-01, 4.7766e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,033][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.0830, 0.0023, 0.0613, 0.0425, 0.0410, 0.0245, 0.4695, 0.2759],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,033][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.0005, 0.0030, 0.0569, 0.0176, 0.0662, 0.0553, 0.4415, 0.3590],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,034][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.7896, 0.0032, 0.0574, 0.0115, 0.0133, 0.0204, 0.0459, 0.0587],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,034][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.2056, 0.0017, 0.0460, 0.0244, 0.0225, 0.0347, 0.2887, 0.3766],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,035][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([9.0610e-01, 8.7384e-04, 2.1835e-02, 1.1729e-02, 3.2345e-03, 1.1365e-02,
        1.8356e-02, 2.6502e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,036][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([6.2289e-03, 5.1030e-15, 6.0941e-08, 6.5450e-07, 4.3213e-09, 9.7146e-11,
        2.8665e-03, 9.9090e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,038][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([9.4091e-04, 3.5384e-05, 2.5947e-03, 1.8366e-03, 5.1529e-03, 2.3656e-03,
        2.9269e-01, 6.3815e-01, 5.6234e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,040][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([6.9733e-03, 4.2370e-04, 1.1088e-02, 1.3534e-02, 9.4616e-03, 8.6395e-03,
        2.1920e-01, 6.2990e-01, 1.0078e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,044][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.5682, 0.0109, 0.0682, 0.0600, 0.0244, 0.0300, 0.0856, 0.0635, 0.0892],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,045][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([2.2553e-03, 2.4144e-04, 2.0460e-02, 7.0659e-03, 9.5986e-03, 1.4018e-02,
        2.5675e-01, 6.2017e-01, 6.9442e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,045][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([9.9330e-01, 1.4505e-04, 1.2946e-03, 1.2762e-03, 1.4025e-04, 1.7927e-04,
        5.5587e-04, 1.3039e-03, 1.8026e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,046][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([5.6877e-03, 4.2969e-04, 2.3269e-02, 9.5671e-03, 1.2501e-02, 6.7017e-03,
        3.4291e-01, 4.8628e-01, 1.1266e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,046][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0136, 0.0024, 0.0376, 0.0303, 0.0442, 0.0252, 0.4079, 0.2842, 0.1544],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,047][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([1.1626e-04, 2.9125e-03, 5.2693e-02, 1.4130e-02, 9.1210e-02, 7.0644e-02,
        3.9192e-01, 2.2654e-01, 1.4984e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,047][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.7036, 0.0062, 0.0711, 0.0169, 0.0174, 0.0280, 0.0478, 0.0753, 0.0338],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,047][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0626, 0.0024, 0.0372, 0.0285, 0.0337, 0.0496, 0.3046, 0.3629, 0.1185],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,048][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.8708, 0.0022, 0.0256, 0.0128, 0.0064, 0.0135, 0.0207, 0.0243, 0.0237],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,049][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([1.0695e-10, 1.0916e-14, 1.5321e-09, 3.2749e-08, 1.5899e-09, 8.9159e-12,
        6.8252e-05, 9.9993e-01, 9.2582e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,050][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([1.1615e-02, 8.8035e-07, 3.8165e-04, 3.0146e-04, 2.6386e-04, 1.6144e-04,
        3.2705e-02, 6.5693e-02, 8.2067e-03, 8.8067e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,052][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([3.1889e-02, 1.4321e-05, 2.1539e-03, 2.9378e-03, 8.0665e-04, 8.7035e-04,
        3.4024e-02, 9.2083e-02, 2.3032e-02, 8.1219e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,056][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.7138, 0.0024, 0.0257, 0.0304, 0.0065, 0.0095, 0.0345, 0.0329, 0.0432,
        0.1011], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,058][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([5.0548e-02, 2.4404e-05, 5.7762e-03, 3.2136e-03, 1.6101e-03, 2.9108e-03,
        4.9111e-02, 2.0006e-01, 2.5746e-02, 6.6101e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,059][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([9.9673e-01, 6.1585e-05, 4.0331e-04, 5.0024e-04, 4.3502e-05, 4.9453e-05,
        1.7601e-04, 4.8304e-04, 6.3528e-04, 9.2157e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,059][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([6.6780e-02, 2.4196e-05, 3.3146e-03, 3.2714e-03, 1.2046e-03, 7.2013e-04,
        5.2717e-02, 8.0488e-02, 2.8104e-02, 7.6338e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,060][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([5.1377e-02, 1.7297e-04, 1.1851e-02, 8.1979e-03, 6.9330e-03, 5.2921e-03,
        1.0408e-01, 7.5336e-02, 5.6388e-02, 6.8037e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,060][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([5.5599e-04, 9.6552e-04, 1.0408e-02, 1.4015e-02, 2.6472e-02, 1.7457e-02,
        1.3869e-01, 9.1014e-02, 5.4215e-02, 6.4621e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,060][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.2579e-01, 8.2021e-04, 1.6914e-02, 9.4339e-03, 2.8158e-03, 5.2102e-03,
        1.0925e-02, 1.5310e-02, 8.5108e-03, 4.2728e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,061][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([2.4381e-01, 2.6366e-04, 1.2002e-02, 9.1693e-03, 4.9193e-03, 8.3453e-03,
        7.4684e-02, 1.0435e-01, 3.6124e-02, 5.0633e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,063][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.8763, 0.0017, 0.0258, 0.0100, 0.0069, 0.0112, 0.0169, 0.0195, 0.0199,
        0.0118], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,065][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([8.6962e-06, 1.1654e-22, 2.9503e-13, 2.3223e-12, 3.2135e-15, 1.0441e-16,
        3.1163e-09, 6.2579e-06, 7.5824e-11, 9.9999e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,067][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([1.3878e-03, 2.7034e-06, 6.5795e-04, 2.4378e-04, 3.6097e-04, 1.7649e-04,
        4.2963e-02, 1.0920e-01, 1.0558e-02, 8.0186e-01, 3.2595e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,069][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([1.8264e-03, 2.7014e-05, 1.8014e-03, 1.1311e-03, 7.6479e-04, 5.2088e-04,
        2.8597e-02, 9.8513e-02, 1.5033e-02, 8.4077e-01, 1.1013e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,072][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.4841, 0.0051, 0.0476, 0.0501, 0.0159, 0.0164, 0.0628, 0.0648, 0.0703,
        0.1546, 0.0283], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,072][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([9.5758e-04, 4.5565e-05, 5.4485e-03, 1.5790e-03, 1.6906e-03, 2.2300e-03,
        5.2532e-02, 1.6883e-01, 1.4957e-02, 7.2530e-01, 2.6426e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,072][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([9.9441e-01, 7.0443e-05, 6.8268e-04, 7.6214e-04, 8.2015e-05, 9.7955e-05,
        3.5502e-04, 8.7179e-04, 1.0541e-03, 1.2122e-03, 4.0417e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,073][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([2.2255e-03, 3.6725e-05, 4.6983e-03, 1.4848e-03, 7.7333e-04, 5.5455e-04,
        5.7463e-02, 7.0865e-02, 1.6726e-02, 8.2418e-01, 2.0992e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,073][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([3.7004e-03, 2.5556e-04, 8.1586e-03, 4.7324e-03, 5.6855e-03, 2.9631e-03,
        7.3196e-02, 5.5509e-02, 3.0009e-02, 7.6548e-01, 5.0307e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,074][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([1.1257e-04, 1.0221e-03, 2.6866e-02, 6.6244e-03, 4.2540e-02, 2.9365e-02,
        2.0821e-01, 1.0909e-01, 6.8112e-02, 4.0583e-01, 1.0222e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,074][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.7430, 0.0030, 0.0504, 0.0137, 0.0130, 0.0192, 0.0390, 0.0697, 0.0229,
        0.0146, 0.0115], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,074][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([4.1949e-02, 3.6909e-04, 9.0772e-03, 6.1947e-03, 6.0186e-03, 8.1480e-03,
        9.4116e-02, 1.4268e-01, 4.1269e-02, 5.5745e-01, 9.2727e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,076][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([9.3979e-01, 7.1865e-04, 9.7852e-03, 5.4482e-03, 2.6470e-03, 4.8306e-03,
        7.9357e-03, 1.1358e-02, 8.6652e-03, 3.6285e-03, 5.1884e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,077][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([5.8217e-14, 2.7581e-21, 6.7123e-14, 8.6380e-14, 6.0806e-15, 1.8719e-17,
        5.6773e-10, 4.8824e-06, 9.1947e-13, 1.0000e+00, 2.3043e-10],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,079][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([2.1365e-03, 6.9104e-06, 6.7647e-04, 4.5481e-04, 8.4199e-04, 4.0502e-04,
        3.4617e-02, 7.8954e-02, 9.7180e-03, 6.2879e-01, 6.2247e-02, 1.8116e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,081][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([7.3266e-03, 6.2080e-05, 2.8121e-03, 2.5496e-03, 1.7416e-03, 1.4093e-03,
        3.2302e-02, 7.0927e-02, 2.1637e-02, 6.7284e-01, 2.0194e-02, 1.6620e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,085][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4535, 0.0054, 0.0385, 0.0424, 0.0138, 0.0157, 0.0488, 0.0419, 0.0576,
        0.1131, 0.0299, 0.1393], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,085][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([5.0745e-03, 8.2447e-05, 9.4794e-03, 2.8012e-03, 2.6111e-03, 3.3757e-03,
        5.3667e-02, 1.5855e-01, 2.0797e-02, 4.7125e-01, 4.6627e-02, 2.2569e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,086][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([9.8362e-01, 2.3922e-04, 1.6077e-03, 1.6934e-03, 2.0286e-04, 2.0159e-04,
        7.4467e-04, 1.6294e-03, 2.1084e-03, 2.9719e-03, 5.5508e-04, 4.4269e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,086][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([7.9750e-03, 9.4811e-05, 4.9641e-03, 3.3435e-03, 1.6384e-03, 1.0418e-03,
        5.1357e-02, 7.9263e-02, 2.2548e-02, 6.4347e-01, 3.2312e-02, 1.5199e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,087][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([1.0525e-02, 4.5616e-04, 8.9808e-03, 6.4692e-03, 7.5965e-03, 3.8627e-03,
        6.9143e-02, 5.6061e-02, 3.1718e-02, 5.7224e-01, 5.4751e-02, 1.7820e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,087][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([1.3799e-04, 1.5474e-03, 2.4148e-02, 6.4259e-03, 3.4465e-02, 2.4459e-02,
        1.5702e-01, 1.0334e-01, 6.7540e-02, 3.7860e-01, 8.9796e-02, 1.1252e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,088][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.5735, 0.0072, 0.0680, 0.0222, 0.0203, 0.0252, 0.0500, 0.0874, 0.0304,
        0.0236, 0.0179, 0.0743], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,089][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0499, 0.0005, 0.0121, 0.0078, 0.0083, 0.0100, 0.0688, 0.1010, 0.0335,
        0.3479, 0.1159, 0.2443], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,092][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.8044, 0.0028, 0.0252, 0.0128, 0.0064, 0.0135, 0.0175, 0.0239, 0.0236,
        0.0119, 0.0112, 0.0468], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,094][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([5.3860e-12, 1.8013e-19, 2.9219e-13, 1.7064e-12, 7.7665e-14, 1.6496e-16,
        1.5568e-09, 1.0335e-05, 4.4898e-12, 9.9997e-01, 3.0410e-09, 1.6565e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,096][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([7.2566e-04, 2.1330e-07, 6.2776e-05, 5.5990e-05, 5.2958e-05, 2.0569e-05,
        5.0059e-03, 9.8679e-03, 1.3340e-03, 2.4972e-01, 6.9489e-03, 5.5535e-02,
        6.7067e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,098][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([3.7056e-03, 5.3751e-06, 6.5618e-04, 4.7812e-04, 2.4353e-04, 2.2930e-04,
        1.1357e-02, 2.4104e-02, 5.6227e-03, 3.0034e-01, 5.2172e-03, 8.6826e-02,
        5.6122e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,099][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.4753, 0.0034, 0.0299, 0.0379, 0.0112, 0.0112, 0.0372, 0.0318, 0.0450,
        0.1131, 0.0229, 0.1207, 0.0603], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,099][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([4.3856e-03, 1.4580e-05, 1.9964e-03, 9.5002e-04, 3.3035e-04, 6.7578e-04,
        1.6771e-02, 6.0445e-02, 6.6873e-03, 3.1784e-01, 1.2116e-02, 1.3284e-01,
        4.4494e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,100][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([9.8755e-01, 1.5436e-04, 8.1165e-04, 1.1702e-03, 1.4314e-04, 1.3058e-04,
        4.0596e-04, 5.7622e-04, 1.2587e-03, 1.8570e-03, 4.4633e-04, 2.4849e-03,
        3.0092e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,100][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([5.5812e-03, 1.2568e-05, 1.5438e-03, 1.1047e-03, 3.5403e-04, 2.1489e-04,
        1.9340e-02, 3.0097e-02, 9.0926e-03, 4.8883e-01, 1.0318e-02, 1.1452e-01,
        3.1899e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,101][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([1.6999e-02, 1.0062e-04, 4.5510e-03, 3.0211e-03, 3.6294e-03, 1.8580e-03,
        4.2443e-02, 4.9308e-02, 2.2389e-02, 4.3128e-01, 3.6414e-02, 1.6705e-01,
        2.2096e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,101][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([1.6136e-04, 6.5135e-04, 8.4615e-03, 4.2316e-03, 1.2137e-02, 9.3919e-03,
        6.7861e-02, 4.7981e-02, 2.7848e-02, 2.9416e-01, 3.7560e-02, 5.7394e-02,
        4.3216e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,101][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.8009, 0.0036, 0.0304, 0.0130, 0.0066, 0.0121, 0.0200, 0.0268, 0.0145,
        0.0117, 0.0069, 0.0371, 0.0163], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,103][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([4.3747e-02, 1.3472e-04, 5.2367e-03, 3.6207e-03, 2.6420e-03, 3.5999e-03,
        3.7564e-02, 4.1904e-02, 1.3328e-02, 2.2447e-01, 4.5303e-02, 1.8133e-01,
        3.9713e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,105][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.8157, 0.0023, 0.0201, 0.0081, 0.0050, 0.0073, 0.0134, 0.0179, 0.0186,
        0.0135, 0.0129, 0.0427, 0.0225], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,107][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([1.7403e-11, 4.6298e-23, 6.7853e-16, 1.1132e-14, 1.6665e-17, 3.9851e-20,
        8.7151e-12, 1.3540e-07, 2.9524e-14, 1.5440e-01, 1.1619e-11, 6.2847e-06,
        8.4559e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,110][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.9580e-04, 1.2852e-07, 4.2863e-05, 3.9037e-05, 3.8056e-05, 1.0463e-05,
        5.8711e-03, 1.0791e-02, 1.6366e-03, 2.1396e-01, 5.5935e-03, 6.5256e-02,
        6.3160e-01, 6.4173e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,112][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([3.9522e-03, 2.6943e-06, 3.3182e-04, 3.8200e-04, 1.6984e-04, 1.0078e-04,
        8.2166e-03, 1.6315e-02, 4.9189e-03, 2.3750e-01, 2.2350e-03, 7.2282e-02,
        4.9241e-01, 1.6119e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,113][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.5029, 0.0021, 0.0254, 0.0273, 0.0078, 0.0101, 0.0376, 0.0368, 0.0453,
        0.0963, 0.0200, 0.1192, 0.0320, 0.0371], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,113][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([2.1921e-03, 4.6157e-06, 9.9645e-04, 4.5114e-04, 3.2074e-04, 3.1622e-04,
        1.2659e-02, 4.5649e-02, 5.0772e-03, 2.1542e-01, 8.3710e-03, 9.8368e-02,
        3.8253e-01, 2.2765e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,113][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.9365e-01, 4.1425e-05, 5.2052e-04, 5.4271e-04, 4.9630e-05, 6.5295e-05,
        2.4413e-04, 4.3441e-04, 8.8869e-04, 9.5778e-04, 1.7917e-04, 1.6535e-03,
        6.5752e-04, 1.1870e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,114][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.1543e-03, 3.8313e-06, 8.0304e-04, 3.4188e-04, 1.9425e-04, 7.2992e-05,
        1.7211e-02, 2.0344e-02, 6.5648e-03, 3.0895e-01, 6.7487e-03, 1.0475e-01,
        4.5764e-01, 7.4217e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,114][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([6.1790e-03, 5.2593e-05, 2.9015e-03, 1.7971e-03, 1.8621e-03, 8.7688e-04,
        2.8085e-02, 2.0947e-02, 1.2320e-02, 3.3242e-01, 1.9359e-02, 1.0429e-01,
        2.9986e-01, 1.6905e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,115][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([4.8505e-05, 1.8695e-04, 5.7465e-03, 1.7445e-03, 7.6779e-03, 6.7372e-03,
        5.7093e-02, 3.0166e-02, 2.2339e-02, 1.7226e-01, 2.1338e-02, 4.3635e-02,
        2.9918e-01, 3.3185e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,118][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.6710, 0.0024, 0.0518, 0.0097, 0.0136, 0.0201, 0.0330, 0.0429, 0.0234,
        0.0128, 0.0146, 0.0651, 0.0212, 0.0183], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,120][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([4.2580e-02, 7.3112e-05, 3.4021e-03, 2.6888e-03, 2.0105e-03, 2.1278e-03,
        2.9402e-02, 3.0741e-02, 1.3062e-02, 2.1300e-01, 3.6588e-02, 1.4796e-01,
        3.0488e-01, 1.7149e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,122][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([8.9829e-01, 7.7865e-04, 1.1890e-02, 5.0910e-03, 3.3754e-03, 5.2450e-03,
        7.8990e-03, 8.5333e-03, 1.2602e-02, 3.9582e-03, 5.6116e-03, 2.6370e-02,
        4.3406e-03, 6.0196e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,125][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([5.8292e-15, 1.5337e-25, 2.5302e-18, 8.4799e-17, 8.8461e-19, 9.5289e-23,
        2.8450e-13, 1.0652e-08, 2.3864e-16, 1.4578e-02, 1.3034e-13, 9.5683e-08,
        9.8542e-01, 1.4386e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,126][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:37,127][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[33632],
        [16892],
        [ 4907],
        [18291],
        [11036],
        [ 3705],
        [ 9826],
        [21025],
        [16124],
        [42982],
        [24368],
        [20814],
        [33336],
        [18207]], device='cuda:0')
[2024-07-24 10:30:37,128][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[33019],
        [ 8028],
        [ 3401],
        [ 7534],
        [ 4407],
        [ 1120],
        [ 3865],
        [13826],
        [ 7763],
        [36579],
        [10101],
        [10822],
        [20483],
        [ 7652]], device='cuda:0')
[2024-07-24 10:30:37,130][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[31232],
        [41433],
        [43583],
        [43567],
        [43726],
        [44118],
        [44037],
        [44152],
        [44330],
        [44429],
        [44435],
        [44499],
        [44510],
        [44654]], device='cuda:0')
[2024-07-24 10:30:37,131][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[13200],
        [ 3561],
        [ 3932],
        [ 3265],
        [ 3211],
        [ 3389],
        [ 3904],
        [ 5222],
        [ 5346],
        [ 5256],
        [ 5694],
        [ 5662],
        [ 5649],
        [ 5486]], device='cuda:0')
[2024-07-24 10:30:37,133][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[22881],
        [22371],
        [34763],
        [36734],
        [33348],
        [36632],
        [43422],
        [41774],
        [40092],
        [39827],
        [38957],
        [38220],
        [39414],
        [38647]], device='cuda:0')
[2024-07-24 10:30:37,134][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 1315],
        [29520],
        [10416],
        [19043],
        [ 9997],
        [ 8331],
        [ 5517],
        [ 6626],
        [ 6481],
        [ 5695],
        [ 6004],
        [ 6888],
        [14184],
        [13967]], device='cuda:0')
[2024-07-24 10:30:37,136][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[20849],
        [28859],
        [15330],
        [39690],
        [26915],
        [22272],
        [24064],
        [22302],
        [21249],
        [23810],
        [21328],
        [22406],
        [24344],
        [19933]], device='cuda:0')
[2024-07-24 10:30:37,139][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[15772],
        [34352],
        [21180],
        [28084],
        [25504],
        [26018],
        [35773],
        [43549],
        [43293],
        [33292],
        [33063],
        [32373],
        [29699],
        [27571]], device='cuda:0')
[2024-07-24 10:30:37,142][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[47282],
        [46392],
        [46068],
        [45693],
        [45553],
        [45519],
        [45562],
        [45693],
        [45793],
        [45568],
        [45597],
        [45641],
        [45692],
        [45805]], device='cuda:0')
[2024-07-24 10:30:37,143][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[16926],
        [17167],
        [15944],
        [21426],
        [23166],
        [22226],
        [21824],
        [21997],
        [22093],
        [22230],
        [22886],
        [22137],
        [22166],
        [21930]], device='cuda:0')
[2024-07-24 10:30:37,143][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[40645],
        [40514],
        [40384],
        [40466],
        [38791],
        [38153],
        [38912],
        [38511],
        [37328],
        [38600],
        [36782],
        [37683],
        [37213],
        [35379]], device='cuda:0')
[2024-07-24 10:30:37,145][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[13969],
        [24523],
        [38564],
        [39931],
        [38640],
        [39873],
        [38811],
        [43578],
        [42951],
        [36905],
        [40304],
        [41766],
        [43445],
        [42966]], device='cuda:0')
[2024-07-24 10:30:37,146][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[24834],
        [24904],
        [27259],
        [30981],
        [28308],
        [31235],
        [31204],
        [37698],
        [41101],
        [39793],
        [33927],
        [43852],
        [43315],
        [38371]], device='cuda:0')
[2024-07-24 10:30:37,147][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[46287],
        [46287],
        [15302],
        [19773],
        [37704],
        [17817],
        [ 9128],
        [38081],
        [38059],
        [29985],
        [29985],
        [29985],
        [31828],
        [32023]], device='cuda:0')
[2024-07-24 10:30:37,149][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[37437],
        [36314],
        [23539],
        [29113],
        [32545],
        [31103],
        [29093],
        [31004],
        [29197],
        [33965],
        [38679],
        [27040],
        [33496],
        [37743]], device='cuda:0')
[2024-07-24 10:30:37,150][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[17614],
        [23469],
        [12003],
        [14308],
        [17918],
        [22524],
        [11598],
        [10782],
        [10389],
        [23440],
        [23777],
        [22158],
        [17659],
        [17162]], device='cuda:0')
[2024-07-24 10:30:37,153][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[44283],
        [36137],
        [30751],
        [33778],
        [33890],
        [32900],
        [35321],
        [34903],
        [34191],
        [36073],
        [35451],
        [35260],
        [36108],
        [34324]], device='cuda:0')
[2024-07-24 10:30:37,155][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[36069],
        [39076],
        [36306],
        [38033],
        [38970],
        [39212],
        [39525],
        [37813],
        [38002],
        [39591],
        [38472],
        [38282],
        [38847],
        [39167]], device='cuda:0')
[2024-07-24 10:30:37,158][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[34237],
        [31481],
        [34585],
        [37499],
        [36957],
        [37340],
        [35175],
        [38298],
        [38406],
        [35184],
        [34654],
        [35560],
        [39210],
        [40299]], device='cuda:0')
[2024-07-24 10:30:37,159][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[44644],
        [44641],
        [44641],
        [44561],
        [44594],
        [44594],
        [44610],
        [44601],
        [44536],
        [44561],
        [44529],
        [44288],
        [44260],
        [44500]], device='cuda:0')
[2024-07-24 10:30:37,160][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[37514],
        [43303],
        [41478],
        [47491],
        [44990],
        [44052],
        [35573],
        [41186],
        [42414],
        [33867],
        [33180],
        [34611],
        [30851],
        [29869]], device='cuda:0')
[2024-07-24 10:30:37,161][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[43639],
        [45739],
        [37191],
        [37282],
        [42943],
        [42852],
        [42851],
        [42413],
        [42000],
        [40668],
        [40667],
        [39748],
        [39871],
        [39434]], device='cuda:0')
[2024-07-24 10:30:37,162][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[27280],
        [26609],
        [19953],
        [19034],
        [19187],
        [17990],
        [16608],
        [17007],
        [16500],
        [17583],
        [17194],
        [16828],
        [16111],
        [14573]], device='cuda:0')
[2024-07-24 10:30:37,163][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[44584],
        [44573],
        [44205],
        [43307],
        [43373],
        [41690],
        [43739],
        [44180],
        [44110],
        [44651],
        [44172],
        [43338],
        [44432],
        [43979]], device='cuda:0')
[2024-07-24 10:30:37,166][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[41734],
        [34172],
        [32088],
        [31206],
        [34033],
        [31888],
        [24941],
        [29672],
        [27869],
        [18905],
        [18194],
        [20259],
        [24123],
        [24733]], device='cuda:0')
[2024-07-24 10:30:37,167][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[42488],
        [42473],
        [41691],
        [39669],
        [41068],
        [39426],
        [39541],
        [36444],
        [33877],
        [34744],
        [38379],
        [31591],
        [33103],
        [36298]], device='cuda:0')
[2024-07-24 10:30:37,169][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[43153],
        [43152],
        [29005],
        [27225],
        [26852],
        [28782],
        [35090],
        [13736],
        [13730],
        [39885],
        [39886],
        [39885],
        [19996],
        [17648]], device='cuda:0')
[2024-07-24 10:30:37,172][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 4443],
        [ 5283],
        [10841],
        [ 9063],
        [ 8275],
        [ 9079],
        [11353],
        [12014],
        [13047],
        [11383],
        [10681],
        [12434],
        [13567],
        [12594]], device='cuda:0')
[2024-07-24 10:30:37,174][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[20923],
        [ 5914],
        [14944],
        [19745],
        [ 9210],
        [12132],
        [13312],
        [ 9105],
        [15945],
        [17882],
        [13307],
        [25728],
        [22582],
        [16323]], device='cuda:0')
[2024-07-24 10:30:37,175][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442],
        [31442]], device='cuda:0')
[2024-07-24 10:30:37,217][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:37,219][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,221][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,224][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,227][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,227][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,228][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,228][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,228][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,228][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,229][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,229][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,229][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,230][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([9.9914e-01, 8.6299e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,231][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0409, 0.9591], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,231][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.7924, 0.2076], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,231][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.9040, 0.0960], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,232][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0430, 0.9570], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,232][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.9839, 0.0161], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,233][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.9833, 0.0167], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,233][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.5265, 0.4735], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,233][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.8901, 0.1099], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,234][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.1448, 0.8552], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,234][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([9.9997e-01, 2.5197e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,234][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.9929, 0.0071], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,235][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.8101e-01, 8.0713e-04, 1.8184e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,235][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0187, 0.6841, 0.2973], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,235][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.8298, 0.0511, 0.1191], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,236][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.9757, 0.0173, 0.0070], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,236][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0168, 0.7370, 0.2462], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,236][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.9920, 0.0037, 0.0043], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,237][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9897, 0.0086, 0.0017], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,237][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.7532, 0.0454, 0.2015], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,237][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.8439, 0.0677, 0.0884], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,238][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0863, 0.4202, 0.4935], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,238][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ and] are: tensor([9.9947e-01, 4.2867e-05, 4.8434e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,238][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.9456, 0.0013, 0.0531], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,240][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.7885, 0.0115, 0.1100, 0.0900], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,243][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.0121, 0.4933, 0.2397, 0.2549], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,247][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.1939, 0.1472, 0.4144, 0.2445], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,248][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.7773, 0.0839, 0.0930, 0.0459], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,249][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0030, 0.2486, 0.1414, 0.6070], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,249][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.7780, 0.0337, 0.1126, 0.0757], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,250][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.0261, 0.0015, 0.9714, 0.0011], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,250][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.4105, 0.0757, 0.2277, 0.2862], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,250][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.3168, 0.1036, 0.3526, 0.2270], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,251][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.0627, 0.2976, 0.3343, 0.3054], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,251][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([9.8902e-01, 5.7071e-04, 3.7299e-03, 6.6801e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,251][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.8422, 0.0094, 0.0917, 0.0567], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,253][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.9301, 0.0017, 0.0239, 0.0384, 0.0059], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,256][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0124, 0.3227, 0.1919, 0.2514, 0.2215], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,260][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.3552, 0.0533, 0.2519, 0.2719, 0.0677], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,262][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.7281, 0.0649, 0.0516, 0.0707, 0.0847], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,262][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.0029, 0.2131, 0.0846, 0.4395, 0.2599], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,263][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.9178, 0.0123, 0.0212, 0.0410, 0.0076], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,263][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.5277, 0.0072, 0.3335, 0.1304, 0.0012], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,264][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.5530, 0.0211, 0.1066, 0.1544, 0.1650], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,264][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.4074, 0.0330, 0.2008, 0.2586, 0.1002], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,264][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.0416, 0.2049, 0.2386, 0.2488, 0.2662], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,265][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ went] are: tensor([9.9888e-01, 2.6583e-05, 3.4122e-04, 6.5578e-04, 9.5175e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,266][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.9204, 0.0016, 0.0426, 0.0184, 0.0169], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,270][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.9338, 0.0009, 0.0196, 0.0310, 0.0067, 0.0079], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,273][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0097, 0.2864, 0.1424, 0.1993, 0.2406, 0.1217], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,277][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1928, 0.0980, 0.2911, 0.2890, 0.0975, 0.0315], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,278][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.7276, 0.0317, 0.0206, 0.0225, 0.1149, 0.0827], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,278][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0004, 0.1913, 0.0775, 0.3582, 0.3364, 0.0361], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,278][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.7921, 0.0253, 0.0378, 0.1047, 0.0325, 0.0075], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,279][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1571, 0.0739, 0.0094, 0.7441, 0.0138, 0.0017], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,279][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1159, 0.0427, 0.1697, 0.1482, 0.3720, 0.1515], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,279][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2343, 0.0749, 0.1741, 0.3065, 0.1730, 0.0372], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,280][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0328, 0.1587, 0.1939, 0.1950, 0.2207, 0.1989], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,280][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.9828e-01, 3.2512e-05, 3.8557e-04, 8.0788e-04, 2.4303e-04, 2.5121e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,281][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.9183, 0.0013, 0.0386, 0.0146, 0.0107, 0.0164], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,283][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.4783e-01, 6.2520e-04, 1.3692e-02, 2.0788e-02, 2.8859e-03, 5.9949e-03,
        8.1863e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,286][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0076, 0.2330, 0.1330, 0.1719, 0.2351, 0.1349, 0.0845],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,289][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.5135, 0.0178, 0.0806, 0.1505, 0.0295, 0.0166, 0.1916],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,291][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.7149, 0.0361, 0.0149, 0.0292, 0.0734, 0.0721, 0.0594],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,291][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0038, 0.0573, 0.0296, 0.1388, 0.1870, 0.0167, 0.5668],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,292][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.8958, 0.0074, 0.0160, 0.0579, 0.0085, 0.0054, 0.0088],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,292][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2021, 0.0560, 0.0127, 0.6555, 0.0226, 0.0498, 0.0014],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,293][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.4112, 0.0143, 0.0806, 0.1003, 0.1337, 0.0704, 0.1895],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,293][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.6510, 0.0130, 0.0601, 0.1552, 0.0345, 0.0112, 0.0751],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,293][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0310, 0.1331, 0.1558, 0.1553, 0.1791, 0.1604, 0.1852],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,294][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([9.9834e-01, 3.1242e-05, 3.6752e-04, 7.6161e-04, 1.1232e-04, 1.6488e-04,
        2.2572e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,294][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.8803, 0.0017, 0.0448, 0.0164, 0.0076, 0.0096, 0.0396],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,295][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ office] are: tensor([9.1695e-01, 8.9644e-04, 1.2376e-02, 3.5878e-02, 4.0204e-03, 8.8175e-03,
        1.0471e-02, 1.0596e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,298][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0059, 0.1887, 0.1060, 0.1363, 0.1923, 0.0999, 0.0749, 0.1959],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,301][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.5353, 0.0138, 0.0376, 0.0893, 0.0146, 0.0111, 0.1185, 0.1797],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,305][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.5408, 0.0320, 0.0209, 0.0286, 0.0839, 0.1711, 0.1143, 0.0084],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,305][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.0049, 0.0325, 0.0129, 0.1078, 0.0869, 0.0138, 0.5729, 0.1683],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,306][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.8855, 0.0055, 0.0145, 0.0466, 0.0088, 0.0041, 0.0140, 0.0209],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,306][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.6701, 0.0036, 0.0376, 0.0609, 0.0040, 0.0948, 0.1250, 0.0040],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,306][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.5435, 0.0125, 0.0336, 0.1141, 0.0639, 0.0311, 0.0990, 0.1022],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,307][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.5333, 0.0149, 0.0601, 0.1495, 0.0544, 0.0141, 0.1228, 0.0509],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,307][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.0233, 0.1157, 0.1337, 0.1412, 0.1549, 0.1349, 0.1643, 0.1319],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,308][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ office] are: tensor([9.9818e-01, 2.8359e-05, 2.9550e-04, 4.5738e-04, 1.0958e-04, 1.4900e-04,
        2.6548e-04, 5.1842e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,308][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ office] are: tensor([9.3331e-01, 6.7305e-04, 1.3585e-02, 8.8621e-03, 3.0909e-03, 4.5382e-03,
        1.5121e-02, 2.0825e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,311][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.8542, 0.0020, 0.0188, 0.0423, 0.0081, 0.0131, 0.0171, 0.0164, 0.0280],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,314][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0067, 0.1817, 0.0954, 0.1381, 0.1647, 0.0958, 0.0751, 0.1797, 0.0627],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,318][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2311, 0.0230, 0.0588, 0.1220, 0.0269, 0.0204, 0.1655, 0.2877, 0.0647],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,319][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.6485, 0.0200, 0.0099, 0.0347, 0.0974, 0.0702, 0.0952, 0.0185, 0.0056],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,319][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [,] are: tensor([2.2405e-04, 5.4402e-02, 1.1472e-02, 9.1994e-02, 1.1406e-01, 1.0694e-02,
        4.2512e-01, 2.8047e-01, 1.1570e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,320][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.8477, 0.0113, 0.0211, 0.0551, 0.0117, 0.0067, 0.0089, 0.0225, 0.0149],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,320][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [,] are: tensor([1.1088e-01, 5.8233e-03, 5.4019e-04, 2.7198e-01, 9.3464e-03, 3.1719e-03,
        5.0049e-01, 9.7619e-02, 1.4628e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,320][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1953, 0.0221, 0.0792, 0.1235, 0.1235, 0.0679, 0.1685, 0.1546, 0.0655],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,321][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.3933, 0.0266, 0.0769, 0.1777, 0.0618, 0.0193, 0.1042, 0.0720, 0.0682],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,321][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0202, 0.0965, 0.1133, 0.1199, 0.1347, 0.1200, 0.1386, 0.1265, 0.1302],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,322][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [,] are: tensor([9.9310e-01, 1.2300e-04, 1.1076e-03, 1.6336e-03, 3.7376e-04, 4.8947e-04,
        7.6178e-04, 9.2885e-04, 1.4846e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,323][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.8256, 0.0020, 0.0381, 0.0164, 0.0083, 0.0109, 0.0333, 0.0242, 0.0413],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,326][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.8906, 0.0009, 0.0187, 0.0269, 0.0061, 0.0106, 0.0118, 0.0094, 0.0175,
        0.0074], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,330][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0063, 0.1367, 0.0808, 0.1135, 0.1293, 0.0815, 0.0660, 0.1702, 0.0832,
        0.1325], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,332][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.3628, 0.0027, 0.0288, 0.0362, 0.0089, 0.0071, 0.0715, 0.1138, 0.0328,
        0.3353], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,333][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.2385, 0.0411, 0.0503, 0.0441, 0.1470, 0.2121, 0.1735, 0.0281, 0.0274,
        0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,333][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0098, 0.0043, 0.0035, 0.0166, 0.0137, 0.0024, 0.0948, 0.0272, 0.0027,
        0.8251], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,334][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.9502, 0.0015, 0.0039, 0.0188, 0.0042, 0.0013, 0.0026, 0.0060, 0.0068,
        0.0048], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,334][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([2.7885e-03, 9.6312e-05, 2.9049e-01, 4.4326e-03, 6.8508e-05, 5.9738e-02,
        5.8465e-01, 4.3253e-03, 5.3337e-02, 8.2061e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,334][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.4953, 0.0043, 0.0241, 0.0736, 0.0303, 0.0147, 0.0500, 0.0648, 0.0239,
        0.2189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,335][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.6528, 0.0028, 0.0333, 0.0582, 0.0171, 0.0049, 0.0340, 0.0148, 0.0205,
        0.1616], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,335][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.0169, 0.0844, 0.1023, 0.1138, 0.1203, 0.1048, 0.1253, 0.1171, 0.1222,
        0.0929], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,336][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([9.9886e-01, 1.1260e-05, 1.5738e-04, 2.8178e-04, 4.2345e-05, 6.5310e-05,
        1.0104e-04, 1.3379e-04, 1.9209e-04, 1.5865e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,339][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.8711, 0.0011, 0.0284, 0.0117, 0.0055, 0.0070, 0.0239, 0.0133, 0.0206,
        0.0174], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,342][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.8797, 0.0010, 0.0103, 0.0322, 0.0040, 0.0057, 0.0090, 0.0089, 0.0175,
        0.0082, 0.0236], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,346][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0070, 0.1445, 0.0803, 0.1051, 0.1011, 0.0739, 0.0619, 0.1468, 0.0621,
        0.1279, 0.0894], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,347][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0966, 0.0060, 0.0233, 0.0391, 0.0084, 0.0056, 0.0711, 0.1136, 0.0268,
        0.5788, 0.0308], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,347][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.5164, 0.0286, 0.0172, 0.0238, 0.0527, 0.0876, 0.1632, 0.0123, 0.0169,
        0.0344, 0.0468], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,347][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([6.4119e-05, 5.2400e-03, 1.0808e-03, 9.4926e-03, 5.1140e-03, 6.6511e-04,
        2.1401e-02, 2.3286e-02, 1.1300e-03, 9.3156e-01, 9.6362e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,348][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.7948, 0.0090, 0.0126, 0.0450, 0.0135, 0.0044, 0.0074, 0.0184, 0.0124,
        0.0557, 0.0269], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,348][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.2697, 0.0048, 0.1301, 0.0280, 0.0007, 0.1550, 0.3021, 0.0233, 0.0782,
        0.0065, 0.0016], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,349][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.3951, 0.0071, 0.0316, 0.0644, 0.0416, 0.0217, 0.0690, 0.1174, 0.0330,
        0.1898, 0.0292], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,349][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.2457, 0.0067, 0.0155, 0.0459, 0.0130, 0.0034, 0.0261, 0.0143, 0.0203,
        0.5762, 0.0328], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,351][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0176, 0.0792, 0.0896, 0.0936, 0.1082, 0.0979, 0.1121, 0.1030, 0.1079,
        0.0912, 0.0997], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,353][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([9.9747e-01, 3.7551e-05, 2.8986e-04, 5.7932e-04, 7.4075e-05, 1.0268e-04,
        1.8756e-04, 4.0721e-04, 3.6459e-04, 2.9623e-04, 1.9343e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,355][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([8.9128e-01, 8.3857e-04, 2.1241e-02, 7.5542e-03, 4.1329e-03, 5.3960e-03,
        1.8902e-02, 1.1219e-02, 1.8941e-02, 1.1300e-02, 9.1984e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,358][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.7735, 0.0028, 0.0225, 0.0335, 0.0094, 0.0124, 0.0147, 0.0180, 0.0288,
        0.0125, 0.0340, 0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,360][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0059, 0.1417, 0.0803, 0.0836, 0.1140, 0.0801, 0.0550, 0.1262, 0.0552,
        0.1188, 0.0961, 0.0429], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,360][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1669, 0.0083, 0.0255, 0.0538, 0.0112, 0.0087, 0.0646, 0.0943, 0.0281,
        0.3432, 0.0407, 0.1546], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,361][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.6002, 0.0135, 0.0129, 0.0175, 0.0506, 0.0593, 0.0575, 0.0139, 0.0093,
        0.0253, 0.0771, 0.0630], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,361][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([3.1242e-05, 3.4531e-03, 8.1473e-04, 5.7745e-03, 1.2018e-02, 8.8842e-04,
        2.6142e-02, 2.0549e-02, 1.0461e-03, 9.2368e-01, 1.5682e-03, 4.0383e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,362][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.5082, 0.0236, 0.0463, 0.0708, 0.0291, 0.0149, 0.0259, 0.0507, 0.0334,
        0.0522, 0.0794, 0.0655], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,362][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.4374, 0.0224, 0.0161, 0.3416, 0.0097, 0.0716, 0.0039, 0.0409, 0.0136,
        0.0282, 0.0120, 0.0025], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,362][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1216, 0.0153, 0.0463, 0.0655, 0.0835, 0.0415, 0.0980, 0.1020, 0.0439,
        0.2285, 0.0697, 0.0843], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,363][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2759, 0.0128, 0.0361, 0.0587, 0.0216, 0.0069, 0.0364, 0.0332, 0.0318,
        0.2871, 0.0802, 0.1194], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,365][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0169, 0.0713, 0.0834, 0.0841, 0.0981, 0.0864, 0.0977, 0.0907, 0.0959,
        0.0825, 0.0953, 0.0978], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,367][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([9.8202e-01, 1.9322e-04, 1.4957e-03, 2.5759e-03, 6.3896e-04, 7.2044e-04,
        1.0386e-03, 1.3066e-03, 1.8864e-03, 1.5205e-03, 7.8686e-04, 5.8139e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,370][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.6954, 0.0032, 0.0403, 0.0190, 0.0091, 0.0119, 0.0357, 0.0311, 0.0337,
        0.0219, 0.0178, 0.0808], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,374][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.3737, 0.0034, 0.0563, 0.0427, 0.0250, 0.0538, 0.0496, 0.0249, 0.0816,
        0.0270, 0.0757, 0.0934, 0.0930], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,374][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0025, 0.1197, 0.0623, 0.0715, 0.0902, 0.0536, 0.0491, 0.1233, 0.0548,
        0.1133, 0.0937, 0.0574, 0.1085], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,374][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0578, 0.0041, 0.0222, 0.0237, 0.0062, 0.0055, 0.0597, 0.0929, 0.0209,
        0.3722, 0.0264, 0.1498, 0.1585], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,375][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.2497, 0.0248, 0.0217, 0.0221, 0.0673, 0.1272, 0.1342, 0.0105, 0.0141,
        0.0273, 0.0975, 0.1822, 0.0214], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,375][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([8.2294e-05, 3.7295e-03, 1.7858e-03, 1.3215e-02, 1.0425e-02, 1.3129e-03,
        6.1647e-02, 2.6449e-02, 1.1339e-03, 8.4858e-01, 1.0665e-03, 8.6585e-03,
        2.1914e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,376][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.8070, 0.0036, 0.0068, 0.0282, 0.0050, 0.0027, 0.0058, 0.0086, 0.0118,
        0.0228, 0.0224, 0.0357, 0.0397], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,376][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0104, 0.0012, 0.2248, 0.0022, 0.0004, 0.2059, 0.3074, 0.0082, 0.1311,
        0.0019, 0.0055, 0.1005, 0.0005], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,377][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0836, 0.0072, 0.0346, 0.0541, 0.0404, 0.0233, 0.0918, 0.0710, 0.0331,
        0.3161, 0.0309, 0.0897, 0.1242], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,378][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.1064, 0.0049, 0.0365, 0.0382, 0.0219, 0.0075, 0.0584, 0.0204, 0.0294,
        0.3908, 0.0670, 0.1459, 0.0727], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,381][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0146, 0.0672, 0.0750, 0.0791, 0.0886, 0.0787, 0.0914, 0.0851, 0.0861,
        0.0764, 0.0841, 0.0926, 0.0810], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,383][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([9.9480e-01, 3.7116e-05, 4.5619e-04, 5.4929e-04, 1.2382e-04, 1.6713e-04,
        2.6036e-04, 2.0603e-04, 3.7450e-04, 4.7735e-04, 1.2536e-04, 2.1851e-03,
        2.3838e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,387][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.6456, 0.0034, 0.0420, 0.0232, 0.0100, 0.0125, 0.0378, 0.0351, 0.0332,
        0.0348, 0.0196, 0.0729, 0.0296], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,388][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([9.1206e-01, 3.7045e-04, 2.7624e-03, 1.1548e-02, 1.7015e-03, 1.6253e-03,
        3.1147e-03, 4.5481e-03, 8.4085e-03, 5.3732e-03, 8.0458e-03, 1.4794e-02,
        2.1999e-02, 3.6464e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,388][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0043, 0.1239, 0.0607, 0.0822, 0.0974, 0.0508, 0.0547, 0.1009, 0.0381,
        0.1027, 0.0825, 0.0512, 0.0992, 0.0513], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,389][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1703, 0.0019, 0.0112, 0.0239, 0.0031, 0.0011, 0.0272, 0.0525, 0.0126,
        0.3319, 0.0183, 0.1270, 0.1764, 0.0427], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,389][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.5203, 0.0137, 0.0103, 0.0121, 0.0652, 0.0330, 0.0399, 0.0140, 0.0094,
        0.0194, 0.0717, 0.1160, 0.0137, 0.0613], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,390][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.7819e-05, 2.0475e-03, 7.1069e-04, 5.6038e-03, 5.4960e-03, 3.6258e-04,
        1.9276e-02, 1.3001e-02, 7.2828e-04, 9.1642e-01, 1.1289e-03, 5.2370e-03,
        2.5922e-02, 3.9736e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,390][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.5826, 0.0070, 0.0147, 0.0470, 0.0136, 0.0028, 0.0091, 0.0323, 0.0165,
        0.0537, 0.0582, 0.0503, 0.1041, 0.0079], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,391][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0644, 0.0197, 0.0020, 0.2499, 0.0043, 0.0005, 0.3681, 0.1054, 0.0071,
        0.0515, 0.0045, 0.1132, 0.0087, 0.0008], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,394][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.2012, 0.0034, 0.0266, 0.0420, 0.0366, 0.0152, 0.0780, 0.0600, 0.0339,
        0.2079, 0.0355, 0.0815, 0.1060, 0.0723], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,397][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2454, 0.0043, 0.0153, 0.0412, 0.0101, 0.0018, 0.0225, 0.0214, 0.0200,
        0.3595, 0.0519, 0.1195, 0.0686, 0.0187], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,401][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0136, 0.0586, 0.0685, 0.0701, 0.0819, 0.0707, 0.0801, 0.0770, 0.0796,
        0.0667, 0.0786, 0.0809, 0.0879, 0.0858], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,402][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.9607e-01, 1.6318e-05, 2.1929e-04, 4.0657e-04, 1.7447e-04, 1.4270e-04,
        1.8429e-04, 1.5733e-04, 4.5931e-04, 2.9265e-04, 1.3590e-04, 1.6027e-03,
        5.8723e-05, 8.3427e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,402][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([8.8412e-01, 4.9224e-04, 1.3380e-02, 6.1787e-03, 2.6030e-03, 3.9264e-03,
        1.1552e-02, 8.3847e-03, 1.3264e-02, 7.1053e-03, 4.5063e-03, 3.1526e-02,
        5.4666e-03, 7.4978e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,443][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:37,445][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,446][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,446][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,446][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,447][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,447][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,447][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,448][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,448][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,448][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,449][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,449][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,449][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([9.9914e-01, 8.6299e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,450][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.9908, 0.0092], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,450][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.7924, 0.2076], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,450][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.4717, 0.5283], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,451][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.2266, 0.7734], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,451][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.9839, 0.0161], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,451][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.6406, 0.3594], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,452][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.5265, 0.4735], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,455][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.9362, 0.0638], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,457][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([9.9991e-01, 8.8946e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,457][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([9.9997e-01, 2.5197e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,457][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.9929, 0.0071], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,457][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([9.8101e-01, 8.0713e-04, 1.8184e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,458][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9934, 0.0016, 0.0049], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,458][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.8298, 0.0511, 0.1191], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,459][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5000, 0.1079, 0.3921], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,461][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.3624, 0.1020, 0.5356], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,461][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9920, 0.0037, 0.0043], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,462][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2648, 0.1807, 0.5546], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,462][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.7532, 0.0454, 0.2015], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,462][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.9166, 0.0144, 0.0690], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,463][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([9.9859e-01, 2.9885e-04, 1.1074e-03], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,463][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([9.9947e-01, 4.2867e-05, 4.8434e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,463][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.9456, 0.0013, 0.0531], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,464][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.7885, 0.0115, 0.1100, 0.0900], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,465][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.3881, 0.0459, 0.3070, 0.2590], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,468][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.1939, 0.1472, 0.4144, 0.2445], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,468][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.1472, 0.1567, 0.3932, 0.3029], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,469][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.0360, 0.1433, 0.5088, 0.3119], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,469][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.7780, 0.0337, 0.1126, 0.0757], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,469][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.1014, 0.1680, 0.4471, 0.2835], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,470][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.4105, 0.0757, 0.2277, 0.2862], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,470][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.3630, 0.0433, 0.3694, 0.2243], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,470][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.9821, 0.0020, 0.0056, 0.0103], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,471][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([9.8902e-01, 5.7071e-04, 3.7299e-03, 6.6801e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,471][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.8422, 0.0094, 0.0917, 0.0567], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,471][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.9301, 0.0017, 0.0239, 0.0384, 0.0059], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,472][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.7112, 0.0101, 0.0440, 0.1487, 0.0860], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,472][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.3552, 0.0533, 0.2519, 0.2719, 0.0677], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,473][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.1641, 0.0949, 0.3262, 0.2530, 0.1618], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,473][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.0514, 0.0465, 0.3055, 0.2837, 0.3130], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,473][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.9178, 0.0123, 0.0212, 0.0410, 0.0076], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,474][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0280, 0.0502, 0.4006, 0.1724, 0.3488], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,478][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.5530, 0.0211, 0.1066, 0.1544, 0.1650], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,480][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.5316, 0.0098, 0.1664, 0.2217, 0.0704], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,481][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([9.9418e-01, 2.3340e-04, 1.4187e-03, 3.7147e-03, 4.5042e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,481][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([9.9888e-01, 2.6583e-05, 3.4122e-04, 6.5578e-04, 9.5175e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,481][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.9204, 0.0016, 0.0426, 0.0184, 0.0169], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,482][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.9338, 0.0009, 0.0196, 0.0310, 0.0067, 0.0079], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,482][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.4591, 0.0152, 0.0341, 0.1475, 0.2021, 0.1420], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,482][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1928, 0.0980, 0.2911, 0.2890, 0.0975, 0.0315], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,483][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0852, 0.0935, 0.2870, 0.1946, 0.2178, 0.1220], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,483][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0108, 0.0473, 0.2488, 0.1787, 0.4168, 0.0976], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,485][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.7921, 0.0253, 0.0378, 0.1047, 0.0325, 0.0075], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,488][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0066, 0.0348, 0.2657, 0.1038, 0.4941, 0.0950], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,492][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1159, 0.0427, 0.1697, 0.1482, 0.3720, 0.1515], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,494][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.3669, 0.0198, 0.1661, 0.2733, 0.1201, 0.0538], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,494][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.8381e-01, 6.7607e-04, 3.5632e-03, 9.1766e-03, 1.7346e-03, 1.0347e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,495][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.9828e-01, 3.2512e-05, 3.8557e-04, 8.0788e-04, 2.4303e-04, 2.5121e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,495][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.9183, 0.0013, 0.0386, 0.0146, 0.0107, 0.0164], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,495][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([9.4783e-01, 6.2520e-04, 1.3692e-02, 2.0788e-02, 2.8859e-03, 5.9949e-03,
        8.1863e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,496][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.8187, 0.0029, 0.0141, 0.0680, 0.0244, 0.0380, 0.0339],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,496][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.5135, 0.0178, 0.0806, 0.1505, 0.0295, 0.0166, 0.1916],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,497][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2945, 0.0351, 0.1533, 0.1722, 0.0991, 0.0660, 0.1797],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,497][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0698, 0.0096, 0.0789, 0.1293, 0.1307, 0.0357, 0.5460],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,499][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.8958, 0.0074, 0.0160, 0.0579, 0.0085, 0.0054, 0.0088],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,501][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0402, 0.0196, 0.1608, 0.1435, 0.2152, 0.0683, 0.3525],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,505][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.4112, 0.0143, 0.0806, 0.1003, 0.1337, 0.0704, 0.1895],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,508][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.7044, 0.0034, 0.0464, 0.1155, 0.0219, 0.0143, 0.0941],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,508][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([9.8961e-01, 3.1379e-04, 1.5485e-03, 6.0552e-03, 7.1943e-04, 5.5219e-04,
        1.1981e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,508][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([9.9834e-01, 3.1242e-05, 3.6752e-04, 7.6161e-04, 1.1232e-04, 1.6488e-04,
        2.2572e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,509][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.8803, 0.0017, 0.0448, 0.0164, 0.0076, 0.0096, 0.0396],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,509][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([9.1695e-01, 8.9644e-04, 1.2376e-02, 3.5878e-02, 4.0204e-03, 8.8175e-03,
        1.0471e-02, 1.0596e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,510][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.4966, 0.0062, 0.0546, 0.0962, 0.0741, 0.1037, 0.1052, 0.0634],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,510][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.5353, 0.0138, 0.0376, 0.0893, 0.0146, 0.0111, 0.1185, 0.1797],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,510][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.3171, 0.0207, 0.0875, 0.1342, 0.0600, 0.0433, 0.1532, 0.1840],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,512][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.0803, 0.0057, 0.0409, 0.0786, 0.0646, 0.0253, 0.4038, 0.3007],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,515][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.8855, 0.0055, 0.0145, 0.0466, 0.0088, 0.0041, 0.0140, 0.0209],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,519][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.0303, 0.0120, 0.0681, 0.0589, 0.0909, 0.0318, 0.1898, 0.5182],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,521][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.5435, 0.0125, 0.0336, 0.1141, 0.0639, 0.0311, 0.0990, 0.1022],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,522][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.5742, 0.0044, 0.0480, 0.1201, 0.0346, 0.0177, 0.1423, 0.0588],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,522][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([9.8997e-01, 3.1125e-04, 1.2649e-03, 5.3243e-03, 4.7615e-04, 5.1885e-04,
        1.1183e-03, 1.0201e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,522][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([9.9818e-01, 2.8359e-05, 2.9550e-04, 4.5738e-04, 1.0958e-04, 1.4900e-04,
        2.6548e-04, 5.1842e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,523][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([9.3331e-01, 6.7305e-04, 1.3585e-02, 8.8621e-03, 3.0909e-03, 4.5382e-03,
        1.5121e-02, 2.0825e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,523][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.8542, 0.0020, 0.0188, 0.0423, 0.0081, 0.0131, 0.0171, 0.0164, 0.0280],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,524][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.7814, 0.0061, 0.0102, 0.0726, 0.0233, 0.0189, 0.0193, 0.0267, 0.0414],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,524][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2311, 0.0230, 0.0588, 0.1220, 0.0269, 0.0204, 0.1655, 0.2877, 0.0647],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,525][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1595, 0.0319, 0.0962, 0.1294, 0.0743, 0.0627, 0.1574, 0.1667, 0.1217],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,528][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0097, 0.0100, 0.0451, 0.0548, 0.1006, 0.0299, 0.3209, 0.3708, 0.0582],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,531][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.8477, 0.0113, 0.0211, 0.0551, 0.0117, 0.0067, 0.0089, 0.0225, 0.0149],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,535][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0081, 0.0170, 0.0768, 0.0536, 0.1114, 0.0387, 0.1383, 0.5195, 0.0364],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,535][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1953, 0.0221, 0.0792, 0.1235, 0.1235, 0.0679, 0.1685, 0.1546, 0.0655],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,536][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.4464, 0.0077, 0.0612, 0.1363, 0.0431, 0.0255, 0.1305, 0.0761, 0.0731],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,536][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([9.7175e-01, 8.1976e-04, 3.1821e-03, 1.1323e-02, 1.3382e-03, 1.0869e-03,
        2.3433e-03, 4.4436e-03, 3.7086e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,537][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([9.9310e-01, 1.2300e-04, 1.1076e-03, 1.6336e-03, 3.7376e-04, 4.8947e-04,
        7.6178e-04, 9.2885e-04, 1.4846e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,537][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.8256, 0.0020, 0.0381, 0.0164, 0.0083, 0.0109, 0.0333, 0.0242, 0.0413],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,537][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.8906, 0.0009, 0.0187, 0.0269, 0.0061, 0.0106, 0.0118, 0.0094, 0.0175,
        0.0074], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,538][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.4547, 0.0042, 0.0444, 0.0747, 0.0733, 0.1015, 0.0638, 0.0292, 0.1028,
        0.0514], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,540][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.3628, 0.0027, 0.0288, 0.0362, 0.0089, 0.0071, 0.0715, 0.1138, 0.0328,
        0.3353], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,542][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.3819, 0.0078, 0.0583, 0.0686, 0.0327, 0.0205, 0.0785, 0.0793, 0.0589,
        0.2135], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,546][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.0354, 0.0011, 0.0140, 0.0213, 0.0166, 0.0062, 0.0876, 0.0802, 0.0208,
        0.7168], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,549][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.9502, 0.0015, 0.0039, 0.0188, 0.0042, 0.0013, 0.0026, 0.0060, 0.0068,
        0.0048], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,549][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0229, 0.0039, 0.0428, 0.0506, 0.0341, 0.0156, 0.0894, 0.3558, 0.0316,
        0.3532], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,549][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.4953, 0.0043, 0.0241, 0.0736, 0.0303, 0.0147, 0.0500, 0.0648, 0.0239,
        0.2189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,550][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.6510, 0.0010, 0.0299, 0.0507, 0.0125, 0.0075, 0.0471, 0.0179, 0.0299,
        0.1525], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,550][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([9.9017e-01, 1.3331e-04, 7.9834e-04, 4.7952e-03, 3.5142e-04, 3.3817e-04,
        6.2084e-04, 9.4261e-04, 1.2536e-03, 5.9488e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,551][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([9.9886e-01, 1.1260e-05, 1.5738e-04, 2.8178e-04, 4.2345e-05, 6.5310e-05,
        1.0104e-04, 1.3379e-04, 1.9209e-04, 1.5865e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,551][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.8711, 0.0011, 0.0284, 0.0117, 0.0055, 0.0070, 0.0239, 0.0133, 0.0206,
        0.0174], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,552][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.8797, 0.0010, 0.0103, 0.0322, 0.0040, 0.0057, 0.0090, 0.0089, 0.0175,
        0.0082, 0.0236], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,553][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.5959, 0.0122, 0.0125, 0.0650, 0.0352, 0.0245, 0.0216, 0.0191, 0.0659,
        0.1050, 0.0431], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,556][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0966, 0.0060, 0.0233, 0.0391, 0.0084, 0.0056, 0.0711, 0.1136, 0.0268,
        0.5788, 0.0308], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,560][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0932, 0.0136, 0.0517, 0.0623, 0.0275, 0.0202, 0.0927, 0.0863, 0.0635,
        0.3860, 0.1030], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,562][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0048, 0.0009, 0.0064, 0.0093, 0.0082, 0.0023, 0.0515, 0.0618, 0.0110,
        0.8252, 0.0186], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,563][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.7948, 0.0090, 0.0126, 0.0450, 0.0135, 0.0044, 0.0074, 0.0184, 0.0124,
        0.0557, 0.0269], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,563][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0037, 0.0045, 0.0365, 0.0181, 0.0430, 0.0130, 0.0702, 0.3820, 0.0217,
        0.3627, 0.0446], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,563][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.3951, 0.0071, 0.0316, 0.0644, 0.0416, 0.0217, 0.0690, 0.1174, 0.0330,
        0.1898, 0.0292], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,564][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.2764, 0.0021, 0.0165, 0.0422, 0.0104, 0.0054, 0.0393, 0.0197, 0.0293,
        0.5187, 0.0401], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,564][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([9.8435e-01, 2.9717e-04, 1.1443e-03, 5.5400e-03, 4.4730e-04, 3.4224e-04,
        1.0331e-03, 1.8135e-03, 1.6974e-03, 2.1992e-03, 1.1336e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,565][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([9.9747e-01, 3.7551e-05, 2.8986e-04, 5.7932e-04, 7.4075e-05, 1.0268e-04,
        1.8756e-04, 4.0721e-04, 3.6459e-04, 2.9623e-04, 1.9343e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,565][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([8.9128e-01, 8.3857e-04, 2.1241e-02, 7.5542e-03, 4.1329e-03, 5.3960e-03,
        1.8902e-02, 1.1219e-02, 1.8941e-02, 1.1300e-02, 9.1984e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,567][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.7735, 0.0028, 0.0225, 0.0335, 0.0094, 0.0124, 0.0147, 0.0180, 0.0288,
        0.0125, 0.0340, 0.0379], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,569][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.3492, 0.0138, 0.0396, 0.0552, 0.0447, 0.0398, 0.0409, 0.0518, 0.0800,
        0.0774, 0.1000, 0.1076], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,574][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1669, 0.0083, 0.0255, 0.0538, 0.0112, 0.0087, 0.0646, 0.0943, 0.0281,
        0.3432, 0.0407, 0.1546], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,576][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1307, 0.0156, 0.0467, 0.0687, 0.0383, 0.0274, 0.0716, 0.0842, 0.0571,
        0.2213, 0.1080, 0.1305], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,576][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0155, 0.0025, 0.0137, 0.0195, 0.0252, 0.0079, 0.0664, 0.0945, 0.0228,
        0.5891, 0.0494, 0.0935], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,577][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.5082, 0.0236, 0.0463, 0.0708, 0.0291, 0.0149, 0.0259, 0.0507, 0.0334,
        0.0522, 0.0794, 0.0655], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,577][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0095, 0.0112, 0.0443, 0.0343, 0.0630, 0.0254, 0.0854, 0.2614, 0.0323,
        0.3130, 0.0690, 0.0511], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,577][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1216, 0.0153, 0.0463, 0.0655, 0.0835, 0.0415, 0.0980, 0.1020, 0.0439,
        0.2285, 0.0697, 0.0843], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,578][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2397, 0.0049, 0.0344, 0.0505, 0.0180, 0.0106, 0.0523, 0.0387, 0.0381,
        0.2788, 0.0849, 0.1489], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,578][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.9359, 0.0014, 0.0040, 0.0151, 0.0022, 0.0015, 0.0032, 0.0061, 0.0056,
        0.0056, 0.0046, 0.0149], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,579][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([9.8202e-01, 1.9322e-04, 1.4957e-03, 2.5759e-03, 6.3896e-04, 7.2044e-04,
        1.0386e-03, 1.3066e-03, 1.8864e-03, 1.5205e-03, 7.8686e-04, 5.8139e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,581][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.6954, 0.0032, 0.0403, 0.0190, 0.0091, 0.0119, 0.0357, 0.0311, 0.0337,
        0.0219, 0.0178, 0.0808], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,584][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.3737, 0.0034, 0.0563, 0.0427, 0.0250, 0.0538, 0.0496, 0.0249, 0.0816,
        0.0270, 0.0757, 0.0934, 0.0930], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,587][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.0819, 0.0050, 0.0516, 0.0339, 0.0680, 0.1452, 0.0631, 0.0283, 0.1139,
        0.0770, 0.0796, 0.0959, 0.1565], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,589][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.0578, 0.0041, 0.0222, 0.0237, 0.0062, 0.0055, 0.0597, 0.0929, 0.0209,
        0.3722, 0.0264, 0.1498, 0.1585], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,590][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0829, 0.0085, 0.0381, 0.0440, 0.0266, 0.0174, 0.0683, 0.0783, 0.0507,
        0.2511, 0.0921, 0.1373, 0.1047], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,590][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.0041, 0.0010, 0.0089, 0.0097, 0.0119, 0.0034, 0.0612, 0.0607, 0.0130,
        0.6004, 0.0217, 0.0805, 0.1235], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,591][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.8070, 0.0036, 0.0068, 0.0282, 0.0050, 0.0027, 0.0058, 0.0086, 0.0118,
        0.0228, 0.0224, 0.0357, 0.0397], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,591][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.0041, 0.0054, 0.0377, 0.0198, 0.0338, 0.0164, 0.0845, 0.2378, 0.0247,
        0.3201, 0.0451, 0.0506, 0.1201], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,592][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0836, 0.0072, 0.0346, 0.0541, 0.0404, 0.0233, 0.0918, 0.0710, 0.0331,
        0.3161, 0.0309, 0.0897, 0.1242], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,592][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.2061, 0.0015, 0.0294, 0.0361, 0.0130, 0.0090, 0.0609, 0.0225, 0.0343,
        0.2447, 0.0544, 0.1656, 0.1223], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,593][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([9.6804e-01, 4.8466e-04, 1.4065e-03, 6.8152e-03, 7.3622e-04, 5.6093e-04,
        1.4644e-03, 2.7203e-03, 2.0202e-03, 3.4548e-03, 1.9649e-03, 8.7138e-03,
        1.6147e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,594][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([9.9480e-01, 3.7116e-05, 4.5619e-04, 5.4929e-04, 1.2382e-04, 1.6713e-04,
        2.6036e-04, 2.0603e-04, 3.7450e-04, 4.7735e-04, 1.2536e-04, 2.1851e-03,
        2.3838e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,597][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.6456, 0.0034, 0.0420, 0.0232, 0.0100, 0.0125, 0.0378, 0.0351, 0.0332,
        0.0348, 0.0196, 0.0729, 0.0296], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,599][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.1206e-01, 3.7045e-04, 2.7624e-03, 1.1548e-02, 1.7015e-03, 1.6253e-03,
        3.1147e-03, 4.5481e-03, 8.4085e-03, 5.3732e-03, 8.0458e-03, 1.4794e-02,
        2.1999e-02, 3.6464e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,603][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.2607, 0.0065, 0.0121, 0.0449, 0.0557, 0.0319, 0.0260, 0.0485, 0.0609,
        0.1091, 0.0798, 0.0878, 0.1497, 0.0266], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,603][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1703, 0.0019, 0.0112, 0.0239, 0.0031, 0.0011, 0.0272, 0.0525, 0.0126,
        0.3319, 0.0183, 0.1270, 0.1764, 0.0427], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,604][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1329, 0.0054, 0.0284, 0.0430, 0.0163, 0.0096, 0.0460, 0.0503, 0.0484,
        0.2351, 0.0820, 0.1458, 0.0995, 0.0573], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,604][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([7.0400e-03, 3.1403e-04, 4.0591e-03, 7.3216e-03, 7.1291e-03, 1.4068e-03,
        3.4946e-02, 3.9021e-02, 8.7669e-03, 5.8616e-01, 1.9000e-02, 7.1301e-02,
        1.6110e-01, 5.2436e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,605][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.5826, 0.0070, 0.0147, 0.0470, 0.0136, 0.0028, 0.0091, 0.0323, 0.0165,
        0.0537, 0.0582, 0.0503, 0.1041, 0.0079], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,605][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0057, 0.0024, 0.0200, 0.0142, 0.0370, 0.0060, 0.0575, 0.1720, 0.0130,
        0.2811, 0.0293, 0.0489, 0.2504, 0.0624], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,606][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.2012, 0.0034, 0.0266, 0.0420, 0.0366, 0.0152, 0.0780, 0.0600, 0.0339,
        0.2079, 0.0355, 0.0815, 0.1060, 0.0723], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,606][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.2469, 0.0010, 0.0117, 0.0302, 0.0061, 0.0024, 0.0282, 0.0198, 0.0208,
        0.2820, 0.0465, 0.1390, 0.1318, 0.0334], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,607][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.6610e-01, 3.9213e-04, 1.8030e-03, 6.5512e-03, 7.7780e-04, 5.2298e-04,
        1.5142e-03, 2.3439e-03, 2.6106e-03, 3.2422e-03, 1.7094e-03, 9.0507e-03,
        2.7835e-03, 5.9882e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,609][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.9607e-01, 1.6318e-05, 2.1929e-04, 4.0657e-04, 1.7447e-04, 1.4270e-04,
        1.8429e-04, 1.5733e-04, 4.5931e-04, 2.9265e-04, 1.3590e-04, 1.6027e-03,
        5.8723e-05, 8.3427e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,610][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([8.8412e-01, 4.9224e-04, 1.3380e-02, 6.1787e-03, 2.6030e-03, 3.9264e-03,
        1.1552e-02, 8.3847e-03, 1.3264e-02, 7.1053e-03, 4.5063e-03, 3.1526e-02,
        5.4666e-03, 7.4978e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,612][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:37,614][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[33731],
        [ 2640],
        [ 5869],
        [ 4337],
        [ 2620],
        [ 4031],
        [ 7873],
        [ 5542],
        [ 2241],
        [15520],
        [14092],
        [19629],
        [21208],
        [15477]], device='cuda:0')
[2024-07-24 10:30:37,617][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[33092],
        [ 3060],
        [ 7521],
        [ 5829],
        [ 4218],
        [ 7611],
        [12810],
        [ 7588],
        [ 2762],
        [16487],
        [15996],
        [22747],
        [23126],
        [18070]], device='cuda:0')
[2024-07-24 10:30:37,619][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[41270],
        [41342],
        [42189],
        [48611],
        [45534],
        [45172],
        [44459],
        [45925],
        [47268],
        [46469],
        [46554],
        [47479],
        [45941],
        [44941]], device='cuda:0')
[2024-07-24 10:30:37,620][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[23682],
        [21279],
        [26257],
        [20526],
        [20372],
        [21615],
        [22346],
        [22487],
        [22843],
        [22921],
        [22710],
        [23164],
        [22742],
        [22648]], device='cuda:0')
[2024-07-24 10:30:37,621][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 8434],
        [12255],
        [18874],
        [33858],
        [30114],
        [32407],
        [28471],
        [29581],
        [34096],
        [28789],
        [30199],
        [31138],
        [33732],
        [32621]], device='cuda:0')
[2024-07-24 10:30:37,622][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[23156],
        [26301],
        [24195],
        [27392],
        [20243],
        [23853],
        [26055],
        [30335],
        [25283],
        [30334],
        [29647],
        [25352],
        [26840],
        [24920]], device='cuda:0')
[2024-07-24 10:30:37,623][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[28573],
        [35545],
        [28457],
        [20028],
        [20492],
        [20392],
        [18487],
        [18003],
        [18316],
        [26933],
        [28036],
        [27906],
        [26836],
        [27542]], device='cuda:0')
[2024-07-24 10:30:37,625][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[26138],
        [27617],
        [26759],
        [40719],
        [32790],
        [41061],
        [34520],
        [34092],
        [36202],
        [29560],
        [38682],
        [42909],
        [35363],
        [41207]], device='cuda:0')
[2024-07-24 10:30:37,627][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[42826],
        [42610],
        [42721],
        [34759],
        [33526],
        [13218],
        [13612],
        [36496],
        [31609],
        [37075],
        [35042],
        [20018],
        [35569],
        [32835]], device='cuda:0')
[2024-07-24 10:30:37,628][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 8506],
        [22179],
        [ 5570],
        [ 9431],
        [ 6324],
        [ 5440],
        [ 5490],
        [ 7656],
        [ 7103],
        [ 9989],
        [ 9298],
        [ 8985],
        [15105],
        [12370]], device='cuda:0')
[2024-07-24 10:30:37,630][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[23765],
        [15653],
        [19391],
        [16630],
        [17972],
        [17996],
        [18015],
        [18006],
        [18277],
        [18833],
        [18545],
        [22006],
        [22298],
        [21578]], device='cuda:0')
[2024-07-24 10:30:37,633][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[25950],
        [16618],
        [12750],
        [14583],
        [14106],
        [14595],
        [15611],
        [16044],
        [15474],
        [15334],
        [15191],
        [15184],
        [15084],
        [15159]], device='cuda:0')
[2024-07-24 10:30:37,635][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[15249],
        [15249],
        [15305],
        [16483],
        [15376],
        [15454],
        [15455],
        [15443],
        [15990],
        [15359],
        [15493],
        [16939],
        [15740],
        [15615]], device='cuda:0')
[2024-07-24 10:30:37,636][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[40775],
        [40036],
        [36182],
        [17026],
        [30168],
        [30678],
        [27565],
        [34960],
        [24843],
        [29996],
        [32886],
        [21398],
        [17275],
        [34706]], device='cuda:0')
[2024-07-24 10:30:37,637][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[42721],
        [42331],
        [40807],
        [43414],
        [42973],
        [41522],
        [42339],
        [44001],
        [45062],
        [45346],
        [44960],
        [45426],
        [44567],
        [44336]], device='cuda:0')
[2024-07-24 10:30:37,638][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[33096],
        [33104],
        [32707],
        [33359],
        [32016],
        [31942],
        [31997],
        [31555],
        [30714],
        [30786],
        [30581],
        [31840],
        [39040],
        [30247]], device='cuda:0')
[2024-07-24 10:30:37,640][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[41051],
        [41340],
        [41335],
        [47815],
        [45779],
        [45654],
        [45048],
        [46317],
        [45475],
        [46657],
        [46894],
        [47505],
        [47441],
        [47489]], device='cuda:0')
[2024-07-24 10:30:37,641][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[35636],
        [34221],
        [40679],
        [38117],
        [40800],
        [39707],
        [44848],
        [41077],
        [38652],
        [39427],
        [38284],
        [38390],
        [37341],
        [37525]], device='cuda:0')
[2024-07-24 10:30:37,643][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[18314],
        [26481],
        [24382],
        [29693],
        [30887],
        [31848],
        [33650],
        [34156],
        [34716],
        [30032],
        [31986],
        [31714],
        [31679],
        [30939]], device='cuda:0')
[2024-07-24 10:30:37,644][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[32771],
        [23919],
        [31726],
        [36427],
        [43034],
        [44902],
        [45909],
        [39649],
        [38876],
        [34630],
        [34113],
        [34973],
        [37207],
        [38304]], device='cuda:0')
[2024-07-24 10:30:37,647][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[40381],
        [40139],
        [40317],
        [36600],
        [39716],
        [37456],
        [39505],
        [39509],
        [38931],
        [40176],
        [38422],
        [30506],
        [39574],
        [34295]], device='cuda:0')
[2024-07-24 10:30:37,649][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[29406],
        [49723],
        [50199],
        [50217],
        [50230],
        [50234],
        [50237],
        [50225],
        [50223],
        [50208],
        [50196],
        [50224],
        [50208],
        [50197]], device='cuda:0')
[2024-07-24 10:30:37,652][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[11489],
        [ 3374],
        [ 5889],
        [ 3614],
        [ 5497],
        [ 6694],
        [ 9657],
        [ 6186],
        [ 5656],
        [ 6922],
        [ 6167],
        [ 8549],
        [ 9620],
        [ 9370]], device='cuda:0')
[2024-07-24 10:30:37,653][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[47292],
        [47836],
        [47994],
        [47794],
        [49305],
        [48731],
        [48819],
        [49089],
        [49250],
        [48937],
        [47264],
        [47035],
        [46931],
        [47355]], device='cuda:0')
[2024-07-24 10:30:37,654][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[19219],
        [19214],
        [19161],
        [18608],
        [19017],
        [18635],
        [18825],
        [18838],
        [18137],
        [18847],
        [18582],
        [16464],
        [17857],
        [17742]], device='cuda:0')
[2024-07-24 10:30:37,655][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[49874],
        [49874],
        [49875],
        [49875],
        [49874],
        [49875],
        [49874],
        [49878],
        [49882],
        [49875],
        [49881],
        [49911],
        [49884],
        [49883]], device='cuda:0')
[2024-07-24 10:30:37,656][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[25528],
        [26242],
        [32268],
        [42552],
        [33964],
        [32858],
        [36243],
        [28614],
        [34007],
        [31517],
        [28649],
        [35442],
        [38109],
        [27310]], device='cuda:0')
[2024-07-24 10:30:37,657][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[9427],
        [7872],
        [5454],
        [5816],
        [4051],
        [4801],
        [2982],
        [5006],
        [5483],
        [5200],
        [6261],
        [6179],
        [4822],
        [6717]], device='cuda:0')
[2024-07-24 10:30:37,658][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[7713],
        [9464],
        [1902],
        [1335],
        [ 775],
        [1152],
        [ 277],
        [ 476],
        [1116],
        [ 383],
        [1241],
        [1245],
        [ 714],
        [ 630]], device='cuda:0')
[2024-07-24 10:30:37,660][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125],
        [14125]], device='cuda:0')
[2024-07-24 10:30:37,703][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:37,703][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,703][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,704][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,704][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,706][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,708][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,710][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,713][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,725][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,727][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,730][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,730][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,730][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.5387, 0.4613], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,731][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.4228, 0.5772], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,731][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.9038, 0.0962], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,731][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.8464, 0.1536], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,732][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.9301, 0.0699], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,732][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.3921, 0.6079], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,732][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.2483, 0.7517], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,734][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.0597, 0.9403], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,736][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.8660, 0.1340], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,739][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.3340, 0.6660], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,743][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0716, 0.9284], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,744][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0374, 0.9626], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,744][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3182, 0.3070, 0.3748], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,744][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.2528, 0.3733, 0.3739], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,745][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.5270, 0.3831, 0.0899], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,745][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.7560, 0.0705, 0.1736], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,745][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9111, 0.0491, 0.0398], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,746][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.6303, 0.0380, 0.3317], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,747][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1862, 0.6262, 0.1877], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,750][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0574, 0.4673, 0.4753], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,754][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6029, 0.0658, 0.3313], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,756][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.1224, 0.4078, 0.4698], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,757][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0155, 0.6012, 0.3833], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,757][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0460, 0.3152, 0.6388], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,757][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.2508, 0.2632, 0.3600, 0.1260], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,758][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.1676, 0.2935, 0.2903, 0.2486], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,758][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.3435, 0.2021, 0.1609, 0.2935], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,758][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.3139, 0.1410, 0.2650, 0.2801], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,759][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.8624, 0.0473, 0.0111, 0.0793], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,759][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.2232, 0.1002, 0.5981, 0.0786], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,761][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.1249, 0.5807, 0.1990, 0.0954], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,763][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.0395, 0.3589, 0.3172, 0.2843], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,767][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.1962, 0.1277, 0.4240, 0.2522], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,770][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.1907, 0.2566, 0.3790, 0.1737], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,770][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0135, 0.5804, 0.2597, 0.1464], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,770][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.0070, 0.2734, 0.5155, 0.2041], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:37,771][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.1703, 0.2285, 0.2852, 0.1267, 0.1893], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,771][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.1380, 0.2313, 0.2337, 0.2005, 0.1965], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,772][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.5306, 0.0720, 0.0848, 0.2993, 0.0134], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,772][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.5201, 0.0498, 0.1490, 0.2324, 0.0487], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,772][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.8621, 0.0328, 0.0192, 0.0563, 0.0296], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,774][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.4037, 0.0698, 0.3964, 0.0833, 0.0467], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,777][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.1143, 0.4305, 0.1601, 0.0865, 0.2086], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,781][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.0217, 0.2540, 0.2702, 0.2296, 0.2245], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,783][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.1930, 0.0453, 0.4305, 0.2985, 0.0327], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,783][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.0746, 0.2074, 0.2457, 0.1154, 0.3569], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,784][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.0065, 0.3235, 0.3215, 0.1181, 0.2304], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,784][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.0060, 0.1952, 0.5437, 0.1715, 0.0836], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:37,784][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1333, 0.1951, 0.2421, 0.1071, 0.1634, 0.1590], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,785][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.1176, 0.1915, 0.1940, 0.1685, 0.1645, 0.1640], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,785][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1415, 0.1048, 0.0827, 0.4293, 0.0473, 0.1944], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,785][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.3771, 0.0681, 0.1937, 0.2602, 0.0664, 0.0345], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,786][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.7225, 0.0591, 0.0355, 0.0757, 0.0533, 0.0539], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,788][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.3999, 0.0452, 0.4033, 0.0462, 0.0453, 0.0600], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,790][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0994, 0.4216, 0.1539, 0.0713, 0.2066, 0.0472], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,794][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0183, 0.2114, 0.2250, 0.1849, 0.1972, 0.1633], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,796][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0931, 0.0961, 0.4541, 0.2798, 0.0581, 0.0188], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,797][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0204, 0.1171, 0.2052, 0.1097, 0.2834, 0.2642], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,797][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0108, 0.3666, 0.1859, 0.0678, 0.1725, 0.1964], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,797][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0011, 0.2404, 0.4108, 0.1358, 0.1516, 0.0602], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:37,798][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1222, 0.1600, 0.2070, 0.0885, 0.1429, 0.1384, 0.1408],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,798][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0920, 0.1678, 0.1665, 0.1442, 0.1426, 0.1431, 0.1439],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,799][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2040, 0.1015, 0.0336, 0.5261, 0.0348, 0.0878, 0.0122],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,800][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.4124, 0.0503, 0.1276, 0.2123, 0.0480, 0.0298, 0.1197],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,803][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.6849, 0.0502, 0.0331, 0.0769, 0.0522, 0.0513, 0.0515],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,807][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.5381, 0.0317, 0.2186, 0.0293, 0.0375, 0.0446, 0.1002],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,809][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0881, 0.4817, 0.1337, 0.0596, 0.1837, 0.0325, 0.0207],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,810][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0156, 0.1741, 0.1716, 0.1500, 0.1594, 0.1386, 0.1908],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,810][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2743, 0.0344, 0.2632, 0.2427, 0.0327, 0.0151, 0.1376],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,811][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0428, 0.1028, 0.1683, 0.0731, 0.1974, 0.1956, 0.2201],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,811][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0092, 0.4099, 0.1657, 0.0448, 0.1317, 0.1012, 0.1375],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,811][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0068, 0.0991, 0.3012, 0.1803, 0.0973, 0.0541, 0.2612],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:37,812][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.1390, 0.1325, 0.1693, 0.0662, 0.1160, 0.1140, 0.1159, 0.1473],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,812][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0882, 0.1405, 0.1406, 0.1248, 0.1216, 0.1199, 0.1245, 0.1399],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,813][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.0892, 0.1133, 0.1207, 0.2127, 0.0475, 0.1864, 0.0665, 0.1638],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,816][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.5002, 0.0295, 0.0718, 0.1770, 0.0374, 0.0193, 0.0905, 0.0742],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,819][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.8195, 0.0159, 0.0101, 0.0326, 0.0191, 0.0137, 0.0166, 0.0724],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,823][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.5018, 0.0179, 0.2360, 0.0335, 0.0433, 0.0574, 0.0655, 0.0446],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,823][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.0887, 0.4254, 0.1342, 0.0607, 0.1714, 0.0359, 0.0214, 0.0623],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,824][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0151, 0.1637, 0.1520, 0.1370, 0.1450, 0.1219, 0.1627, 0.1027],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,824][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.3114, 0.0246, 0.1503, 0.1808, 0.0224, 0.0134, 0.1114, 0.1857],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,825][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.0591, 0.0646, 0.0980, 0.0247, 0.1322, 0.1558, 0.1213, 0.3443],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,825][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.0232, 0.3936, 0.0876, 0.0579, 0.1705, 0.1022, 0.0846, 0.0804],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,825][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.0140, 0.0578, 0.1916, 0.1584, 0.0707, 0.0335, 0.2053, 0.2687],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:37,827][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0922, 0.1192, 0.1581, 0.0620, 0.1054, 0.1098, 0.1070, 0.1465, 0.0998],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,830][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0757, 0.1257, 0.1243, 0.1108, 0.1077, 0.1078, 0.1097, 0.1261, 0.1123],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,834][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2261, 0.0741, 0.0351, 0.3291, 0.0558, 0.0682, 0.0045, 0.1346, 0.0726],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,836][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.2707, 0.0577, 0.1184, 0.1927, 0.0540, 0.0373, 0.1215, 0.0902, 0.0576],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,837][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.4919, 0.0357, 0.0324, 0.0658, 0.0343, 0.0393, 0.0471, 0.1705, 0.0831],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,837][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.7932, 0.0124, 0.0576, 0.0202, 0.0150, 0.0109, 0.0301, 0.0251, 0.0356],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,837][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0812, 0.4144, 0.1353, 0.0544, 0.1658, 0.0339, 0.0198, 0.0553, 0.0399],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,838][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0139, 0.1446, 0.1442, 0.1214, 0.1266, 0.1123, 0.1543, 0.0950, 0.0878],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,838][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0556, 0.0509, 0.2191, 0.1710, 0.0344, 0.0191, 0.1062, 0.2937, 0.0501],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,839][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0227, 0.0470, 0.1234, 0.0422, 0.1070, 0.1000, 0.1165, 0.2019, 0.2392],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,839][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0056, 0.3118, 0.0890, 0.0383, 0.1453, 0.0904, 0.1082, 0.0566, 0.1548],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,841][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0020, 0.1497, 0.2671, 0.0924, 0.0671, 0.0354, 0.1536, 0.1964, 0.0364],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:37,843][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0755, 0.1133, 0.1458, 0.0578, 0.0949, 0.0937, 0.0986, 0.1323, 0.0912,
        0.0968], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,848][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0670, 0.1079, 0.1106, 0.0955, 0.0950, 0.0948, 0.0968, 0.1106, 0.0983,
        0.1233], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,850][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.3475, 0.0284, 0.0265, 0.2709, 0.0211, 0.0337, 0.0105, 0.0878, 0.1010,
        0.0728], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,850][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.3468, 0.0211, 0.0884, 0.1223, 0.0355, 0.0215, 0.0987, 0.0610, 0.0509,
        0.1538], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,851][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.4940, 0.0414, 0.0197, 0.0693, 0.0319, 0.0354, 0.0242, 0.1471, 0.0702,
        0.0668], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,851][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.2273, 0.0170, 0.1658, 0.0358, 0.0340, 0.0623, 0.0940, 0.0519, 0.2164,
        0.0954], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,851][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0814, 0.3105, 0.1126, 0.0604, 0.1410, 0.0364, 0.0228, 0.0580, 0.0461,
        0.1311], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,852][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.0151, 0.1182, 0.1097, 0.1099, 0.1001, 0.0842, 0.1224, 0.0740, 0.0755,
        0.1910], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,852][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.1835, 0.0112, 0.1337, 0.1185, 0.0147, 0.0093, 0.0689, 0.1765, 0.0314,
        0.2524], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,854][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.0190, 0.0477, 0.0848, 0.0389, 0.1301, 0.0766, 0.0826, 0.1733, 0.1653,
        0.1818], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,857][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0053, 0.1366, 0.0700, 0.0643, 0.1237, 0.1050, 0.1291, 0.1038, 0.1706,
        0.0915], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,861][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0046, 0.0176, 0.0849, 0.0497, 0.0156, 0.0093, 0.0633, 0.0820, 0.0129,
        0.6600], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:37,863][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0667, 0.1011, 0.1294, 0.0579, 0.0847, 0.0836, 0.0866, 0.1177, 0.0819,
        0.0885, 0.1018], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,863][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0587, 0.0978, 0.0991, 0.0862, 0.0851, 0.0844, 0.0862, 0.0993, 0.0885,
        0.1120, 0.1026], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,864][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1642, 0.0521, 0.0201, 0.2360, 0.0108, 0.0486, 0.0141, 0.1831, 0.0876,
        0.1515, 0.0319], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,864][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.3857, 0.0221, 0.0633, 0.1359, 0.0226, 0.0139, 0.0755, 0.0509, 0.0409,
        0.1629, 0.0263], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,865][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.5830, 0.0256, 0.0097, 0.0470, 0.0319, 0.0170, 0.0208, 0.1018, 0.0435,
        0.0394, 0.0802], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,865][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.1597, 0.0244, 0.1952, 0.0264, 0.0181, 0.0570, 0.0825, 0.0524, 0.1938,
        0.1344, 0.0562], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,865][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0727, 0.3399, 0.1201, 0.0491, 0.1487, 0.0300, 0.0169, 0.0481, 0.0325,
        0.1225, 0.0195], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,866][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0107, 0.1035, 0.1078, 0.0977, 0.0894, 0.0747, 0.1156, 0.0742, 0.0663,
        0.1874, 0.0727], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,868][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0557, 0.0160, 0.1151, 0.0855, 0.0140, 0.0061, 0.0616, 0.1556, 0.0352,
        0.4133, 0.0418], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,870][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0152, 0.0397, 0.0828, 0.0233, 0.0839, 0.0604, 0.0727, 0.1987, 0.1705,
        0.0990, 0.1538], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,874][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0050, 0.2881, 0.0701, 0.0533, 0.1500, 0.0756, 0.0684, 0.0485, 0.0622,
        0.0845, 0.0945], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,877][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0017, 0.0280, 0.0789, 0.0311, 0.0141, 0.0073, 0.0605, 0.0830, 0.0120,
        0.6613, 0.0220], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:37,877][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0680, 0.0938, 0.1254, 0.0474, 0.0797, 0.0769, 0.0786, 0.1136, 0.0737,
        0.0789, 0.0964, 0.0677], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,877][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0472, 0.0936, 0.0915, 0.0785, 0.0788, 0.0785, 0.0782, 0.0916, 0.0808,
        0.1018, 0.0939, 0.0856], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,878][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0511, 0.0788, 0.0234, 0.1728, 0.0278, 0.0652, 0.0364, 0.2056, 0.0860,
        0.1916, 0.0472, 0.0141], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,878][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.2160, 0.0391, 0.0904, 0.1207, 0.0370, 0.0267, 0.0811, 0.0681, 0.0479,
        0.1324, 0.0446, 0.0959], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,879][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.6285, 0.0249, 0.0128, 0.0398, 0.0211, 0.0179, 0.0134, 0.0890, 0.0404,
        0.0300, 0.0631, 0.0192], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,879][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.3835, 0.0125, 0.0692, 0.0232, 0.0181, 0.0217, 0.0500, 0.0385, 0.0898,
        0.1018, 0.0531, 0.1386], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,881][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0703, 0.3587, 0.1073, 0.0431, 0.1525, 0.0276, 0.0144, 0.0449, 0.0301,
        0.1275, 0.0166, 0.0070], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,885][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0159, 0.0918, 0.0948, 0.0908, 0.0829, 0.0704, 0.1023, 0.0705, 0.0640,
        0.1516, 0.0692, 0.0956], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,888][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0829, 0.0247, 0.1118, 0.1056, 0.0230, 0.0140, 0.0703, 0.1527, 0.0395,
        0.2380, 0.0566, 0.0807], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,890][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0057, 0.0191, 0.0695, 0.0202, 0.0552, 0.0449, 0.0721, 0.1145, 0.1658,
        0.1179, 0.1350, 0.1801], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,890][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0086, 0.3190, 0.0485, 0.0337, 0.1106, 0.0695, 0.0822, 0.0354, 0.0684,
        0.0961, 0.0608, 0.0672], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,891][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0020, 0.0451, 0.0797, 0.0403, 0.0370, 0.0163, 0.0717, 0.0704, 0.0241,
        0.5013, 0.0484, 0.0638], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:37,891][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0475, 0.0922, 0.1141, 0.0446, 0.0719, 0.0705, 0.0774, 0.0999, 0.0672,
        0.0779, 0.0904, 0.0683, 0.0781], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,891][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0399, 0.0879, 0.0863, 0.0718, 0.0745, 0.0748, 0.0724, 0.0849, 0.0750,
        0.0934, 0.0874, 0.0787, 0.0731], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,892][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0485, 0.0604, 0.0428, 0.2068, 0.0247, 0.0669, 0.0598, 0.1037, 0.1291,
        0.1190, 0.0588, 0.0205, 0.0590], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,892][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.1953, 0.0224, 0.0735, 0.0958, 0.0296, 0.0189, 0.0898, 0.0526, 0.0455,
        0.1499, 0.0383, 0.1120, 0.0764], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,893][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.5090, 0.0307, 0.0098, 0.0617, 0.0248, 0.0135, 0.0124, 0.1064, 0.0347,
        0.0468, 0.0519, 0.0229, 0.0755], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,896][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.1071, 0.0089, 0.0764, 0.0166, 0.0199, 0.0353, 0.0634, 0.0172, 0.1734,
        0.0586, 0.0725, 0.2502, 0.1004], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,899][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0676, 0.3085, 0.1080, 0.0468, 0.1451, 0.0328, 0.0180, 0.0485, 0.0361,
        0.1266, 0.0202, 0.0093, 0.0323], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,903][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0131, 0.0871, 0.0832, 0.0841, 0.0702, 0.0594, 0.0897, 0.0584, 0.0573,
        0.1513, 0.0527, 0.0845, 0.1090], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,904][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.0409, 0.0133, 0.1011, 0.0576, 0.0153, 0.0090, 0.0624, 0.1448, 0.0341,
        0.3098, 0.0514, 0.0838, 0.0763], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,904][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0194, 0.0321, 0.0507, 0.0294, 0.0674, 0.0530, 0.0563, 0.0835, 0.1095,
        0.1342, 0.1286, 0.1491, 0.0869], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,904][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0032, 0.1332, 0.0492, 0.0346, 0.1167, 0.0813, 0.0819, 0.0849, 0.1111,
        0.0991, 0.0769, 0.0892, 0.0387], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,905][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0044, 0.0254, 0.1232, 0.0213, 0.0113, 0.0056, 0.0633, 0.0394, 0.0107,
        0.5191, 0.0174, 0.0923, 0.0666], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:37,905][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0631, 0.0779, 0.0974, 0.0426, 0.0665, 0.0641, 0.0664, 0.0858, 0.0626,
        0.0694, 0.0796, 0.0596, 0.0724, 0.0926], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,906][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0416, 0.0793, 0.0784, 0.0672, 0.0673, 0.0672, 0.0673, 0.0770, 0.0692,
        0.0861, 0.0796, 0.0739, 0.0693, 0.0766], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,908][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0487, 0.0437, 0.0296, 0.1598, 0.0172, 0.0617, 0.0162, 0.0909, 0.0654,
        0.1448, 0.0449, 0.0121, 0.1365, 0.1286], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,910][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.3489, 0.0157, 0.0533, 0.1250, 0.0190, 0.0101, 0.0520, 0.0399, 0.0304,
        0.1076, 0.0245, 0.0853, 0.0583, 0.0301], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,914][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.4813, 0.0308, 0.0127, 0.0475, 0.0268, 0.0198, 0.0189, 0.0949, 0.0421,
        0.0364, 0.0692, 0.0285, 0.0617, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,917][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.3218, 0.0071, 0.0702, 0.0170, 0.0133, 0.0138, 0.0340, 0.0238, 0.0867,
        0.0746, 0.0540, 0.1493, 0.0765, 0.0578], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,917][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0706, 0.3485, 0.1035, 0.0404, 0.1464, 0.0267, 0.0137, 0.0432, 0.0293,
        0.1256, 0.0161, 0.0065, 0.0262, 0.0033], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,918][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0094, 0.0685, 0.0754, 0.0711, 0.0650, 0.0520, 0.0792, 0.0539, 0.0497,
        0.1357, 0.0531, 0.0777, 0.1092, 0.1002], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,918][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0814, 0.0088, 0.0700, 0.0770, 0.0092, 0.0032, 0.0408, 0.0981, 0.0240,
        0.3408, 0.0305, 0.0685, 0.0815, 0.0661], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,918][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0036, 0.0212, 0.0498, 0.0274, 0.0520, 0.0393, 0.0580, 0.0773, 0.1188,
        0.1149, 0.1094, 0.1633, 0.0630, 0.1020], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,919][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0114, 0.2388, 0.0671, 0.0371, 0.1226, 0.0867, 0.0806, 0.0471, 0.0596,
        0.0865, 0.0428, 0.0705, 0.0234, 0.0259], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,919][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0017, 0.0141, 0.0598, 0.0337, 0.0098, 0.0036, 0.0417, 0.0492, 0.0113,
        0.5869, 0.0158, 0.0572, 0.0694, 0.0457], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:37,975][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:37,978][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,981][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,981][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,981][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,982][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,982][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,982][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,982][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,983][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,983][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,983][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,984][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:37,984][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.7388, 0.2612], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,984][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.2467, 0.7533], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,985][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.4848, 0.5152], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,985][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.8464, 0.1536], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,985][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([9.9989e-01, 1.1154e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,986][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.4758, 0.5242], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,988][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.9974, 0.0026], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,989][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.5353, 0.4647], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,989][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.8660, 0.1340], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,989][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0468, 0.9532], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,990][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.9066, 0.0934], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,990][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.0496, 0.9504], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:37,990][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7487, 0.0844, 0.1669], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,991][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0547, 0.1143, 0.8311], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,992][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.2995, 0.3315, 0.3690], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,995][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.7560, 0.0705, 0.1736], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:37,997][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([9.9660e-01, 3.7626e-04, 3.0253e-03], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,001][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3468, 0.2743, 0.3790], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,001][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.9895, 0.0024, 0.0081], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,002][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.3667, 0.1676, 0.4657], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,002][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.6029, 0.0658, 0.3313], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,003][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0446, 0.4133, 0.5421], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,003][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.8389, 0.0623, 0.0988], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,003][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0686, 0.3324, 0.5990], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,004][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.2060, 0.1856, 0.2755, 0.3329], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,004][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.0199, 0.1840, 0.6506, 0.1455], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,004][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.2141, 0.2552, 0.2873, 0.2434], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,006][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.3139, 0.1410, 0.2650, 0.2801], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,009][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.9397, 0.0024, 0.0129, 0.0450], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,013][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.1412, 0.3081, 0.4930, 0.0577], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,015][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.8763, 0.0112, 0.0256, 0.0869], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,015][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.1522, 0.2280, 0.3347, 0.2851], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,016][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.1962, 0.1277, 0.4240, 0.2522], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,016][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.0021, 0.1936, 0.7722, 0.0321], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,016][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.3622, 0.2184, 0.2035, 0.2160], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,017][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.0184, 0.3237, 0.4192, 0.2387], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,017][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.3531, 0.0716, 0.1839, 0.3296, 0.0618], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,017][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0230, 0.0631, 0.6880, 0.1555, 0.0705], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,019][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.1707, 0.2013, 0.2202, 0.1950, 0.2128], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,023][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.5201, 0.0498, 0.1490, 0.2324, 0.0487], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,025][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([9.7316e-01, 5.0734e-04, 5.3506e-03, 1.9061e-02, 1.9240e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,028][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.2514, 0.2102, 0.3430, 0.0663, 0.1290], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,028][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.9618, 0.0014, 0.0036, 0.0301, 0.0032], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,029][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.2209, 0.0821, 0.3132, 0.2811, 0.1028], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,029][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.1930, 0.0453, 0.4305, 0.2985, 0.0327], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,029][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.0206, 0.1581, 0.4565, 0.0938, 0.2710], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,030][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.4455, 0.0910, 0.1905, 0.1674, 0.1056], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,030][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.0169, 0.1948, 0.3879, 0.2646, 0.1359], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,030][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1689, 0.1220, 0.2334, 0.3123, 0.1239, 0.0395], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,031][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0098, 0.0739, 0.6412, 0.1181, 0.0882, 0.0688], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,032][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1528, 0.1657, 0.1872, 0.1611, 0.1795, 0.1536], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,035][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.3771, 0.0681, 0.1937, 0.2602, 0.0664, 0.0345], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,039][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.9425, 0.0011, 0.0102, 0.0375, 0.0049, 0.0039], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,041][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.2284, 0.1778, 0.2705, 0.0420, 0.1225, 0.1588], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,042][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.9320, 0.0029, 0.0089, 0.0468, 0.0053, 0.0043], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,042][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1198, 0.0971, 0.2886, 0.2255, 0.1747, 0.0943], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,042][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0931, 0.0961, 0.4541, 0.2798, 0.0581, 0.0188], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,043][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0017, 0.1536, 0.2773, 0.0328, 0.3118, 0.2228], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,043][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.5081, 0.0632, 0.1442, 0.1333, 0.0670, 0.0842], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,044][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0060, 0.2403, 0.3066, 0.2100, 0.1928, 0.0443], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,045][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3218, 0.0465, 0.1186, 0.2605, 0.0712, 0.0260, 0.1554],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,048][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0369, 0.0483, 0.3706, 0.1631, 0.0568, 0.0476, 0.2766],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,052][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1299, 0.1432, 0.1624, 0.1409, 0.1554, 0.1322, 0.1361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,054][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.4124, 0.0503, 0.1276, 0.2123, 0.0480, 0.0298, 0.1197],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,055][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.4068e-01, 7.5161e-04, 8.1081e-03, 3.3828e-02, 3.9247e-03, 3.3767e-03,
        9.3313e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,055][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1544, 0.1658, 0.2433, 0.0357, 0.1166, 0.1686, 0.1157],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,056][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.9198, 0.0031, 0.0100, 0.0455, 0.0059, 0.0044, 0.0113],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,056][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1316, 0.0520, 0.2084, 0.1811, 0.0952, 0.0666, 0.2650],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,056][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2743, 0.0344, 0.2632, 0.2427, 0.0327, 0.0151, 0.1376],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,057][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0049, 0.1318, 0.2959, 0.0451, 0.1829, 0.1750, 0.1644],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,057][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3219, 0.0751, 0.1548, 0.1137, 0.0874, 0.1100, 0.1371],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,058][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0173, 0.1172, 0.2514, 0.2367, 0.1435, 0.0438, 0.1900],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,060][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.4065, 0.0288, 0.0799, 0.1728, 0.0430, 0.0190, 0.1311, 0.1189],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,063][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.0372, 0.0390, 0.2338, 0.1287, 0.0383, 0.0375, 0.2321, 0.2535],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,066][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.1046, 0.1258, 0.1410, 0.1223, 0.1338, 0.1124, 0.1176, 0.1426],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,068][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.5002, 0.0295, 0.0718, 0.1770, 0.0374, 0.0193, 0.0905, 0.0742],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,068][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([9.5823e-01, 5.3864e-04, 4.2082e-03, 2.0097e-02, 2.2774e-03, 1.6513e-03,
        6.3661e-03, 6.6308e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,069][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.0934, 0.1702, 0.2673, 0.0420, 0.1120, 0.1734, 0.0939, 0.0478],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,069][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.9412, 0.0021, 0.0060, 0.0309, 0.0037, 0.0026, 0.0064, 0.0072],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,070][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.1438, 0.0520, 0.1429, 0.1623, 0.0734, 0.0511, 0.1930, 0.1816],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,070][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.3114, 0.0246, 0.1503, 0.1808, 0.0224, 0.0134, 0.1114, 0.1857],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,070][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.0016, 0.0633, 0.2312, 0.0216, 0.1547, 0.1918, 0.1798, 0.1560],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,072][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.4882, 0.0418, 0.0714, 0.0886, 0.0551, 0.0520, 0.0714, 0.1316],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,075][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0290, 0.0846, 0.1747, 0.2116, 0.1198, 0.0346, 0.1680, 0.1776],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,079][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.1395, 0.0491, 0.1178, 0.1913, 0.0592, 0.0344, 0.1544, 0.1873, 0.0672],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,081][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0128, 0.0526, 0.2646, 0.0924, 0.0348, 0.0426, 0.1696, 0.2551, 0.0755],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,082][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1031, 0.1120, 0.1266, 0.1085, 0.1205, 0.0999, 0.1044, 0.1279, 0.0971],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,082][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.2707, 0.0577, 0.1184, 0.1927, 0.0540, 0.0373, 0.1215, 0.0902, 0.0576],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,082][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.8518, 0.0026, 0.0172, 0.0505, 0.0089, 0.0068, 0.0171, 0.0243, 0.0207],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,083][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1914, 0.1402, 0.1874, 0.0446, 0.1001, 0.1127, 0.0917, 0.0570, 0.0749],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,083][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.8906, 0.0046, 0.0107, 0.0452, 0.0057, 0.0039, 0.0094, 0.0094, 0.0205],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,084][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0728, 0.0519, 0.1647, 0.1218, 0.0713, 0.0626, 0.2123, 0.1647, 0.0779],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,084][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0556, 0.0509, 0.2191, 0.1710, 0.0344, 0.0191, 0.1062, 0.2937, 0.0501],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,086][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0070, 0.1126, 0.1591, 0.0500, 0.1213, 0.1054, 0.1159, 0.2260, 0.1026],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,088][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.1717, 0.0659, 0.1222, 0.0841, 0.0730, 0.0951, 0.0873, 0.1367, 0.1641],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,092][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0059, 0.1580, 0.2073, 0.1465, 0.1177, 0.0394, 0.1252, 0.1409, 0.0591],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,095][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.2585, 0.0142, 0.0558, 0.1187, 0.0220, 0.0118, 0.0828, 0.1122, 0.0339,
        0.2900], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,095][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.0308, 0.0202, 0.1944, 0.0668, 0.0254, 0.0279, 0.1547, 0.1869, 0.0603,
        0.2327], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,095][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.0908, 0.0968, 0.1147, 0.1012, 0.1098, 0.0929, 0.0957, 0.1168, 0.0909,
        0.0905], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,096][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.3468, 0.0211, 0.0884, 0.1223, 0.0355, 0.0215, 0.0987, 0.0610, 0.0509,
        0.1538], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,096][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([9.5136e-01, 2.8910e-04, 4.5132e-03, 1.9994e-02, 1.6688e-03, 1.8633e-03,
        5.5191e-03, 4.5605e-03, 6.9796e-03, 3.2564e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,097][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.0645, 0.1100, 0.2407, 0.0358, 0.0914, 0.1844, 0.0936, 0.0499, 0.0989,
        0.0308], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,097][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.8905, 0.0032, 0.0083, 0.0423, 0.0053, 0.0032, 0.0087, 0.0076, 0.0153,
        0.0153], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,100][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.1319, 0.0243, 0.0977, 0.1168, 0.0389, 0.0337, 0.1392, 0.1148, 0.0620,
        0.2406], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,103][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.1835, 0.0112, 0.1337, 0.1185, 0.0147, 0.0093, 0.0689, 0.1765, 0.0314,
        0.2524], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,107][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0228, 0.0464, 0.1440, 0.0550, 0.1590, 0.1176, 0.1176, 0.1411, 0.0984,
        0.0981], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,109][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.2727, 0.0321, 0.0728, 0.0755, 0.0456, 0.0644, 0.0864, 0.1141, 0.1467,
        0.0896], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,110][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.0108, 0.0381, 0.1096, 0.1004, 0.0481, 0.0169, 0.0858, 0.0939, 0.0317,
        0.4648], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,110][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.1066, 0.0253, 0.0580, 0.1126, 0.0251, 0.0114, 0.0834, 0.1047, 0.0335,
        0.3986, 0.0408], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,111][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0109, 0.0192, 0.1774, 0.0599, 0.0223, 0.0223, 0.1431, 0.1704, 0.0562,
        0.2475, 0.0707], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,111][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0840, 0.0917, 0.1055, 0.0912, 0.1008, 0.0848, 0.0867, 0.1069, 0.0821,
        0.0824, 0.0839], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,112][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.3857, 0.0221, 0.0633, 0.1359, 0.0226, 0.0139, 0.0755, 0.0509, 0.0409,
        0.1629, 0.0263], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,112][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.9238, 0.0016, 0.0057, 0.0284, 0.0022, 0.0014, 0.0051, 0.0135, 0.0071,
        0.0071, 0.0042], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,112][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.1209, 0.1139, 0.2123, 0.0279, 0.0659, 0.1484, 0.0895, 0.0412, 0.1042,
        0.0311, 0.0447], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,114][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.9400, 0.0015, 0.0034, 0.0240, 0.0021, 0.0011, 0.0033, 0.0042, 0.0087,
        0.0066, 0.0052], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,117][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0876, 0.0297, 0.0879, 0.1013, 0.0340, 0.0293, 0.1329, 0.1288, 0.0566,
        0.2536, 0.0582], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,121][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0557, 0.0160, 0.1151, 0.0855, 0.0140, 0.0061, 0.0616, 0.1556, 0.0352,
        0.4133, 0.0418], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,123][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0046, 0.0688, 0.1134, 0.0326, 0.1513, 0.0802, 0.0782, 0.1129, 0.0774,
        0.1846, 0.0961], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,124][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.3167, 0.0409, 0.0600, 0.0777, 0.0403, 0.0514, 0.0493, 0.1237, 0.1079,
        0.0608, 0.0714], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,124][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0047, 0.0518, 0.0984, 0.0738, 0.0465, 0.0132, 0.0767, 0.0834, 0.0304,
        0.4690, 0.0520], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,124][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1104, 0.0349, 0.0651, 0.1077, 0.0419, 0.0198, 0.0826, 0.1030, 0.0430,
        0.2541, 0.0602, 0.0774], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,125][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0126, 0.0330, 0.1364, 0.0554, 0.0287, 0.0285, 0.1135, 0.1358, 0.0612,
        0.2044, 0.0931, 0.0975], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,125][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0793, 0.0835, 0.0986, 0.0870, 0.0923, 0.0784, 0.0810, 0.1007, 0.0760,
        0.0768, 0.0784, 0.0679], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,126][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.2160, 0.0391, 0.0904, 0.1207, 0.0370, 0.0267, 0.0811, 0.0681, 0.0479,
        0.1324, 0.0446, 0.0959], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,126][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.8246, 0.0022, 0.0108, 0.0456, 0.0064, 0.0057, 0.0135, 0.0178, 0.0177,
        0.0128, 0.0090, 0.0337], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,128][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0981, 0.1282, 0.1800, 0.0312, 0.0832, 0.1298, 0.0831, 0.0520, 0.0891,
        0.0364, 0.0477, 0.0413], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,131][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.7460, 0.0075, 0.0173, 0.0579, 0.0104, 0.0085, 0.0182, 0.0158, 0.0351,
        0.0231, 0.0181, 0.0421], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,135][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0581, 0.0379, 0.0908, 0.0837, 0.0541, 0.0414, 0.1151, 0.1028, 0.0588,
        0.1704, 0.0751, 0.1117], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,137][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0829, 0.0247, 0.1118, 0.1056, 0.0230, 0.0140, 0.0703, 0.1527, 0.0395,
        0.2380, 0.0566, 0.0807], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,137][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0011, 0.1052, 0.2069, 0.0171, 0.1134, 0.1007, 0.0899, 0.1143, 0.0619,
        0.0687, 0.0946, 0.0262], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,138][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.1769, 0.0492, 0.0745, 0.0734, 0.0470, 0.0550, 0.0625, 0.0925, 0.1085,
        0.0723, 0.0741, 0.1141], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,138][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0097, 0.0766, 0.0898, 0.0865, 0.0659, 0.0246, 0.0689, 0.0774, 0.0415,
        0.3327, 0.0791, 0.0474], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,138][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.0672, 0.0209, 0.0550, 0.0690, 0.0209, 0.0111, 0.0867, 0.1064, 0.0312,
        0.3246, 0.0355, 0.0809, 0.0906], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,139][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.0140, 0.0156, 0.1177, 0.0352, 0.0173, 0.0219, 0.1067, 0.1334, 0.0631,
        0.2089, 0.0717, 0.1088, 0.0856], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,139][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.0697, 0.0774, 0.0897, 0.0803, 0.0868, 0.0735, 0.0753, 0.0917, 0.0718,
        0.0720, 0.0725, 0.0624, 0.0772], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,141][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.1953, 0.0224, 0.0735, 0.0958, 0.0296, 0.0189, 0.0898, 0.0526, 0.0455,
        0.1499, 0.0383, 0.1120, 0.0764], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,142][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([8.9585e-01, 8.4356e-04, 7.9564e-03, 2.6433e-02, 2.7080e-03, 2.6966e-03,
        8.4540e-03, 7.5048e-03, 1.0244e-02, 8.2782e-03, 3.0660e-03, 2.2474e-02,
        3.4911e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,145][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.0477, 0.0939, 0.1965, 0.0276, 0.0826, 0.1635, 0.0881, 0.0403, 0.1037,
        0.0251, 0.0576, 0.0459, 0.0274], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,149][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.7254, 0.0070, 0.0161, 0.0615, 0.0111, 0.0073, 0.0189, 0.0156, 0.0332,
        0.0303, 0.0193, 0.0413, 0.0129], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,150][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0577, 0.0238, 0.0763, 0.0664, 0.0284, 0.0277, 0.1074, 0.0800, 0.0525,
        0.2074, 0.0375, 0.1110, 0.1240], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,151][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.0409, 0.0133, 0.1011, 0.0576, 0.0153, 0.0090, 0.0624, 0.1448, 0.0341,
        0.3098, 0.0514, 0.0838, 0.0763], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,151][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([0.0190, 0.0348, 0.0999, 0.0426, 0.0930, 0.0894, 0.0834, 0.0755, 0.0760,
        0.1159, 0.1172, 0.0674, 0.0859], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,152][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.1173, 0.0325, 0.0678, 0.0582, 0.0381, 0.0772, 0.0757, 0.0820, 0.1212,
        0.0577, 0.0655, 0.1103, 0.0964], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,152][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0055, 0.0377, 0.0921, 0.0528, 0.0328, 0.0121, 0.0642, 0.0630, 0.0293,
        0.4031, 0.0510, 0.0491, 0.1073], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,153][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1363, 0.0122, 0.0366, 0.0929, 0.0180, 0.0059, 0.0578, 0.0628, 0.0276,
        0.3017, 0.0309, 0.0772, 0.1091, 0.0309], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,153][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0168, 0.0126, 0.1059, 0.0463, 0.0114, 0.0109, 0.0802, 0.1250, 0.0425,
        0.1832, 0.0499, 0.0939, 0.0649, 0.1565], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,155][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0673, 0.0719, 0.0835, 0.0730, 0.0798, 0.0674, 0.0698, 0.0848, 0.0671,
        0.0671, 0.0678, 0.0591, 0.0734, 0.0681], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,157][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.3489, 0.0157, 0.0533, 0.1250, 0.0190, 0.0101, 0.0520, 0.0399, 0.0304,
        0.1076, 0.0245, 0.0853, 0.0583, 0.0301], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,160][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.2204e-01, 4.4325e-04, 3.9297e-03, 2.2369e-02, 1.9749e-03, 1.4202e-03,
        5.6641e-03, 6.2935e-03, 8.0673e-03, 5.1124e-03, 2.7473e-03, 1.6233e-02,
        2.5239e-03, 1.1817e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,164][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.1562, 0.1063, 0.1555, 0.0312, 0.0786, 0.0853, 0.0617, 0.0419, 0.0685,
        0.0286, 0.0506, 0.0385, 0.0246, 0.0725], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,164][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.8950, 0.0021, 0.0050, 0.0323, 0.0030, 0.0024, 0.0061, 0.0045, 0.0141,
        0.0090, 0.0055, 0.0172, 0.0022, 0.0016], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,165][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0796, 0.0126, 0.0519, 0.0681, 0.0252, 0.0162, 0.0759, 0.0754, 0.0435,
        0.1803, 0.0466, 0.1113, 0.1203, 0.0930], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,165][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0814, 0.0088, 0.0700, 0.0770, 0.0092, 0.0032, 0.0408, 0.0981, 0.0240,
        0.3408, 0.0305, 0.0685, 0.0815, 0.0661], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,166][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0008, 0.0514, 0.0972, 0.0134, 0.1178, 0.0685, 0.0774, 0.1207, 0.0600,
        0.1060, 0.1084, 0.0242, 0.0590, 0.0952], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,166][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.3916, 0.0176, 0.0344, 0.0527, 0.0168, 0.0235, 0.0355, 0.0512, 0.0622,
        0.0459, 0.0389, 0.0932, 0.0931, 0.0434], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,167][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0077, 0.0264, 0.0656, 0.0874, 0.0295, 0.0076, 0.0493, 0.0593, 0.0257,
        0.4099, 0.0389, 0.0438, 0.0938, 0.0552], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,168][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:38,169][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[33083],
        [43165],
        [18404],
        [21341],
        [43459],
        [16043],
        [19372],
        [10842],
        [39265],
        [43156],
        [18525],
        [19852],
        [11486],
        [15651]], device='cuda:0')
[2024-07-24 10:30:38,171][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[33838],
        [ 4641],
        [ 9953],
        [16261],
        [ 7068],
        [ 9781],
        [13870],
        [ 7024],
        [ 3210],
        [17271],
        [16674],
        [20676],
        [20319],
        [15473]], device='cuda:0')
[2024-07-24 10:30:38,173][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[46845],
        [48189],
        [48878],
        [48742],
        [48174],
        [48077],
        [48223],
        [48472],
        [48478],
        [48521],
        [48434],
        [48481],
        [48545],
        [48633]], device='cuda:0')
[2024-07-24 10:30:38,174][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[43177],
        [40840],
        [42206],
        [43180],
        [42429],
        [41776],
        [41856],
        [41914],
        [41453],
        [41241],
        [41427],
        [41673],
        [41484],
        [41243]], device='cuda:0')
[2024-07-24 10:30:38,176][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 1670],
        [ 3000],
        [13439],
        [ 4523],
        [ 2398],
        [ 2760],
        [ 3785],
        [ 6872],
        [ 6185],
        [ 4336],
        [ 9535],
        [11017],
        [ 6706],
        [ 9371]], device='cuda:0')
[2024-07-24 10:30:38,179][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 4669],
        [12260],
        [15746],
        [19190],
        [14490],
        [17515],
        [18007],
        [16809],
        [20285],
        [18458],
        [17041],
        [18826],
        [19310],
        [17927]], device='cuda:0')
[2024-07-24 10:30:38,181][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[41863],
        [37455],
        [39552],
        [21810],
        [27204],
        [25134],
        [26443],
        [30902],
        [23838],
        [19528],
        [23438],
        [25371],
        [15884],
        [19861]], device='cuda:0')
[2024-07-24 10:30:38,182][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[  559],
        [10805],
        [ 6061],
        [ 9258],
        [ 7983],
        [ 7717],
        [ 4675],
        [ 5700],
        [ 2507],
        [ 6849],
        [ 7670],
        [ 5521],
        [ 5636],
        [ 5206]], device='cuda:0')
[2024-07-24 10:30:38,183][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[14567],
        [19511],
        [17504],
        [15844],
        [11837],
        [11235],
        [12057],
        [11066],
        [10734],
        [ 9300],
        [ 9925],
        [10199],
        [ 9403],
        [10103]], device='cuda:0')
[2024-07-24 10:30:38,184][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 3503],
        [35377],
        [34313],
        [34666],
        [33790],
        [34915],
        [32939],
        [32573],
        [31001],
        [28290],
        [28074],
        [27775],
        [28452],
        [28033]], device='cuda:0')
[2024-07-24 10:30:38,186][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[27100],
        [30077],
        [32928],
        [33607],
        [32689],
        [32790],
        [29518],
        [26701],
        [26231],
        [22375],
        [20266],
        [20832],
        [19344],
        [18381]], device='cuda:0')
[2024-07-24 10:30:38,187][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[48727],
        [44310],
        [42053],
        [42977],
        [40159],
        [40307],
        [39838],
        [37928],
        [38053],
        [38138],
        [36979],
        [37147],
        [36936],
        [37353]], device='cuda:0')
[2024-07-24 10:30:38,189][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[36914],
        [25153],
        [26349],
        [25394],
        [25428],
        [25431],
        [25675],
        [25111],
        [24610],
        [24076],
        [24089],
        [23757],
        [23219],
        [23745]], device='cuda:0')
[2024-07-24 10:30:38,190][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[18704],
        [13656],
        [ 7820],
        [ 9117],
        [ 8657],
        [ 9091],
        [ 9306],
        [ 8673],
        [ 8571],
        [17234],
        [17141],
        [15202],
        [15072],
        [16455]], device='cuda:0')
[2024-07-24 10:30:38,193][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[37810],
        [48915],
        [46542],
        [45365],
        [49010],
        [48084],
        [47124],
        [47013],
        [49682],
        [48584],
        [47133],
        [46415],
        [35013],
        [45039]], device='cuda:0')
[2024-07-24 10:30:38,195][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[20604],
        [29016],
        [27521],
        [40521],
        [41087],
        [42986],
        [42626],
        [41277],
        [42205],
        [40146],
        [40237],
        [40658],
        [40547],
        [40565]], device='cuda:0')
[2024-07-24 10:30:38,198][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[43642],
        [39522],
        [16406],
        [19360],
        [20482],
        [20759],
        [25516],
        [28382],
        [27948],
        [29211],
        [30741],
        [33572],
        [34318],
        [32815]], device='cuda:0')
[2024-07-24 10:30:38,199][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[45849],
        [37493],
        [40958],
        [40975],
        [41483],
        [42184],
        [42455],
        [42497],
        [42845],
        [42499],
        [42268],
        [42205],
        [42158],
        [42228]], device='cuda:0')
[2024-07-24 10:30:38,200][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[34943],
        [47223],
        [47048],
        [47892],
        [48010],
        [47773],
        [47736],
        [47753],
        [47807],
        [47902],
        [48099],
        [48062],
        [47951],
        [47937]], device='cuda:0')
[2024-07-24 10:30:38,201][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[30463],
        [30466],
        [30505],
        [30452],
        [30407],
        [30320],
        [30353],
        [30452],
        [30394],
        [30597],
        [30628],
        [30869],
        [31092],
        [30932]], device='cuda:0')
[2024-07-24 10:30:38,202][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[32004],
        [ 7677],
        [12953],
        [13017],
        [14014],
        [14146],
        [14701],
        [15108],
        [15797],
        [15894],
        [15825],
        [15845],
        [16293],
        [15828]], device='cuda:0')
[2024-07-24 10:30:38,203][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[46970],
        [46988],
        [46884],
        [46659],
        [46749],
        [46611],
        [46474],
        [46512],
        [46086],
        [45672],
        [46352],
        [43622],
        [43376],
        [45567]], device='cuda:0')
[2024-07-24 10:30:38,205][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[48007],
        [18288],
        [34274],
        [29071],
        [32920],
        [32583],
        [37528],
        [38394],
        [38284],
        [39935],
        [39305],
        [38246],
        [39453],
        [41024]], device='cuda:0')
[2024-07-24 10:30:38,207][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[26200],
        [29432],
        [40511],
        [48203],
        [47995],
        [48551],
        [45973],
        [45778],
        [48267],
        [43892],
        [42938],
        [43874],
        [42415],
        [39819]], device='cuda:0')
[2024-07-24 10:30:38,209][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[33906],
        [12868],
        [18212],
        [20361],
        [14399],
        [13242],
        [15926],
        [17223],
        [18567],
        [18210],
        [17741],
        [18064],
        [19400],
        [19044]], device='cuda:0')
[2024-07-24 10:30:38,212][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[41143],
        [47704],
        [49198],
        [49227],
        [49471],
        [49475],
        [49371],
        [49417],
        [49202],
        [49163],
        [49214],
        [49170],
        [49212],
        [49271]], device='cuda:0')
[2024-07-24 10:30:38,214][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[26988],
        [40635],
        [42454],
        [46366],
        [47233],
        [46954],
        [47127],
        [46879],
        [46124],
        [42549],
        [40670],
        [42167],
        [40033],
        [41258]], device='cuda:0')
[2024-07-24 10:30:38,215][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[2615],
        [3570],
        [1963],
        [ 803],
        [ 703],
        [ 728],
        [ 689],
        [ 647],
        [ 722],
        [1119],
        [1276],
        [1236],
        [1324],
        [1189]], device='cuda:0')
[2024-07-24 10:30:38,216][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 8306],
        [38112],
        [20418],
        [14145],
        [40716],
        [11741],
        [ 6828],
        [11772],
        [27835],
        [37953],
        [ 6844],
        [ 5785],
        [ 8989],
        [10107]], device='cuda:0')
[2024-07-24 10:30:38,217][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342],
        [16342]], device='cuda:0')
[2024-07-24 10:30:38,263][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:38,263][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,264][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,264][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,264][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,265][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,265][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,265][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,266][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,266][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,267][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,267][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,268][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,268][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.2991, 0.7009], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,268][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.4572, 0.5428], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,269][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.0072, 0.9928], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,269][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.2082, 0.7918], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,269][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.4830, 0.5170], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,270][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.9989, 0.0011], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,270][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.2788, 0.7212], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,270][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.2058, 0.7942], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,271][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.6181, 0.3819], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,271][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.3087, 0.6913], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,271][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0914, 0.9086], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,272][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0430, 0.9570], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,274][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0085, 0.8313, 0.1602], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,277][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.2695, 0.3744, 0.3561], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,277][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0024, 0.5146, 0.4830], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,278][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0977, 0.4446, 0.4577], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,278][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0901, 0.2324, 0.6775], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,278][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ and] are: tensor([9.9888e-01, 2.6679e-04, 8.5515e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,279][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1494, 0.4322, 0.4184], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,282][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0105, 0.6455, 0.3440], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,283][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0950, 0.4284, 0.4766], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,284][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0470, 0.3741, 0.5789], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,284][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0023, 0.3547, 0.6430], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,284][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0491, 0.2396, 0.7113], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,285][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.0062, 0.7051, 0.2322, 0.0565], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,285][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.1638, 0.2954, 0.2763, 0.2645], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,285][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.0020, 0.3650, 0.3529, 0.2801], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,286][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.0683, 0.3984, 0.3603, 0.1729], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,286][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0161, 0.2679, 0.4743, 0.2417], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,288][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.7549, 0.0152, 0.0364, 0.1935], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,290][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.1141, 0.2988, 0.2946, 0.2925], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,294][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.0010, 0.7155, 0.2486, 0.0350], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,297][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.0257, 0.3916, 0.3205, 0.2622], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,297][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.0142, 0.3221, 0.4422, 0.2215], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,297][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([1.1929e-04, 2.6880e-01, 7.2650e-01, 4.5810e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,298][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.0293, 0.0372, 0.8401, 0.0934], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,298][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0106, 0.7655, 0.0917, 0.0642, 0.0680], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,298][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.1496, 0.2164, 0.2088, 0.2102, 0.2150], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,299][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.0022, 0.2650, 0.2532, 0.2035, 0.2762], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,299][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0653, 0.2756, 0.2763, 0.1466, 0.2362], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,300][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.0736, 0.0569, 0.2609, 0.5543, 0.0543], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,301][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ went] are: tensor([0.8799, 0.0010, 0.0043, 0.1129, 0.0020], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,304][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.0895, 0.2265, 0.2262, 0.2269, 0.2308], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,308][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.0169, 0.2460, 0.1749, 0.3827, 0.1794], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,310][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.0681, 0.1297, 0.2520, 0.4968, 0.0534], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,311][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ went] are: tensor([0.0283, 0.0955, 0.3894, 0.3841, 0.1026], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,311][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.0153, 0.1208, 0.5164, 0.0625, 0.2850], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,311][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ went] are: tensor([2.1778e-04, 5.8411e-03, 9.5745e-01, 7.9713e-03, 2.8519e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,312][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0012, 0.8032, 0.0861, 0.0253, 0.0773, 0.0069], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,312][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.1226, 0.1769, 0.1704, 0.1750, 0.1792, 0.1760], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,312][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0009, 0.2182, 0.2069, 0.1625, 0.2315, 0.1799], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,313][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0393, 0.2502, 0.2341, 0.1243, 0.1967, 0.1554], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,313][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0177, 0.1422, 0.3019, 0.3867, 0.0660, 0.0855], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,314][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([8.6932e-01, 8.6133e-04, 3.0598e-03, 1.2017e-01, 2.0535e-03, 4.5346e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,317][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0601, 0.1901, 0.1870, 0.1861, 0.1955, 0.1813], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,320][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0074, 0.3193, 0.1928, 0.1393, 0.2286, 0.1126], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,324][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0280, 0.1794, 0.2967, 0.3618, 0.0832, 0.0509], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,324][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0076, 0.2383, 0.2743, 0.2612, 0.1453, 0.0732], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,324][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.2451e-04, 1.4812e-01, 3.7295e-01, 5.2263e-03, 3.0380e-01, 1.6978e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,325][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0289, 0.0407, 0.4838, 0.0841, 0.1542, 0.2083], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,325][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0113, 0.4241, 0.1029, 0.0839, 0.1417, 0.0187, 0.2175],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,326][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0907, 0.1639, 0.1544, 0.1497, 0.1585, 0.1567, 0.1263],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,326][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0011, 0.1768, 0.1670, 0.1331, 0.1855, 0.1458, 0.1907],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,326][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0426, 0.1876, 0.1955, 0.0980, 0.1646, 0.1321, 0.1796],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,327][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0318, 0.0543, 0.1974, 0.3057, 0.0507, 0.0741, 0.2860],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,328][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.8475, 0.0011, 0.0045, 0.1243, 0.0032, 0.0059, 0.0134],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,331][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0584, 0.1619, 0.1597, 0.1567, 0.1627, 0.1529, 0.1477],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,335][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0111, 0.2663, 0.1814, 0.1373, 0.2063, 0.1067, 0.0910],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,337][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0389, 0.0840, 0.2003, 0.3757, 0.0801, 0.0496, 0.1712],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,338][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0194, 0.1156, 0.1851, 0.2525, 0.1165, 0.0692, 0.2417],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,338][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([2.9551e-04, 1.7553e-01, 3.5397e-01, 8.1290e-03, 2.2268e-01, 1.4989e-01,
        8.9505e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,338][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0181, 0.0216, 0.4811, 0.0650, 0.0730, 0.1790, 0.1622],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,339][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0055, 0.3152, 0.0800, 0.0606, 0.0970, 0.0120, 0.2245, 0.2051],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,339][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0908, 0.1353, 0.1325, 0.1315, 0.1355, 0.1349, 0.1146, 0.1251],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,340][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.0008, 0.1540, 0.1469, 0.1149, 0.1634, 0.1278, 0.1713, 0.1209],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,340][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.0346, 0.1609, 0.1665, 0.0878, 0.1374, 0.1093, 0.1560, 0.1474],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,342][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.0401, 0.0529, 0.1504, 0.2590, 0.0367, 0.0545, 0.2675, 0.1390],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,344][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ office] are: tensor([9.0816e-01, 4.6453e-04, 1.6913e-03, 7.1591e-02, 1.1777e-03, 2.7334e-03,
        5.7716e-03, 8.4063e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,347][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.0539, 0.1418, 0.1388, 0.1413, 0.1400, 0.1358, 0.1306, 0.1178],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,351][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0072, 0.2651, 0.1148, 0.0879, 0.1671, 0.0770, 0.0958, 0.1852],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,351][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.0420, 0.0510, 0.1146, 0.2625, 0.0475, 0.0442, 0.2069, 0.2312],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,351][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.0174, 0.0926, 0.1663, 0.1906, 0.0727, 0.0479, 0.2088, 0.2038],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,352][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ office] are: tensor([1.9960e-04, 1.7377e-01, 2.6818e-01, 4.7322e-03, 2.1681e-01, 1.2652e-01,
        1.1906e-01, 9.0732e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,352][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.0083, 0.0212, 0.3826, 0.0444, 0.0645, 0.1603, 0.1684, 0.1503],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,353][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0018, 0.6611, 0.0434, 0.0223, 0.0775, 0.0063, 0.0469, 0.1253, 0.0154],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,353][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0800, 0.1243, 0.1175, 0.1197, 0.1220, 0.1216, 0.1005, 0.1134, 0.1010],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,353][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0006, 0.1360, 0.1299, 0.1027, 0.1462, 0.1110, 0.1488, 0.1064, 0.1184],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,355][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0379, 0.1452, 0.1369, 0.0769, 0.1155, 0.0941, 0.1262, 0.1278, 0.1394],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,357][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0279, 0.0474, 0.1306, 0.2939, 0.0356, 0.0636, 0.2098, 0.1216, 0.0697],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,360][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.6680, 0.0062, 0.0139, 0.2014, 0.0091, 0.0114, 0.0235, 0.0564, 0.0102],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,364][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0459, 0.1245, 0.1203, 0.1232, 0.1263, 0.1159, 0.1127, 0.1090, 0.1223],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,365][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0020, 0.1959, 0.1015, 0.0798, 0.1076, 0.0874, 0.0647, 0.1733, 0.1878],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,365][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0208, 0.0986, 0.2065, 0.1840, 0.0562, 0.0455, 0.1298, 0.1945, 0.0642],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,365][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0117, 0.0852, 0.1307, 0.1737, 0.0678, 0.0496, 0.1421, 0.2180, 0.1212],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,366][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0005, 0.1591, 0.2357, 0.0125, 0.2074, 0.1133, 0.0767, 0.1048, 0.0900],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,366][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0034, 0.0226, 0.4468, 0.0364, 0.0421, 0.1406, 0.0976, 0.1391, 0.0715],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,367][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0355, 0.3399, 0.0363, 0.0368, 0.0448, 0.0034, 0.0834, 0.0940, 0.0364,
        0.2896], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,367][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0706, 0.1088, 0.1072, 0.1060, 0.1084, 0.1080, 0.0917, 0.1001, 0.0907,
        0.1085], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,369][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.0007, 0.1232, 0.1148, 0.0951, 0.1302, 0.0993, 0.1307, 0.0949, 0.1070,
        0.1040], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,371][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.0355, 0.1170, 0.1174, 0.0630, 0.1019, 0.0818, 0.1106, 0.1047, 0.1211,
        0.1470], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,375][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.0561, 0.0299, 0.1039, 0.2084, 0.0331, 0.0429, 0.2096, 0.0970, 0.0673,
        0.1518], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,378][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.8140, 0.0014, 0.0066, 0.1053, 0.0037, 0.0062, 0.0198, 0.0282, 0.0066,
        0.0083], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,378][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0438, 0.1093, 0.1057, 0.1075, 0.1098, 0.1042, 0.1017, 0.0957, 0.1098,
        0.1126], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,379][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.0094, 0.0798, 0.0867, 0.0756, 0.0997, 0.0953, 0.0951, 0.1217, 0.2005,
        0.1360], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,379][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.0366, 0.0343, 0.1149, 0.1561, 0.0425, 0.0375, 0.1440, 0.1489, 0.0619,
        0.2233], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,379][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([0.0236, 0.0235, 0.0806, 0.1249, 0.0310, 0.0226, 0.1015, 0.0981, 0.0560,
        0.4383], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,380][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0035, 0.1081, 0.2067, 0.0258, 0.1903, 0.1179, 0.0862, 0.1055, 0.0734,
        0.0825], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,380][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0010, 0.0078, 0.5699, 0.0136, 0.0260, 0.1843, 0.0741, 0.0665, 0.0402,
        0.0165], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,381][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0099, 0.4486, 0.0280, 0.0185, 0.0318, 0.0022, 0.0671, 0.1014, 0.0252,
        0.2251, 0.0422], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,382][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0632, 0.1025, 0.0966, 0.0965, 0.0988, 0.0975, 0.0812, 0.0910, 0.0816,
        0.0976, 0.0934], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,385][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0007, 0.1137, 0.1062, 0.0854, 0.1186, 0.0910, 0.1200, 0.0884, 0.0969,
        0.0915, 0.0876], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,389][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0234, 0.1109, 0.1075, 0.0577, 0.0908, 0.0715, 0.0991, 0.0982, 0.1132,
        0.1328, 0.0948], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,391][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0146, 0.0375, 0.1043, 0.1484, 0.0263, 0.0352, 0.1848, 0.0900, 0.0740,
        0.1982, 0.0867], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,392][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.7853, 0.0024, 0.0054, 0.1268, 0.0034, 0.0059, 0.0145, 0.0245, 0.0058,
        0.0116, 0.0145], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,392][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0386, 0.0999, 0.0983, 0.0966, 0.0984, 0.0942, 0.0898, 0.0873, 0.0978,
        0.1037, 0.0953], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,392][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0038, 0.1449, 0.0831, 0.0535, 0.1030, 0.0660, 0.0612, 0.1459, 0.1044,
        0.0791, 0.1552], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,393][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0140, 0.0723, 0.1207, 0.1285, 0.0312, 0.0263, 0.1225, 0.1417, 0.0538,
        0.2248, 0.0643], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,393][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0046, 0.0405, 0.0651, 0.0654, 0.0280, 0.0171, 0.0823, 0.0808, 0.0605,
        0.5181, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,394][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([1.4632e-04, 1.3849e-01, 2.3645e-01, 5.2305e-03, 1.9602e-01, 9.6495e-02,
        6.8630e-02, 6.3186e-02, 6.9912e-02, 9.2687e-02, 3.2747e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,394][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0065, 0.0296, 0.1468, 0.0210, 0.0777, 0.0673, 0.1298, 0.0735, 0.1602,
        0.1343, 0.1532], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,396][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0076, 0.3354, 0.0406, 0.0248, 0.0662, 0.0105, 0.0709, 0.1066, 0.0372,
        0.1900, 0.0728, 0.0373], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,398][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0515, 0.0966, 0.0913, 0.0871, 0.0937, 0.0931, 0.0751, 0.0844, 0.0766,
        0.0911, 0.0866, 0.0730], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,402][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0006, 0.1029, 0.0960, 0.0775, 0.1070, 0.0825, 0.1078, 0.0811, 0.0882,
        0.0828, 0.0802, 0.0934], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,405][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0240, 0.1029, 0.0999, 0.0513, 0.0815, 0.0655, 0.0884, 0.0890, 0.1019,
        0.1215, 0.0852, 0.0891], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,405][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0093, 0.0618, 0.1335, 0.0924, 0.0461, 0.0669, 0.1533, 0.0948, 0.0731,
        0.1399, 0.0916, 0.0373], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,405][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.6438, 0.0054, 0.0101, 0.1444, 0.0088, 0.0154, 0.0254, 0.0332, 0.0151,
        0.0158, 0.0248, 0.0579], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,406][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0400, 0.0897, 0.0885, 0.0888, 0.0899, 0.0857, 0.0814, 0.0797, 0.0911,
        0.0935, 0.0880, 0.0836], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,406][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0058, 0.2865, 0.1003, 0.0443, 0.1053, 0.0542, 0.0391, 0.0731, 0.0945,
        0.0609, 0.0973, 0.0386], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,407][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0060, 0.1089, 0.1443, 0.0807, 0.0703, 0.0435, 0.0899, 0.1090, 0.0529,
        0.1876, 0.0812, 0.0257], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,407][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0045, 0.0766, 0.0737, 0.0519, 0.0520, 0.0402, 0.0819, 0.0972, 0.0649,
        0.3557, 0.0639, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,408][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([1.1447e-04, 1.7744e-01, 2.5724e-01, 3.9303e-03, 1.7912e-01, 7.5746e-02,
        5.8243e-02, 4.2249e-02, 7.7272e-02, 8.2140e-02, 3.9180e-02, 7.3269e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,409][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0152, 0.0208, 0.2309, 0.0467, 0.0452, 0.1062, 0.0902, 0.0982, 0.0760,
        0.0473, 0.1616, 0.0616], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,412][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0017, 0.4495, 0.0217, 0.0067, 0.0394, 0.0021, 0.0611, 0.0484, 0.0336,
        0.2636, 0.0403, 0.0200, 0.0120], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,416][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0437, 0.0887, 0.0881, 0.0802, 0.0889, 0.0893, 0.0720, 0.0790, 0.0731,
        0.0856, 0.0810, 0.0684, 0.0619], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,418][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0012, 0.0882, 0.0843, 0.0700, 0.0926, 0.0747, 0.0936, 0.0718, 0.0810,
        0.0769, 0.0729, 0.0844, 0.1084], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,419][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.0198, 0.0936, 0.0872, 0.0459, 0.0747, 0.0602, 0.0842, 0.0768, 0.0931,
        0.1163, 0.0774, 0.0810, 0.0897], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,419][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.0034, 0.0656, 0.1375, 0.0620, 0.0353, 0.0466, 0.1614, 0.0853, 0.0718,
        0.1810, 0.0838, 0.0241, 0.0420], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,419][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.5958, 0.0035, 0.0141, 0.1428, 0.0077, 0.0111, 0.0333, 0.0411, 0.0111,
        0.0256, 0.0220, 0.0591, 0.0326], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,420][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0374, 0.0812, 0.0815, 0.0824, 0.0822, 0.0798, 0.0765, 0.0722, 0.0825,
        0.0831, 0.0834, 0.0786, 0.0793], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,420][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0035, 0.1343, 0.0855, 0.0243, 0.1193, 0.1236, 0.0651, 0.0607, 0.1185,
        0.0948, 0.0972, 0.0562, 0.0170], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,421][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.0023, 0.0562, 0.1738, 0.0414, 0.0528, 0.0272, 0.0883, 0.0606, 0.0659,
        0.2954, 0.0597, 0.0263, 0.0500], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,422][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0022, 0.0398, 0.0899, 0.0404, 0.0354, 0.0273, 0.0916, 0.0676, 0.0518,
        0.4156, 0.0378, 0.0287, 0.0720], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,425][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0004, 0.1733, 0.1131, 0.0071, 0.1976, 0.0888, 0.0585, 0.0541, 0.0711,
        0.1351, 0.0553, 0.0163, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,429][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0020, 0.0123, 0.3080, 0.0137, 0.0358, 0.1638, 0.1156, 0.0531, 0.0746,
        0.0277, 0.1313, 0.0411, 0.0210], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,432][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0487, 0.1141, 0.0190, 0.0585, 0.0199, 0.0032, 0.0780, 0.0983, 0.0238,
        0.2126, 0.0503, 0.0906, 0.0620, 0.1209], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,432][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0474, 0.0818, 0.0782, 0.0770, 0.0813, 0.0801, 0.0663, 0.0725, 0.0671,
        0.0799, 0.0753, 0.0643, 0.0610, 0.0679], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,432][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0007, 0.0817, 0.0762, 0.0627, 0.0848, 0.0672, 0.0868, 0.0658, 0.0713,
        0.0669, 0.0643, 0.0750, 0.1010, 0.0958], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,433][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0237, 0.0828, 0.0790, 0.0492, 0.0653, 0.0528, 0.0718, 0.0738, 0.0811,
        0.0983, 0.0713, 0.0735, 0.0813, 0.0960], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,433][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0198, 0.0168, 0.0756, 0.1561, 0.0142, 0.0270, 0.1489, 0.0695, 0.0537,
        0.1227, 0.0606, 0.0478, 0.0482, 0.1391], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,434][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([8.7287e-01, 3.0074e-04, 1.0364e-03, 7.9029e-02, 7.7805e-04, 1.7341e-03,
        4.1997e-03, 6.4214e-03, 1.5825e-03, 2.0144e-03, 4.4521e-03, 1.6820e-02,
        6.0063e-03, 2.7513e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,434][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0320, 0.0752, 0.0746, 0.0747, 0.0769, 0.0724, 0.0695, 0.0687, 0.0771,
        0.0784, 0.0758, 0.0722, 0.0751, 0.0775], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,436][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0102, 0.1339, 0.0627, 0.0649, 0.0609, 0.0380, 0.0504, 0.0970, 0.1080,
        0.0799, 0.1308, 0.0697, 0.0182, 0.0752], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,438][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0375, 0.0330, 0.0942, 0.1870, 0.0247, 0.0188, 0.0808, 0.0829, 0.0379,
        0.1696, 0.0621, 0.0419, 0.0642, 0.0654], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,441][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0055, 0.0198, 0.0306, 0.0604, 0.0178, 0.0103, 0.0528, 0.0650, 0.0364,
        0.4188, 0.0306, 0.0351, 0.1230, 0.0939], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,443][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.4220e-04, 1.6706e-01, 1.9439e-01, 5.1955e-03, 1.5604e-01, 6.1392e-02,
        5.7895e-02, 5.3837e-02, 7.5285e-02, 9.2881e-02, 4.2689e-02, 7.4252e-03,
        1.7049e-02, 6.8724e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,445][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0122, 0.0209, 0.1243, 0.0340, 0.0493, 0.0566, 0.0834, 0.0797, 0.0917,
        0.1262, 0.1205, 0.0559, 0.0492, 0.0959], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,494][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:38,497][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,501][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,501][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,502][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,502][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,502][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,503][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,503][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,503][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,504][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,504][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,504][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,505][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.4551, 0.5449], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,506][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.9966, 0.0034], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,506][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.0122, 0.9878], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,506][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.8047, 0.1953], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,507][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.4830, 0.5170], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,507][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.9989, 0.0011], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,507][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([9.9901e-01, 9.9044e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,508][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.9946, 0.0054], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,508][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.6181, 0.3819], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,508][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.3087, 0.6913], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,509][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0914, 0.9086], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,509][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.9820, 0.0180], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,509][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0479, 0.4548, 0.4973], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,510][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9856, 0.0075, 0.0070], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,510][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0007, 0.4010, 0.5983], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,510][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.4465, 0.1661, 0.3874], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,511][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0901, 0.2324, 0.6775], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,511][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([9.9888e-01, 2.6679e-04, 8.5515e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,511][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.9876, 0.0033, 0.0090], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,512][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.8474, 0.0592, 0.0934], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,512][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0950, 0.4284, 0.4766], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,512][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0470, 0.3741, 0.5789], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,513][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0023, 0.3547, 0.6430], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,513][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.7152, 0.0773, 0.2076], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,513][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.0175, 0.3427, 0.3832, 0.2565], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,514][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.1376, 0.2577, 0.2771, 0.3277], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,514][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([9.3247e-06, 2.4627e-01, 7.1947e-01, 3.4260e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,514][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.0669, 0.2676, 0.3250, 0.3405], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,515][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.0161, 0.2679, 0.4743, 0.2417], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,515][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.7549, 0.0152, 0.0364, 0.1935], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,518][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.5969, 0.0644, 0.1030, 0.2358], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,521][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.0483, 0.4311, 0.3722, 0.1485], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,525][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.0257, 0.3916, 0.3205, 0.2622], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,525][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.0142, 0.3221, 0.4422, 0.2215], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,526][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([1.1929e-04, 2.6880e-01, 7.2650e-01, 4.5810e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,526][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.0529, 0.4139, 0.3993, 0.1339], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,526][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.0288, 0.2205, 0.1657, 0.5314, 0.0536], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,527][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.9268, 0.0022, 0.0028, 0.0665, 0.0018], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,527][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.0013, 0.1095, 0.2867, 0.1365, 0.4660], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,527][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.3031, 0.0655, 0.1362, 0.4550, 0.0402], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,528][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.0736, 0.0569, 0.2609, 0.5543, 0.0543], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,529][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([0.8799, 0.0010, 0.0043, 0.1129, 0.0020], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,532][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.9268, 0.0022, 0.0053, 0.0626, 0.0031], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,536][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.7314, 0.0169, 0.0281, 0.2145, 0.0091], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,539][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.0681, 0.1297, 0.2520, 0.4968, 0.0534], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,541][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.0283, 0.0955, 0.3894, 0.3841, 0.1026], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,541][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.0153, 0.1208, 0.5164, 0.0625, 0.2850], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,542][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.5564, 0.0422, 0.0920, 0.2417, 0.0677], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,542][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0060, 0.4068, 0.1690, 0.3001, 0.0731, 0.0450], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,542][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.7648, 0.0107, 0.0108, 0.1911, 0.0138, 0.0088], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,543][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([5.9930e-05, 7.8229e-02, 1.2997e-01, 2.6885e-02, 2.3379e-01, 5.3107e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,543][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1128, 0.1553, 0.2136, 0.4096, 0.0625, 0.0461], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,545][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0177, 0.1422, 0.3019, 0.3867, 0.0660, 0.0855], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,546][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([8.6932e-01, 8.6133e-04, 3.0598e-03, 1.2017e-01, 2.0535e-03, 4.5346e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,549][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.8449, 0.0042, 0.0134, 0.1175, 0.0080, 0.0120], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,553][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.4599, 0.0723, 0.1215, 0.2805, 0.0383, 0.0276], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,554][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0280, 0.1794, 0.2967, 0.3618, 0.0832, 0.0509], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,554][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0076, 0.2383, 0.2743, 0.2612, 0.1453, 0.0732], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,555][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.2451e-04, 1.4812e-01, 3.7295e-01, 5.2263e-03, 3.0380e-01, 1.6978e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,555][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.2693, 0.0564, 0.1639, 0.2260, 0.1291, 0.1553], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,556][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0205, 0.1358, 0.1514, 0.3360, 0.0735, 0.0595, 0.2233],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,556][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.6423, 0.0170, 0.0218, 0.2307, 0.0278, 0.0178, 0.0425],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,556][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([3.1383e-05, 5.6758e-02, 1.3087e-01, 1.6641e-02, 1.6433e-01, 3.6117e-01,
        2.7020e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,557][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1363, 0.0519, 0.1368, 0.3275, 0.0501, 0.0463, 0.2511],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,558][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0318, 0.0543, 0.1974, 0.3057, 0.0507, 0.0741, 0.2860],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,561][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.8475, 0.0011, 0.0045, 0.1243, 0.0032, 0.0059, 0.0134],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,565][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.7542, 0.0096, 0.0232, 0.1384, 0.0152, 0.0198, 0.0397],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,567][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.4473, 0.0711, 0.1146, 0.2436, 0.0443, 0.0330, 0.0461],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,568][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0389, 0.0840, 0.2003, 0.3757, 0.0801, 0.0496, 0.1712],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,568][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0194, 0.1156, 0.1851, 0.2525, 0.1165, 0.0692, 0.2417],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,569][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([2.9551e-04, 1.7553e-01, 3.5397e-01, 8.1290e-03, 2.2268e-01, 1.4989e-01,
        8.9505e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,569][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2673, 0.0479, 0.1298, 0.2020, 0.1002, 0.1006, 0.1522],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,569][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.0154, 0.1005, 0.0997, 0.2204, 0.0452, 0.0315, 0.2056, 0.2818],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,570][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.7619, 0.0080, 0.0094, 0.1438, 0.0125, 0.0083, 0.0306, 0.0255],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,571][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([4.9306e-05, 4.9366e-02, 8.8012e-02, 9.3350e-03, 1.5089e-01, 3.0890e-01,
        2.4018e-01, 1.5326e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,573][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.0978, 0.0465, 0.1153, 0.2675, 0.0429, 0.0396, 0.2277, 0.1627],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,576][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.0401, 0.0529, 0.1504, 0.2590, 0.0367, 0.0545, 0.2675, 0.1390],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,578][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([9.0816e-01, 4.6453e-04, 1.6913e-03, 7.1591e-02, 1.1777e-03, 2.7334e-03,
        5.7716e-03, 8.4063e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,581][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.7891, 0.0068, 0.0152, 0.1079, 0.0095, 0.0159, 0.0301, 0.0255],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,581][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.3581, 0.1035, 0.1035, 0.2063, 0.0468, 0.0316, 0.0539, 0.0963],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,582][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.0420, 0.0510, 0.1146, 0.2625, 0.0475, 0.0442, 0.2069, 0.2312],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,582][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.0174, 0.0926, 0.1663, 0.1906, 0.0727, 0.0479, 0.2088, 0.2038],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,582][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([1.9960e-04, 1.7377e-01, 2.6818e-01, 4.7322e-03, 2.1681e-01, 1.2652e-01,
        1.1906e-01, 9.0732e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,583][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0796, 0.0453, 0.1638, 0.1108, 0.1122, 0.1458, 0.1980, 0.1445],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,583][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0029, 0.2216, 0.1012, 0.1447, 0.0559, 0.0357, 0.0980, 0.3097, 0.0303],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,584][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.4814, 0.0215, 0.0212, 0.2395, 0.0312, 0.0215, 0.0358, 0.0935, 0.0544],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,584][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([5.6679e-05, 4.7636e-02, 7.7529e-02, 1.9638e-02, 1.3606e-01, 2.5684e-01,
        2.0309e-01, 1.5906e-01, 1.0009e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,587][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0761, 0.0583, 0.1002, 0.2700, 0.0356, 0.0366, 0.1797, 0.1549, 0.0885],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,590][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0279, 0.0474, 0.1306, 0.2939, 0.0356, 0.0636, 0.2098, 0.1216, 0.0697],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,594][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.6680, 0.0062, 0.0139, 0.2014, 0.0091, 0.0114, 0.0235, 0.0564, 0.0102],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,595][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.6278, 0.0143, 0.0312, 0.1476, 0.0201, 0.0246, 0.0426, 0.0505, 0.0413],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,595][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.2723, 0.0956, 0.1019, 0.2178, 0.0370, 0.0379, 0.0431, 0.0973, 0.0972],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,595][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0208, 0.0986, 0.2065, 0.1840, 0.0562, 0.0455, 0.1298, 0.1945, 0.0642],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,596][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0117, 0.0852, 0.1307, 0.1737, 0.0678, 0.0496, 0.1421, 0.2180, 0.1212],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,596][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0005, 0.1591, 0.2357, 0.0125, 0.2074, 0.1133, 0.0767, 0.1048, 0.0900],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,597][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.2974, 0.0290, 0.0594, 0.1982, 0.0437, 0.0536, 0.0844, 0.1133, 0.1210],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,598][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.0141, 0.0479, 0.0467, 0.1451, 0.0247, 0.0125, 0.0822, 0.1617, 0.0289,
        0.4362], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,601][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.7082, 0.0041, 0.0102, 0.1489, 0.0086, 0.0082, 0.0255, 0.0280, 0.0220,
        0.0361], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,603][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([8.5933e-05, 2.5962e-02, 5.6647e-02, 1.3143e-02, 1.7132e-01, 2.7764e-01,
        1.6620e-01, 1.0681e-01, 9.7955e-02, 8.4238e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,607][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.1422, 0.0175, 0.0489, 0.2012, 0.0216, 0.0187, 0.1254, 0.0870, 0.0553,
        0.2822], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,608][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.0561, 0.0299, 0.1039, 0.2084, 0.0331, 0.0429, 0.2096, 0.0970, 0.0673,
        0.1518], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,608][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.8140, 0.0014, 0.0066, 0.1053, 0.0037, 0.0062, 0.0198, 0.0282, 0.0066,
        0.0083], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,609][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.8164, 0.0033, 0.0081, 0.0725, 0.0060, 0.0106, 0.0246, 0.0217, 0.0230,
        0.0139], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,609][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.4560, 0.0277, 0.0588, 0.1802, 0.0226, 0.0288, 0.0438, 0.0591, 0.0727,
        0.0503], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,610][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.0366, 0.0343, 0.1149, 0.1561, 0.0425, 0.0375, 0.1440, 0.1489, 0.0619,
        0.2233], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,610][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0236, 0.0235, 0.0806, 0.1249, 0.0310, 0.0226, 0.1015, 0.0981, 0.0560,
        0.4383], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,610][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0035, 0.1081, 0.2067, 0.0258, 0.1903, 0.1179, 0.0862, 0.1055, 0.0734,
        0.0825], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,612][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.2962, 0.0201, 0.0605, 0.1392, 0.0398, 0.0618, 0.0876, 0.0758, 0.1182,
        0.1008], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,615][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0026, 0.1003, 0.0554, 0.0806, 0.0252, 0.0147, 0.0860, 0.1676, 0.0284,
        0.4014, 0.0378], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,619][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.5629, 0.0141, 0.0140, 0.1814, 0.0196, 0.0105, 0.0237, 0.0534, 0.0330,
        0.0498, 0.0376], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,621][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([6.4721e-05, 5.5838e-02, 7.2606e-02, 1.2413e-02, 1.4392e-01, 2.5815e-01,
        1.8052e-01, 1.0360e-01, 9.3366e-02, 4.1332e-02, 3.8188e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,622][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0465, 0.0337, 0.0698, 0.1526, 0.0221, 0.0192, 0.1336, 0.0998, 0.0668,
        0.2932, 0.0627], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,622][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0146, 0.0375, 0.1043, 0.1484, 0.0263, 0.0352, 0.1848, 0.0900, 0.0740,
        0.1982, 0.0867], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,622][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.7853, 0.0024, 0.0054, 0.1268, 0.0034, 0.0059, 0.0145, 0.0245, 0.0058,
        0.0116, 0.0145], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,623][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.6779, 0.0109, 0.0185, 0.1062, 0.0109, 0.0155, 0.0346, 0.0365, 0.0303,
        0.0228, 0.0359], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,623][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.3366, 0.0551, 0.0770, 0.1616, 0.0299, 0.0275, 0.0385, 0.0803, 0.0571,
        0.0424, 0.0941], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,624][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0140, 0.0723, 0.1207, 0.1285, 0.0312, 0.0263, 0.1225, 0.1417, 0.0538,
        0.2248, 0.0643], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,625][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0046, 0.0405, 0.0651, 0.0654, 0.0280, 0.0171, 0.0823, 0.0808, 0.0605,
        0.5181, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,627][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([1.4632e-04, 1.3849e-01, 2.3645e-01, 5.2305e-03, 1.9602e-01, 9.6495e-02,
        6.8630e-02, 6.3186e-02, 6.9912e-02, 9.2687e-02, 3.2747e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,630][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.1514, 0.0231, 0.0548, 0.0878, 0.0497, 0.0649, 0.1085, 0.0838, 0.1587,
        0.1076, 0.1097], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,634][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0035, 0.1394, 0.0780, 0.0660, 0.0549, 0.0345, 0.0867, 0.1498, 0.0397,
        0.2572, 0.0629, 0.0273], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,635][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1335, 0.0321, 0.0455, 0.1350, 0.0663, 0.0484, 0.0717, 0.0763, 0.0966,
        0.1263, 0.0840, 0.0844], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,635][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([9.5862e-06, 5.7451e-02, 7.7821e-02, 4.6210e-03, 1.6321e-01, 2.4972e-01,
        1.3951e-01, 7.8697e-02, 1.0200e-01, 3.7254e-02, 5.2639e-02, 3.7064e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,636][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0319, 0.0508, 0.0866, 0.1124, 0.0400, 0.0395, 0.1144, 0.1068, 0.0764,
        0.2053, 0.0776, 0.0584], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,636][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0093, 0.0618, 0.1335, 0.0924, 0.0461, 0.0669, 0.1533, 0.0948, 0.0731,
        0.1399, 0.0916, 0.0373], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,637][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.6438, 0.0054, 0.0101, 0.1444, 0.0088, 0.0154, 0.0254, 0.0332, 0.0151,
        0.0158, 0.0248, 0.0579], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,637][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.3516, 0.0255, 0.0412, 0.1324, 0.0323, 0.0413, 0.0643, 0.0550, 0.0649,
        0.0444, 0.0599, 0.0874], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,638][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0903, 0.1455, 0.1367, 0.1081, 0.0682, 0.0485, 0.0489, 0.0742, 0.0820,
        0.0562, 0.0885, 0.0528], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,639][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0060, 0.1089, 0.1443, 0.0807, 0.0703, 0.0435, 0.0899, 0.1090, 0.0529,
        0.1876, 0.0812, 0.0257], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,642][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0045, 0.0766, 0.0737, 0.0519, 0.0520, 0.0402, 0.0819, 0.0972, 0.0649,
        0.3557, 0.0639, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,644][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([1.1447e-04, 1.7744e-01, 2.5724e-01, 3.9303e-03, 1.7912e-01, 7.5746e-02,
        5.8243e-02, 4.2249e-02, 7.7272e-02, 8.2140e-02, 3.9180e-02, 7.3269e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,648][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0650, 0.0523, 0.0948, 0.0901, 0.0852, 0.0808, 0.0908, 0.0768, 0.1172,
        0.0728, 0.0934, 0.0808], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,649][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.0006, 0.1249, 0.0372, 0.0281, 0.0287, 0.0080, 0.0657, 0.1179, 0.0332,
        0.4550, 0.0436, 0.0162, 0.0410], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,649][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.3801, 0.0028, 0.0115, 0.1206, 0.0142, 0.0205, 0.0696, 0.0200, 0.0562,
        0.0556, 0.0352, 0.1063, 0.1072], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,649][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([4.0951e-05, 3.7180e-02, 7.0125e-02, 3.3333e-03, 1.7795e-01, 2.6164e-01,
        1.1803e-01, 5.9843e-02, 1.0686e-01, 5.6989e-02, 4.2994e-02, 4.0755e-02,
        2.4252e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,650][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0154, 0.0274, 0.0472, 0.0808, 0.0270, 0.0262, 0.1228, 0.0693, 0.0625,
        0.3399, 0.0618, 0.0483, 0.0714], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,650][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.0034, 0.0656, 0.1375, 0.0620, 0.0353, 0.0466, 0.1614, 0.0853, 0.0718,
        0.1810, 0.0838, 0.0241, 0.0420], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,651][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.5958, 0.0035, 0.0141, 0.1428, 0.0077, 0.0111, 0.0333, 0.0411, 0.0111,
        0.0256, 0.0220, 0.0591, 0.0326], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,653][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.4821, 0.0123, 0.0197, 0.0977, 0.0191, 0.0283, 0.0556, 0.0347, 0.0531,
        0.0309, 0.0456, 0.0857, 0.0353], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,656][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0859, 0.0875, 0.1228, 0.0927, 0.0582, 0.0771, 0.0602, 0.0614, 0.0906,
        0.0718, 0.0941, 0.0618, 0.0357], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,659][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.0023, 0.0562, 0.1738, 0.0414, 0.0528, 0.0272, 0.0883, 0.0606, 0.0659,
        0.2954, 0.0597, 0.0263, 0.0500], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,661][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([0.0022, 0.0398, 0.0899, 0.0404, 0.0354, 0.0273, 0.0916, 0.0676, 0.0518,
        0.4156, 0.0378, 0.0287, 0.0720], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,662][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.0004, 0.1733, 0.1131, 0.0071, 0.1976, 0.0888, 0.0585, 0.0541, 0.0711,
        0.1351, 0.0553, 0.0163, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,662][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0304, 0.0343, 0.0965, 0.0391, 0.0835, 0.1095, 0.1101, 0.0528, 0.1522,
        0.0683, 0.0902, 0.0694, 0.0637], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,663][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0069, 0.0474, 0.0345, 0.1215, 0.0152, 0.0097, 0.0655, 0.1326, 0.0186,
        0.2991, 0.0318, 0.0315, 0.0841, 0.1017], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,663][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.6344, 0.0025, 0.0035, 0.1219, 0.0043, 0.0051, 0.0187, 0.0114, 0.0265,
        0.0303, 0.0167, 0.0525, 0.0647, 0.0075], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,664][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([5.0854e-05, 6.4015e-02, 5.1396e-02, 1.4402e-02, 1.2302e-01, 1.9317e-01,
        1.3883e-01, 9.0872e-02, 7.3051e-02, 4.4028e-02, 4.3893e-02, 5.2395e-02,
        3.7788e-02, 7.3082e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,664][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0827, 0.0176, 0.0381, 0.1741, 0.0129, 0.0141, 0.0870, 0.0676, 0.0473,
        0.2149, 0.0459, 0.0617, 0.0609, 0.0753], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,666][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0198, 0.0168, 0.0756, 0.1561, 0.0142, 0.0270, 0.1489, 0.0695, 0.0537,
        0.1227, 0.0606, 0.0478, 0.0482, 0.1391], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,667][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([8.7287e-01, 3.0074e-04, 1.0364e-03, 7.9029e-02, 7.7805e-04, 1.7341e-03,
        4.1997e-03, 6.4214e-03, 1.5825e-03, 2.0144e-03, 4.4521e-03, 1.6820e-02,
        6.0063e-03, 2.7513e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,670][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.7619, 0.0026, 0.0063, 0.0775, 0.0041, 0.0064, 0.0183, 0.0121, 0.0177,
        0.0126, 0.0156, 0.0413, 0.0171, 0.0064], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,675][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.3574, 0.0424, 0.0576, 0.1537, 0.0195, 0.0177, 0.0323, 0.0518, 0.0544,
        0.0394, 0.0709, 0.0553, 0.0272, 0.0205], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,675][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0375, 0.0330, 0.0942, 0.1870, 0.0247, 0.0188, 0.0808, 0.0829, 0.0379,
        0.1696, 0.0621, 0.0419, 0.0642, 0.0654], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,676][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0055, 0.0198, 0.0306, 0.0604, 0.0178, 0.0103, 0.0528, 0.0650, 0.0364,
        0.4188, 0.0306, 0.0351, 0.1230, 0.0939], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,676][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.4220e-04, 1.6706e-01, 1.9439e-01, 5.1955e-03, 1.5604e-01, 6.1392e-02,
        5.7895e-02, 5.3837e-02, 7.5285e-02, 9.2881e-02, 4.2689e-02, 7.4252e-03,
        1.7049e-02, 6.8724e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,677][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1343, 0.0156, 0.0434, 0.0910, 0.0385, 0.0458, 0.0791, 0.0645, 0.1006,
        0.0771, 0.0621, 0.0831, 0.0877, 0.0774], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,678][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:38,679][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[33030],
        [41242],
        [27661],
        [31958],
        [41619],
        [28283],
        [22000],
        [17178],
        [31215],
        [36149],
        [18590],
        [19686],
        [ 7770],
        [14018]], device='cuda:0')
[2024-07-24 10:30:38,681][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[34314],
        [43173],
        [22451],
        [28317],
        [44427],
        [22709],
        [22044],
        [13227],
        [36703],
        [39837],
        [16790],
        [18116],
        [ 9023],
        [12517]], device='cuda:0')
[2024-07-24 10:30:38,683][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[47380],
        [43256],
        [38907],
        [38379],
        [40164],
        [39850],
        [38419],
        [37181],
        [39550],
        [38644],
        [38441],
        [37800],
        [38524],
        [36899]], device='cuda:0')
[2024-07-24 10:30:38,684][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[39504],
        [29251],
        [21927],
        [25785],
        [25697],
        [23831],
        [21939],
        [21785],
        [21540],
        [21600],
        [20817],
        [19920],
        [20425],
        [19734]], device='cuda:0')
[2024-07-24 10:30:38,686][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[34699],
        [29925],
        [29623],
        [33547],
        [31835],
        [30693],
        [29577],
        [28846],
        [28522],
        [28382],
        [28177],
        [27945],
        [27524],
        [27304]], device='cuda:0')
[2024-07-24 10:30:38,689][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[23529],
        [35557],
        [37428],
        [35224],
        [34549],
        [34504],
        [36170],
        [35068],
        [34756],
        [35182],
        [36066],
        [36922],
        [37101],
        [37553]], device='cuda:0')
[2024-07-24 10:30:38,691][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[15890],
        [ 9014],
        [17752],
        [15990],
        [16741],
        [16900],
        [15315],
        [14613],
        [14726],
        [12815],
        [12180],
        [13410],
        [12919],
        [14610]], device='cuda:0')
[2024-07-24 10:30:38,692][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[29074],
        [29206],
        [29170],
        [48583],
        [44261],
        [44797],
        [45128],
        [40220],
        [48149],
        [44607],
        [45977],
        [46198],
        [46555],
        [41441]], device='cuda:0')
[2024-07-24 10:30:38,693][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[1191],
        [ 896],
        [ 761],
        [1013],
        [ 887],
        [ 852],
        [ 780],
        [ 770],
        [ 755],
        [ 807],
        [ 839],
        [ 754],
        [ 754],
        [ 766]], device='cuda:0')
[2024-07-24 10:30:38,694][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[38003],
        [26359],
        [24472],
        [25273],
        [28589],
        [26366],
        [27246],
        [25276],
        [27785],
        [29345],
        [27329],
        [27614],
        [28955],
        [29446]], device='cuda:0')
[2024-07-24 10:30:38,695][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[13052],
        [10985],
        [ 7449],
        [10881],
        [13372],
        [11305],
        [10190],
        [ 9876],
        [ 8873],
        [ 9573],
        [ 9632],
        [ 8800],
        [ 9020],
        [10214]], device='cuda:0')
[2024-07-24 10:30:38,696][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[32867],
        [45576],
        [41513],
        [36962],
        [29480],
        [35758],
        [34157],
        [34440],
        [31101],
        [25564],
        [25599],
        [28280],
        [24496],
        [18987]], device='cuda:0')
[2024-07-24 10:30:38,698][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[42397],
        [28101],
        [35582],
        [36755],
        [43631],
        [40860],
        [40080],
        [39397],
        [40090],
        [39941],
        [38722],
        [38414],
        [37879],
        [38674]], device='cuda:0')
[2024-07-24 10:30:38,700][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[42281],
        [49998],
        [49687],
        [49538],
        [48906],
        [49855],
        [49716],
        [49593],
        [49558],
        [49301],
        [49907],
        [49785],
        [49573],
        [49605]], device='cuda:0')
[2024-07-24 10:30:38,701][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[31365],
        [27010],
        [21836],
        [24206],
        [22300],
        [26202],
        [20557],
        [26651],
        [16780],
        [22405],
        [24374],
        [22430],
        [15398],
        [24438]], device='cuda:0')
[2024-07-24 10:30:38,703][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[34762],
        [27219],
        [40283],
        [39058],
        [36295],
        [38143],
        [39106],
        [41863],
        [42074],
        [40208],
        [40575],
        [40882],
        [40431],
        [41740]], device='cuda:0')
[2024-07-24 10:30:38,706][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[19532],
        [19136],
        [17996],
        [34573],
        [15446],
        [14913],
        [23461],
        [15884],
        [28817],
        [17605],
        [26615],
        [35026],
        [33302],
        [22229]], device='cuda:0')
[2024-07-24 10:30:38,708][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[28674],
        [43000],
        [38075],
        [36249],
        [38354],
        [37486],
        [37763],
        [38362],
        [38713],
        [39112],
        [38826],
        [39015],
        [39275],
        [39532]], device='cuda:0')
[2024-07-24 10:30:38,709][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 9523],
        [11908],
        [23123],
        [24901],
        [23541],
        [25566],
        [27004],
        [22939],
        [23583],
        [17291],
        [17504],
        [19559],
        [18321],
        [21321]], device='cuda:0')
[2024-07-24 10:30:38,710][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[45705],
        [34402],
        [24580],
        [26800],
        [26870],
        [27430],
        [26704],
        [23956],
        [24075],
        [25683],
        [26599],
        [26013],
        [26407],
        [26652]], device='cuda:0')
[2024-07-24 10:30:38,711][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[32653],
        [32681],
        [32661],
        [19870],
        [25663],
        [25160],
        [24759],
        [28492],
        [19404],
        [26376],
        [24652],
        [22553],
        [23963],
        [28927]], device='cuda:0')
[2024-07-24 10:30:38,713][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[33671],
        [33671],
        [32590],
        [18038],
        [32587],
        [28430],
        [21883],
        [23416],
        [18997],
        [22815],
        [18267],
        [15883],
        [16351],
        [20711]], device='cuda:0')
[2024-07-24 10:30:38,714][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[47118],
        [47346],
        [49150],
        [42019],
        [48539],
        [46631],
        [47103],
        [47131],
        [47851],
        [49099],
        [48582],
        [47649],
        [48460],
        [48947]], device='cuda:0')
[2024-07-24 10:30:38,716][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[31452],
        [26346],
        [17098],
        [16194],
        [11630],
        [11185],
        [ 8667],
        [13159],
        [13239],
        [11447],
        [12380],
        [11230],
        [10148],
        [12185]], device='cuda:0')
[2024-07-24 10:30:38,717][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[23970],
        [23475],
        [ 9310],
        [10636],
        [ 9900],
        [12001],
        [13299],
        [12378],
        [13212],
        [21697],
        [23192],
        [20777],
        [22533],
        [24854]], device='cuda:0')
[2024-07-24 10:30:38,719][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[38239],
        [14268],
        [12646],
        [13754],
        [15468],
        [15829],
        [14903],
        [13637],
        [14759],
        [15747],
        [16792],
        [16521],
        [15884],
        [16233]], device='cuda:0')
[2024-07-24 10:30:38,722][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[22686],
        [25414],
        [27028],
        [33719],
        [31264],
        [28808],
        [28690],
        [27599],
        [30229],
        [30939],
        [30221],
        [29627],
        [27713],
        [28625]], device='cuda:0')
[2024-07-24 10:30:38,725][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 9926],
        [18752],
        [18747],
        [20935],
        [21808],
        [22060],
        [21675],
        [22682],
        [19973],
        [18049],
        [17194],
        [16985],
        [17498],
        [15446]], device='cuda:0')
[2024-07-24 10:30:38,725][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[21961],
        [24438],
        [39043],
        [36054],
        [18537],
        [33319],
        [30955],
        [32143],
        [21415],
        [14111],
        [28274],
        [33401],
        [32774],
        [27912]], device='cuda:0')
[2024-07-24 10:30:38,726][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709],
        [28709]], device='cuda:0')
[2024-07-24 10:30:38,776][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:38,777][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,777][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,777][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,778][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,778][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,778][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,779][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,779][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,780][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,780][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,780][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,781][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:38,781][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([9.9997e-01, 2.6714e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,781][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.9123, 0.0877], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,782][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.9979, 0.0021], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,782][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([9.9999e-01, 4.9933e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,782][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.9961, 0.0039], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,783][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([9.9995e-01, 4.5800e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,783][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([1.0000e+00, 3.4752e-10], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,783][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([9.9952e-01, 4.7612e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,784][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([9.9999e-01, 7.1440e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,784][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([9.9999e-01, 9.9522e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,784][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([9.9980e-01, 2.0224e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,785][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.9660, 0.0340], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:38,785][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.2420e-01, 7.3186e-04, 7.5064e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,785][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0017, 0.1019, 0.8965], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,786][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.8769, 0.0061, 0.1170], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,786][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.7876, 0.0394, 0.1729], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,786][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0249, 0.0347, 0.9404], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,787][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.9414, 0.0026, 0.0560], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,787][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ and] are: tensor([9.5449e-01, 5.6165e-04, 4.4952e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,788][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0194, 0.0414, 0.9392], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,788][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ and] are: tensor([9.4611e-01, 5.6579e-04, 5.3328e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,788][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.9074, 0.0039, 0.0887], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,789][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.3656, 0.0164, 0.6180], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,789][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1337, 0.0435, 0.8228], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:38,790][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.5721, 0.0162, 0.3295, 0.0822], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,790][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([1.4897e-04, 1.8153e-01, 8.1196e-01, 6.3619e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,790][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.4369, 0.0544, 0.1802, 0.3285], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,791][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.1123, 0.3862, 0.2978, 0.2037], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,791][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0188, 0.1592, 0.4794, 0.3426], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,800][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.3539, 0.0141, 0.0811, 0.5508], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,800][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.4504, 0.0255, 0.2270, 0.2971], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,801][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.0230, 0.0820, 0.5288, 0.3662], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,801][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.4225, 0.0237, 0.1806, 0.3733], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,801][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.2778, 0.0874, 0.2540, 0.3808], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,802][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0738, 0.1141, 0.4516, 0.3605], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,802][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.0163, 0.2037, 0.6709, 0.1092], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:38,802][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ went] are: tensor([9.8231e-01, 3.8263e-05, 6.9041e-03, 1.0534e-02, 2.1212e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,803][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.2101, 0.0237, 0.6239, 0.1291, 0.0133], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,804][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.4075, 0.0232, 0.2529, 0.2926, 0.0238], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,806][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ went] are: tensor([9.0839e-01, 6.7865e-06, 2.5245e-03, 8.9025e-02, 5.5144e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,807][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ went] are: tensor([1.4940e-01, 7.8434e-04, 1.2854e-01, 7.2099e-01, 2.9167e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,810][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ went] are: tensor([3.5234e-01, 1.2604e-04, 4.9464e-02, 5.9779e-01, 2.7581e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,812][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ went] are: tensor([9.9977e-01, 9.0965e-10, 4.9124e-07, 2.3236e-04, 5.9967e-10],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,814][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ went] are: tensor([1.8356e-01, 8.1659e-05, 7.7818e-02, 7.3846e-01, 7.5142e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,814][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ went] are: tensor([8.4092e-01, 1.4107e-05, 4.6761e-03, 1.5438e-01, 1.5855e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,815][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ went] are: tensor([8.6754e-01, 8.1576e-05, 1.1980e-02, 1.2023e-01, 1.6764e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,815][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ went] are: tensor([7.8365e-01, 1.8569e-04, 2.0270e-02, 1.9547e-01, 4.2319e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,815][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.2358, 0.0116, 0.2304, 0.5110, 0.0112], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:38,816][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([8.3714e-01, 6.9459e-04, 7.0232e-02, 3.7833e-02, 4.3547e-03, 4.9743e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,816][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([5.3432e-04, 5.1001e-02, 7.6917e-01, 8.3179e-03, 4.8554e-02, 1.2243e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,816][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.4824, 0.0049, 0.1041, 0.2760, 0.0099, 0.1227], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,818][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.2947, 0.0287, 0.1586, 0.3450, 0.0588, 0.1143], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,821][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0073, 0.0061, 0.2681, 0.3532, 0.0050, 0.3605], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,825][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.2355, 0.0023, 0.0393, 0.6899, 0.0032, 0.0298], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,827][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([9.0684e-01, 2.0877e-04, 1.0865e-02, 7.0902e-02, 1.5166e-04, 1.1031e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,827][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0091, 0.0118, 0.3026, 0.4454, 0.0078, 0.2233], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,828][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.5903, 0.0006, 0.0407, 0.3327, 0.0013, 0.0344], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,828][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.5635, 0.0053, 0.0735, 0.2819, 0.0115, 0.0642], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,828][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1297, 0.0086, 0.2473, 0.4547, 0.0102, 0.1495], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,829][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0225, 0.0350, 0.3778, 0.1818, 0.0661, 0.3168], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:38,829][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([5.1884e-01, 5.0993e-04, 3.4171e-02, 2.1415e-02, 2.7299e-03, 2.3419e-02,
        3.9891e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,830][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0029, 0.0479, 0.5209, 0.0096, 0.0363, 0.0984, 0.2840],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,830][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2731, 0.0188, 0.1246, 0.2328, 0.0269, 0.1439, 0.1799],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,832][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3334, 0.0188, 0.0787, 0.3043, 0.0499, 0.0699, 0.1449],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,834][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0131, 0.0051, 0.1276, 0.3479, 0.0065, 0.2417, 0.2582],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,838][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2508, 0.0022, 0.0355, 0.5473, 0.0056, 0.0461, 0.1124],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,840][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([9.5462e-01, 9.4708e-05, 3.0909e-03, 3.4228e-02, 7.3897e-05, 2.5899e-03,
        5.3028e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,841][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0167, 0.0084, 0.2015, 0.3465, 0.0111, 0.2015, 0.2142],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,841][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5759, 0.0008, 0.0288, 0.2863, 0.0017, 0.0280, 0.0785],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,842][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4072, 0.0076, 0.0746, 0.2646, 0.0161, 0.0631, 0.1668],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,842][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1872, 0.0085, 0.1287, 0.3454, 0.0137, 0.0928, 0.2237],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,842][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0444, 0.0244, 0.2163, 0.1915, 0.0504, 0.1651, 0.3078],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:38,843][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.3621, 0.0006, 0.0488, 0.0167, 0.0035, 0.0278, 0.5350, 0.0054],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,844][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0006, 0.0340, 0.4974, 0.0041, 0.0354, 0.0927, 0.2523, 0.0836],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,847][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.1903, 0.0208, 0.0905, 0.1959, 0.0211, 0.1067, 0.1288, 0.2459],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,851][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.2550, 0.0135, 0.0759, 0.2777, 0.0227, 0.0735, 0.1804, 0.1014],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,853][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.0122, 0.0058, 0.0737, 0.2517, 0.0045, 0.1453, 0.2007, 0.3062],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,854][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.3336, 0.0019, 0.0190, 0.4259, 0.0042, 0.0246, 0.0884, 0.1023],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,854][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ office] are: tensor([8.6668e-01, 1.7727e-04, 1.4799e-02, 6.9129e-02, 1.4358e-04, 1.2078e-02,
        3.3824e-02, 3.1673e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,855][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0140, 0.0092, 0.1364, 0.2653, 0.0086, 0.1112, 0.1934, 0.2619],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,855][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ office] are: tensor([6.1537e-01, 2.6954e-04, 2.0425e-02, 2.3915e-01, 4.0254e-04, 1.9401e-02,
        6.0404e-02, 4.4576e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,855][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.3109, 0.0072, 0.0743, 0.2104, 0.0120, 0.0514, 0.1513, 0.1825],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,856][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.1058, 0.0058, 0.1355, 0.2734, 0.0078, 0.0728, 0.1611, 0.2378],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,856][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.0183, 0.0108, 0.2158, 0.1021, 0.0205, 0.1527, 0.2979, 0.1819],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:38,857][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [,] are: tensor([7.2890e-01, 1.5052e-04, 1.9172e-02, 1.6901e-02, 7.6067e-04, 1.0389e-02,
        1.8437e-01, 3.2276e-03, 3.6126e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,860][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0131, 0.0287, 0.3171, 0.0253, 0.0185, 0.1442, 0.2688, 0.1269, 0.0575],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,863][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2271, 0.0170, 0.1078, 0.1803, 0.0209, 0.0936, 0.1011, 0.2134, 0.0387],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,865][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [,] are: tensor([6.5473e-01, 3.0328e-04, 1.4188e-02, 2.2487e-01, 1.2749e-03, 1.8014e-02,
        4.6809e-02, 3.1122e-02, 8.6916e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,867][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0366, 0.0010, 0.0823, 0.4246, 0.0005, 0.0869, 0.1241, 0.2352, 0.0087],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,867][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [,] are: tensor([1.9755e-01, 2.8440e-04, 2.7322e-02, 4.9789e-01, 6.7009e-04, 3.2832e-02,
        9.0618e-02, 1.4831e-01, 4.5231e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,868][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [,] are: tensor([9.9715e-01, 8.7833e-08, 8.7513e-06, 2.7753e-03, 5.9526e-08, 1.6422e-05,
        4.6716e-05, 5.2494e-06, 1.6744e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,868][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0291, 0.0004, 0.1189, 0.3937, 0.0004, 0.0834, 0.1017, 0.2694, 0.0029],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,869][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [,] are: tensor([6.3231e-01, 1.6879e-04, 1.3733e-02, 2.6050e-01, 2.0696e-04, 1.0944e-02,
        3.5168e-02, 4.4759e-02, 2.2139e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,869][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.4476, 0.0018, 0.0534, 0.2087, 0.0030, 0.0412, 0.1122, 0.1080, 0.0241],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,869][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.4173, 0.0014, 0.0434, 0.3036, 0.0019, 0.0312, 0.0748, 0.1165, 0.0096],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,871][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0538, 0.0208, 0.1499, 0.2342, 0.0178, 0.0669, 0.1548, 0.1675, 0.1342],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:38,873][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([8.7220e-01, 7.2745e-05, 7.2812e-03, 1.5000e-02, 3.4144e-04, 6.0947e-03,
        7.9816e-02, 3.4193e-03, 1.3342e-02, 2.4284e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,876][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0295, 0.0141, 0.3293, 0.0298, 0.0131, 0.1524, 0.2631, 0.0691, 0.0525,
        0.0473], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,880][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.4595, 0.0041, 0.0796, 0.1568, 0.0050, 0.0540, 0.0798, 0.1338, 0.0222,
        0.0052], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,881][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([8.4188e-01, 3.8019e-05, 3.5534e-03, 1.1837e-01, 1.9896e-04, 6.6446e-03,
        1.9923e-02, 6.4714e-03, 1.9308e-03, 9.8839e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,881][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.1076, 0.0009, 0.0584, 0.4099, 0.0007, 0.0474, 0.1617, 0.1924, 0.0088,
        0.0123], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,881][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([3.3558e-01, 1.4193e-04, 1.7371e-02, 4.0178e-01, 5.0076e-04, 1.8576e-02,
        9.7821e-02, 1.1559e-01, 4.5296e-03, 8.1047e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,882][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([9.9971e-01, 2.5305e-09, 6.7091e-07, 2.9022e-04, 1.6696e-09, 8.8366e-07,
        2.8064e-06, 1.8261e-07, 4.1449e-08, 2.4587e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,882][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([1.3757e-01, 3.6861e-04, 5.1784e-02, 4.5723e-01, 5.1525e-04, 2.5089e-02,
        9.8181e-02, 2.1655e-01, 3.1495e-03, 9.5622e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,883][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([8.0291e-01, 1.9143e-05, 3.5981e-03, 1.5906e-01, 3.2509e-05, 3.7707e-03,
        1.7562e-02, 1.2339e-02, 4.8493e-04, 2.2540e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,883][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([7.3479e-01, 1.6171e-04, 1.2175e-02, 1.5704e-01, 3.5626e-04, 1.2873e-02,
        4.3105e-02, 3.2390e-02, 5.6409e-03, 1.4704e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,884][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([6.3565e-01, 4.1082e-04, 2.1379e-02, 2.1495e-01, 8.8857e-04, 1.7487e-02,
        4.7550e-02, 5.1872e-02, 4.0057e-03, 5.8075e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,885][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.1196, 0.0043, 0.1037, 0.2704, 0.0064, 0.0626, 0.1661, 0.1595, 0.0644,
        0.0432], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:38,888][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.4188, 0.0005, 0.0494, 0.0196, 0.0027, 0.0274, 0.3840, 0.0052, 0.0690,
        0.0079, 0.0154], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,892][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0006, 0.0182, 0.4594, 0.0064, 0.0143, 0.0674, 0.1905, 0.1047, 0.0191,
        0.0433, 0.0762], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,894][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1682, 0.0160, 0.0774, 0.1600, 0.0164, 0.0738, 0.0922, 0.2129, 0.0333,
        0.0212, 0.1286], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,894][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.2266, 0.0102, 0.0539, 0.2243, 0.0174, 0.0581, 0.1094, 0.0936, 0.0473,
        0.0488, 0.1105], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,895][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0083, 0.0056, 0.1003, 0.1588, 0.0047, 0.1016, 0.2081, 0.1976, 0.0338,
        0.0491, 0.1322], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,895][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.1396, 0.0032, 0.0288, 0.4233, 0.0050, 0.0364, 0.0989, 0.1297, 0.0150,
        0.0432, 0.0769], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,896][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([9.6889e-01, 2.7376e-05, 1.1939e-03, 2.2268e-02, 1.6821e-05, 1.2396e-03,
        3.2368e-03, 5.7669e-04, 1.8606e-04, 1.0282e-04, 2.2619e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,896][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0054, 0.0155, 0.1340, 0.1692, 0.0115, 0.0852, 0.1609, 0.2201, 0.0266,
        0.1077, 0.0638], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,897][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.5193, 0.0006, 0.0281, 0.2418, 0.0007, 0.0200, 0.0778, 0.0622, 0.0066,
        0.0055, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,899][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.2604, 0.0050, 0.0667, 0.1858, 0.0084, 0.0546, 0.1326, 0.1201, 0.0426,
        0.0235, 0.1003], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,902][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0968, 0.0069, 0.0904, 0.2295, 0.0079, 0.0543, 0.1342, 0.1891, 0.0215,
        0.0359, 0.1336], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,905][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0130, 0.0121, 0.1420, 0.0868, 0.0156, 0.0895, 0.1810, 0.1607, 0.1123,
        0.0964, 0.0905], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:38,907][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1779, 0.0018, 0.0446, 0.0162, 0.0070, 0.0289, 0.2972, 0.0070, 0.1001,
        0.0174, 0.0137, 0.2882], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,908][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0003, 0.0574, 0.2937, 0.0052, 0.0473, 0.0820, 0.1767, 0.0631, 0.0440,
        0.1089, 0.0922, 0.0293], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,908][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1298, 0.0236, 0.0752, 0.1262, 0.0251, 0.0835, 0.0962, 0.1505, 0.0399,
        0.0290, 0.1095, 0.1114], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,908][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0780, 0.0470, 0.0781, 0.1152, 0.0661, 0.0646, 0.0960, 0.0881, 0.0841,
        0.0948, 0.0988, 0.0893], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,909][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0028, 0.0249, 0.1218, 0.0754, 0.0195, 0.1420, 0.1283, 0.1827, 0.0541,
        0.0841, 0.1395, 0.0249], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,909][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0747, 0.0129, 0.0504, 0.2511, 0.0179, 0.0708, 0.0964, 0.1462, 0.0300,
        0.0735, 0.1189, 0.0573], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,910][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.5271, 0.0043, 0.0311, 0.1290, 0.0030, 0.0382, 0.0557, 0.0137, 0.0180,
        0.0088, 0.0380, 0.1332], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,910][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0029, 0.0417, 0.1898, 0.0831, 0.0252, 0.1308, 0.1157, 0.1681, 0.0349,
        0.1120, 0.0728, 0.0230], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,912][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2832, 0.0037, 0.0486, 0.2224, 0.0049, 0.0532, 0.0968, 0.0850, 0.0206,
        0.0152, 0.0670, 0.0995], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,916][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0938, 0.0280, 0.0987, 0.1261, 0.0390, 0.0700, 0.1191, 0.0961, 0.0805,
        0.0646, 0.0886, 0.0956], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,919][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0388, 0.0239, 0.1169, 0.1319, 0.0256, 0.0864, 0.1253, 0.1600, 0.0405,
        0.0634, 0.1249, 0.0623], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,921][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0073, 0.0406, 0.1573, 0.0448, 0.0504, 0.1195, 0.1353, 0.0774, 0.1468,
        0.1132, 0.0711, 0.0363], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:38,921][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.2455, 0.0014, 0.0433, 0.0216, 0.0048, 0.0247, 0.2498, 0.0078, 0.0723,
        0.0130, 0.0174, 0.2857, 0.0126], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,922][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0004, 0.0211, 0.2982, 0.0052, 0.0297, 0.1323, 0.1878, 0.0600, 0.0320,
        0.0607, 0.0962, 0.0433, 0.0329], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,922][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.1990, 0.0073, 0.0502, 0.1140, 0.0069, 0.0542, 0.0750, 0.1131, 0.0242,
        0.0136, 0.1176, 0.0931, 0.1319], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,923][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.1149, 0.0135, 0.0553, 0.1981, 0.0227, 0.0581, 0.1021, 0.0583, 0.0532,
        0.0601, 0.0817, 0.1185, 0.0637], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,923][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.0010, 0.0189, 0.1273, 0.0463, 0.0166, 0.0752, 0.1580, 0.1666, 0.0568,
        0.1409, 0.1373, 0.0198, 0.0353], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,924][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.0214, 0.0122, 0.0577, 0.1650, 0.0144, 0.0659, 0.1351, 0.1443, 0.0369,
        0.1009, 0.1241, 0.0457, 0.0765], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,925][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([8.8957e-01, 7.6352e-05, 4.4470e-03, 2.1383e-02, 4.9076e-05, 3.8518e-03,
        7.3645e-03, 6.3428e-04, 6.3314e-04, 2.9566e-04, 6.0060e-03, 5.9103e-02,
        6.5903e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,927][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0006, 0.0466, 0.1819, 0.0329, 0.0234, 0.0409, 0.1222, 0.2012, 0.0394,
        0.1827, 0.0853, 0.0140, 0.0289], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,931][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.3706, 0.0009, 0.0384, 0.2297, 0.0017, 0.0329, 0.1041, 0.0538, 0.0146,
        0.0116, 0.0435, 0.0771, 0.0210], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,934][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0832, 0.0153, 0.0822, 0.1385, 0.0244, 0.0575, 0.1100, 0.0920, 0.0867,
        0.0603, 0.0868, 0.1023, 0.0609], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,934][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0255, 0.0089, 0.0960, 0.1276, 0.0108, 0.0774, 0.1319, 0.1488, 0.0273,
        0.0497, 0.1336, 0.0594, 0.1031], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,935][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0080, 0.0207, 0.1600, 0.0555, 0.0209, 0.0923, 0.1754, 0.1001, 0.1208,
        0.1067, 0.0750, 0.0343, 0.0302], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:38,935][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([4.3542e-01, 1.6612e-04, 2.0385e-02, 1.3546e-02, 8.5684e-04, 1.2633e-02,
        2.0362e-01, 2.5686e-03, 3.0624e-02, 2.9412e-03, 5.4306e-03, 2.4472e-01,
        3.7842e-03, 2.3308e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,936][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0009, 0.0269, 0.2644, 0.0072, 0.0193, 0.0493, 0.1243, 0.0816, 0.0238,
        0.0710, 0.0855, 0.0369, 0.0438, 0.1652], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,936][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2679, 0.0036, 0.0315, 0.1094, 0.0057, 0.0411, 0.0592, 0.1141, 0.0158,
        0.0080, 0.0734, 0.0944, 0.1016, 0.0744], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,937][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.2522, 0.0033, 0.0364, 0.1740, 0.0093, 0.0348, 0.0803, 0.0655, 0.0293,
        0.0259, 0.0583, 0.1127, 0.0500, 0.0681], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,938][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0053, 0.0027, 0.0603, 0.1825, 0.0032, 0.0975, 0.1229, 0.1677, 0.0180,
        0.0418, 0.0822, 0.0247, 0.0237, 0.1674], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,941][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.1318, 0.0011, 0.0156, 0.3639, 0.0030, 0.0227, 0.0594, 0.0857, 0.0082,
        0.0308, 0.0670, 0.0551, 0.0832, 0.0725], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,943][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([9.1264e-01, 2.3734e-05, 1.5502e-03, 2.9069e-02, 2.0004e-05, 2.4365e-03,
        5.2983e-03, 6.0466e-04, 5.6728e-04, 1.1208e-04, 1.8623e-03, 3.3560e-02,
        3.4696e-03, 8.7913e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,947][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0066, 0.0032, 0.0701, 0.1735, 0.0040, 0.0771, 0.0964, 0.1533, 0.0107,
        0.0585, 0.0406, 0.0255, 0.0503, 0.2301], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,948][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([5.3164e-01, 1.2968e-04, 1.1469e-02, 2.4580e-01, 2.9823e-04, 1.4792e-02,
        4.3743e-02, 3.1286e-02, 2.9168e-03, 2.0665e-03, 1.8799e-02, 5.5364e-02,
        1.2481e-02, 2.9219e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,948][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.2595, 0.0044, 0.0429, 0.1427, 0.0096, 0.0336, 0.0996, 0.0577, 0.0430,
        0.0219, 0.0532, 0.1116, 0.0398, 0.0805], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,949][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0812, 0.0020, 0.0607, 0.1897, 0.0030, 0.0461, 0.1073, 0.1203, 0.0125,
        0.0208, 0.0884, 0.0664, 0.0771, 0.1244], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:38,949][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0177, 0.0092, 0.0862, 0.0828, 0.0175, 0.0751, 0.1464, 0.0884, 0.1008,
        0.0622, 0.0552, 0.0387, 0.0223, 0.1974], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,014][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:39,016][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,017][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,017][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,018][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,018][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,018][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,021][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,023][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,023][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,024][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,024][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,024][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,025][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([9.9999e-01, 8.3143e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,025][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([9.9996e-01, 4.2398e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,025][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.9985, 0.0015], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,026][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([9.9999e-01, 4.9933e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,026][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.9961, 0.0039], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,027][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([9.9995e-01, 4.5800e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,029][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 3.3946e-08], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,030][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([9.9952e-01, 4.7612e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,033][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.9999e-01, 7.1440e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,035][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 2.9649e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,037][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([9.9980e-01, 2.0224e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,037][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.9660, 0.0340], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,038][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([8.9677e-01, 2.3925e-04, 1.0299e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,038][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0159, 0.3460, 0.6381], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,038][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.9152, 0.0023, 0.0824], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,039][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.7876, 0.0394, 0.1729], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,039][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0249, 0.0347, 0.9404], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,039][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9414, 0.0026, 0.0560], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,040][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.7384, 0.0252, 0.2363], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,040][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0194, 0.0414, 0.9392], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,041][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([9.4611e-01, 5.6579e-04, 5.3328e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,044][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.9633, 0.0013, 0.0353], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,047][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.3656, 0.0164, 0.6180], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,051][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1337, 0.0435, 0.8228], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,051][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.4789, 0.0084, 0.4329, 0.0798], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,051][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([1.1264e-04, 8.5035e-01, 1.4716e-01, 2.3730e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,052][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.4728, 0.0387, 0.1566, 0.3319], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,052][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.1123, 0.3862, 0.2978, 0.2037], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,052][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.0188, 0.1592, 0.4794, 0.3426], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,053][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.3539, 0.0141, 0.0811, 0.5508], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,053][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.1140, 0.3103, 0.3065, 0.2692], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,054][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.0230, 0.0820, 0.5288, 0.3662], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,055][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.4225, 0.0237, 0.1806, 0.3733], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,058][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.3747, 0.0617, 0.1801, 0.3835], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,062][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.0738, 0.1141, 0.4516, 0.3605], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,064][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.0163, 0.2037, 0.6709, 0.1092], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,065][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([9.7870e-01, 1.7233e-05, 5.9163e-03, 1.5261e-02, 1.0956e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,065][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([8.5012e-01, 3.4551e-04, 1.0625e-02, 1.3843e-01, 4.7573e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,065][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.5305, 0.0117, 0.1389, 0.3070, 0.0119], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,066][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([9.0839e-01, 6.7865e-06, 2.5245e-03, 8.9025e-02, 5.5144e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,066][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([1.4940e-01, 7.8434e-04, 1.2854e-01, 7.2099e-01, 2.9167e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,066][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([3.5234e-01, 1.2604e-04, 4.9464e-02, 5.9779e-01, 2.7581e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,067][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([9.7985e-01, 3.8500e-07, 1.1488e-04, 2.0035e-02, 2.1700e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,067][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([1.8356e-01, 8.1659e-05, 7.7818e-02, 7.3846e-01, 7.5142e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,068][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([8.4092e-01, 1.4107e-05, 4.6761e-03, 1.5438e-01, 1.5855e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,070][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([9.0935e-01, 2.2187e-05, 3.2102e-03, 8.7391e-02, 2.7443e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,071][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([7.8365e-01, 1.8569e-04, 2.0270e-02, 1.9547e-01, 4.2319e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,075][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.2358, 0.0116, 0.2304, 0.5110, 0.0112], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,078][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([7.8564e-01, 2.7102e-04, 8.3397e-02, 4.6729e-02, 2.2805e-03, 8.1686e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,078][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0125, 0.2202, 0.4106, 0.0544, 0.1434, 0.1588], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,079][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.5457, 0.0022, 0.0847, 0.2801, 0.0033, 0.0840], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,079][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.2947, 0.0287, 0.1586, 0.3450, 0.0588, 0.1143], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,079][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0073, 0.0061, 0.2681, 0.3532, 0.0050, 0.3605], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,080][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.2355, 0.0023, 0.0393, 0.6899, 0.0032, 0.0298], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,080][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.4984, 0.0121, 0.0716, 0.3366, 0.0151, 0.0662], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,081][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0091, 0.0118, 0.3026, 0.4454, 0.0078, 0.2233], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,081][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.5903, 0.0006, 0.0407, 0.3327, 0.0013, 0.0344], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,083][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.6325, 0.0034, 0.0506, 0.2775, 0.0055, 0.0304], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,085][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1297, 0.0086, 0.2473, 0.4547, 0.0102, 0.1495], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,089][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0225, 0.0350, 0.3778, 0.1818, 0.0661, 0.3168], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,091][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([4.8319e-01, 2.8628e-04, 4.6220e-02, 2.9168e-02, 1.8825e-03, 4.2405e-02,
        3.9685e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,092][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0217, 0.2725, 0.2647, 0.0536, 0.1213, 0.0789, 0.1873],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,092][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3673, 0.0078, 0.1079, 0.2699, 0.0102, 0.1045, 0.1326],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,093][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.3334, 0.0188, 0.0787, 0.3043, 0.0499, 0.0699, 0.1449],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,093][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0131, 0.0051, 0.1276, 0.3479, 0.0065, 0.2417, 0.2582],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,093][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2508, 0.0022, 0.0355, 0.5473, 0.0056, 0.0461, 0.1124],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,094][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.6055, 0.0085, 0.0355, 0.2379, 0.0124, 0.0310, 0.0692],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,094][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0167, 0.0084, 0.2015, 0.3465, 0.0111, 0.2015, 0.2142],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,096][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.5759, 0.0008, 0.0288, 0.2863, 0.0017, 0.0280, 0.0785],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,098][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.6036, 0.0029, 0.0376, 0.2546, 0.0051, 0.0251, 0.0711],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,102][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1872, 0.0085, 0.1287, 0.3454, 0.0137, 0.0928, 0.2237],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,105][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0444, 0.0244, 0.2163, 0.1915, 0.0504, 0.1651, 0.3078],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,105][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([2.8001e-01, 2.5481e-04, 7.2057e-02, 1.7816e-02, 1.8882e-03, 4.9077e-02,
        5.7274e-01, 6.1556e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,106][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.0056, 0.2409, 0.3093, 0.0215, 0.0976, 0.0797, 0.1597, 0.0857],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,106][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.2656, 0.0090, 0.0753, 0.2280, 0.0093, 0.0827, 0.1009, 0.2292],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,107][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.2550, 0.0135, 0.0759, 0.2777, 0.0227, 0.0735, 0.1804, 0.1014],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,107][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.0122, 0.0058, 0.0737, 0.2517, 0.0045, 0.1453, 0.2007, 0.3062],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,107][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.3336, 0.0019, 0.0190, 0.4259, 0.0042, 0.0246, 0.0884, 0.1023],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,108][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.2792, 0.0155, 0.0720, 0.2577, 0.0205, 0.0499, 0.1291, 0.1762],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,109][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.0140, 0.0092, 0.1364, 0.2653, 0.0086, 0.1112, 0.1934, 0.2619],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,111][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([6.1537e-01, 2.6954e-04, 2.0425e-02, 2.3915e-01, 4.0254e-04, 1.9401e-02,
        6.0404e-02, 4.4576e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,114][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.5824, 0.0022, 0.0315, 0.2105, 0.0032, 0.0181, 0.0609, 0.0911],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,118][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.1058, 0.0058, 0.1355, 0.2734, 0.0078, 0.0728, 0.1611, 0.2378],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,119][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0183, 0.0108, 0.2158, 0.1021, 0.0205, 0.1527, 0.2979, 0.1819],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,119][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([7.7466e-01, 7.8951e-05, 2.3464e-02, 2.5642e-02, 4.6462e-04, 1.6748e-02,
        1.4265e-01, 4.0346e-03, 1.2255e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,120][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.2660, 0.0211, 0.1099, 0.2257, 0.0108, 0.0615, 0.1286, 0.1357, 0.0408],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,120][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2763, 0.0097, 0.1088, 0.2438, 0.0089, 0.0594, 0.0667, 0.2119, 0.0145],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,121][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([6.5473e-01, 3.0328e-04, 1.4188e-02, 2.2487e-01, 1.2749e-03, 1.8014e-02,
        4.6809e-02, 3.1122e-02, 8.6916e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,121][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0366, 0.0010, 0.0823, 0.4246, 0.0005, 0.0869, 0.1241, 0.2352, 0.0087],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,121][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.9755e-01, 2.8440e-04, 2.7322e-02, 4.9789e-01, 6.7009e-04, 3.2832e-02,
        9.0618e-02, 1.4831e-01, 4.5231e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,122][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([9.1800e-01, 3.0481e-05, 6.6022e-04, 7.1384e-02, 6.4164e-05, 1.1000e-03,
        3.3515e-03, 4.7073e-03, 6.9808e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,123][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0291, 0.0004, 0.1189, 0.3937, 0.0004, 0.0834, 0.1017, 0.2694, 0.0029],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,125][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([6.3231e-01, 1.6879e-04, 1.3733e-02, 2.6050e-01, 2.0696e-04, 1.0944e-02,
        3.5168e-02, 4.4759e-02, 2.2139e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,127][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([6.7891e-01, 3.8458e-04, 1.9538e-02, 1.9597e-01, 5.0300e-04, 1.3058e-02,
        3.5457e-02, 5.2994e-02, 3.1865e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,131][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.4173, 0.0014, 0.0434, 0.3036, 0.0019, 0.0312, 0.0748, 0.1165, 0.0096],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,133][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0538, 0.0208, 0.1499, 0.2342, 0.0178, 0.0669, 0.1548, 0.1675, 0.1342],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,133][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([8.8928e-01, 4.8392e-05, 7.3516e-03, 2.5356e-02, 2.3468e-04, 8.9133e-03,
        5.8206e-02, 4.8270e-03, 4.7614e-03, 1.0179e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,133][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.6329, 0.0020, 0.0360, 0.1773, 0.0025, 0.0343, 0.0475, 0.0490, 0.0123,
        0.0062], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,134][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.5783, 0.0029, 0.0554, 0.1690, 0.0030, 0.0296, 0.0435, 0.1083, 0.0073,
        0.0027], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,134][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([8.4188e-01, 3.8019e-05, 3.5534e-03, 1.1837e-01, 1.9896e-04, 6.6446e-03,
        1.9923e-02, 6.4714e-03, 1.9308e-03, 9.8839e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,135][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.1076, 0.0009, 0.0584, 0.4099, 0.0007, 0.0474, 0.1617, 0.1924, 0.0088,
        0.0123], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,135][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([3.3558e-01, 1.4193e-04, 1.7371e-02, 4.0178e-01, 5.0076e-04, 1.8576e-02,
        9.7821e-02, 1.1559e-01, 4.5296e-03, 8.1047e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,135][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([9.6133e-01, 3.5585e-06, 2.8342e-04, 3.4427e-02, 1.5155e-05, 4.9154e-04,
        1.4589e-03, 1.6710e-03, 1.9419e-04, 1.2310e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,136][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([1.3757e-01, 3.6861e-04, 5.1784e-02, 4.5723e-01, 5.1525e-04, 2.5089e-02,
        9.8181e-02, 2.1655e-01, 3.1495e-03, 9.5622e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,138][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([8.0291e-01, 1.9143e-05, 3.5981e-03, 1.5906e-01, 3.2509e-05, 3.7707e-03,
        1.7562e-02, 1.2339e-02, 4.8493e-04, 2.2540e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,140][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([8.1848e-01, 7.5828e-05, 5.2420e-03, 1.4152e-01, 1.2445e-04, 4.7920e-03,
        1.4742e-02, 1.3352e-02, 1.0514e-03, 6.1742e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,142][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([6.3565e-01, 4.1082e-04, 2.1379e-02, 2.1495e-01, 8.8857e-04, 1.7487e-02,
        4.7550e-02, 5.1872e-02, 4.0057e-03, 5.8075e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,146][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.1196, 0.0043, 0.1037, 0.2704, 0.0064, 0.0626, 0.1661, 0.1595, 0.0644,
        0.0432], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,146][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([3.9021e-01, 3.0826e-04, 7.0088e-02, 2.6383e-02, 1.9140e-03, 5.4641e-02,
        3.9592e-01, 6.9861e-03, 3.4068e-02, 3.7434e-03, 1.5745e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,147][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0083, 0.1034, 0.3300, 0.0334, 0.0529, 0.0811, 0.1289, 0.1152, 0.0712,
        0.0338, 0.0418], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,147][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.2466, 0.0095, 0.0681, 0.2023, 0.0087, 0.0545, 0.0715, 0.1984, 0.0133,
        0.0110, 0.1160], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,148][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.2266, 0.0102, 0.0539, 0.2243, 0.0174, 0.0581, 0.1094, 0.0936, 0.0473,
        0.0488, 0.1105], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,148][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0083, 0.0056, 0.1003, 0.1588, 0.0047, 0.1016, 0.2081, 0.1976, 0.0338,
        0.0491, 0.1322], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,149][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.1396, 0.0032, 0.0288, 0.4233, 0.0050, 0.0364, 0.0989, 0.1297, 0.0150,
        0.0432, 0.0769], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,149][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.4759, 0.0090, 0.0251, 0.1880, 0.0098, 0.0227, 0.0585, 0.0959, 0.0294,
        0.0191, 0.0665], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,151][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0054, 0.0155, 0.1340, 0.1692, 0.0115, 0.0852, 0.1609, 0.2201, 0.0266,
        0.1077, 0.0638], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,154][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.5193, 0.0006, 0.0281, 0.2418, 0.0007, 0.0200, 0.0778, 0.0622, 0.0066,
        0.0055, 0.0375], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,158][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.4258, 0.0031, 0.0463, 0.2228, 0.0040, 0.0282, 0.0761, 0.0932, 0.0153,
        0.0137, 0.0715], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,160][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0968, 0.0069, 0.0904, 0.2295, 0.0079, 0.0543, 0.1342, 0.1891, 0.0215,
        0.0359, 0.1336], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,160][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0130, 0.0121, 0.1420, 0.0868, 0.0156, 0.0895, 0.1810, 0.1607, 0.1123,
        0.0964, 0.0905], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,161][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1633, 0.0013, 0.0683, 0.0194, 0.0058, 0.0526, 0.3352, 0.0096, 0.0577,
        0.0111, 0.0147, 0.2611], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,161][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0013, 0.3154, 0.1665, 0.0102, 0.1205, 0.0447, 0.0719, 0.0396, 0.1109,
        0.0838, 0.0248, 0.0103], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,162][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1916, 0.0126, 0.0718, 0.1575, 0.0130, 0.0720, 0.0848, 0.1390, 0.0198,
        0.0157, 0.1094, 0.1129], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,162][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0780, 0.0470, 0.0781, 0.1152, 0.0661, 0.0646, 0.0960, 0.0881, 0.0841,
        0.0948, 0.0988, 0.0893], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,162][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0028, 0.0249, 0.1218, 0.0754, 0.0195, 0.1420, 0.1283, 0.1827, 0.0541,
        0.0841, 0.1395, 0.0249], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,163][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0747, 0.0129, 0.0504, 0.2511, 0.0179, 0.0708, 0.0964, 0.1462, 0.0300,
        0.0735, 0.1189, 0.0573], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,164][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1592, 0.0355, 0.0557, 0.1430, 0.0332, 0.0596, 0.0916, 0.0857, 0.0864,
        0.0531, 0.0890, 0.1080], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,167][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0029, 0.0417, 0.1898, 0.0831, 0.0252, 0.1308, 0.1157, 0.1681, 0.0349,
        0.1120, 0.0728, 0.0230], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,171][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2832, 0.0037, 0.0486, 0.2224, 0.0049, 0.0532, 0.0968, 0.0850, 0.0206,
        0.0152, 0.0670, 0.0995], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,174][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1798, 0.0191, 0.0936, 0.1792, 0.0222, 0.0530, 0.0999, 0.0945, 0.0486,
        0.0423, 0.0798, 0.0880], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,174][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0388, 0.0239, 0.1169, 0.1319, 0.0256, 0.0864, 0.1253, 0.1600, 0.0405,
        0.0634, 0.1249, 0.0623], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,174][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0073, 0.0406, 0.1573, 0.0448, 0.0504, 0.1195, 0.1353, 0.0774, 0.1468,
        0.1132, 0.0711, 0.0363], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,175][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.2161, 0.0011, 0.0672, 0.0264, 0.0040, 0.0466, 0.2869, 0.0096, 0.0435,
        0.0076, 0.0194, 0.2575, 0.0140], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,175][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.0029, 0.1431, 0.2665, 0.0176, 0.0876, 0.0841, 0.0897, 0.0765, 0.0833,
        0.0644, 0.0451, 0.0193, 0.0199], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,176][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.2720, 0.0044, 0.0460, 0.1303, 0.0035, 0.0465, 0.0585, 0.1015, 0.0101,
        0.0080, 0.1096, 0.0855, 0.1240], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,176][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.1149, 0.0135, 0.0553, 0.1981, 0.0227, 0.0581, 0.1021, 0.0583, 0.0532,
        0.0601, 0.0817, 0.1185, 0.0637], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,177][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.0010, 0.0189, 0.1273, 0.0463, 0.0166, 0.0752, 0.1580, 0.1666, 0.0568,
        0.1409, 0.1373, 0.0198, 0.0353], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,179][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.0214, 0.0122, 0.0577, 0.1650, 0.0144, 0.0659, 0.1351, 0.1443, 0.0369,
        0.1009, 0.1241, 0.0457, 0.0765], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,182][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.1895, 0.0147, 0.0550, 0.1210, 0.0187, 0.0419, 0.0782, 0.0805, 0.0655,
        0.0332, 0.1003, 0.1035, 0.0980], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,185][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0006, 0.0466, 0.1819, 0.0329, 0.0234, 0.0409, 0.1222, 0.2012, 0.0394,
        0.1827, 0.0853, 0.0140, 0.0289], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,187][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.3706, 0.0009, 0.0384, 0.2297, 0.0017, 0.0329, 0.1041, 0.0538, 0.0146,
        0.0116, 0.0435, 0.0771, 0.0210], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,188][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([0.1447, 0.0133, 0.0809, 0.1957, 0.0163, 0.0435, 0.0928, 0.0851, 0.0540,
        0.0468, 0.0780, 0.0883, 0.0605], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,188][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.0255, 0.0089, 0.0960, 0.1276, 0.0108, 0.0774, 0.1319, 0.1488, 0.0273,
        0.0497, 0.1336, 0.0594, 0.1031], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,189][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0080, 0.0207, 0.1600, 0.0555, 0.0209, 0.0923, 0.1754, 0.1001, 0.1208,
        0.1067, 0.0750, 0.0343, 0.0302], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,189][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([4.0575e-01, 9.1896e-05, 2.9457e-02, 1.8249e-02, 6.3453e-04, 2.6372e-02,
        2.2950e-01, 3.1675e-03, 1.5114e-02, 1.4524e-03, 6.2000e-03, 2.2920e-01,
        4.4346e-03, 3.0387e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,189][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0154, 0.1226, 0.1952, 0.0476, 0.0568, 0.0613, 0.1194, 0.1013, 0.0712,
        0.0426, 0.0363, 0.0249, 0.0270, 0.0785], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,190][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.4498, 0.0011, 0.0211, 0.1270, 0.0016, 0.0246, 0.0365, 0.0808, 0.0037,
        0.0026, 0.0549, 0.0793, 0.0803, 0.0367], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,191][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.2522, 0.0033, 0.0364, 0.1740, 0.0093, 0.0348, 0.0803, 0.0655, 0.0293,
        0.0259, 0.0583, 0.1127, 0.0500, 0.0681], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,194][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0053, 0.0027, 0.0603, 0.1825, 0.0032, 0.0975, 0.1229, 0.1677, 0.0180,
        0.0418, 0.0822, 0.0247, 0.0237, 0.1674], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,198][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.1318, 0.0011, 0.0156, 0.3639, 0.0030, 0.0227, 0.0594, 0.0857, 0.0082,
        0.0308, 0.0670, 0.0551, 0.0832, 0.0725], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,201][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.4576, 0.0018, 0.0163, 0.1594, 0.0028, 0.0199, 0.0437, 0.0446, 0.0219,
        0.0076, 0.0341, 0.0836, 0.0561, 0.0505], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,201][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0066, 0.0032, 0.0701, 0.1735, 0.0040, 0.0771, 0.0964, 0.1533, 0.0107,
        0.0585, 0.0406, 0.0255, 0.0503, 0.2301], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,202][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([5.3164e-01, 1.2968e-04, 1.1469e-02, 2.4580e-01, 2.9823e-04, 1.4792e-02,
        4.3743e-02, 3.1286e-02, 2.9168e-03, 2.0665e-03, 1.8799e-02, 5.5364e-02,
        1.2481e-02, 2.9219e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,202][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.5204, 0.0015, 0.0224, 0.1781, 0.0025, 0.0136, 0.0501, 0.0377, 0.0109,
        0.0082, 0.0300, 0.0695, 0.0270, 0.0280], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,203][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0812, 0.0020, 0.0607, 0.1897, 0.0030, 0.0461, 0.1073, 0.1203, 0.0125,
        0.0208, 0.0884, 0.0664, 0.0771, 0.1244], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,203][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0177, 0.0092, 0.0862, 0.0828, 0.0175, 0.0751, 0.1464, 0.0884, 0.1008,
        0.0622, 0.0552, 0.0387, 0.0223, 0.1974], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,204][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:39,206][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[36545],
        [38069],
        [27072],
        [35850],
        [42474],
        [28008],
        [23834],
        [15564],
        [30000],
        [34759],
        [16778],
        [20541],
        [ 9494],
        [14704]], device='cuda:0')
[2024-07-24 10:30:39,207][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[33159],
        [38943],
        [24235],
        [32396],
        [40574],
        [24877],
        [18767],
        [12318],
        [27710],
        [34401],
        [14748],
        [14919],
        [ 6171],
        [10007]], device='cuda:0')
[2024-07-24 10:30:39,209][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[25188],
        [25184],
        [25930],
        [27805],
        [25187],
        [26166],
        [26760],
        [26970],
        [25697],
        [25211],
        [26144],
        [23540],
        [22999],
        [23288]], device='cuda:0')
[2024-07-24 10:30:39,210][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[31118],
        [25152],
        [14102],
        [13715],
        [26336],
        [12873],
        [ 7465],
        [ 7875],
        [ 8090],
        [ 9055],
        [11526],
        [10337],
        [12224],
        [17025]], device='cuda:0')
[2024-07-24 10:30:39,213][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 2162],
        [ 2173],
        [ 4041],
        [11925],
        [11368],
        [ 6626],
        [ 8386],
        [ 6780],
        [ 7147],
        [ 5252],
        [ 7653],
        [ 8433],
        [ 8319],
        [ 6659]], device='cuda:0')
[2024-07-24 10:30:39,215][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[37071],
        [37071],
        [42955],
        [44493],
        [39169],
        [39862],
        [37106],
        [35764],
        [38902],
        [39302],
        [36924],
        [37130],
        [35523],
        [34241]], device='cuda:0')
[2024-07-24 10:30:39,218][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[37611],
        [37343],
        [36150],
        [33039],
        [38984],
        [37952],
        [40611],
        [41807],
        [41642],
        [42065],
        [42270],
        [40813],
        [41437],
        [42296]], device='cuda:0')
[2024-07-24 10:30:39,219][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[14979],
        [14980],
        [19573],
        [37495],
        [37100],
        [38269],
        [38023],
        [37435],
        [38967],
        [37784],
        [41332],
        [42665],
        [44268],
        [42836]], device='cuda:0')
[2024-07-24 10:30:39,220][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[26365],
        [26365],
        [26791],
        [40312],
        [26384],
        [34295],
        [30490],
        [33737],
        [26700],
        [26393],
        [28816],
        [31771],
        [28294],
        [29222]], device='cuda:0')
[2024-07-24 10:30:39,221][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[36211],
        [36209],
        [25840],
        [26331],
        [30824],
        [26302],
        [25815],
        [28844],
        [30122],
        [31159],
        [24737],
        [22702],
        [21013],
        [21815]], device='cuda:0')
[2024-07-24 10:30:39,222][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[34675],
        [34676],
        [28808],
        [43137],
        [47021],
        [47617],
        [44808],
        [41718],
        [44367],
        [45591],
        [34925],
        [19779],
        [25005],
        [36093]], device='cuda:0')
[2024-07-24 10:30:39,223][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[17219],
        [17222],
        [27802],
        [36179],
        [17465],
        [30110],
        [36067],
        [39940],
        [37630],
        [26835],
        [39951],
        [42521],
        [41822],
        [38117]], device='cuda:0')
[2024-07-24 10:30:39,226][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[47907],
        [47899],
        [28872],
        [15632],
        [28543],
        [16927],
        [21437],
        [27004],
        [24094],
        [27721],
        [25631],
        [25827],
        [24706],
        [27178]], device='cuda:0')
[2024-07-24 10:30:39,227][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[47122],
        [48885],
        [49801],
        [50193],
        [50257],
        [50060],
        [49884],
        [48678],
        [50136],
        [50193],
        [49173],
        [49210],
        [49081],
        [49040]], device='cuda:0')
[2024-07-24 10:30:39,229][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[48043],
        [47205],
        [45897],
        [47299],
        [46996],
        [45268],
        [46830],
        [46425],
        [46466],
        [46750],
        [46165],
        [47166],
        [46578],
        [46403]], device='cuda:0')
[2024-07-24 10:30:39,232][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[6047],
        [6050],
        [5537],
        [5268],
        [5683],
        [4625],
        [2845],
        [2707],
        [3579],
        [4592],
        [3362],
        [3557],
        [3575],
        [3005]], device='cuda:0')
[2024-07-24 10:30:39,234][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[14108],
        [14101],
        [15960],
        [ 8508],
        [ 7959],
        [17412],
        [20241],
        [18844],
        [10871],
        [ 7632],
        [18908],
        [21893],
        [21176],
        [19838]], device='cuda:0')
[2024-07-24 10:30:39,235][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[47260],
        [47160],
        [34211],
        [ 5156],
        [ 6366],
        [11371],
        [10898],
        [22369],
        [19579],
        [22540],
        [23626],
        [21977],
        [26471],
        [28113]], device='cuda:0')
[2024-07-24 10:30:39,236][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[44334],
        [44334],
        [34886],
        [39804],
        [38508],
        [35050],
        [35595],
        [36295],
        [32652],
        [35630],
        [34205],
        [36606],
        [34650],
        [34926]], device='cuda:0')
[2024-07-24 10:30:39,237][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[29068],
        [28972],
        [20206],
        [18959],
        [11194],
        [16235],
        [18150],
        [14752],
        [12294],
        [12307],
        [17853],
        [19007],
        [18802],
        [15985]], device='cuda:0')
[2024-07-24 10:30:39,238][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[41666],
        [41665],
        [40722],
        [33663],
        [34307],
        [33917],
        [34367],
        [34785],
        [33917],
        [34600],
        [32908],
        [31208],
        [29707],
        [33257]], device='cuda:0')
[2024-07-24 10:30:39,239][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[26622],
        [26622],
        [ 8444],
        [16815],
        [23386],
        [ 4497],
        [ 3793],
        [ 5489],
        [13074],
        [20230],
        [ 4307],
        [ 8238],
        [ 7557],
        [ 3667]], device='cuda:0')
[2024-07-24 10:30:39,240][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[16302],
        [16287],
        [36783],
        [38963],
        [35059],
        [40790],
        [41565],
        [41919],
        [40645],
        [38852],
        [45990],
        [46472],
        [47408],
        [46910]], device='cuda:0')
[2024-07-24 10:30:39,243][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25165],
        [25164],
        [23199],
        [29464],
        [25440],
        [35247],
        [36929],
        [34914],
        [34567],
        [30078],
        [34104],
        [32303],
        [33286],
        [35958]], device='cuda:0')
[2024-07-24 10:30:39,244][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[46201],
        [46201],
        [45747],
        [37525],
        [45000],
        [40409],
        [41897],
        [45732],
        [44668],
        [43052],
        [45901],
        [44543],
        [45027],
        [45483]], device='cuda:0')
[2024-07-24 10:30:39,246][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 9921],
        [ 9881],
        [30242],
        [36619],
        [11708],
        [37477],
        [37030],
        [35109],
        [31181],
        [21166],
        [34246],
        [35321],
        [37088],
        [37398]], device='cuda:0')
[2024-07-24 10:30:39,249][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[42209],
        [42944],
        [39394],
        [42782],
        [40761],
        [37709],
        [31883],
        [33722],
        [36004],
        [36767],
        [37400],
        [36863],
        [36831],
        [38341]], device='cuda:0')
[2024-07-24 10:30:39,251][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 9611],
        [ 9475],
        [12184],
        [22141],
        [29070],
        [20634],
        [21394],
        [17968],
        [21808],
        [24066],
        [17882],
        [16227],
        [14485],
        [14661]], device='cuda:0')
[2024-07-24 10:30:39,252][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[1505],
        [1695],
        [3018],
        [2337],
        [3607],
        [4578],
        [3194],
        [3483],
        [3298],
        [2923],
        [2950],
        [2374],
        [2718],
        [2740]], device='cuda:0')
[2024-07-24 10:30:39,253][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122],
        [33122]], device='cuda:0')
[2024-07-24 10:30:39,307][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:39,308][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,308][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,308][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,308][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,309][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,309][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,309][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,309][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,310][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,312][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,314][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,317][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,317][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.1834, 0.8166], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,317][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([1.0000e+00, 7.4273e-07], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,318][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.1719, 0.8281], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,318][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([1.0000e+00, 4.3058e-13], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,318][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([9.9999e-01, 8.2369e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,319][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([1.0000e+00, 9.6046e-07], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,319][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([9.9993e-01, 7.0606e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,319][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,320][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.0603, 0.9397], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,321][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([9.5297e-04, 9.9905e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,325][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.6715, 0.3285], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,329][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0974, 0.9026], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,329][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0119, 0.8900, 0.0980], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,330][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0012, 0.9559, 0.0429], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,330][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0276, 0.7474, 0.2250], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,330][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ and] are: tensor([9.9979e-01, 9.8226e-05, 1.0822e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,331][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0014, 0.9595, 0.0391], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,331][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.8877, 0.0035, 0.1088], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,331][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1927, 0.6358, 0.1715], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,332][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.4477, 0.0233, 0.5289], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,333][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0190, 0.4925, 0.4885], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,339][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0024, 0.5482, 0.4494], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,341][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0092, 0.7743, 0.2165], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,341][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0135, 0.8095, 0.1771], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,342][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([1.3038e-04, 9.8560e-01, 1.3714e-02, 5.5336e-04], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,342][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([1.8058e-05, 9.9530e-01, 3.0808e-03, 1.6032e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,342][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.0034, 0.8408, 0.1149, 0.0408], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,343][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([9.2689e-01, 8.2042e-05, 7.0182e-02, 2.8495e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,343][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([9.0690e-04, 9.5467e-01, 2.3346e-02, 2.1074e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,343][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.4552, 0.1326, 0.1507, 0.2615], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,344][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.0044, 0.7256, 0.2604, 0.0096], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,347][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.1847, 0.2081, 0.2122, 0.3949], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,351][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.0084, 0.3696, 0.3476, 0.2744], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,353][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.0018, 0.2835, 0.2817, 0.4330], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,354][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0081, 0.6338, 0.2012, 0.1569], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,354][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([0.0110, 0.6943, 0.1985, 0.0962], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,354][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0159, 0.5999, 0.1202, 0.0252, 0.2388], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,355][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ went] are: tensor([2.2143e-01, 5.2470e-06, 6.2755e-03, 7.7227e-01, 1.8264e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,355][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.0633, 0.3472, 0.2520, 0.1441, 0.1935], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,355][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ went] are: tensor([9.9999e-01, 8.2518e-12, 6.0913e-06, 8.2406e-07, 9.0901e-12],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,356][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ went] are: tensor([9.1253e-01, 8.1849e-04, 6.2694e-02, 2.2660e-02, 1.3008e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,356][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ went] are: tensor([9.4746e-01, 6.6185e-06, 6.0618e-04, 5.1894e-02, 3.3904e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,357][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ went] are: tensor([5.9867e-01, 5.5768e-04, 1.3307e-01, 2.6694e-01, 7.6113e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,362][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.6082, 0.0021, 0.0709, 0.3170, 0.0017], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,365][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ went] are: tensor([0.0086, 0.2462, 0.2293, 0.1684, 0.3475], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,366][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ went] are: tensor([3.0914e-04, 2.2234e-01, 2.3657e-01, 3.7406e-01, 1.6672e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,366][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.1144, 0.2244, 0.2481, 0.2057, 0.2073], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,366][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.0645, 0.3601, 0.2177, 0.1578, 0.1999], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,367][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0026, 0.7882, 0.0407, 0.0050, 0.1401, 0.0234], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,367][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0011, 0.5080, 0.0353, 0.0604, 0.3792, 0.0160], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,368][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0097, 0.5469, 0.1400, 0.0685, 0.1599, 0.0750], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,368][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([9.7710e-01, 6.1109e-04, 1.3413e-02, 7.8379e-03, 2.9332e-04, 7.4195e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,368][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([5.3514e-04, 6.2389e-01, 1.9725e-02, 1.4370e-02, 3.2298e-01, 1.8496e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,371][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.3996, 0.0033, 0.0713, 0.3648, 0.0086, 0.1525], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,376][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1312, 0.3692, 0.2025, 0.0332, 0.1073, 0.1566], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,378][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1276, 0.0078, 0.1443, 0.5918, 0.0061, 0.1224], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,378][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0052, 0.1864, 0.1782, 0.1428, 0.2787, 0.2087], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,378][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0006, 0.1642, 0.1626, 0.2655, 0.1641, 0.2429], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,379][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0042, 0.3838, 0.1145, 0.0894, 0.3082, 0.0998], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,379][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0053, 0.4877, 0.1209, 0.0532, 0.2128, 0.1202], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,379][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0045, 0.7052, 0.0558, 0.0074, 0.1560, 0.0350, 0.0361],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,380][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0033, 0.4081, 0.0436, 0.0897, 0.3644, 0.0157, 0.0754],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,380][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0203, 0.4002, 0.1508, 0.0799, 0.1423, 0.0883, 0.1181],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,381][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([9.9773e-01, 2.1996e-06, 9.8983e-04, 7.0654e-04, 3.0471e-06, 4.0928e-04,
        1.5536e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,386][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0031, 0.4634, 0.0430, 0.0276, 0.3270, 0.0474, 0.0886],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,390][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.6710, 0.0021, 0.0319, 0.2012, 0.0055, 0.0469, 0.0413],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,390][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6287, 0.0654, 0.1379, 0.0395, 0.0219, 0.0711, 0.0355],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,390][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2497, 0.0091, 0.0825, 0.4544, 0.0079, 0.0947, 0.1016],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,391][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0035, 0.1618, 0.1468, 0.1125, 0.2460, 0.1763, 0.1531],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,391][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0005, 0.1576, 0.1442, 0.2197, 0.1322, 0.1967, 0.1490],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,391][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0081, 0.2660, 0.1153, 0.1004, 0.2474, 0.1150, 0.1479],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,392][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0115, 0.3531, 0.1280, 0.0584, 0.2208, 0.1349, 0.0932],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,392][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0010, 0.8142, 0.0292, 0.0017, 0.1086, 0.0152, 0.0151, 0.0151],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,393][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ office] are: tensor([4.0811e-04, 6.1814e-01, 1.3574e-02, 1.3255e-02, 2.9874e-01, 6.5081e-03,
        2.5279e-02, 2.4097e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,396][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.0143, 0.4081, 0.1269, 0.0567, 0.1292, 0.0658, 0.0946, 0.1044],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,398][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ office] are: tensor([9.9539e-01, 7.0857e-07, 3.2229e-03, 8.7812e-04, 2.7501e-07, 1.8073e-04,
        2.3252e-04, 9.5799e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,404][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.0027, 0.3881, 0.0598, 0.0378, 0.2537, 0.0528, 0.1223, 0.0829],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,404][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.5840, 0.0048, 0.0397, 0.1848, 0.0095, 0.0489, 0.0372, 0.0910],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,405][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.0142, 0.0784, 0.5659, 0.0116, 0.0266, 0.1370, 0.1422, 0.0241],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,405][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.1253, 0.0093, 0.0996, 0.4158, 0.0067, 0.0696, 0.0838, 0.1898],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,405][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.0045, 0.1286, 0.1254, 0.1096, 0.2005, 0.1555, 0.1355, 0.1404],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,406][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.0004, 0.1090, 0.1050, 0.1477, 0.0988, 0.1498, 0.1099, 0.2795],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,406][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.0048, 0.2696, 0.0927, 0.0753, 0.2392, 0.0880, 0.1276, 0.1030],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,410][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ office] are: tensor([0.0058, 0.3435, 0.1380, 0.0471, 0.2074, 0.0977, 0.0958, 0.0647],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,415][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0047, 0.5715, 0.0487, 0.0085, 0.1534, 0.0348, 0.0455, 0.0373, 0.0956],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,416][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [,] are: tensor([2.6243e-02, 5.5776e-04, 1.5723e-02, 8.2735e-01, 6.2212e-04, 2.8263e-02,
        3.7808e-02, 5.8534e-02, 4.9046e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,416][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0142, 0.3421, 0.1093, 0.0585, 0.1209, 0.0677, 0.1021, 0.0869, 0.0983],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,416][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [,] are: tensor([9.9999e-01, 3.1772e-10, 3.8741e-06, 5.5511e-06, 3.0296e-10, 6.0023e-07,
        6.5209e-07, 4.0354e-06, 1.5432e-11], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,417][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.2235, 0.0350, 0.1474, 0.0737, 0.0332, 0.1207, 0.1235, 0.1200, 0.1231],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,417][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [,] are: tensor([9.3830e-01, 1.2003e-05, 8.4607e-04, 5.2093e-02, 4.1608e-05, 1.2542e-03,
        1.5613e-03, 5.8553e-03, 4.0148e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,418][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.7218, 0.0011, 0.0377, 0.0913, 0.0007, 0.0634, 0.0430, 0.0340, 0.0071],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,418][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.2768, 0.0011, 0.0859, 0.3373, 0.0009, 0.0547, 0.0405, 0.1975, 0.0053],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,421][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0032, 0.1219, 0.1090, 0.0813, 0.1747, 0.1236, 0.1081, 0.1149, 0.1633],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,426][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0004, 0.1084, 0.0904, 0.1551, 0.0902, 0.1425, 0.1117, 0.2463, 0.0549],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,427][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0294, 0.1493, 0.1171, 0.1008, 0.1367, 0.1105, 0.1215, 0.1035, 0.1313],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,428][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0231, 0.3662, 0.0801, 0.0514, 0.1554, 0.1114, 0.0778, 0.0531, 0.0814],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,428][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0052, 0.4243, 0.0429, 0.0076, 0.1146, 0.0263, 0.0281, 0.0331, 0.0876,
        0.2304], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,429][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([2.3967e-01, 2.4331e-04, 2.0992e-02, 5.7651e-01, 6.8370e-04, 5.5781e-02,
        4.0619e-02, 6.0433e-02, 3.6230e-03, 1.4517e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,429][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.0173, 0.2374, 0.1072, 0.0548, 0.0990, 0.0685, 0.0866, 0.0866, 0.0841,
        0.1586], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,430][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([9.9991e-01, 6.1419e-11, 6.5941e-05, 1.3276e-05, 2.4645e-10, 3.3319e-06,
        3.6166e-06, 3.2356e-06, 7.0471e-11, 2.2572e-10], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,430][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.4316, 0.0084, 0.1316, 0.0506, 0.0098, 0.1034, 0.0948, 0.0832, 0.0485,
        0.0380], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,431][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([9.1685e-01, 9.2705e-06, 7.4028e-04, 7.5889e-02, 3.5661e-05, 1.2226e-03,
        1.4103e-03, 3.7265e-03, 3.5379e-05, 8.1262e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,436][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.3975, 0.0005, 0.1282, 0.1580, 0.0008, 0.1254, 0.1398, 0.0304, 0.0186,
        0.0007], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,439][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.4641, 0.0011, 0.0374, 0.2927, 0.0010, 0.0315, 0.0441, 0.1127, 0.0072,
        0.0080], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,440][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.0046, 0.0931, 0.0933, 0.0729, 0.1362, 0.1079, 0.0959, 0.0961, 0.1380,
        0.1620], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,440][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([1.8453e-04, 8.6693e-02, 8.9366e-02, 1.2251e-01, 6.8841e-02, 1.2084e-01,
        8.7725e-02, 2.1055e-01, 5.4780e-02, 1.5851e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,440][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0241, 0.1332, 0.1024, 0.0835, 0.1213, 0.0929, 0.1144, 0.0999, 0.1179,
        0.1102], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,441][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0077, 0.2450, 0.1034, 0.0575, 0.1247, 0.0649, 0.0680, 0.0541, 0.1616,
        0.1131], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,441][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0012, 0.5357, 0.0283, 0.0028, 0.0939, 0.0170, 0.0202, 0.0171, 0.0705,
        0.1965, 0.0169], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,442][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0028, 0.3115, 0.0354, 0.0657, 0.1606, 0.0188, 0.0596, 0.0650, 0.0718,
        0.1041, 0.1046], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,442][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0071, 0.2915, 0.0849, 0.0419, 0.0920, 0.0480, 0.0671, 0.0656, 0.0768,
        0.1837, 0.0415], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,443][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([9.2514e-01, 4.3194e-05, 4.6657e-02, 9.1145e-03, 2.1864e-05, 4.3919e-03,
        8.2310e-03, 6.2155e-03, 1.8995e-05, 3.9627e-05, 1.2224e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,448][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0012, 0.3140, 0.0278, 0.0193, 0.1958, 0.0250, 0.0552, 0.0512, 0.1051,
        0.1626, 0.0429], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,451][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.5513, 0.0033, 0.0222, 0.2101, 0.0076, 0.0403, 0.0393, 0.0950, 0.0025,
        0.0093, 0.0192], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,452][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.1831, 0.0833, 0.0694, 0.0291, 0.0215, 0.0544, 0.1061, 0.0272, 0.1113,
        0.1614, 0.1532], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,452][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0838, 0.0048, 0.0869, 0.3226, 0.0035, 0.0651, 0.0657, 0.2254, 0.0102,
        0.0183, 0.1136], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,453][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0024, 0.0862, 0.0778, 0.0629, 0.1265, 0.0909, 0.0792, 0.0851, 0.1209,
        0.1526, 0.1158], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,453][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0006, 0.0818, 0.0725, 0.1147, 0.0600, 0.1068, 0.0864, 0.1845, 0.0524,
        0.1525, 0.0879], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,454][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0025, 0.2006, 0.0610, 0.0508, 0.1696, 0.0580, 0.0849, 0.0672, 0.1220,
        0.1151, 0.0683], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,454][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0059, 0.2768, 0.0882, 0.0329, 0.1481, 0.0692, 0.0742, 0.0369, 0.0944,
        0.1389, 0.0344], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,457][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0013, 0.4625, 0.0290, 0.0024, 0.0995, 0.0196, 0.0247, 0.0152, 0.0943,
        0.2259, 0.0182, 0.0075], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,463][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0015, 0.3781, 0.0305, 0.0463, 0.2171, 0.0125, 0.0357, 0.0411, 0.0707,
        0.0890, 0.0600, 0.0174], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,464][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0090, 0.2665, 0.0883, 0.0431, 0.0795, 0.0457, 0.0698, 0.0606, 0.0741,
        0.1705, 0.0390, 0.0541], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,464][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([9.7924e-01, 9.4203e-06, 1.1784e-02, 4.4753e-03, 9.7848e-06, 1.5922e-03,
        6.6133e-04, 1.7896e-03, 2.0431e-06, 7.2708e-06, 1.8884e-04, 2.4446e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,464][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0016, 0.2790, 0.0269, 0.0242, 0.1943, 0.0239, 0.0593, 0.0570, 0.1110,
        0.1305, 0.0477, 0.0445], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,465][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.2109, 0.0159, 0.1028, 0.1639, 0.0261, 0.1068, 0.0908, 0.1307, 0.0096,
        0.0283, 0.0302, 0.0840], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,465][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0156, 0.1852, 0.0835, 0.0080, 0.0398, 0.0338, 0.0419, 0.0184, 0.1552,
        0.3220, 0.0632, 0.0334], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,466][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0518, 0.0257, 0.1133, 0.1597, 0.0172, 0.0823, 0.0736, 0.2005, 0.0353,
        0.0566, 0.1055, 0.0785], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,466][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0013, 0.0826, 0.0697, 0.0510, 0.1243, 0.0808, 0.0697, 0.0769, 0.1126,
        0.1497, 0.1028, 0.0785], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,470][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0005, 0.0753, 0.0718, 0.1075, 0.0671, 0.0903, 0.0763, 0.1832, 0.0437,
        0.1234, 0.0773, 0.0836], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,475][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0027, 0.1754, 0.0637, 0.0500, 0.1508, 0.0593, 0.0797, 0.0645, 0.1134,
        0.1032, 0.0658, 0.0716], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,476][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0076, 0.2336, 0.0731, 0.0507, 0.0900, 0.0997, 0.0527, 0.0506, 0.0889,
        0.1853, 0.0308, 0.0369], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,476][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([2.2121e-04, 6.1963e-01, 1.5687e-02, 9.5901e-04, 8.7605e-02, 7.6950e-03,
        9.6091e-03, 9.5770e-03, 4.7911e-02, 1.7858e-01, 8.1707e-03, 2.4492e-03,
        1.1905e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,477][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0004, 0.4352, 0.0209, 0.0135, 0.1565, 0.0109, 0.0201, 0.0337, 0.0626,
        0.1653, 0.0515, 0.0105, 0.0189], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,477][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0026, 0.3229, 0.0683, 0.0253, 0.0861, 0.0299, 0.0513, 0.0521, 0.0690,
        0.1800, 0.0269, 0.0350, 0.0506], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,478][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([6.6619e-01, 2.9996e-03, 2.3763e-01, 1.5372e-02, 1.1051e-03, 1.4924e-02,
        1.8920e-02, 3.5636e-02, 2.6518e-04, 4.7011e-04, 2.1837e-03, 3.2723e-03,
        1.0258e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,478][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.0022, 0.2032, 0.0373, 0.0256, 0.1460, 0.0368, 0.0728, 0.0459, 0.1139,
        0.1510, 0.0468, 0.0680, 0.0506], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,481][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([0.0994, 0.0097, 0.1041, 0.2064, 0.0166, 0.1210, 0.0757, 0.1196, 0.0105,
        0.0336, 0.0291, 0.0604, 0.1138], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,487][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.0165, 0.0396, 0.0922, 0.0119, 0.0156, 0.0469, 0.1373, 0.0209, 0.3230,
        0.0654, 0.0761, 0.1166, 0.0381], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,488][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0363, 0.0070, 0.1092, 0.1576, 0.0056, 0.0652, 0.0768, 0.1929, 0.0223,
        0.0507, 0.0985, 0.0653, 0.1124], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,488][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([0.0014, 0.0688, 0.0632, 0.0467, 0.1064, 0.0750, 0.0653, 0.0671, 0.1001,
        0.1304, 0.0945, 0.0746, 0.1064], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,489][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0002, 0.0653, 0.0687, 0.0892, 0.0588, 0.0936, 0.0639, 0.1514, 0.0383,
        0.1149, 0.0674, 0.0822, 0.1061], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,489][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0020, 0.1895, 0.0514, 0.0402, 0.1503, 0.0464, 0.0731, 0.0585, 0.1063,
        0.1017, 0.0583, 0.0608, 0.0613], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,489][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0061, 0.1637, 0.1073, 0.0384, 0.0926, 0.0830, 0.0740, 0.0421, 0.1021,
        0.0811, 0.0384, 0.0625, 0.1088], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,490][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0020, 0.4660, 0.0271, 0.0035, 0.0945, 0.0181, 0.0235, 0.0205, 0.0673,
        0.2051, 0.0168, 0.0077, 0.0248, 0.0232], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,491][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0023, 0.2988, 0.0325, 0.0993, 0.1344, 0.0125, 0.0363, 0.0581, 0.0499,
        0.1284, 0.0546, 0.0156, 0.0443, 0.0329], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,495][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0073, 0.2348, 0.0746, 0.0391, 0.0734, 0.0408, 0.0615, 0.0607, 0.0651,
        0.1507, 0.0350, 0.0453, 0.0625, 0.0492], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,498][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([7.7003e-01, 7.7464e-04, 3.5873e-02, 5.6456e-02, 3.9261e-04, 3.5352e-03,
        6.9757e-03, 1.1348e-01, 5.6757e-05, 9.6845e-04, 1.8583e-03, 7.1600e-03,
        1.8256e-03, 6.1325e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,500][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0024, 0.2142, 0.0292, 0.0236, 0.1554, 0.0284, 0.0586, 0.0554, 0.1014,
        0.1333, 0.0441, 0.0492, 0.0478, 0.0571], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,500][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.3435, 0.0009, 0.0337, 0.2869, 0.0022, 0.0631, 0.0427, 0.0660, 0.0017,
        0.0039, 0.0099, 0.0606, 0.0380, 0.0468], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,501][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0777, 0.0934, 0.0566, 0.0165, 0.0309, 0.0385, 0.0777, 0.0251, 0.0839,
        0.1090, 0.1096, 0.1141, 0.0872, 0.0799], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,501][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0637, 0.0019, 0.0520, 0.3104, 0.0016, 0.0536, 0.0501, 0.1797, 0.0057,
        0.0105, 0.0622, 0.0618, 0.0582, 0.0885], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,502][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0016, 0.0641, 0.0596, 0.0479, 0.0958, 0.0692, 0.0600, 0.0647, 0.0916,
        0.1145, 0.0869, 0.0697, 0.0964, 0.0780], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,502][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0002, 0.0521, 0.0553, 0.0835, 0.0557, 0.0784, 0.0637, 0.1286, 0.0346,
        0.0954, 0.0688, 0.0773, 0.1113, 0.0951], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,505][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0039, 0.1438, 0.0561, 0.0492, 0.1259, 0.0535, 0.0700, 0.0586, 0.0945,
        0.0906, 0.0582, 0.0620, 0.0604, 0.0732], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,510][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0075, 0.1647, 0.0624, 0.0380, 0.0851, 0.0722, 0.0578, 0.0353, 0.0756,
        0.1152, 0.0381, 0.0492, 0.0974, 0.1016], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,555][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:39,555][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,556][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,556][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,556][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,557][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,557][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,557][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,558][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,558][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,558][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,558][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,559][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,559][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 2.7831e-10], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,559][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([9.9986e-01, 1.3961e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,560][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 3.2615e-08], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,560][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 4.4289e-08], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,560][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.0068, 0.9932], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,561][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 6.8303e-07], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,561][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.9980, 0.0020], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,561][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,562][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.9944e-01, 5.6019e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,562][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.9862, 0.0138], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,562][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([9.9999e-01, 9.5948e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,563][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.9921, 0.0079], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,563][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([9.9485e-01, 4.5865e-05, 5.1005e-03], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,563][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0023, 0.1325, 0.8652], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,564][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.8555, 0.0027, 0.1419], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,564][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0284, 0.9450, 0.0266], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,564][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([7.1966e-07, 9.9999e-01, 1.1981e-05], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,565][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([9.3206e-01, 8.0447e-04, 6.7138e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,565][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([5.6301e-04, 2.7231e-01, 7.2713e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,566][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.4477, 0.0233, 0.5289], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,570][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.8434, 0.0494, 0.1072], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,572][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([3.9499e-05, 3.3181e-01, 6.6815e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,574][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0564, 0.0102, 0.9333], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,575][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0065, 0.0080, 0.9856], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,575][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.3095, 0.0064, 0.0320, 0.6521], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,575][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([1.1325e-04, 8.2114e-01, 1.7398e-01, 4.7574e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,576][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.2407, 0.1149, 0.2681, 0.3763], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,576][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.3502, 0.4675, 0.1058, 0.0765], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,576][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([3.2794e-07, 9.9999e-01, 9.6193e-06, 1.0331e-07], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,577][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.3691, 0.0577, 0.1853, 0.3879], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,578][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([1.0487e-05, 8.3185e-01, 1.6720e-01, 9.4574e-04], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,584][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.1847, 0.2081, 0.2122, 0.3949], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,586][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.0940, 0.1590, 0.2796, 0.4674], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,586][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([1.8931e-06, 6.6862e-01, 3.3130e-01, 8.3476e-05], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,587][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([0.0780, 0.0290, 0.2989, 0.5940], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,587][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([3.2971e-04, 1.1547e-01, 8.7817e-01, 6.0295e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,587][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([9.6282e-01, 5.0711e-09, 6.7742e-05, 3.7108e-02, 4.5059e-09],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,588][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.4334, 0.0016, 0.1511, 0.4082, 0.0056], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,588][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([8.9583e-01, 6.9857e-07, 2.2398e-03, 1.0193e-01, 2.2422e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,588][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([9.8498e-01, 1.3809e-07, 4.5707e-04, 1.4559e-02, 1.3103e-07],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,589][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.0036, 0.7930, 0.0207, 0.0015, 0.1811], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,590][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([9.4355e-01, 5.2832e-06, 6.0621e-04, 5.5817e-02, 2.5825e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,595][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.2728, 0.0070, 0.1478, 0.5620, 0.0104], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,598][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.6082, 0.0021, 0.0709, 0.3170, 0.0017], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,599][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([0.6474, 0.0037, 0.0599, 0.2824, 0.0066], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,599][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([0.2388, 0.0531, 0.3741, 0.1427, 0.1913], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,599][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([3.1306e-01, 1.2082e-05, 5.0523e-02, 6.3640e-01, 6.9908e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,600][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([0.3465, 0.0078, 0.2945, 0.3437, 0.0075], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,600][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([3.7140e-01, 1.4519e-05, 2.6916e-03, 6.1818e-01, 1.9160e-05, 7.6887e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,600][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0011, 0.0914, 0.4888, 0.0449, 0.1521, 0.2217], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,601][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.3944, 0.0005, 0.0664, 0.3650, 0.0028, 0.1709], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,604][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0188, 0.5677, 0.0366, 0.0074, 0.3501, 0.0193], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,608][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([7.6429e-08, 9.9982e-01, 2.5203e-06, 3.3169e-09, 1.8204e-04, 2.2568e-08],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,610][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.4433, 0.0010, 0.0500, 0.4124, 0.0026, 0.0905], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,610][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([2.4605e-04, 1.8823e-01, 3.9107e-01, 1.0731e-02, 2.6155e-01, 1.4817e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,611][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1276, 0.0078, 0.1443, 0.5918, 0.0061, 0.1224], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,611][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.2261, 0.0164, 0.0514, 0.6068, 0.0094, 0.0898], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,612][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([6.7141e-06, 1.7631e-01, 1.0742e-01, 1.1755e-04, 6.8967e-01, 2.6471e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,612][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([2.7953e-02, 6.7478e-04, 1.1823e-01, 6.4464e-01, 6.2567e-04, 2.0788e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,612][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0036, 0.0095, 0.6715, 0.0372, 0.0169, 0.2613], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,613][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([3.3347e-01, 2.7756e-05, 4.3017e-03, 6.3475e-01, 3.7101e-05, 9.2597e-03,
        1.8159e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,613][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0011, 0.1107, 0.3488, 0.0207, 0.1521, 0.1190, 0.2476],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,616][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4510, 0.0007, 0.0446, 0.2974, 0.0032, 0.0940, 0.1092],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,622][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1232, 0.2642, 0.1149, 0.0807, 0.2644, 0.1351, 0.0175],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,623][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([4.3087e-07, 9.9913e-01, 7.1279e-06, 4.7212e-08, 8.6409e-04, 1.1763e-07,
        2.5428e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,623][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.5992, 0.0012, 0.0347, 0.2771, 0.0027, 0.0435, 0.0416],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,623][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0010, 0.2370, 0.2411, 0.0253, 0.2622, 0.0947, 0.1388],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,624][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2497, 0.0091, 0.0825, 0.4544, 0.0079, 0.0947, 0.1016],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,624][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1941, 0.0350, 0.0659, 0.5047, 0.0213, 0.1057, 0.0733],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,625][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([3.9476e-05, 2.5961e-01, 5.6740e-02, 3.3318e-04, 6.2820e-01, 1.8057e-02,
        3.7016e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,628][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0255, 0.0005, 0.1025, 0.4451, 0.0007, 0.3117, 0.1140],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,632][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0085, 0.0125, 0.5853, 0.0539, 0.0118, 0.1844, 0.1436],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,634][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([4.4139e-01, 4.4655e-06, 1.5688e-03, 5.2892e-01, 2.7189e-06, 6.3828e-03,
        1.0653e-02, 1.1080e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,634][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.0004, 0.1609, 0.2204, 0.0098, 0.1656, 0.1118, 0.2191, 0.1120],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,635][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.4769, 0.0006, 0.0273, 0.2988, 0.0016, 0.0501, 0.0799, 0.0647],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,635][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.2004, 0.3835, 0.0584, 0.0548, 0.2340, 0.0430, 0.0152, 0.0108],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,636][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([1.3512e-06, 9.9978e-01, 6.0368e-06, 9.9116e-08, 2.1499e-04, 1.3529e-07,
        5.1939e-07, 7.4604e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,636][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.5271, 0.0020, 0.0405, 0.2507, 0.0036, 0.0425, 0.0356, 0.0981],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,636][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([2.5965e-04, 2.3533e-01, 2.5962e-01, 9.0121e-03, 2.7088e-01, 1.0734e-01,
        8.9338e-02, 2.8214e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,637][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.1253, 0.0093, 0.0996, 0.4158, 0.0067, 0.0696, 0.0838, 0.1898],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,638][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.1445, 0.0173, 0.0543, 0.4755, 0.0128, 0.0906, 0.0589, 0.1460],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,642][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([5.0087e-06, 2.7751e-01, 1.1585e-01, 8.5588e-05, 5.4837e-01, 2.4238e-02,
        3.1899e-02, 2.0502e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,646][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([0.0182, 0.0020, 0.0757, 0.3593, 0.0020, 0.1980, 0.1840, 0.1609],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,647][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0027, 0.0130, 0.5109, 0.0235, 0.0163, 0.2141, 0.1102, 0.1093],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,647][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([7.3854e-01, 1.2784e-07, 3.4302e-04, 2.5917e-01, 7.1235e-08, 3.6592e-04,
        8.5376e-04, 7.2254e-04, 1.3038e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,648][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0051, 0.0103, 0.2334, 0.0664, 0.0151, 0.2298, 0.2104, 0.1332, 0.0964],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,648][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([6.8889e-01, 1.6363e-05, 6.5201e-03, 2.7264e-01, 2.7002e-05, 4.8291e-03,
        1.2714e-02, 1.4183e-02, 1.8140e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,648][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([9.1782e-01, 1.5933e-05, 3.8813e-03, 7.2890e-02, 2.2627e-05, 1.9413e-03,
        9.5483e-05, 3.3137e-03, 2.3843e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,649][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([3.0117e-05, 9.9456e-01, 2.0850e-04, 2.4933e-06, 4.8716e-03, 8.3382e-06,
        2.0039e-05, 1.6814e-06, 3.0011e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,650][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([9.2329e-01, 6.7128e-06, 9.4736e-04, 6.5516e-02, 2.3534e-05, 1.3333e-03,
        1.8204e-03, 7.0270e-03, 3.4735e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,656][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0370, 0.0284, 0.1916, 0.2824, 0.0261, 0.1255, 0.1073, 0.1426, 0.0590],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,658][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.2768, 0.0011, 0.0859, 0.3373, 0.0009, 0.0547, 0.0405, 0.1975, 0.0053],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,658][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.3125, 0.0134, 0.0605, 0.3807, 0.0095, 0.0475, 0.0334, 0.1204, 0.0220],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,659][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0025, 0.0710, 0.1786, 0.0102, 0.2037, 0.0746, 0.0924, 0.0108, 0.3560],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,659][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([1.4215e-01, 8.7311e-06, 5.0799e-02, 6.6715e-01, 7.5014e-06, 6.0185e-02,
        3.0712e-02, 4.8884e-02, 1.0345e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,660][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.1143, 0.0115, 0.2210, 0.2336, 0.0088, 0.1455, 0.1216, 0.1245, 0.0190],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,660][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([9.3242e-01, 7.7386e-09, 1.1350e-04, 6.6977e-02, 5.5207e-09, 1.1817e-04,
        2.4793e-04, 1.2168e-04, 1.1816e-07, 1.4532e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,661][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.0514, 0.0031, 0.1349, 0.1005, 0.0115, 0.2907, 0.2049, 0.1249, 0.0598,
        0.0183], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,661][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([8.6031e-01, 1.2600e-06, 2.0515e-03, 1.2949e-01, 3.5914e-06, 1.2594e-03,
        4.5537e-03, 2.2958e-03, 3.3049e-05, 7.5484e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,662][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([9.7187e-01, 9.7001e-06, 2.5500e-03, 2.2231e-02, 1.0349e-05, 6.5609e-04,
        1.6493e-04, 2.4750e-03, 1.3785e-05, 2.3192e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,665][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([2.0027e-05, 9.9483e-01, 5.5174e-04, 3.5855e-05, 2.1833e-03, 5.2568e-05,
        1.1155e-04, 2.6926e-05, 5.8512e-04, 1.6057e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,668][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([9.2681e-01, 4.1352e-06, 5.6724e-04, 6.6849e-02, 1.7770e-05, 9.8118e-04,
        1.2924e-03, 3.4032e-03, 2.3381e-05, 5.1116e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,671][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.1383, 0.0118, 0.0934, 0.3674, 0.0182, 0.1232, 0.0717, 0.1101, 0.0303,
        0.0355], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,671][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.4641, 0.0011, 0.0374, 0.2927, 0.0010, 0.0315, 0.0441, 0.1127, 0.0072,
        0.0080], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,671][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([0.5491, 0.0021, 0.0426, 0.2817, 0.0036, 0.0355, 0.0246, 0.0509, 0.0086,
        0.0013], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,672][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.0202, 0.0289, 0.1671, 0.0372, 0.0774, 0.0700, 0.0667, 0.0150, 0.1483,
        0.3692], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,672][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([3.7080e-01, 9.7136e-06, 2.1665e-02, 5.1188e-01, 8.6461e-06, 1.8658e-02,
        3.4009e-02, 4.2774e-02, 1.0071e-04, 8.7419e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,673][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([0.2417, 0.0068, 0.0976, 0.2904, 0.0053, 0.1338, 0.0986, 0.1110, 0.0114,
        0.0033], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,673][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([4.0524e-01, 7.5533e-06, 3.4761e-03, 5.5899e-01, 4.8411e-06, 5.9762e-03,
        1.2344e-02, 1.1298e-02, 2.6610e-05, 8.2903e-06, 2.6301e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,676][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0003, 0.0613, 0.2173, 0.0090, 0.0570, 0.1130, 0.1522, 0.0860, 0.1186,
        0.0682, 0.1170], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,679][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([4.2498e-01, 3.7449e-04, 3.8055e-02, 2.9566e-01, 7.9163e-04, 5.1644e-02,
        7.0427e-02, 7.6691e-02, 2.7714e-03, 1.0051e-03, 3.7600e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,682][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0453, 0.2804, 0.0602, 0.0371, 0.2352, 0.0950, 0.0173, 0.0094, 0.1259,
        0.0840, 0.0101], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,683][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([1.1592e-05, 9.9537e-01, 2.4782e-05, 4.5880e-07, 4.3918e-03, 1.7894e-06,
        6.2308e-06, 3.3451e-07, 3.5927e-05, 1.5505e-04, 1.2031e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,683][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.4818, 0.0014, 0.0264, 0.2769, 0.0030, 0.0383, 0.0384, 0.1039, 0.0019,
        0.0060, 0.0221], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,684][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0004, 0.1110, 0.1320, 0.0107, 0.1262, 0.0560, 0.0526, 0.0212, 0.1441,
        0.2667, 0.0790], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,684][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0838, 0.0048, 0.0869, 0.3226, 0.0035, 0.0651, 0.0657, 0.2254, 0.0102,
        0.0183, 0.1136], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,685][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.1512, 0.0293, 0.0577, 0.3999, 0.0160, 0.0671, 0.0430, 0.1254, 0.0232,
        0.0129, 0.0742], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,685][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([1.4632e-05, 4.1051e-02, 7.9348e-02, 1.5274e-04, 1.0449e-01, 1.5689e-02,
        1.8596e-02, 1.4012e-03, 1.3534e-01, 5.8647e-01, 1.7450e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,685][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0091, 0.0010, 0.0946, 0.3345, 0.0010, 0.2057, 0.1546, 0.1008, 0.0082,
        0.0069, 0.0836], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,689][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0079, 0.0250, 0.2571, 0.0339, 0.0367, 0.1421, 0.1001, 0.1372, 0.0530,
        0.0295, 0.1773], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,693][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1832, 0.0007, 0.0270, 0.5804, 0.0006, 0.0422, 0.0626, 0.0403, 0.0017,
        0.0008, 0.0171, 0.0434], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,695][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0002, 0.1915, 0.1445, 0.0061, 0.1456, 0.0565, 0.0675, 0.0479, 0.1325,
        0.1268, 0.0703, 0.0106], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,695][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1559, 0.0054, 0.0881, 0.2192, 0.0127, 0.1277, 0.1229, 0.0706, 0.0279,
        0.0136, 0.0550, 0.1009], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,696][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0455, 0.3622, 0.0645, 0.0233, 0.1835, 0.0582, 0.0113, 0.0085, 0.1280,
        0.0909, 0.0064, 0.0178], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,696][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([2.2472e-06, 9.9866e-01, 1.9890e-05, 2.9643e-07, 1.2406e-03, 6.0255e-07,
        2.4486e-06, 1.2432e-07, 1.5949e-05, 5.7664e-05, 3.0287e-07, 1.8819e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,697][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.2032, 0.0101, 0.1024, 0.2045, 0.0147, 0.0937, 0.0835, 0.1382, 0.0085,
        0.0225, 0.0358, 0.0828], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,697][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([3.8334e-05, 1.7860e-01, 5.5939e-02, 1.6446e-03, 1.6593e-01, 2.0207e-02,
        2.5917e-02, 6.7975e-03, 2.1585e-01, 3.0619e-01, 1.9853e-02, 3.0324e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,700][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0518, 0.0257, 0.1133, 0.1597, 0.0172, 0.0823, 0.0736, 0.2005, 0.0353,
        0.0566, 0.1055, 0.0785], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,705][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0537, 0.0805, 0.1160, 0.2353, 0.0414, 0.0959, 0.0602, 0.1132, 0.0559,
        0.0372, 0.0689, 0.0419], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,707][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([8.1547e-07, 7.9663e-02, 2.6057e-02, 2.2816e-05, 1.6722e-01, 5.6852e-03,
        7.9756e-03, 4.1409e-04, 1.5021e-01, 5.5816e-01, 4.0828e-03, 5.0958e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,707][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0052, 0.0063, 0.1875, 0.1348, 0.0053, 0.2413, 0.1227, 0.1274, 0.0204,
        0.0187, 0.1052, 0.0254], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,707][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0018, 0.0288, 0.3980, 0.0163, 0.0337, 0.1784, 0.0995, 0.0654, 0.0422,
        0.0266, 0.0855, 0.0239], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,708][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([4.3220e-01, 2.4292e-06, 8.9284e-03, 4.3765e-01, 2.5392e-06, 1.8679e-02,
        5.2599e-02, 6.0298e-03, 4.8779e-05, 8.1478e-06, 1.9593e-03, 3.9705e-02,
        2.1920e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,708][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([1.1437e-04, 1.6778e-01, 1.9580e-01, 3.6039e-03, 1.0259e-01, 8.8460e-02,
        7.1536e-02, 4.8218e-02, 1.0299e-01, 1.1905e-01, 7.5241e-02, 8.8754e-03,
        1.5739e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,709][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.2894, 0.0005, 0.0654, 0.2784, 0.0027, 0.0874, 0.1178, 0.0262, 0.0083,
        0.0027, 0.0263, 0.0835, 0.0113], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,709][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0426, 0.4270, 0.0417, 0.0082, 0.1457, 0.0186, 0.0159, 0.0039, 0.1693,
        0.0774, 0.0083, 0.0235, 0.0179], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,710][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([1.2031e-07, 9.9982e-01, 2.2245e-06, 1.9636e-08, 1.6079e-04, 5.2223e-08,
        1.3766e-07, 5.2079e-09, 3.4937e-06, 1.3494e-05, 1.8659e-08, 2.0635e-07,
        3.4580e-09], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,713][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.1479, 0.0032, 0.0925, 0.2559, 0.0060, 0.0993, 0.0657, 0.1137, 0.0059,
        0.0174, 0.0282, 0.0648, 0.0994], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,717][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([1.1381e-04, 7.3813e-02, 1.0759e-01, 4.9159e-03, 1.3628e-01, 6.3819e-02,
        5.5402e-02, 1.5136e-02, 1.7349e-01, 3.0096e-01, 3.9000e-02, 6.5752e-03,
        2.2901e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,719][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0363, 0.0070, 0.1092, 0.1576, 0.0056, 0.0652, 0.0768, 0.1929, 0.0223,
        0.0507, 0.0985, 0.0653, 0.1124], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,719][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.0962, 0.0359, 0.0966, 0.2667, 0.0320, 0.0891, 0.0790, 0.0798, 0.0485,
        0.0338, 0.0765, 0.0446, 0.0214], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,720][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([5.6242e-07, 3.2049e-02, 7.3351e-02, 3.1189e-05, 1.2002e-01, 1.1453e-02,
        1.2449e-02, 6.6203e-04, 1.2924e-01, 6.1290e-01, 4.9179e-03, 5.1999e-04,
        2.3991e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,720][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([0.0012, 0.0040, 0.1372, 0.1026, 0.0029, 0.2493, 0.1672, 0.1087, 0.0166,
        0.0195, 0.1413, 0.0182, 0.0313], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,721][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0044, 0.0172, 0.2410, 0.0209, 0.0276, 0.1890, 0.0968, 0.1000, 0.0418,
        0.0231, 0.1368, 0.0380, 0.0635], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,721][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([3.2890e-01, 3.4094e-06, 2.3525e-03, 6.2687e-01, 4.1402e-06, 4.8807e-03,
        1.4384e-02, 3.8570e-03, 2.8976e-05, 4.7161e-06, 9.3508e-04, 1.1997e-02,
        1.1593e-03, 4.6283e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,724][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0007, 0.0484, 0.1661, 0.0188, 0.0457, 0.0762, 0.1034, 0.0933, 0.0777,
        0.0690, 0.0906, 0.0186, 0.0438, 0.1479], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,727][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([4.4368e-01, 1.0402e-04, 2.0283e-02, 3.1363e-01, 4.8915e-04, 4.2372e-02,
        4.6682e-02, 2.6283e-02, 1.8767e-03, 5.0406e-04, 1.3351e-02, 5.6105e-02,
        4.1323e-03, 3.0506e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,731][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0159, 0.2661, 0.0450, 0.0517, 0.2447, 0.0596, 0.0079, 0.0069, 0.1105,
        0.1320, 0.0056, 0.0136, 0.0105, 0.0301], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,731][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([3.4687e-07, 9.9971e-01, 2.5112e-06, 1.0530e-08, 2.6909e-04, 4.2614e-08,
        2.2913e-07, 5.5905e-09, 2.5878e-06, 1.3575e-05, 1.8609e-08, 2.9757e-07,
        3.3259e-09, 2.3557e-08], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,732][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([3.9318e-01, 3.2752e-04, 2.7883e-02, 2.9971e-01, 8.9468e-04, 4.4914e-02,
        3.3838e-02, 5.9354e-02, 1.0220e-03, 2.2373e-03, 9.6114e-03, 5.5144e-02,
        3.4371e-02, 3.7510e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,732][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0002, 0.0986, 0.1237, 0.0082, 0.1146, 0.0419, 0.0587, 0.0202, 0.1859,
        0.2276, 0.0251, 0.0072, 0.0159, 0.0722], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,732][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0637, 0.0019, 0.0520, 0.3104, 0.0016, 0.0536, 0.0501, 0.1797, 0.0057,
        0.0105, 0.0622, 0.0618, 0.0582, 0.0885], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,733][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.2197, 0.0232, 0.0409, 0.4235, 0.0125, 0.0538, 0.0261, 0.0674, 0.0154,
        0.0108, 0.0280, 0.0291, 0.0090, 0.0406], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,733][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([2.8551e-06, 4.6478e-02, 3.4727e-02, 5.8687e-05, 1.4429e-01, 6.6225e-03,
        1.3485e-02, 4.9219e-04, 1.4819e-01, 5.7745e-01, 2.5347e-03, 7.5809e-04,
        2.6429e-03, 2.2269e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,734][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.6406e-03, 1.3995e-04, 5.3168e-02, 3.5120e-01, 2.1924e-04, 1.8212e-01,
        5.1491e-02, 4.6856e-02, 1.4103e-03, 1.4323e-03, 2.4091e-02, 1.6101e-02,
        6.7190e-03, 2.5541e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,737][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0062, 0.0094, 0.2824, 0.0419, 0.0134, 0.1544, 0.1046, 0.0924, 0.0233,
        0.0117, 0.0871, 0.0297, 0.0524, 0.0913], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,739][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:39,741][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[40483],
        [33583],
        [ 6237],
        [12388],
        [25970],
        [ 2669],
        [ 7583],
        [ 2282],
        [14672],
        [20693],
        [ 4238],
        [ 8868],
        [ 2483],
        [ 3181]], device='cuda:0')
[2024-07-24 10:30:39,744][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[35595],
        [34829],
        [21150],
        [35795],
        [40090],
        [25514],
        [25304],
        [16861],
        [27539],
        [32627],
        [17904],
        [21109],
        [11371],
        [14626]], device='cuda:0')
[2024-07-24 10:30:39,746][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[39911],
        [22435],
        [20020],
        [21219],
        [15445],
        [18722],
        [17850],
        [19111],
        [17020],
        [12361],
        [13871],
        [12987],
        [14803],
        [13014]], device='cuda:0')
[2024-07-24 10:30:39,747][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[23761],
        [23761],
        [ 6696],
        [ 6987],
        [ 6698],
        [ 6699],
        [ 5938],
        [ 6563],
        [ 6097],
        [ 6195],
        [ 3582],
        [ 4053],
        [ 3774],
        [ 3213]], device='cuda:0')
[2024-07-24 10:30:39,748][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[6904],
        [ 352],
        [ 370],
        [ 209],
        [  25],
        [  78],
        [  37],
        [  64],
        [  45],
        [  30],
        [  35],
        [  32],
        [  36],
        [  21]], device='cuda:0')
[2024-07-24 10:30:39,749][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[40749],
        [40749],
        [40742],
        [37190],
        [40749],
        [39540],
        [40634],
        [40539],
        [40749],
        [40748],
        [36691],
        [39680],
        [21179],
        [24352]], device='cuda:0')
[2024-07-24 10:30:39,752][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[35302],
        [35302],
        [33078],
        [33087],
        [39435],
        [33825],
        [35186],
        [36797],
        [40468],
        [41305],
        [35487],
        [35934],
        [36686],
        [36437]], device='cuda:0')
[2024-07-24 10:30:39,754][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[44617],
        [44617],
        [30121],
        [ 1938],
        [37349],
        [ 6650],
        [10823],
        [16848],
        [37484],
        [32905],
        [15728],
        [14859],
        [13789],
        [12700]], device='cuda:0')
[2024-07-24 10:30:39,757][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[42717],
        [42713],
        [ 5997],
        [ 6120],
        [ 1934],
        [ 9230],
        [21025],
        [25398],
        [15145],
        [12155],
        [15900],
        [10315],
        [25187],
        [21494]], device='cuda:0')
[2024-07-24 10:30:39,759][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[  241],
        [  267],
        [23815],
        [40572],
        [ 7284],
        [20839],
        [18660],
        [19999],
        [15711],
        [11513],
        [26128],
        [34672],
        [33771],
        [24461]], device='cuda:0')
[2024-07-24 10:30:39,760][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 8716],
        [15837],
        [ 5730],
        [ 5975],
        [ 2761],
        [ 1995],
        [ 1560],
        [ 1258],
        [  997],
        [  914],
        [  771],
        [  690],
        [  632],
        [  626]], device='cuda:0')
[2024-07-24 10:30:39,761][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[1646],
        [8766],
        [5577],
        [   1],
        [   1],
        [   1],
        [   2],
        [   3],
        [   3],
        [  15],
        [  13],
        [  10],
        [  15],
        [  16]], device='cuda:0')
[2024-07-24 10:30:39,762][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[5183],
        [1260],
        [3755],
        [3266],
        [2768],
        [3623],
        [3349],
        [3650],
        [3491],
        [3691],
        [3752],
        [3810],
        [4013],
        [4210]], device='cuda:0')
[2024-07-24 10:30:39,764][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[17788],
        [ 1224],
        [ 1251],
        [  979],
        [ 1344],
        [ 1265],
        [ 1143],
        [  939],
        [  747],
        [  350],
        [  349],
        [  202],
        [  248],
        [  195]], device='cuda:0')
[2024-07-24 10:30:39,766][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[48691],
        [34409],
        [35082],
        [39882],
        [17533],
        [21255],
        [34179],
        [22776],
        [31160],
        [29687],
        [34248],
        [42393],
        [34427],
        [35427]], device='cuda:0')
[2024-07-24 10:30:39,769][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[17120],
        [17120],
        [17331],
        [19213],
        [15737],
        [17535],
        [18905],
        [17570],
        [10552],
        [15385],
        [18356],
        [29259],
        [22595],
        [19324]], device='cuda:0')
[2024-07-24 10:30:39,771][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 2655],
        [ 2657],
        [25890],
        [22461],
        [ 7208],
        [19529],
        [20114],
        [22294],
        [16832],
        [13339],
        [21329],
        [22252],
        [21368],
        [18051]], device='cuda:0')
[2024-07-24 10:30:39,774][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[30966],
        [30966],
        [43304],
        [46688],
        [20088],
        [48651],
        [48929],
        [48577],
        [32403],
        [20555],
        [49152],
        [49887],
        [49616],
        [48299]], device='cuda:0')
[2024-07-24 10:30:39,775][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 8809],
        [ 8809],
        [39181],
        [43707],
        [ 8618],
        [39066],
        [43955],
        [41240],
        [ 8747],
        [ 8536],
        [41421],
        [40808],
        [40084],
        [40910]], device='cuda:0')
[2024-07-24 10:30:39,776][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[5681],
        [1107],
        [1107],
        [1107],
        [ 948],
        [1107],
        [1107],
        [1107],
        [1102],
        [1101],
        [1104],
        [1107],
        [1107],
        [1107]], device='cuda:0')
[2024-07-24 10:30:39,777][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 1052],
        [ 1052],
        [ 2323],
        [29454],
        [ 1600],
        [35504],
        [30034],
        [32950],
        [ 1893],
        [ 1876],
        [35284],
        [44670],
        [43802],
        [39585]], device='cuda:0')
[2024-07-24 10:30:39,778][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[10938],
        [11157],
        [43677],
        [39306],
        [20785],
        [45386],
        [45469],
        [44979],
        [43902],
        [39718],
        [46053],
        [45211],
        [46526],
        [46639]], device='cuda:0')
[2024-07-24 10:30:39,780][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[28076],
        [27888],
        [ 1845],
        [ 2062],
        [ 1120],
        [ 1126],
        [ 1476],
        [ 3128],
        [ 3537],
        [ 3214],
        [ 4286],
        [ 7522],
        [ 7483],
        [ 4232]], device='cuda:0')
[2024-07-24 10:30:39,783][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[39718],
        [39724],
        [45632],
        [43332],
        [36082],
        [36810],
        [42565],
        [46194],
        [45621],
        [43993],
        [47668],
        [49505],
        [49529],
        [46412]], device='cuda:0')
[2024-07-24 10:30:39,786][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[30390],
        [32077],
        [45346],
        [47932],
        [48024],
        [48419],
        [48521],
        [48423],
        [46294],
        [47346],
        [47521],
        [47886],
        [47572],
        [47653]], device='cuda:0')
[2024-07-24 10:30:39,788][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[26291],
        [26294],
        [42543],
        [40451],
        [38709],
        [38133],
        [37980],
        [38367],
        [38308],
        [39014],
        [38667],
        [39335],
        [39224],
        [38374]], device='cuda:0')
[2024-07-24 10:30:39,791][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[11780],
        [11959],
        [21151],
        [20825],
        [21952],
        [20814],
        [20797],
        [19931],
        [20240],
        [20069],
        [18905],
        [19964],
        [19775],
        [20418]], device='cuda:0')
[2024-07-24 10:30:39,792][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[43851],
        [42276],
        [19574],
        [11682],
        [41193],
        [15720],
        [11748],
        [ 9861],
        [32179],
        [35323],
        [ 8062],
        [ 3877],
        [ 4836],
        [ 8654]], device='cuda:0')
[2024-07-24 10:30:39,793][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 2531],
        [44810],
        [ 5837],
        [ 6169],
        [44219],
        [ 9656],
        [ 6302],
        [14928],
        [29055],
        [36054],
        [ 7600],
        [ 2871],
        [ 7210],
        [ 7513]], device='cuda:0')
[2024-07-24 10:30:39,794][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242],
        [22242]], device='cuda:0')
[2024-07-24 10:30:39,847][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:39,847][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,848][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,848][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,848][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,849][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,849][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,849][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,850][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,850][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,850][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,851][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,851][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:39,851][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.2262, 0.7738], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,852][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.4975, 0.5025], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,852][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.9460, 0.0540], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,852][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.4881, 0.5119], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,853][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([9.9944e-01, 5.5654e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,853][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.4819, 0.5181], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,853][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0292, 0.9708], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,854][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([9.9989e-01, 1.1325e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,854][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([0.9919, 0.0081], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,854][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([9.9992e-01, 8.1156e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,855][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0236, 0.9764], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,855][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([1.0000e+00, 6.0044e-09], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:39,855][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0662, 0.7671, 0.1668], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,856][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ and] are: tensor([8.7983e-04, 9.5597e-01, 4.3147e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,856][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0127, 0.8301, 0.1573], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,856][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0315, 0.8168, 0.1517], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,857][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9466, 0.0410, 0.0124], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,857][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0021, 0.9809, 0.0170], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,857][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1376, 0.4962, 0.3662], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,857][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.9362, 0.0045, 0.0593], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,858][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.9811, 0.0030, 0.0159], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,858][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.9153, 0.0387, 0.0459], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,858][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0264, 0.1834, 0.7902], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,859][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ and] are: tensor([5.1509e-05, 9.1687e-01, 8.3082e-02], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:39,859][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.0071, 0.7810, 0.1530, 0.0588], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,860][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([3.4709e-06, 9.9379e-01, 5.7739e-03, 4.3036e-04], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,860][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.0082, 0.7301, 0.1827, 0.0790], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,860][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.0119, 0.8247, 0.0814, 0.0820], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,863][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.4677, 0.0967, 0.1133, 0.3223], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,866][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.0622, 0.7654, 0.0059, 0.1665], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,870][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.1679, 0.4551, 0.1561, 0.2209], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,870][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.7367, 0.0173, 0.0284, 0.2176], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,871][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.9681, 0.0023, 0.0153, 0.0144], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,871][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.9307, 0.0105, 0.0331, 0.0257], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,872][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([0.0966, 0.1731, 0.4490, 0.2813], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,872][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([1.0073e-04, 9.0602e-01, 8.7300e-02, 6.5805e-03], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:39,872][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.0973, 0.4230, 0.2166, 0.1128, 0.1504], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,873][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0487, 0.2869, 0.2996, 0.1416, 0.2232], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,873][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ went] are: tensor([0.2034, 0.0556, 0.2779, 0.3926, 0.0705], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,873][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.0385, 0.2763, 0.1960, 0.2339, 0.2554], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,875][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ went] are: tensor([0.9199, 0.0027, 0.0284, 0.0469, 0.0020], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,877][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ went] are: tensor([7.6895e-05, 3.8319e-04, 2.7150e-03, 9.9662e-01, 2.0967e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,880][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.0347, 0.2650, 0.2372, 0.2316, 0.2315], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,882][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ went] are: tensor([9.5057e-01, 1.8734e-04, 1.5728e-02, 3.2727e-02, 7.9139e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,884][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ went] are: tensor([9.7411e-01, 2.8102e-03, 1.1233e-02, 1.1343e-02, 5.0710e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,885][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ went] are: tensor([9.9800e-01, 8.6401e-05, 1.3344e-03, 5.7882e-04, 3.9612e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,885][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ went] are: tensor([0.0148, 0.1268, 0.3965, 0.3056, 0.1562], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,885][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ went] are: tensor([7.6684e-01, 1.3092e-07, 1.3229e-02, 2.1993e-01, 4.5067e-07],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:39,886][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0175, 0.5032, 0.0841, 0.1232, 0.1931, 0.0790], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,886][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.6369e-04, 5.0246e-01, 1.7291e-02, 1.8101e-03, 4.6092e-01, 1.7248e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,886][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0035, 0.4803, 0.0815, 0.0381, 0.2977, 0.0989], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,887][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0134, 0.3513, 0.0807, 0.0920, 0.3408, 0.1218], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,887][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.7948, 0.0480, 0.0264, 0.1039, 0.0121, 0.0148], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,889][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0020, 0.3174, 0.0088, 0.3422, 0.3155, 0.0141], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,892][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1263, 0.2671, 0.1421, 0.1434, 0.2003, 0.1209], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,896][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.6359, 0.0035, 0.0581, 0.1943, 0.0041, 0.1041], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,898][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([9.7037e-01, 2.1766e-03, 1.1145e-02, 1.0520e-02, 6.4317e-04, 5.1492e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,898][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.6792, 0.0330, 0.0993, 0.1640, 0.0069, 0.0175], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,899][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1294, 0.1992, 0.2891, 0.1911, 0.1425, 0.0488], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,899][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.0923e-04, 2.2880e-01, 6.8822e-02, 1.8654e-02, 3.2414e-01, 3.5947e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:39,899][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0809, 0.4457, 0.1017, 0.0963, 0.1327, 0.0863, 0.0564],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,900][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0007, 0.6383, 0.0402, 0.0020, 0.2506, 0.0281, 0.0401],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,900][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0107, 0.3549, 0.1086, 0.0527, 0.2226, 0.1244, 0.1261],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,901][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0180, 0.2643, 0.0631, 0.0997, 0.3121, 0.1282, 0.1145],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,901][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.7632, 0.0268, 0.0370, 0.1282, 0.0064, 0.0230, 0.0153],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,903][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0054, 0.1339, 0.0104, 0.6096, 0.1977, 0.0184, 0.0246],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,905][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1163, 0.2322, 0.1283, 0.1334, 0.1651, 0.1057, 0.1189],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,909][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.8746, 0.0009, 0.0124, 0.0721, 0.0014, 0.0150, 0.0236],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,912][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([9.7320e-01, 1.9984e-03, 8.6994e-03, 7.7033e-03, 5.3348e-04, 3.9204e-03,
        3.9436e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,912][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.9017, 0.0078, 0.0254, 0.0508, 0.0015, 0.0079, 0.0049],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,912][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1006, 0.1869, 0.3013, 0.2308, 0.1141, 0.0477, 0.0186],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,913][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([3.6837e-04, 7.6766e-02, 1.1492e-01, 2.6847e-02, 1.5090e-01, 4.5909e-01,
        1.7110e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:39,913][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.0142, 0.3850, 0.0844, 0.0944, 0.1147, 0.0717, 0.0518, 0.1838],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,913][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0007, 0.2228, 0.0361, 0.0027, 0.5066, 0.0455, 0.0645, 0.1211],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,914][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.0095, 0.2420, 0.1106, 0.0556, 0.1802, 0.1242, 0.1248, 0.1532],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,914][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.0143, 0.2664, 0.0649, 0.0744, 0.2628, 0.1034, 0.1202, 0.0936],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,915][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ office] are: tensor([0.6631, 0.0133, 0.0484, 0.1171, 0.0032, 0.0242, 0.0205, 0.1102],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,916][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.0011, 0.3095, 0.0041, 0.2976, 0.3523, 0.0073, 0.0163, 0.0117],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,918][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.1193, 0.2001, 0.0848, 0.1181, 0.1408, 0.0772, 0.0906, 0.1691],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,919][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.6158, 0.0023, 0.0355, 0.1706, 0.0024, 0.0442, 0.0376, 0.0916],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,919][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ office] are: tensor([9.6992e-01, 2.0322e-03, 8.7267e-03, 7.8672e-03, 4.5250e-04, 3.6480e-03,
        3.1654e-03, 4.1903e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,919][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ office] are: tensor([9.8790e-01, 1.5591e-03, 4.1127e-03, 3.3711e-03, 1.0434e-04, 2.7524e-04,
        4.2215e-04, 2.2552e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,922][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ office] are: tensor([0.1052, 0.1429, 0.2684, 0.1703, 0.0924, 0.0367, 0.0152, 0.1690],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,925][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ office] are: tensor([3.5888e-04, 8.0843e-02, 6.2878e-02, 3.4612e-02, 1.0032e-01, 4.1319e-01,
        2.1879e-01, 8.9002e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:39,927][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0332, 0.4714, 0.0497, 0.0683, 0.1253, 0.0483, 0.0458, 0.1016, 0.0565],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,927][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0140, 0.1383, 0.0763, 0.0186, 0.2078, 0.1053, 0.2317, 0.0780, 0.1298],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,927][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0427, 0.0980, 0.1221, 0.1238, 0.0842, 0.1418, 0.1421, 0.1480, 0.0973],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,928][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0216, 0.2551, 0.0644, 0.0928, 0.1790, 0.0825, 0.0875, 0.0816, 0.1355],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,928][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.8091, 0.0016, 0.0224, 0.1116, 0.0011, 0.0269, 0.0080, 0.0179, 0.0014],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,929][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [,] are: tensor([3.4148e-04, 1.4828e-03, 2.1485e-03, 9.7177e-01, 1.2955e-03, 3.6043e-03,
        4.9348e-03, 1.1316e-02, 3.1029e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,929][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0271, 0.1512, 0.1000, 0.1123, 0.1110, 0.0975, 0.1025, 0.1918, 0.1066],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,929][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [,] are: tensor([9.0729e-01, 1.3259e-04, 1.0465e-02, 3.3351e-02, 3.9861e-04, 2.1350e-02,
        1.3312e-02, 1.2869e-02, 8.2771e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,931][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [,] are: tensor([9.7071e-01, 1.7703e-03, 8.0040e-03, 6.9347e-03, 4.3696e-04, 4.1236e-03,
        3.2610e-03, 3.1553e-03, 1.5994e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,932][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [,] are: tensor([9.8370e-01, 1.0165e-04, 3.1036e-03, 1.0051e-02, 1.0302e-05, 8.2262e-04,
        5.1811e-04, 1.6676e-03, 2.6284e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,935][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.1020, 0.1110, 0.2568, 0.1946, 0.0879, 0.0492, 0.0170, 0.1776, 0.0040],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,937][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [,] are: tensor([3.0451e-01, 1.6531e-06, 1.7234e-02, 3.3503e-01, 3.3743e-06, 1.2909e-02,
        3.8787e-03, 3.2641e-01, 1.6918e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:39,940][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.0111, 0.1707, 0.0558, 0.0611, 0.0896, 0.0420, 0.0490, 0.0871, 0.0390,
        0.3946], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,940][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.0025, 0.1478, 0.0510, 0.0173, 0.2432, 0.0488, 0.0902, 0.0690, 0.1787,
        0.1514], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,941][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([0.0242, 0.0591, 0.1330, 0.1214, 0.0653, 0.1441, 0.1341, 0.1396, 0.0862,
        0.0931], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,941][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.0072, 0.1584, 0.0630, 0.0588, 0.1616, 0.0950, 0.0856, 0.0490, 0.0956,
        0.2258], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,942][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([0.7311, 0.0022, 0.0317, 0.1488, 0.0026, 0.0405, 0.0208, 0.0167, 0.0026,
        0.0030], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,942][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0023, 0.0019, 0.0055, 0.9032, 0.0023, 0.0075, 0.0167, 0.0247, 0.0069,
        0.0290], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,943][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.0104, 0.1069, 0.0776, 0.1134, 0.0883, 0.0987, 0.1031, 0.1917, 0.0921,
        0.1177], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,943][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([8.0094e-01, 4.9798e-04, 2.4046e-02, 4.9845e-02, 1.3549e-03, 5.9050e-02,
        2.7084e-02, 3.3800e-02, 2.1143e-03, 1.2697e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,944][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([9.5351e-01, 2.8901e-03, 1.2698e-02, 1.3088e-02, 5.0756e-04, 5.2706e-03,
        4.2320e-03, 4.3134e-03, 1.7057e-03, 1.7813e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,945][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([9.9739e-01, 8.1664e-05, 1.0428e-03, 8.1980e-04, 1.0322e-05, 1.7546e-04,
        1.3878e-04, 3.1513e-04, 1.3317e-05, 8.1891e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,949][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([0.0052, 0.0333, 0.0772, 0.0995, 0.0333, 0.0264, 0.0080, 0.1172, 0.0015,
        0.5984], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,951][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([3.1411e-01, 2.8410e-06, 2.5139e-02, 3.0062e-01, 7.2357e-06, 2.0021e-02,
        6.5231e-03, 3.3354e-01, 1.1938e-05, 2.8243e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:39,954][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0233, 0.1935, 0.0392, 0.0290, 0.0676, 0.0277, 0.0256, 0.0455, 0.0342,
        0.4689, 0.0455], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,954][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0019, 0.2344, 0.0190, 0.0033, 0.1356, 0.0214, 0.0389, 0.0961, 0.0889,
        0.2968, 0.0637], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,954][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0103, 0.1741, 0.0654, 0.0378, 0.1094, 0.0731, 0.0788, 0.0885, 0.0997,
        0.1771, 0.0857], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,955][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0052, 0.2264, 0.0300, 0.0317, 0.1811, 0.0562, 0.0487, 0.0435, 0.0803,
        0.2092, 0.0876], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,955][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([9.3594e-01, 2.9334e-03, 6.1278e-03, 2.9302e-02, 1.0467e-03, 2.5933e-03,
        2.1753e-03, 1.3102e-02, 7.0337e-04, 4.8221e-03, 1.2517e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,956][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0008, 0.0937, 0.0031, 0.3625, 0.1203, 0.0061, 0.0112, 0.0084, 0.0675,
        0.3172, 0.0092], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,956][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0689, 0.1406, 0.0604, 0.0824, 0.0976, 0.0509, 0.0625, 0.1300, 0.0704,
        0.1125, 0.1237], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,956][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.6722, 0.0021, 0.0199, 0.1517, 0.0019, 0.0183, 0.0232, 0.0433, 0.0015,
        0.0031, 0.0629], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,957][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([9.6166e-01, 1.7021e-03, 8.7689e-03, 1.0126e-02, 3.9581e-04, 4.0162e-03,
        3.2001e-03, 3.6449e-03, 1.6406e-03, 1.5412e-03, 3.3016e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,958][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([9.4221e-01, 4.6303e-03, 1.3570e-02, 2.2050e-02, 4.6145e-04, 2.2741e-03,
        2.4024e-03, 1.0175e-02, 4.4293e-04, 5.3637e-04, 1.2503e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,960][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0149, 0.0396, 0.0828, 0.0552, 0.0312, 0.0121, 0.0040, 0.0722, 0.0010,
        0.6074, 0.0797], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,963][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0044, 0.0655, 0.0424, 0.0786, 0.0752, 0.1976, 0.0774, 0.0909, 0.0419,
        0.1015, 0.2246], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:39,967][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0099, 0.1790, 0.0303, 0.0244, 0.0848, 0.0299, 0.0190, 0.0399, 0.0297,
        0.5032, 0.0394, 0.0105], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,968][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([3.7330e-05, 2.7007e-01, 1.1275e-02, 6.5259e-04, 1.5632e-01, 8.7482e-03,
        3.2513e-02, 7.6345e-02, 8.1840e-02, 3.0760e-01, 3.0488e-02, 2.4115e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,968][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0070, 0.2018, 0.0529, 0.0359, 0.1027, 0.0605, 0.0603, 0.0661, 0.0922,
        0.1936, 0.0626, 0.0644], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,969][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0060, 0.2270, 0.0449, 0.0312, 0.1852, 0.0550, 0.0563, 0.0546, 0.0789,
        0.1690, 0.0682, 0.0238], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,969][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.7411, 0.0062, 0.0222, 0.1007, 0.0025, 0.0129, 0.0079, 0.0319, 0.0029,
        0.0169, 0.0070, 0.0478], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,969][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0017, 0.0778, 0.0024, 0.4255, 0.1204, 0.0057, 0.0071, 0.0064, 0.0593,
        0.2762, 0.0047, 0.0128], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,970][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0839, 0.1278, 0.0684, 0.0747, 0.0960, 0.0560, 0.0586, 0.1107, 0.0719,
        0.1025, 0.1031, 0.0463], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,970][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.3766, 0.0082, 0.0500, 0.1356, 0.0058, 0.0338, 0.0595, 0.0919, 0.0075,
        0.0111, 0.0824, 0.1374], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,970][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([9.2908e-01, 2.2325e-03, 1.1169e-02, 1.1052e-02, 6.8582e-04, 5.7142e-03,
        4.7031e-03, 4.9596e-03, 2.9156e-03, 2.3377e-03, 4.5960e-03, 2.0554e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,972][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.7200, 0.0154, 0.0642, 0.0650, 0.0026, 0.0146, 0.0056, 0.0624, 0.0021,
        0.0021, 0.0203, 0.0258], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,975][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0698, 0.0460, 0.0549, 0.0454, 0.0300, 0.0084, 0.0037, 0.0465, 0.0015,
        0.6212, 0.0637, 0.0089], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,977][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([1.1622e-04, 9.4581e-02, 5.4728e-02, 1.1481e-02, 1.1009e-01, 1.7526e-01,
        1.0359e-01, 2.2512e-02, 1.7733e-01, 1.4242e-01, 9.4205e-02, 1.3681e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:39,981][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.0016, 0.1458, 0.0350, 0.0254, 0.0934, 0.0329, 0.0311, 0.0547, 0.0492,
        0.3022, 0.0513, 0.0219, 0.1555], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,981][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([2.4665e-08, 3.6364e-01, 5.3756e-04, 3.1240e-06, 2.6820e-01, 1.0280e-03,
        1.1282e-03, 5.4310e-03, 2.7714e-01, 7.9646e-02, 3.1306e-03, 1.1867e-04,
        8.9476e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,982][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0068, 0.1665, 0.0610, 0.0234, 0.0998, 0.0612, 0.0640, 0.0693, 0.1025,
        0.1577, 0.0646, 0.0582, 0.0650], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,982][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.0040, 0.2261, 0.0434, 0.0234, 0.1993, 0.0489, 0.0517, 0.0391, 0.0732,
        0.1924, 0.0632, 0.0183, 0.0170], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,983][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([0.4433, 0.0340, 0.0465, 0.1096, 0.0113, 0.0245, 0.0218, 0.0686, 0.0106,
        0.0391, 0.0098, 0.0645, 0.1165], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,983][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([2.2377e-04, 1.8330e-01, 3.8367e-03, 8.9458e-02, 2.0151e-01, 6.6181e-03,
        1.4979e-02, 8.9776e-03, 1.2418e-01, 3.2188e-01, 1.2758e-02, 2.5481e-02,
        6.8023e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,983][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.1034, 0.1318, 0.0565, 0.0696, 0.0857, 0.0409, 0.0430, 0.0905, 0.0589,
        0.1004, 0.0977, 0.0392, 0.0822], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,984][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.1331, 0.0019, 0.0603, 0.1162, 0.0021, 0.0455, 0.0833, 0.1950, 0.0074,
        0.0079, 0.1070, 0.1368, 0.1035], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,984][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([9.5394e-01, 1.5154e-03, 8.3301e-03, 7.6641e-03, 3.5240e-04, 3.1589e-03,
        2.9094e-03, 3.3682e-03, 1.6948e-03, 1.0664e-03, 2.4983e-03, 1.2753e-02,
        7.4723e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,986][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.4246, 0.0688, 0.2254, 0.0259, 0.0034, 0.0202, 0.0282, 0.0882, 0.0083,
        0.0033, 0.0291, 0.0607, 0.0141], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,988][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([0.0273, 0.0247, 0.0840, 0.0614, 0.0303, 0.0197, 0.0073, 0.0968, 0.0023,
        0.4297, 0.0882, 0.0189, 0.1094], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,993][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.0005, 0.0358, 0.0804, 0.0248, 0.0849, 0.2720, 0.0906, 0.0525, 0.0676,
        0.0862, 0.0824, 0.0179, 0.1043], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:39,995][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0031, 0.1436, 0.0219, 0.0377, 0.0722, 0.0226, 0.0158, 0.0532, 0.0272,
        0.3427, 0.0389, 0.0148, 0.1737, 0.0327], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,995][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.1680e-05, 2.8197e-01, 4.4799e-03, 1.9411e-04, 4.5753e-01, 4.3933e-03,
        5.4824e-03, 1.4965e-02, 5.7839e-02, 1.5230e-01, 1.4905e-02, 1.8400e-03,
        3.2352e-04, 3.7535e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,995][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0030, 0.1980, 0.0401, 0.0210, 0.1187, 0.0456, 0.0517, 0.0505, 0.0947,
        0.1631, 0.0514, 0.0516, 0.0500, 0.0606], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,996][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0077, 0.1713, 0.0355, 0.0375, 0.1571, 0.0635, 0.0528, 0.0443, 0.0798,
        0.1803, 0.0678, 0.0263, 0.0244, 0.0518], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,996][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([8.4044e-01, 3.3051e-03, 7.0035e-03, 7.0440e-02, 1.0551e-03, 3.5043e-03,
        3.8957e-03, 2.0338e-02, 6.8671e-04, 8.3962e-03, 1.5264e-03, 1.6235e-02,
        1.6269e-02, 6.9049e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,997][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0017, 0.0776, 0.0050, 0.4375, 0.0870, 0.0105, 0.0139, 0.0116, 0.0623,
        0.2328, 0.0108, 0.0277, 0.0068, 0.0148], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,997][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0578, 0.1072, 0.0623, 0.0657, 0.0839, 0.0504, 0.0494, 0.0890, 0.0661,
        0.0884, 0.0945, 0.0412, 0.0782, 0.0658], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,997][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.5636, 0.0012, 0.0216, 0.1178, 0.0014, 0.0280, 0.0399, 0.0337, 0.0026,
        0.0045, 0.0227, 0.1045, 0.0202, 0.0383], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,998][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([9.3484e-01, 1.9786e-03, 1.0212e-02, 8.7174e-03, 5.6033e-04, 5.0753e-03,
        3.8092e-03, 3.9993e-03, 2.1322e-03, 1.5605e-03, 3.6175e-03, 1.6730e-02,
        8.8395e-04, 5.8811e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:39,999][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3009, 0.0120, 0.0599, 0.3337, 0.0029, 0.0267, 0.0229, 0.1267, 0.0026,
        0.0029, 0.0172, 0.0679, 0.0076, 0.0159], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,002][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1192, 0.0686, 0.0412, 0.0395, 0.0334, 0.0102, 0.0047, 0.0392, 0.0023,
        0.5120, 0.0566, 0.0076, 0.0632, 0.0024], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,006][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0010, 0.0082, 0.0250, 0.0492, 0.0109, 0.1124, 0.0271, 0.0455, 0.0149,
        0.0211, 0.0548, 0.0109, 0.0680, 0.5509], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,042][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:40,042][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,042][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,043][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,043][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,043][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,044][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,044][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,044][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,045][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,045][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,045][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,046][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,046][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.9530, 0.0470], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,046][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.9786, 0.0214], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,047][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([9.9941e-01, 5.8930e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,047][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.9987, 0.0013], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,047][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([0.9978, 0.0022], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,048][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([0.9950, 0.0050], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,048][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.9901, 0.0099], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,049][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([9.9975e-01, 2.4794e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,049][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.9998e-01, 2.4402e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,049][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([0.9886, 0.0114], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,050][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.9783, 0.0217], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,050][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 2.2164e-09], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,050][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0126, 0.1784, 0.8090], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,051][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([9.7173e-06, 6.2639e-01, 3.7360e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,051][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0236, 0.4026, 0.5738], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,051][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.1111, 0.0460, 0.8429], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,052][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.8969, 0.0015, 0.1015], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,052][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0497, 0.0705, 0.8798], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,052][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0014, 0.4253, 0.5733], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,055][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.9752, 0.0012, 0.0236], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,058][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.1655, 0.0072, 0.8273], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,060][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0142, 0.0133, 0.9725], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,060][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([1.0191e-04, 9.9913e-01, 7.6450e-04], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,061][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0012, 0.3877, 0.6111], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,061][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.0027, 0.4371, 0.4412, 0.1190], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,061][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([2.2388e-06, 8.9322e-01, 1.0606e-01, 7.1646e-04], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,061][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([5.2958e-04, 8.8834e-01, 9.9289e-02, 1.1844e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,062][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.0475, 0.3231, 0.3966, 0.2328], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,062][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.3265, 0.0298, 0.2077, 0.4361], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,062][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.1165, 0.0397, 0.0827, 0.7611], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,063][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([8.5935e-05, 8.3949e-01, 1.4996e-01, 1.0470e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,063][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.5864, 0.0232, 0.0496, 0.3409], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,065][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.0202, 0.0807, 0.8010, 0.0981], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,067][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.0591, 0.0231, 0.0681, 0.8497], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,069][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([2.7677e-05, 9.6212e-01, 1.9440e-03, 3.5910e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,072][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([4.4667e-04, 7.5461e-01, 2.2244e-01, 2.2508e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,074][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.0210, 0.0105, 0.8516, 0.0945, 0.0224], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,074][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0631, 0.0108, 0.4588, 0.4509, 0.0163], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,075][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([0.2415, 0.0011, 0.3030, 0.4519, 0.0025], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,075][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([0.4066, 0.0056, 0.2747, 0.2937, 0.0194], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,075][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([0.1578, 0.0021, 0.1882, 0.6492, 0.0027], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,075][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([4.3577e-02, 4.4245e-04, 1.8712e-01, 7.6869e-01, 1.7433e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,076][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0600, 0.0111, 0.6635, 0.2317, 0.0337], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,076][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([9.0136e-01, 4.0755e-04, 2.1067e-02, 7.5793e-02, 1.3712e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,076][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([8.8561e-01, 6.1317e-05, 4.1686e-03, 1.0997e-01, 1.8689e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,077][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([5.2523e-02, 3.6721e-04, 5.2020e-02, 8.9497e-01, 1.1608e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,077][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([0.0494, 0.0091, 0.1877, 0.7450, 0.0088], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,078][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([8.2320e-01, 7.7264e-08, 1.8092e-02, 1.5871e-01, 2.7888e-07],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,081][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0018, 0.0123, 0.2627, 0.1403, 0.0206, 0.5624], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,083][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([9.3826e-06, 3.1292e-01, 2.1957e-01, 2.5972e-03, 3.3193e-01, 1.3298e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,087][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0114, 0.1416, 0.1711, 0.0959, 0.1988, 0.3811], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,088][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0310, 0.0056, 0.1483, 0.2604, 0.0119, 0.5428], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,088][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.4950, 0.0005, 0.0405, 0.3877, 0.0016, 0.0747], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,089][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.0822e-02, 7.3141e-04, 2.0058e-02, 8.7665e-01, 7.4378e-04, 9.0999e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,089][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0004, 0.1957, 0.2061, 0.0683, 0.2826, 0.2469], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,089][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.6735, 0.0018, 0.0343, 0.2413, 0.0023, 0.0467], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,089][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.1012, 0.0086, 0.2333, 0.2438, 0.0186, 0.3945], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,090][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([7.6011e-03, 2.6232e-04, 1.9156e-02, 8.0138e-01, 2.5806e-04, 1.7135e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,090][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.3427e-04, 4.4900e-01, 2.0753e-03, 4.5637e-01, 6.3406e-02, 2.9017e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,090][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([3.8622e-04, 3.1010e-02, 1.3046e-01, 5.7624e-02, 5.1006e-02, 7.2952e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,092][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0045, 0.0302, 0.2074, 0.1370, 0.0462, 0.3802, 0.1945],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,094][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([3.6181e-05, 2.4652e-01, 1.7826e-01, 3.7386e-03, 2.4051e-01, 8.7958e-02,
        2.4298e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,097][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0165, 0.1740, 0.0799, 0.0895, 0.2097, 0.1673, 0.2632],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,101][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0757, 0.0122, 0.0958, 0.2574, 0.0268, 0.3390, 0.1931],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,101][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2341, 0.0010, 0.0923, 0.4582, 0.0017, 0.1392, 0.0735],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,102][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([8.2684e-03, 2.4033e-04, 3.7983e-02, 6.7578e-01, 3.4947e-04, 2.6321e-01,
        1.4169e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,102][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0011, 0.1244, 0.2418, 0.0550, 0.2093, 0.1348, 0.2336],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,102][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.7652, 0.0016, 0.0187, 0.1582, 0.0023, 0.0179, 0.0362],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,103][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2320, 0.0082, 0.1305, 0.2831, 0.0161, 0.1407, 0.1895],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,103][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([3.2160e-03, 1.6209e-04, 3.7996e-02, 4.5149e-01, 3.0959e-04, 4.9012e-01,
        1.6705e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,103][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0008, 0.3514, 0.0024, 0.5663, 0.0458, 0.0180, 0.0154],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,104][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0007, 0.0188, 0.1371, 0.0433, 0.0363, 0.5808, 0.1830],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,104][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.0034, 0.0533, 0.1496, 0.1858, 0.0483, 0.3714, 0.1274, 0.0607],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,104][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([4.5519e-06, 2.3881e-01, 2.5174e-01, 1.1074e-03, 2.4599e-01, 9.9512e-02,
        1.4570e-01, 1.7145e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,106][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([0.0083, 0.2301, 0.0710, 0.0845, 0.2293, 0.1591, 0.1465, 0.0712],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,108][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([0.0445, 0.0147, 0.0935, 0.1338, 0.0258, 0.4038, 0.2016, 0.0823],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,111][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([0.4018, 0.0008, 0.0431, 0.4120, 0.0013, 0.0737, 0.0303, 0.0370],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,115][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([0.0049, 0.0023, 0.0459, 0.5772, 0.0028, 0.2360, 0.0561, 0.0749],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,116][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([0.0013, 0.1097, 0.2071, 0.1166, 0.1211, 0.1970, 0.1387, 0.1085],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,116][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.5766, 0.0025, 0.0362, 0.2317, 0.0028, 0.0340, 0.0402, 0.0760],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,116][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.0753, 0.0065, 0.2805, 0.1513, 0.0130, 0.2360, 0.1455, 0.0920],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,117][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.0031, 0.0015, 0.0366, 0.6394, 0.0011, 0.2033, 0.0289, 0.0860],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,117][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([6.8688e-05, 5.0504e-01, 2.2684e-03, 2.6453e-01, 4.8145e-02, 1.5790e-02,
        7.5938e-03, 1.5657e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,117][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([0.0015, 0.0216, 0.0746, 0.0682, 0.0284, 0.4363, 0.1887, 0.1807],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,118][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0038, 0.0061, 0.3308, 0.0833, 0.0085, 0.2691, 0.1350, 0.0865, 0.0768],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,118][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0017, 0.0273, 0.2838, 0.0856, 0.0270, 0.1960, 0.1786, 0.0758, 0.1242],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,118][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0810, 0.0014, 0.1110, 0.2597, 0.0024, 0.2521, 0.1759, 0.0969, 0.0196],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,120][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1492, 0.0029, 0.0895, 0.2208, 0.0064, 0.1989, 0.1664, 0.1198, 0.0462],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,122][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([2.3575e-01, 1.2210e-04, 3.8218e-02, 5.8525e-01, 1.5318e-04, 6.0814e-02,
        2.5861e-02, 5.2682e-02, 1.1454e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,124][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.4326e-02, 4.6095e-05, 4.6200e-02, 6.9698e-01, 3.3040e-05, 1.8108e-01,
        1.0336e-02, 5.0873e-02, 1.1958e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,127][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0077, 0.0103, 0.3079, 0.1363, 0.0171, 0.1601, 0.1174, 0.2042, 0.0389],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,129][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([8.5427e-01, 2.2463e-04, 1.2959e-02, 7.6815e-02, 5.7502e-04, 1.8268e-02,
        1.7955e-02, 1.7583e-02, 1.3516e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,130][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([7.1812e-01, 1.3857e-04, 1.5403e-02, 2.1185e-01, 3.4523e-04, 2.5933e-02,
        1.1577e-02, 1.3863e-02, 2.7682e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,130][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([1.6821e-02, 1.5768e-05, 1.5247e-02, 8.6700e-01, 1.2887e-05, 3.6562e-02,
        2.2133e-03, 6.2096e-02, 3.4172e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,130][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0033, 0.0233, 0.0128, 0.7170, 0.0059, 0.0565, 0.0199, 0.1550, 0.0062],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,131][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([4.0655e-01, 5.9482e-07, 2.5221e-02, 3.7506e-01, 1.2399e-06, 1.7144e-02,
        9.2559e-03, 1.6675e-01, 1.5480e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,131][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.0151, 0.0040, 0.4059, 0.1049, 0.0084, 0.2273, 0.1151, 0.0503, 0.0536,
        0.0154], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,131][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.0082, 0.0135, 0.3035, 0.1244, 0.0208, 0.1840, 0.1527, 0.0552, 0.0729,
        0.0648], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,132][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([0.1655, 0.0028, 0.1278, 0.3053, 0.0054, 0.1786, 0.1084, 0.0787, 0.0215,
        0.0060], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,132][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.2918, 0.0043, 0.0639, 0.1899, 0.0121, 0.1425, 0.1555, 0.0723, 0.0538,
        0.0139], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,133][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([2.5662e-01, 4.5220e-04, 3.2453e-02, 5.5203e-01, 5.9150e-04, 5.6721e-02,
        3.3793e-02, 6.3206e-02, 2.7284e-03, 1.4058e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,135][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([4.7527e-02, 2.4871e-04, 5.5713e-02, 6.8048e-01, 1.6917e-04, 9.2963e-02,
        3.2215e-02, 8.9331e-02, 5.0079e-04, 8.5184e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,138][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0246, 0.0059, 0.2299, 0.2522, 0.0135, 0.1664, 0.0750, 0.1976, 0.0188,
        0.0160], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,140][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([7.9624e-01, 5.3862e-04, 2.1263e-02, 8.2282e-02, 1.3536e-03, 3.7469e-02,
        2.6338e-02, 3.1097e-02, 2.3665e-03, 1.0481e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,143][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([8.5433e-01, 8.8388e-05, 4.0353e-03, 1.1952e-01, 2.5951e-04, 9.7228e-03,
        4.2337e-03, 6.5541e-03, 7.3336e-04, 5.2136e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,143][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([6.5265e-02, 1.9281e-04, 8.6772e-03, 7.7934e-01, 1.3195e-04, 1.0646e-02,
        7.8282e-03, 1.2723e-01, 3.4557e-04, 3.4476e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,143][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([0.0120, 0.0112, 0.0971, 0.4154, 0.0089, 0.2574, 0.0545, 0.1158, 0.0176,
        0.0102], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,144][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([5.2649e-01, 7.7445e-07, 2.9266e-02, 2.8845e-01, 2.0527e-06, 1.6924e-02,
        9.7614e-03, 1.2909e-01, 8.8016e-06, 7.0569e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,144][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0039, 0.0278, 0.1913, 0.1281, 0.0257, 0.2437, 0.0958, 0.0400, 0.0670,
        0.0483, 0.1284], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,144][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([1.9226e-05, 1.2726e-01, 1.3560e-01, 2.8419e-03, 1.2992e-01, 5.1067e-02,
        5.3343e-02, 1.3792e-02, 2.3517e-01, 1.9588e-01, 5.5107e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,145][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0155, 0.1100, 0.0506, 0.0756, 0.0999, 0.0692, 0.0613, 0.0558, 0.2261,
        0.1605, 0.0755], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,145][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0282, 0.0139, 0.0889, 0.1414, 0.0265, 0.3045, 0.1327, 0.0910, 0.0526,
        0.0302, 0.0902], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,146][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([5.4765e-01, 3.0765e-04, 2.0797e-02, 3.2472e-01, 7.6448e-04, 3.6261e-02,
        1.9279e-02, 2.5617e-02, 2.2249e-03, 1.6357e-03, 2.0745e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,147][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0015, 0.0005, 0.0446, 0.4538, 0.0007, 0.3722, 0.0272, 0.0550, 0.0020,
        0.0026, 0.0399], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,149][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0016, 0.0640, 0.1738, 0.1387, 0.0491, 0.1216, 0.0748, 0.1266, 0.0488,
        0.0765, 0.1245], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,152][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.5979, 0.0026, 0.0246, 0.2241, 0.0025, 0.0182, 0.0305, 0.0483, 0.0027,
        0.0033, 0.0454], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,156][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.1697, 0.0054, 0.1206, 0.2652, 0.0108, 0.1414, 0.0753, 0.0520, 0.0318,
        0.0281, 0.0998], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,157][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([2.8162e-03, 5.7189e-04, 1.3037e-02, 6.1947e-01, 6.0568e-04, 2.4373e-01,
        1.7400e-02, 5.5462e-02, 2.0595e-03, 1.6965e-03, 4.3150e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,157][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([2.6432e-04, 3.4904e-01, 3.7293e-03, 3.4695e-01, 4.2837e-02, 2.1950e-02,
        7.8174e-03, 1.6397e-01, 4.8927e-03, 1.3663e-02, 4.4891e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,158][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0070, 0.0170, 0.0660, 0.1162, 0.0191, 0.2670, 0.0933, 0.1596, 0.0201,
        0.0270, 0.2078], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,158][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0011, 0.0759, 0.1650, 0.0435, 0.0764, 0.1987, 0.0952, 0.0339, 0.1566,
        0.0788, 0.0564, 0.0185], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,158][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([2.6666e-06, 1.4576e-01, 5.4119e-02, 3.6680e-04, 1.3107e-01, 1.9387e-02,
        4.2957e-02, 3.9045e-03, 3.7071e-01, 2.0974e-01, 2.0258e-02, 1.7248e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,159][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0018, 0.1451, 0.0226, 0.0098, 0.1462, 0.0312, 0.0389, 0.0144, 0.3273,
        0.2239, 0.0238, 0.0150], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,159][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0318, 0.0300, 0.1054, 0.1147, 0.0502, 0.2444, 0.1111, 0.0712, 0.0708,
        0.0391, 0.0597, 0.0716], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,159][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2961, 0.0014, 0.0633, 0.2695, 0.0027, 0.0835, 0.0537, 0.0417, 0.0092,
        0.0061, 0.0496, 0.1230], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,161][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0032, 0.0026, 0.1087, 0.2505, 0.0029, 0.3880, 0.0316, 0.1126, 0.0074,
        0.0078, 0.0645, 0.0204], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,164][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0003, 0.1210, 0.1262, 0.0156, 0.1392, 0.0661, 0.0659, 0.0306, 0.1694,
        0.2052, 0.0501, 0.0105], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,168][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.3222, 0.0126, 0.0611, 0.1819, 0.0090, 0.0370, 0.0694, 0.0910, 0.0132,
        0.0135, 0.0724, 0.1166], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,170][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0356, 0.0245, 0.1528, 0.0995, 0.0375, 0.1481, 0.1280, 0.0445, 0.1077,
        0.0649, 0.0706, 0.0864], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,171][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0035, 0.0036, 0.0854, 0.3345, 0.0026, 0.2743, 0.0266, 0.1539, 0.0059,
        0.0050, 0.0909, 0.0139], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,171][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([6.4293e-05, 7.4279e-01, 1.4782e-03, 7.4084e-02, 6.8100e-02, 7.3513e-03,
        5.4419e-03, 5.8293e-02, 1.0616e-02, 1.5530e-02, 1.5002e-02, 1.2515e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,171][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.8738e-04, 5.9954e-02, 7.8356e-02, 1.7593e-02, 7.1191e-02, 2.7391e-01,
        1.1437e-01, 4.7171e-02, 1.1362e-01, 8.6551e-02, 1.2045e-01, 1.6638e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,172][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.0021, 0.0254, 0.1647, 0.0924, 0.0467, 0.2970, 0.1148, 0.0302, 0.0987,
        0.0425, 0.0441, 0.0185, 0.0229], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,172][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([4.5546e-06, 4.2036e-02, 3.3121e-01, 8.6338e-04, 4.4041e-02, 8.5034e-02,
        8.7947e-02, 1.3353e-02, 1.8905e-01, 1.4210e-01, 4.7958e-02, 3.5414e-03,
        1.2862e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,172][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([0.0050, 0.0938, 0.0660, 0.0279, 0.1178, 0.0545, 0.0593, 0.0339, 0.3258,
        0.1342, 0.0293, 0.0177, 0.0348], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,173][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0105, 0.0100, 0.1079, 0.0620, 0.0239, 0.3259, 0.1560, 0.0507, 0.0659,
        0.0352, 0.0517, 0.0577, 0.0427], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,173][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([0.2095, 0.0009, 0.0373, 0.2975, 0.0021, 0.0643, 0.0439, 0.0432, 0.0087,
        0.0066, 0.0554, 0.1083, 0.1223], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,174][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([1.8233e-04, 1.5811e-03, 1.0501e-01, 1.1862e-01, 1.6574e-03, 4.1457e-01,
        4.9839e-02, 1.2006e-01, 8.6137e-03, 9.7914e-03, 1.2852e-01, 1.0082e-02,
        3.1476e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,177][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([0.0004, 0.0292, 0.3498, 0.0521, 0.0466, 0.1593, 0.0727, 0.0482, 0.0565,
        0.0825, 0.0527, 0.0127, 0.0371], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,180][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.2357, 0.0018, 0.0727, 0.1746, 0.0024, 0.0424, 0.0759, 0.1412, 0.0082,
        0.0062, 0.0601, 0.1070, 0.0719], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,184][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([0.0485, 0.0131, 0.1350, 0.1085, 0.0252, 0.1519, 0.1018, 0.0523, 0.0698,
        0.0433, 0.0651, 0.0733, 0.1122], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,184][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([2.0714e-04, 1.5528e-03, 9.9801e-02, 2.5001e-01, 9.3009e-04, 2.6837e-01,
        3.8603e-02, 1.5392e-01, 4.7973e-03, 3.8588e-03, 1.6282e-01, 5.8028e-03,
        9.3312e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,185][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([8.0914e-05, 3.2597e-01, 2.0381e-02, 1.4558e-01, 8.7860e-02, 6.8968e-02,
        2.6255e-02, 1.6245e-01, 2.4141e-02, 3.6726e-02, 6.2661e-02, 3.0271e-03,
        3.5904e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,185][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([0.0007, 0.0110, 0.1039, 0.0341, 0.0247, 0.3612, 0.1113, 0.0770, 0.0319,
        0.0290, 0.0950, 0.0246, 0.0954], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,185][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0026, 0.0243, 0.1312, 0.1547, 0.0327, 0.2179, 0.0801, 0.0340, 0.0564,
        0.0222, 0.0286, 0.0170, 0.0192, 0.1790], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,186][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([7.1028e-06, 1.2703e-01, 7.9815e-02, 1.2958e-03, 1.1433e-01, 3.0663e-02,
        5.8520e-02, 7.1376e-03, 2.6489e-01, 1.7854e-01, 1.5786e-02, 1.9677e-03,
        5.1001e-03, 1.1492e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,186][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0155, 0.0509, 0.0299, 0.0565, 0.0613, 0.0484, 0.0812, 0.0225, 0.2286,
        0.1557, 0.0238, 0.0337, 0.0459, 0.1462], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,186][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0435, 0.0066, 0.0658, 0.2071, 0.0135, 0.2272, 0.0942, 0.0406, 0.0259,
        0.0124, 0.0213, 0.0602, 0.0292, 0.1525], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,187][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([6.7757e-01, 3.5362e-05, 5.5740e-03, 2.1766e-01, 1.1245e-04, 1.1895e-02,
        8.2823e-03, 3.4914e-03, 3.2562e-04, 2.7405e-04, 3.8728e-03, 3.4244e-02,
        1.6676e-02, 1.9983e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,188][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.6301e-03, 8.1130e-05, 1.1499e-02, 6.0829e-01, 1.2862e-04, 2.4309e-01,
        5.9181e-03, 2.6113e-02, 3.6978e-04, 6.4742e-04, 1.1485e-02, 7.6562e-03,
        4.6958e-03, 7.8399e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,191][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0009, 0.0622, 0.1307, 0.0906, 0.0802, 0.1014, 0.0697, 0.0694, 0.0690,
        0.1028, 0.0481, 0.0118, 0.0264, 0.1369], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,193][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.5804, 0.0010, 0.0211, 0.1653, 0.0013, 0.0209, 0.0360, 0.0326, 0.0029,
        0.0031, 0.0163, 0.0776, 0.0165, 0.0251], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,198][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0964, 0.0059, 0.0581, 0.1949, 0.0112, 0.1002, 0.0788, 0.0208, 0.0305,
        0.0203, 0.0183, 0.0658, 0.0519, 0.2467], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,198][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.1837e-03, 4.2465e-05, 7.2725e-03, 7.0552e-01, 7.7838e-05, 2.0481e-01,
        2.9004e-03, 1.4072e-02, 1.3722e-04, 1.8391e-04, 8.2289e-03, 2.7718e-03,
        5.7135e-04, 5.2229e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,198][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.6260e-04, 3.9476e-01, 1.1582e-03, 3.9965e-01, 3.7486e-02, 1.0839e-02,
        5.6569e-03, 1.1175e-01, 3.9953e-03, 6.2344e-03, 1.0198e-02, 1.2563e-03,
        1.5144e-02, 1.7199e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,199][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0010, 0.0023, 0.0376, 0.0551, 0.0033, 0.1673, 0.0397, 0.0651, 0.0100,
        0.0066, 0.0458, 0.0139, 0.0626, 0.4895], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,200][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:40,201][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[18746],
        [ 5840],
        [   23],
        [   41],
        [  455],
        [    9],
        [    6],
        [    3],
        [   10],
        [   92],
        [   10],
        [    3],
        [    2],
        [    6]], device='cuda:0')
[2024-07-24 10:30:40,202][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[7586],
        [8011],
        [  61],
        [ 143],
        [1287],
        [  32],
        [  29],
        [  16],
        [  26],
        [ 281],
        [  37],
        [  25],
        [   7],
        [  32]], device='cuda:0')
[2024-07-24 10:30:40,203][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 5941],
        [32087],
        [32046],
        [25172],
        [12366],
        [14218],
        [15973],
        [15239],
        [21773],
        [25217],
        [31232],
        [31645],
        [26154],
        [23760]], device='cuda:0')
[2024-07-24 10:30:40,206][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[12015],
        [ 6452],
        [ 6868],
        [ 7280],
        [  775],
        [ 5620],
        [ 4849],
        [ 3883],
        [ 2225],
        [ 2523],
        [ 3262],
        [ 3367],
        [ 3876],
        [ 5164]], device='cuda:0')
[2024-07-24 10:30:40,207][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 3839],
        [ 5028],
        [25568],
        [19946],
        [ 2260],
        [17337],
        [14396],
        [11152],
        [ 7381],
        [ 6583],
        [ 9875],
        [10245],
        [11166],
        [11955]], device='cuda:0')
[2024-07-24 10:30:40,209][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[27010],
        [31724],
        [29999],
        [30911],
        [26478],
        [25587],
        [24492],
        [22917],
        [22606],
        [22935],
        [23583],
        [23170],
        [23561],
        [22579]], device='cuda:0')
[2024-07-24 10:30:40,212][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[45244],
        [45265],
        [46553],
        [29811],
        [42970],
        [39716],
        [37037],
        [32362],
        [37082],
        [34155],
        [43565],
        [33698],
        [24760],
        [37222]], device='cuda:0')
[2024-07-24 10:30:40,214][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[32428],
        [19308],
        [18750],
        [19724],
        [30512],
        [21796],
        [25618],
        [21422],
        [30422],
        [29921],
        [22079],
        [23067],
        [19692],
        [24084]], device='cuda:0')
[2024-07-24 10:30:40,215][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[534],
        [ 78],
        [ 59],
        [  4],
        [  4],
        [  7],
        [  6],
        [  8],
        [  9],
        [  7],
        [ 15],
        [ 16],
        [ 14],
        [ 14]], device='cuda:0')
[2024-07-24 10:30:40,216][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[31224],
        [31196],
        [26552],
        [    8],
        [10375],
        [   51],
        [ 1499],
        [  252],
        [10149],
        [ 5436],
        [  210],
        [ 2500],
        [ 7769],
        [ 1881]], device='cuda:0')
[2024-07-24 10:30:40,217][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[42085],
        [42162],
        [42235],
        [42452],
        [42386],
        [42431],
        [42399],
        [42412],
        [42412],
        [42585],
        [42514],
        [42856],
        [42610],
        [42826]], device='cuda:0')
[2024-07-24 10:30:40,218][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[46613],
        [46614],
        [47559],
        [45796],
        [46621],
        [30689],
        [43352],
        [46481],
        [46087],
        [46591],
        [45180],
        [35354],
        [32323],
        [11799]], device='cuda:0')
[2024-07-24 10:30:40,219][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[12553],
        [ 2470],
        [ 1121],
        [  897],
        [  702],
        [  804],
        [  775],
        [  700],
        [  664],
        [  922],
        [  933],
        [ 1013],
        [  727],
        [  954]], device='cuda:0')
[2024-07-24 10:30:40,220][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[41167],
        [41167],
        [19481],
        [19533],
        [44526],
        [13942],
        [11866],
        [11653],
        [33288],
        [31498],
        [10369],
        [10377],
        [10083],
        [ 7469]], device='cuda:0')
[2024-07-24 10:30:40,222][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[46526],
        [28322],
        [12726],
        [ 8055],
        [27701],
        [ 9166],
        [ 8916],
        [ 8385],
        [16368],
        [23153],
        [11866],
        [ 6389],
        [ 8928],
        [ 9806]], device='cuda:0')
[2024-07-24 10:30:40,224][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[25449],
        [33126],
        [36539],
        [40377],
        [33295],
        [34221],
        [34643],
        [32711],
        [32025],
        [33186],
        [36500],
        [39594],
        [36028],
        [32407]], device='cuda:0')
[2024-07-24 10:30:40,226][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[49595],
        [49599],
        [38048],
        [35020],
        [44048],
        [42452],
        [43653],
        [44068],
        [47096],
        [46534],
        [45449],
        [44810],
        [47218],
        [44771]], device='cuda:0')
[2024-07-24 10:30:40,228][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[39168],
        [39169],
        [30150],
        [32552],
        [25314],
        [30706],
        [32918],
        [30439],
        [32265],
        [30329],
        [30911],
        [34205],
        [33878],
        [33503]], device='cuda:0')
[2024-07-24 10:30:40,231][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[45907],
        [45754],
        [29032],
        [32430],
        [29493],
        [28927],
        [28414],
        [27564],
        [26651],
        [26997],
        [26932],
        [27400],
        [27313],
        [26083]], device='cuda:0')
[2024-07-24 10:30:40,232][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[30541],
        [30399],
        [28960],
        [31876],
        [33480],
        [33391],
        [33149],
        [32879],
        [33522],
        [33168],
        [32241],
        [28222],
        [28282],
        [31537]], device='cuda:0')
[2024-07-24 10:30:40,233][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[37295],
        [36986],
        [27551],
        [23189],
        [24465],
        [22766],
        [21027],
        [19922],
        [20909],
        [20680],
        [19090],
        [18909],
        [18670],
        [19334]], device='cuda:0')
[2024-07-24 10:30:40,234][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[27866],
        [28012],
        [35836],
        [40513],
        [39380],
        [38166],
        [38413],
        [39633],
        [38659],
        [39075],
        [34940],
        [33447],
        [35684],
        [35889]], device='cuda:0')
[2024-07-24 10:30:40,235][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[39079],
        [39083],
        [40545],
        [38757],
        [42980],
        [41985],
        [44825],
        [40953],
        [43988],
        [44924],
        [41420],
        [42823],
        [42059],
        [42959]], device='cuda:0')
[2024-07-24 10:30:40,236][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[24652],
        [24654],
        [39593],
        [40719],
        [27770],
        [38246],
        [36966],
        [38732],
        [31820],
        [28937],
        [38628],
        [41592],
        [41027],
        [41069]], device='cuda:0')
[2024-07-24 10:30:40,237][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[34567],
        [34946],
        [47908],
        [46084],
        [45569],
        [45721],
        [46387],
        [45726],
        [45502],
        [44995],
        [45755],
        [46111],
        [45961],
        [45526]], device='cuda:0')
[2024-07-24 10:30:40,239][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[49569],
        [49110],
        [38013],
        [38634],
        [49374],
        [45780],
        [46674],
        [44709],
        [48795],
        [49556],
        [46738],
        [40880],
        [46542],
        [46109]], device='cuda:0')
[2024-07-24 10:30:40,242][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12391],
        [12391],
        [28458],
        [31736],
        [11226],
        [10967],
        [10468],
        [15507],
        [22940],
        [19805],
        [15525],
        [14049],
        [12504],
        [ 9830]], device='cuda:0')
[2024-07-24 10:30:40,243][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 5997],
        [ 5883],
        [ 7812],
        [ 7192],
        [ 9733],
        [10382],
        [ 9282],
        [ 9746],
        [ 6640],
        [ 5749],
        [ 9437],
        [10924],
        [ 9082],
        [11054]], device='cuda:0')
[2024-07-24 10:30:40,245][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 3083],
        [19282],
        [31734],
        [31826],
        [23989],
        [36562],
        [36962],
        [37094],
        [33075],
        [26618],
        [32808],
        [35560],
        [32358],
        [35774]], device='cuda:0')
[2024-07-24 10:30:40,248][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232],
        [28232]], device='cuda:0')
[2024-07-24 10:30:40,286][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:40,287][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,287][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,287][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,288][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,288][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,288][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,289][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,289][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,289][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,290][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,290][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,290][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,291][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.9988, 0.0012], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,291][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([0.1198, 0.8802], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,291][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([1.9584e-05, 9.9998e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,292][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.9233, 0.0767], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,292][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([5.3409e-04, 9.9947e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,292][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([9.9908e-01, 9.2004e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,293][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.9364, 0.0636], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,293][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([0.8235, 0.1765], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,293][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([1.0000e+00, 1.1745e-14], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,294][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([1.0000e+00, 1.2653e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,294][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([9.9998e-01, 1.6268e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,294][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.2048, 0.7952], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,295][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3267, 0.0096, 0.6637], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,295][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.4349, 0.1151, 0.4501], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,295][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0028, 0.1235, 0.8737], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,296][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0282, 0.8896, 0.0822], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,296][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ and] are: tensor([4.5819e-04, 2.9736e-02, 9.6981e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,297][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0025, 0.8206, 0.1769], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,297][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9843, 0.0047, 0.0109], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,297][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0091, 0.0134, 0.9775], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,299][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.9131, 0.0024, 0.0846], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,300][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0208, 0.0125, 0.9667], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,301][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ and] are: tensor([2.9173e-01, 5.5354e-06, 7.0826e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,302][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.9610, 0.0048, 0.0342], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,304][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Samantha] are: tensor([0.5533, 0.0115, 0.0102, 0.4250], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,305][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Samantha] are: tensor([0.1195, 0.0022, 0.0021, 0.8762], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,307][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Samantha] are: tensor([0.0085, 0.0804, 0.2961, 0.6150], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,308][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Samantha] are: tensor([0.0321, 0.7399, 0.1281, 0.0999], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,308][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Samantha] are: tensor([0.0052, 0.2207, 0.5463, 0.2278], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,308][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Samantha] are: tensor([0.0026, 0.7394, 0.0633, 0.1947], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,309][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Samantha] are: tensor([0.4577, 0.0075, 0.0192, 0.5156], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,309][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Samantha] are: tensor([0.1300, 0.0202, 0.0484, 0.8014], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,309][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Samantha] are: tensor([0.4636, 0.0366, 0.0634, 0.4363], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,310][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Samantha] are: tensor([0.0059, 0.1306, 0.0366, 0.8269], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,310][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Samantha] are: tensor([5.9717e-01, 4.7895e-04, 9.8838e-03, 3.9247e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,310][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Samantha] are: tensor([9.7502e-01, 2.0579e-04, 3.3827e-04, 2.4441e-02], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,311][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ went] are: tensor([0.9436, 0.0014, 0.0303, 0.0216, 0.0030], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,312][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ went] are: tensor([0.0030, 0.1707, 0.5044, 0.1686, 0.1533], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,313][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ went] are: tensor([1.3027e-05, 5.5033e-01, 4.8511e-02, 6.0143e-03, 3.9513e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,314][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ went] are: tensor([0.2459, 0.0727, 0.3777, 0.1701, 0.1336], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,315][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ went] are: tensor([7.2383e-05, 2.0062e-01, 5.2444e-01, 3.3336e-02, 2.4153e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,316][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ went] are: tensor([4.8908e-03, 1.7464e-04, 6.9862e-02, 9.1247e-01, 1.2605e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,317][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ went] are: tensor([0.4131, 0.0221, 0.0153, 0.5047, 0.0448], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,318][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ went] are: tensor([0.0010, 0.0017, 0.8462, 0.1441, 0.0071], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,319][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ went] are: tensor([9.9947e-01, 5.3717e-14, 5.6333e-08, 5.3418e-04, 2.1794e-14],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,320][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ went] are: tensor([2.3784e-01, 3.5044e-06, 1.3310e-02, 7.4884e-01, 1.0539e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,321][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ went] are: tensor([4.4620e-01, 3.3047e-06, 3.9968e-01, 1.5410e-01, 1.4278e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,323][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ went] are: tensor([0.1125, 0.2003, 0.1642, 0.4554, 0.0676], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,324][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1691, 0.0010, 0.2580, 0.1269, 0.0012, 0.4438], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,325][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0045, 0.0036, 0.0136, 0.9667, 0.0013, 0.0104], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,327][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0009, 0.0479, 0.3520, 0.1835, 0.0171, 0.3987], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,328][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0176, 0.4587, 0.0621, 0.0611, 0.3153, 0.0853], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,329][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([5.7681e-05, 1.9223e-02, 4.5645e-01, 1.7163e-02, 2.9581e-02, 4.7752e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,330][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([3.9618e-04, 1.4150e-01, 2.7745e-02, 5.1485e-02, 7.2452e-01, 5.4350e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,331][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.6074, 0.0014, 0.0053, 0.3759, 0.0024, 0.0076], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,333][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0005, 0.0011, 0.2197, 0.5073, 0.0006, 0.2708], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,334][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([8.2608e-01, 1.1958e-05, 1.9722e-03, 1.7081e-01, 2.0438e-05, 1.1002e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,334][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([2.4019e-03, 6.4858e-04, 2.3242e-02, 7.5717e-01, 6.3289e-04, 2.1590e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,334][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([5.4896e-02, 2.2126e-06, 6.1499e-02, 9.2226e-02, 2.7409e-05, 7.9135e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,335][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([8.8739e-01, 2.3586e-04, 4.1166e-03, 1.0141e-01, 1.5359e-04, 6.6886e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,335][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.3755, 0.0006, 0.1807, 0.0961, 0.0007, 0.2287, 0.1178],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,335][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0084, 0.0080, 0.0410, 0.8711, 0.0056, 0.0204, 0.0455],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,336][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0010, 0.0861, 0.2357, 0.1390, 0.0370, 0.2155, 0.2856],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,336][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0244, 0.3812, 0.0681, 0.0559, 0.2643, 0.0859, 0.1203],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,337][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([4.9638e-05, 1.2995e-02, 3.3790e-01, 2.2574e-02, 1.9746e-02, 4.5455e-01,
        1.5218e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,337][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0007, 0.0213, 0.0299, 0.0569, 0.5764, 0.0776, 0.2373],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,339][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6641, 0.0018, 0.0035, 0.3125, 0.0026, 0.0055, 0.0099],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,340][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0008, 0.0021, 0.1686, 0.4694, 0.0018, 0.2220, 0.1353],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,341][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([9.7165e-01, 9.0115e-08, 6.1629e-05, 2.8229e-02, 1.3710e-07, 6.1381e-05,
        2.6801e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,342][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.2311e-02, 1.3454e-04, 1.4270e-02, 7.7745e-01, 1.8418e-04, 1.2149e-01,
        7.4165e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,343][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([2.4681e-02, 2.5729e-07, 7.6930e-02, 4.4300e-02, 3.4923e-06, 8.3813e-01,
        1.5955e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,344][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5459, 0.0015, 0.0260, 0.3232, 0.0014, 0.0514, 0.0505],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,345][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ office] are: tensor([0.1660, 0.0038, 0.1572, 0.2405, 0.0043, 0.3013, 0.0828, 0.0440],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,346][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ office] are: tensor([0.0142, 0.0083, 0.0099, 0.9447, 0.0019, 0.0035, 0.0029, 0.0147],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,348][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ office] are: tensor([0.0009, 0.0427, 0.1509, 0.2632, 0.0106, 0.2091, 0.1600, 0.1626],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,349][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ office] are: tensor([0.0480, 0.2324, 0.0688, 0.0814, 0.2007, 0.0919, 0.1505, 0.1262],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,350][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ office] are: tensor([2.8243e-04, 2.3316e-02, 2.9394e-01, 5.2839e-02, 2.4324e-02, 4.5246e-01,
        1.2219e-01, 3.0643e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,351][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ office] are: tensor([0.0021, 0.0606, 0.0453, 0.2771, 0.2405, 0.1113, 0.1355, 0.1275],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,353][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ office] are: tensor([0.6144, 0.0013, 0.0050, 0.3137, 0.0019, 0.0065, 0.0100, 0.0473],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,354][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ office] are: tensor([0.0070, 0.0023, 0.0382, 0.8211, 0.0018, 0.0433, 0.0167, 0.0696],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,356][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ office] are: tensor([0.2336, 0.0099, 0.0477, 0.6699, 0.0081, 0.0204, 0.0015, 0.0088],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,357][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ office] are: tensor([0.0021, 0.0035, 0.0225, 0.5670, 0.0024, 0.1180, 0.0263, 0.2583],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,358][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ office] are: tensor([8.5207e-02, 7.7714e-06, 8.5872e-02, 2.3810e-01, 6.1269e-05, 5.7117e-01,
        1.4479e-02, 5.1024e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,359][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ office] are: tensor([8.8905e-01, 2.2173e-04, 2.0987e-03, 9.6038e-02, 1.3904e-04, 3.9742e-03,
        5.4455e-03, 3.0287e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,360][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.4083, 0.0015, 0.2475, 0.0587, 0.0024, 0.1389, 0.0967, 0.0406, 0.0054],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,361][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0019, 0.0429, 0.2202, 0.3202, 0.0198, 0.0101, 0.0606, 0.1395, 0.1848],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,361][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [,] are: tensor([8.5074e-06, 1.9145e-01, 4.4524e-02, 6.1926e-03, 7.9073e-02, 2.0716e-02,
        4.1224e-02, 2.0841e-02, 5.9597e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,361][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0937, 0.1227, 0.0922, 0.0798, 0.1448, 0.0889, 0.1255, 0.1512, 0.1012],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,362][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [,] are: tensor([1.3605e-05, 4.5090e-03, 2.4023e-01, 1.0276e-02, 8.6095e-03, 1.9081e-01,
        7.0535e-02, 3.1882e-02, 4.4313e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,362][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0047, 0.0008, 0.0600, 0.2894, 0.0357, 0.1680, 0.3675, 0.0410, 0.0327],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,362][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.4117, 0.0079, 0.0067, 0.4287, 0.0122, 0.0109, 0.0166, 0.0971, 0.0081],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,363][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [,] are: tensor([4.3907e-04, 3.1501e-04, 3.2967e-01, 1.8073e-01, 6.2753e-04, 3.2862e-01,
        1.2343e-01, 2.7829e-02, 8.3325e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,364][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [,] are: tensor([9.9910e-01, 2.3842e-12, 5.3508e-07, 8.9933e-04, 1.9991e-12, 1.7510e-07,
        5.0192e-11, 2.6511e-08, 2.1149e-11], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,364][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [,] are: tensor([2.0493e-02, 3.4872e-06, 1.4374e-02, 6.4244e-01, 6.8797e-06, 1.2965e-01,
        2.0614e-02, 1.7239e-01, 1.8477e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,365][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [,] are: tensor([3.9825e-01, 5.0607e-07, 1.9377e-01, 1.7900e-01, 2.7334e-06, 1.9780e-01,
        1.6597e-02, 1.4424e-02, 1.4721e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,367][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2685, 0.0258, 0.0742, 0.3590, 0.0099, 0.0654, 0.1318, 0.0390, 0.0264],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,368][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Kimberly] are: tensor([0.8165, 0.0016, 0.0493, 0.0298, 0.0035, 0.0263, 0.0447, 0.0209, 0.0033,
        0.0040], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,369][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Kimberly] are: tensor([3.4420e-04, 7.4108e-02, 1.5654e-01, 4.0615e-02, 7.3845e-02, 2.5606e-03,
        3.2355e-02, 1.9061e-02, 5.0535e-01, 9.5222e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,370][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Kimberly] are: tensor([6.1244e-07, 2.2232e-01, 8.1614e-03, 6.2355e-04, 1.2982e-01, 2.3785e-03,
        5.7214e-03, 1.0417e-03, 3.1751e-01, 3.1242e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,371][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Kimberly] are: tensor([0.1104, 0.0533, 0.1264, 0.0897, 0.0950, 0.1006, 0.1325, 0.1318, 0.0895,
        0.0708], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,372][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Kimberly] are: tensor([5.9472e-06, 3.4257e-02, 7.6441e-02, 8.3719e-03, 3.8600e-02, 8.2688e-02,
        2.7241e-02, 3.2401e-02, 4.8458e-01, 2.1542e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,374][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Kimberly] are: tensor([0.0029, 0.0012, 0.0498, 0.2570, 0.0684, 0.1390, 0.3953, 0.0294, 0.0548,
        0.0022], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,375][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Kimberly] are: tensor([0.2333, 0.0245, 0.0113, 0.4203, 0.0515, 0.0201, 0.0290, 0.1552, 0.0281,
        0.0265], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,376][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Kimberly] are: tensor([1.2982e-04, 1.0273e-03, 3.5237e-01, 7.4968e-02, 4.7386e-03, 3.8351e-01,
        1.3947e-01, 7.1413e-03, 3.3720e-02, 2.9237e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,377][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Kimberly] are: tensor([9.9982e-01, 2.5997e-14, 1.0415e-08, 1.8037e-04, 1.1774e-14, 6.6562e-09,
        5.3371e-13, 4.8517e-10, 1.3794e-13, 1.0364e-15], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,378][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Kimberly] are: tensor([6.6323e-02, 1.0428e-05, 1.7330e-02, 6.0693e-01, 2.1568e-05, 1.4677e-01,
        2.0411e-02, 1.4218e-01, 2.1103e-05, 7.1476e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,379][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Kimberly] are: tensor([1.4534e-01, 3.1104e-06, 4.9257e-01, 1.0337e-01, 1.5424e-05, 2.1051e-01,
        4.1375e-02, 5.5624e-03, 6.6407e-04, 5.9341e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,380][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Kimberly] are: tensor([0.0056, 0.0790, 0.1138, 0.1318, 0.0453, 0.1112, 0.3083, 0.0313, 0.1402,
        0.0336], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,382][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.5671, 0.0028, 0.0228, 0.1821, 0.0039, 0.0431, 0.0403, 0.0399, 0.0033,
        0.0078, 0.0868], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,383][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0066, 0.0115, 0.0300, 0.8188, 0.0041, 0.0030, 0.0044, 0.0207, 0.0047,
        0.0029, 0.0932], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,385][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0004, 0.0814, 0.1413, 0.1234, 0.0168, 0.1037, 0.1083, 0.1297, 0.1006,
        0.0387, 0.1557], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,386][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0246, 0.1897, 0.0509, 0.0527, 0.1285, 0.0619, 0.0864, 0.0869, 0.1130,
        0.1401, 0.0654], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,386][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([5.7154e-05, 1.9991e-02, 1.5971e-01, 3.1770e-02, 1.3795e-02, 1.8810e-01,
        4.5462e-02, 4.0862e-02, 1.4112e-01, 1.4594e-01, 2.1320e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,387][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0017, 0.0186, 0.0179, 0.1205, 0.1541, 0.0361, 0.0778, 0.0468, 0.0595,
        0.0151, 0.4520], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,387][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.5639, 0.0028, 0.0044, 0.3321, 0.0032, 0.0060, 0.0096, 0.0603, 0.0015,
        0.0017, 0.0147], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,387][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([1.5456e-03, 4.4638e-04, 8.4026e-02, 7.3952e-01, 3.0605e-04, 5.5987e-02,
        1.3306e-02, 5.8232e-02, 1.0894e-03, 2.7416e-04, 4.5272e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,388][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([7.6905e-01, 1.8533e-04, 5.9317e-04, 2.2948e-01, 1.0922e-04, 3.8297e-04,
        8.0663e-06, 1.3073e-04, 3.9328e-05, 1.3042e-05, 8.1924e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,388][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([6.2484e-03, 2.1834e-04, 5.9135e-03, 7.4988e-01, 8.3392e-05, 4.4193e-02,
        4.6624e-03, 1.6330e-01, 6.2648e-05, 2.6503e-05, 2.5406e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,389][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([2.9214e-02, 9.9804e-06, 1.0426e-01, 1.9218e-01, 6.1567e-05, 6.4646e-01,
        1.8627e-02, 6.0152e-03, 3.9724e-04, 1.0749e-03, 1.6973e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,389][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([7.7843e-01, 1.2257e-03, 5.6478e-03, 1.8179e-01, 5.2351e-04, 8.5734e-03,
        1.3124e-02, 6.1691e-03, 6.8137e-04, 5.3965e-04, 3.2898e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,390][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3315, 0.0037, 0.0495, 0.1477, 0.0034, 0.0842, 0.0491, 0.0772, 0.0057,
        0.0086, 0.1643, 0.0751], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,391][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0302, 0.0061, 0.0111, 0.8650, 0.0017, 0.0049, 0.0051, 0.0287, 0.0016,
        0.0012, 0.0295, 0.0149], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,393][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0012, 0.0506, 0.1119, 0.1263, 0.0113, 0.1165, 0.1332, 0.1542, 0.0768,
        0.0191, 0.1658, 0.0332], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,394][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0141, 0.1801, 0.0392, 0.0357, 0.1503, 0.0510, 0.0629, 0.0829, 0.1149,
        0.1536, 0.0586, 0.0566], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,395][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([1.5351e-04, 2.9347e-02, 1.7725e-01, 3.1125e-02, 1.9855e-02, 2.3797e-01,
        8.9804e-02, 2.3605e-02, 1.7720e-01, 7.1219e-02, 1.1759e-01, 2.4886e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,396][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([2.4291e-04, 5.4800e-03, 5.7127e-03, 3.0524e-02, 9.7298e-02, 1.4691e-02,
        4.0574e-02, 1.1860e-02, 3.4734e-02, 5.8282e-03, 4.4866e-01, 3.0439e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,397][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3619, 0.0068, 0.0070, 0.4349, 0.0057, 0.0129, 0.0200, 0.0827, 0.0041,
        0.0032, 0.0339, 0.0268], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,399][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0101, 0.0034, 0.0545, 0.6213, 0.0014, 0.0467, 0.0234, 0.1374, 0.0026,
        0.0009, 0.0417, 0.0566], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,400][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([8.2591e-01, 3.0508e-04, 2.1037e-03, 1.6744e-01, 3.2706e-04, 1.9011e-03,
        1.9569e-04, 6.8902e-04, 1.7281e-04, 9.6397e-05, 1.6929e-04, 6.9395e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,401][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.1301e-02, 1.5121e-03, 1.5769e-02, 5.5739e-01, 1.0897e-03, 8.4575e-02,
        3.2827e-02, 2.2023e-01, 1.6756e-03, 3.3192e-04, 3.8101e-02, 3.5198e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,401][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([8.6753e-02, 1.0438e-04, 9.9257e-02, 2.2700e-01, 3.0514e-04, 4.8187e-01,
        2.5788e-02, 2.5697e-02, 1.2017e-03, 2.1142e-03, 3.3030e-03, 4.6613e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,402][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([8.1418e-01, 1.5745e-03, 5.3167e-03, 1.3400e-01, 7.4723e-04, 7.7128e-03,
        1.0097e-02, 7.9893e-03, 7.1458e-04, 5.7030e-04, 4.5568e-03, 1.2544e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,404][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ kiss] are: tensor([0.4018, 0.0035, 0.0659, 0.0984, 0.0036, 0.0935, 0.0593, 0.0433, 0.0061,
        0.0085, 0.1297, 0.0618, 0.0246], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,405][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ kiss] are: tensor([0.0075, 0.0474, 0.0273, 0.7158, 0.0141, 0.0054, 0.0086, 0.0311, 0.0091,
        0.0100, 0.0716, 0.0154, 0.0366], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,407][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ kiss] are: tensor([0.0002, 0.0709, 0.1614, 0.0485, 0.0144, 0.0938, 0.1276, 0.0711, 0.1385,
        0.0533, 0.1715, 0.0228, 0.0260], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,408][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ kiss] are: tensor([0.0059, 0.2069, 0.0270, 0.0338, 0.1501, 0.0400, 0.0603, 0.0769, 0.0959,
        0.1392, 0.0569, 0.0589, 0.0482], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,409][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ kiss] are: tensor([5.9507e-05, 3.4672e-02, 1.2852e-01, 2.8877e-02, 2.6284e-02, 2.0269e-01,
        5.7544e-02, 2.6201e-02, 2.3546e-01, 1.4076e-01, 8.4041e-02, 1.2095e-02,
        2.2805e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,410][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ kiss] are: tensor([5.4111e-05, 2.7975e-02, 6.2096e-03, 1.0723e-02, 3.4131e-01, 1.2321e-02,
        5.1727e-02, 1.3003e-02, 8.5081e-02, 7.7355e-03, 2.2836e-01, 2.0715e-01,
        8.3445e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,412][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ kiss] are: tensor([0.2656, 0.0061, 0.0096, 0.4494, 0.0085, 0.0168, 0.0268, 0.0981, 0.0057,
        0.0048, 0.0385, 0.0307, 0.0395], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,413][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ kiss] are: tensor([0.0007, 0.0125, 0.1396, 0.3328, 0.0127, 0.0948, 0.0423, 0.0669, 0.0214,
        0.0132, 0.0898, 0.0391, 0.1343], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,413][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ kiss] are: tensor([4.2494e-01, 6.4604e-04, 1.5832e-02, 5.2626e-01, 4.7311e-04, 1.6970e-02,
        4.7996e-04, 8.9182e-03, 8.7813e-04, 2.8811e-04, 5.8860e-04, 5.2118e-04,
        3.2036e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,413][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ kiss] are: tensor([0.0056, 0.0019, 0.0488, 0.2888, 0.0013, 0.1621, 0.0279, 0.1623, 0.0018,
        0.0004, 0.0219, 0.0308, 0.2463], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,414][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ kiss] are: tensor([2.9190e-02, 2.7660e-05, 6.9565e-02, 2.5169e-01, 8.9966e-05, 5.9034e-01,
        2.0031e-02, 8.3069e-03, 6.5362e-04, 8.1272e-04, 1.7893e-03, 1.8356e-02,
        9.1436e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,414][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ kiss] are: tensor([0.7529, 0.0023, 0.0066, 0.1757, 0.0010, 0.0087, 0.0152, 0.0104, 0.0014,
        0.0011, 0.0062, 0.0157, 0.0029], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,415][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1577, 0.0007, 0.1166, 0.1320, 0.0006, 0.1849, 0.0752, 0.0412, 0.0020,
        0.0025, 0.1474, 0.0756, 0.0159, 0.0478], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,415][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([1.4439e-02, 1.8809e-03, 3.6258e-03, 9.4946e-01, 6.0853e-04, 1.4051e-03,
        1.7820e-03, 9.1540e-03, 4.8530e-04, 3.0821e-04, 8.6421e-03, 4.1793e-03,
        3.3658e-03, 6.5919e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,416][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0011, 0.0297, 0.0890, 0.1246, 0.0070, 0.0650, 0.0849, 0.0919, 0.0438,
        0.0278, 0.0939, 0.0237, 0.0197, 0.2977], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,417][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0086, 0.2039, 0.0239, 0.0315, 0.1394, 0.0358, 0.0467, 0.0718, 0.0920,
        0.1507, 0.0463, 0.0519, 0.0441, 0.0535], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,418][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.1563e-05, 7.0604e-03, 9.8371e-02, 2.0262e-02, 7.4613e-03, 1.2938e-01,
        3.7398e-02, 6.7391e-03, 6.4676e-02, 4.7758e-02, 3.3003e-02, 1.2030e-02,
        6.4653e-03, 5.2931e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,419][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.0474e-04, 8.2841e-03, 5.3580e-03, 1.1724e-02, 1.4723e-01, 1.2159e-02,
        4.8948e-02, 9.6604e-03, 3.5088e-02, 4.0784e-03, 3.1905e-01, 3.6420e-01,
        1.0847e-02, 2.3272e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,421][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.6245, 0.0009, 0.0018, 0.2957, 0.0012, 0.0031, 0.0059, 0.0349, 0.0007,
        0.0007, 0.0072, 0.0098, 0.0105, 0.0030], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,422][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([2.0086e-03, 3.5358e-04, 4.1677e-02, 7.4919e-01, 1.6460e-04, 3.4428e-02,
        1.4456e-02, 5.7296e-02, 4.5474e-04, 1.6430e-04, 1.4060e-02, 2.0541e-02,
        2.7890e-02, 3.7320e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,423][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([4.7715e-01, 2.1112e-04, 8.0230e-03, 5.0053e-01, 5.2365e-04, 8.7706e-03,
        3.0551e-04, 1.4681e-03, 5.2390e-04, 2.9007e-04, 1.9622e-04, 4.1118e-04,
        7.5954e-04, 8.3998e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,423][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([8.0107e-03, 1.2504e-04, 8.3826e-03, 5.5618e-01, 1.2191e-04, 5.4616e-02,
        1.2415e-02, 1.0237e-01, 1.2277e-04, 4.9889e-05, 4.6135e-03, 1.7815e-02,
        1.5079e-01, 8.4398e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,424][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([8.6839e-02, 2.1831e-06, 2.6527e-02, 2.6420e-01, 1.1255e-05, 5.3091e-01,
        3.0202e-03, 4.8660e-03, 3.9844e-05, 1.5319e-04, 1.3708e-04, 1.0861e-02,
        2.0247e-03, 7.0411e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,425][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([9.2018e-01, 7.0773e-05, 1.3520e-03, 6.5793e-02, 4.1192e-05, 2.0740e-03,
        2.6049e-03, 1.4925e-03, 6.7868e-05, 5.0212e-05, 7.8039e-04, 3.5436e-03,
        6.1565e-04, 1.3325e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,474][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:30:40,475][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,475][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,476][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,476][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,476][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,477][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,478][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,478][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,480][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,481][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,482][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,483][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [After] are: tensor([1.], device='cuda:0') for source tokens [After]
[2024-07-24 10:30:40,483][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.9988, 0.0012], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,485][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([0.1198, 0.8802], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,486][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([1.2492e-05, 9.9999e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,487][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([0.9929, 0.0071], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,488][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([5.3409e-04, 9.9947e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,489][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([9.9929e-01, 7.0501e-04], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,490][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([0.0149, 0.9851], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,492][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([0.8235, 0.1765], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,493][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 1.1745e-14], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,494][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([1.0000e+00, 1.0954e-06], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,495][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([9.9998e-01, 1.6268e-05], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,495][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([2.3121e-05, 9.9998e-01], device='cuda:0') for source tokens [After Kimberly]
[2024-07-24 10:30:40,497][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3267, 0.0096, 0.6637], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,498][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.4349, 0.1151, 0.4501], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,499][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([7.0241e-05, 8.0836e-03, 9.9185e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,500][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0936, 0.0044, 0.9020], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,500][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([4.5819e-04, 2.9736e-02, 9.6981e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,500][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0278, 0.0014, 0.9708], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,501][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0270, 0.1978, 0.7752], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,501][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0091, 0.0134, 0.9775], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,501][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.9131, 0.0024, 0.0846], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,502][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0468, 0.0098, 0.9434], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,502][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([2.9173e-01, 5.5354e-06, 7.0826e-01], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,502][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1159, 0.0537, 0.8304], device='cuda:0') for source tokens [After Kimberly and]
[2024-07-24 10:30:40,503][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Samantha] are: tensor([0.5533, 0.0115, 0.0102, 0.4250], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,504][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Samantha] are: tensor([0.1195, 0.0022, 0.0021, 0.8762], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,505][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Samantha] are: tensor([0.0014, 0.0170, 0.1364, 0.8451], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,506][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Samantha] are: tensor([0.4612, 0.0007, 0.0182, 0.5199], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,508][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Samantha] are: tensor([0.0052, 0.2207, 0.5463, 0.2278], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,509][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Samantha] are: tensor([0.0319, 0.0454, 0.0646, 0.8581], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,511][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Samantha] are: tensor([0.0014, 0.0050, 0.0294, 0.9641], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,512][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Samantha] are: tensor([0.1300, 0.0202, 0.0484, 0.8014], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,513][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Samantha] are: tensor([0.4636, 0.0366, 0.0634, 0.4363], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,515][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Samantha] are: tensor([0.0059, 0.1226, 0.0618, 0.8096], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,516][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Samantha] are: tensor([5.9717e-01, 4.7895e-04, 9.8838e-03, 3.9247e-01], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,517][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Samantha] are: tensor([0.6468, 0.0012, 0.0012, 0.3508], device='cuda:0') for source tokens [After Kimberly and Samantha]
[2024-07-24 10:30:40,519][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ went] are: tensor([0.9436, 0.0014, 0.0303, 0.0216, 0.0030], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,520][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ went] are: tensor([0.0030, 0.1707, 0.5044, 0.1686, 0.1533], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,521][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ went] are: tensor([2.1082e-06, 4.1395e-01, 5.7676e-02, 7.8531e-03, 5.2052e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,522][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ went] are: tensor([3.9449e-03, 1.3602e-04, 9.8511e-01, 1.0526e-02, 2.8697e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,523][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ went] are: tensor([7.2383e-05, 2.0062e-01, 5.2444e-01, 3.3336e-02, 2.4153e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,524][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ went] are: tensor([3.1672e-02, 1.0369e-04, 2.0692e-01, 7.6064e-01, 6.6421e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,525][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ went] are: tensor([0.0025, 0.3801, 0.0173, 0.2539, 0.3461], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,526][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ went] are: tensor([0.0010, 0.0017, 0.8462, 0.1441, 0.0071], device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,526][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ went] are: tensor([9.9947e-01, 5.3717e-14, 5.6333e-08, 5.3418e-04, 2.1794e-14],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,527][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ went] are: tensor([5.1640e-01, 4.5341e-06, 1.0279e-02, 4.7330e-01, 1.2373e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,527][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ went] are: tensor([4.4620e-01, 3.3047e-06, 3.9968e-01, 1.5410e-01, 1.4278e-05],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,527][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ went] are: tensor([5.6285e-06, 1.6380e-01, 7.6443e-01, 3.2208e-02, 3.9557e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went]
[2024-07-24 10:30:40,528][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1691, 0.0010, 0.2580, 0.1269, 0.0012, 0.4438], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,528][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0045, 0.0036, 0.0136, 0.9667, 0.0013, 0.0104], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,528][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([8.1716e-06, 4.1120e-03, 3.8870e-01, 1.2388e-01, 1.8898e-03, 4.8141e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,529][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0054, 0.0008, 0.1838, 0.4509, 0.0007, 0.3583], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,530][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([5.7681e-05, 1.9223e-02, 4.5645e-01, 1.7163e-02, 2.9581e-02, 4.7752e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,531][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.3788e-03, 4.4717e-04, 6.4049e-02, 4.3470e-01, 5.3943e-04, 4.9789e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,532][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([2.5525e-04, 3.9171e-03, 2.0391e-02, 9.3565e-01, 2.1872e-03, 3.7604e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,533][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0005, 0.0011, 0.2197, 0.5073, 0.0006, 0.2708], device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,534][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([8.2608e-01, 1.1958e-05, 1.9722e-03, 1.7081e-01, 2.0438e-05, 1.1002e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,535][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.9439e-03, 6.4072e-04, 3.1705e-02, 7.2999e-01, 6.3554e-04, 2.3109e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,536][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([5.4896e-02, 2.2126e-06, 6.1499e-02, 9.2226e-02, 2.7409e-05, 7.9135e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,537][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([5.9501e-03, 3.1718e-04, 1.7912e-02, 8.1678e-01, 1.5763e-04, 1.5888e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to]
[2024-07-24 10:30:40,538][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3755, 0.0006, 0.1807, 0.0961, 0.0007, 0.2287, 0.1178],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,540][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0084, 0.0080, 0.0410, 0.8711, 0.0056, 0.0204, 0.0455],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,541][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([5.2084e-05, 1.5397e-02, 3.1392e-01, 1.6347e-01, 9.8398e-03, 2.9140e-01,
        2.0592e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,542][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.1199e-03, 1.2614e-04, 2.4240e-01, 1.3824e-01, 1.3607e-04, 5.3183e-01,
        8.6151e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,542][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([4.9638e-05, 1.2995e-02, 3.3790e-01, 2.2574e-02, 1.9746e-02, 4.5455e-01,
        1.5218e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,543][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([4.9795e-03, 2.5754e-04, 3.4682e-02, 6.4557e-01, 4.2228e-04, 2.3553e-01,
        7.8558e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,545][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0012, 0.0199, 0.0176, 0.9235, 0.0073, 0.0234, 0.0071],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,546][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0008, 0.0021, 0.1686, 0.4694, 0.0018, 0.2220, 0.1353],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,547][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([9.7165e-01, 9.0115e-08, 6.1629e-05, 2.8229e-02, 1.3710e-07, 6.1381e-05,
        2.6801e-07], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,548][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([3.0666e-02, 2.0849e-04, 2.1475e-02, 7.1516e-01, 2.7956e-04, 1.5230e-01,
        7.9902e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,549][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([2.4681e-02, 2.5729e-07, 7.6930e-02, 4.4300e-02, 3.4923e-06, 8.3813e-01,
        1.5955e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,550][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([1.2581e-04, 5.0832e-04, 1.1421e-01, 2.4988e-01, 3.4245e-04, 5.7232e-01,
        6.2618e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the]
[2024-07-24 10:30:40,551][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ office] are: tensor([0.1660, 0.0038, 0.1572, 0.2405, 0.0043, 0.3013, 0.0828, 0.0440],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,552][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ office] are: tensor([0.0142, 0.0083, 0.0099, 0.9447, 0.0019, 0.0035, 0.0029, 0.0147],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,552][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ office] are: tensor([1.8122e-05, 8.9125e-03, 2.0178e-01, 3.8981e-01, 1.9878e-03, 2.4299e-01,
        8.0266e-02, 7.4234e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,553][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ office] are: tensor([6.1987e-03, 3.8087e-04, 6.0295e-02, 6.2946e-01, 1.7373e-04, 1.9411e-01,
        9.3040e-02, 1.6339e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,553][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ office] are: tensor([2.8243e-04, 2.3316e-02, 2.9394e-01, 5.2839e-02, 2.4324e-02, 4.5246e-01,
        1.2219e-01, 3.0643e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,553][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ office] are: tensor([6.3229e-03, 1.4509e-04, 1.9148e-02, 7.3352e-01, 1.7643e-04, 1.4231e-01,
        1.6113e-02, 8.2260e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,554][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ office] are: tensor([3.5147e-04, 3.3046e-03, 1.9875e-02, 8.6944e-01, 1.2632e-03, 3.1665e-02,
        2.7039e-03, 7.1400e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,554][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ office] are: tensor([0.0070, 0.0023, 0.0382, 0.8211, 0.0018, 0.0433, 0.0167, 0.0696],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,555][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ office] are: tensor([0.2336, 0.0099, 0.0477, 0.6699, 0.0081, 0.0204, 0.0015, 0.0088],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,555][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ office] are: tensor([0.0035, 0.0025, 0.0416, 0.5795, 0.0018, 0.1774, 0.0344, 0.1593],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,556][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ office] are: tensor([8.5207e-02, 7.7714e-06, 8.5872e-02, 2.3810e-01, 6.1269e-05, 5.7117e-01,
        1.4479e-02, 5.1024e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,557][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ office] are: tensor([9.6409e-03, 4.4357e-04, 7.4188e-03, 9.1023e-01, 1.2226e-04, 5.8731e-02,
        1.1793e-02, 1.6168e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office]
[2024-07-24 10:30:40,558][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.4083, 0.0015, 0.2475, 0.0587, 0.0024, 0.1389, 0.0967, 0.0406, 0.0054],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,560][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0019, 0.0429, 0.2202, 0.3202, 0.0198, 0.0101, 0.0606, 0.1395, 0.1848],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,561][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([1.2958e-07, 4.6226e-02, 3.6171e-02, 3.1508e-03, 3.4012e-02, 2.5425e-02,
        3.1464e-02, 1.0637e-02, 8.1291e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,561][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([1.9522e-03, 1.3080e-05, 7.7285e-01, 3.0386e-02, 2.8208e-05, 5.6544e-02,
        8.0484e-02, 5.7598e-02, 1.4073e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,562][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([1.3605e-05, 4.5090e-03, 2.4023e-01, 1.0276e-02, 8.6095e-03, 1.9081e-01,
        7.0535e-02, 3.1882e-02, 4.4313e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,563][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([4.7686e-03, 1.5567e-05, 4.3471e-02, 3.9616e-01, 4.4854e-05, 4.1950e-01,
        6.6802e-02, 6.8645e-02, 5.9350e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,565][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0009, 0.1301, 0.0263, 0.5264, 0.0977, 0.0166, 0.0112, 0.1158, 0.0749],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,566][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([4.3907e-04, 3.1501e-04, 3.2967e-01, 1.8073e-01, 6.2753e-04, 3.2862e-01,
        1.2343e-01, 2.7829e-02, 8.3325e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,566][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([9.9910e-01, 2.3842e-12, 5.3508e-07, 8.9933e-04, 1.9991e-12, 1.7510e-07,
        5.0192e-11, 2.6511e-08, 2.1149e-11], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,568][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([7.1235e-02, 6.2413e-06, 1.9669e-02, 6.3684e-01, 1.2101e-05, 1.7191e-01,
        2.7365e-02, 7.2928e-02, 3.9208e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,568][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([3.9825e-01, 5.0607e-07, 1.9377e-01, 1.7900e-01, 2.7334e-06, 1.9780e-01,
        1.6597e-02, 1.4424e-02, 1.4721e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,569][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([1.3411e-06, 7.8405e-03, 4.7767e-01, 2.4955e-02, 2.2193e-03, 1.9376e-01,
        2.1065e-01, 3.9650e-03, 7.8939e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office,]
[2024-07-24 10:30:40,571][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Kimberly] are: tensor([0.8165, 0.0016, 0.0493, 0.0298, 0.0035, 0.0263, 0.0447, 0.0209, 0.0033,
        0.0040], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,572][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Kimberly] are: tensor([3.4420e-04, 7.4108e-02, 1.5654e-01, 4.0615e-02, 7.3845e-02, 2.5606e-03,
        3.2355e-02, 1.9061e-02, 5.0535e-01, 9.5222e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,573][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Kimberly] are: tensor([1.9850e-08, 7.2261e-02, 2.5304e-03, 2.0017e-04, 7.1186e-02, 1.6132e-03,
        3.3980e-03, 2.6103e-04, 5.9654e-01, 2.5201e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,574][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Kimberly] are: tensor([4.0787e-03, 4.8480e-04, 6.6242e-01, 2.7815e-02, 8.5075e-04, 4.8896e-02,
        1.9208e-01, 5.9239e-02, 3.9015e-03, 2.2791e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,574][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Kimberly] are: tensor([5.9472e-06, 3.4257e-02, 7.6441e-02, 8.3719e-03, 3.8600e-02, 8.2688e-02,
        2.7241e-02, 3.2401e-02, 4.8458e-01, 2.1542e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,575][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Kimberly] are: tensor([7.6026e-03, 6.2011e-05, 4.6418e-02, 3.7415e-01, 2.5302e-04, 4.0380e-01,
        9.9118e-02, 6.6854e-02, 1.6898e-03, 5.3048e-05], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,576][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Kimberly] are: tensor([2.4566e-04, 3.0248e-01, 3.5222e-03, 4.7061e-02, 2.8020e-01, 1.9390e-03,
        1.9768e-03, 1.8317e-02, 1.0370e-01, 2.4057e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,577][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Kimberly] are: tensor([1.2982e-04, 1.0273e-03, 3.5237e-01, 7.4968e-02, 4.7386e-03, 3.8351e-01,
        1.3947e-01, 7.1413e-03, 3.3720e-02, 2.9237e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,578][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Kimberly] are: tensor([9.9982e-01, 2.5997e-14, 1.0415e-08, 1.8037e-04, 1.1774e-14, 6.6562e-09,
        5.3371e-13, 4.8517e-10, 1.3794e-13, 1.0364e-15], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,578][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Kimberly] are: tensor([1.7340e-01, 2.4184e-05, 1.7401e-02, 5.8366e-01, 4.3363e-05, 1.4517e-01,
        2.4910e-02, 5.5333e-02, 4.8140e-05, 9.0683e-06], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,579][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Kimberly] are: tensor([1.4534e-01, 3.1104e-06, 4.9257e-01, 1.0337e-01, 1.5424e-05, 2.1051e-01,
        4.1375e-02, 5.5624e-03, 6.6407e-04, 5.9341e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,579][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Kimberly] are: tensor([2.1485e-09, 2.1048e-02, 1.0404e-01, 6.5317e-04, 7.3951e-03, 3.0984e-02,
        1.8285e-01, 5.4241e-04, 6.4753e-01, 4.9605e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly]
[2024-07-24 10:30:40,579][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.5671, 0.0028, 0.0228, 0.1821, 0.0039, 0.0431, 0.0403, 0.0399, 0.0033,
        0.0078, 0.0868], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,580][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0066, 0.0115, 0.0300, 0.8188, 0.0041, 0.0030, 0.0044, 0.0207, 0.0047,
        0.0029, 0.0932], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,580][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([8.8554e-06, 2.8677e-02, 2.9020e-01, 1.5419e-01, 7.3287e-03, 1.7931e-01,
        7.3182e-02, 1.0542e-01, 8.7775e-02, 1.5236e-02, 5.8668e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,581][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([3.5008e-03, 5.7441e-04, 7.2252e-02, 6.9777e-01, 2.8800e-04, 1.2338e-01,
        7.9050e-02, 1.2640e-02, 1.2331e-03, 4.4555e-04, 8.8652e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,582][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([5.7154e-05, 1.9991e-02, 1.5971e-01, 3.1770e-02, 1.3795e-02, 1.8810e-01,
        4.5462e-02, 4.0862e-02, 1.4112e-01, 1.4594e-01, 2.1320e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,583][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([8.8816e-03, 3.4795e-04, 2.0851e-02, 6.8257e-01, 3.0320e-04, 1.3747e-01,
        2.3310e-02, 1.1188e-01, 1.1167e-03, 1.2521e-04, 1.3148e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,583][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([6.0676e-04, 1.8611e-02, 1.6980e-02, 6.2922e-01, 6.5624e-03, 1.7307e-02,
        2.4225e-03, 1.1650e-01, 9.9351e-04, 4.8010e-03, 1.8601e-01],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,584][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([1.5456e-03, 4.4638e-04, 8.4026e-02, 7.3952e-01, 3.0605e-04, 5.5987e-02,
        1.3306e-02, 5.8232e-02, 1.0894e-03, 2.7416e-04, 4.5272e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,585][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([7.6905e-01, 1.8533e-04, 5.9317e-04, 2.2948e-01, 1.0922e-04, 3.8297e-04,
        8.0663e-06, 1.3073e-04, 3.9328e-05, 1.3042e-05, 8.1924e-06],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,586][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([1.6998e-02, 3.0181e-04, 1.1756e-02, 7.7109e-01, 1.1950e-04, 6.9947e-02,
        7.9838e-03, 9.5999e-02, 1.5001e-04, 2.9358e-05, 2.5623e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,587][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([2.9214e-02, 9.9804e-06, 1.0426e-01, 1.9218e-01, 6.1567e-05, 6.4646e-01,
        1.8627e-02, 6.0152e-03, 3.9724e-04, 1.0749e-03, 1.6973e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,588][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([6.5948e-04, 4.3828e-03, 2.5840e-02, 7.8919e-01, 1.0445e-03, 1.2330e-01,
        4.4799e-02, 3.1420e-03, 1.8654e-03, 4.2968e-04, 5.3472e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave]
[2024-07-24 10:30:40,589][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.3315, 0.0037, 0.0495, 0.1477, 0.0034, 0.0842, 0.0491, 0.0772, 0.0057,
        0.0086, 0.1643, 0.0751], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,591][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0302, 0.0061, 0.0111, 0.8650, 0.0017, 0.0049, 0.0051, 0.0287, 0.0016,
        0.0012, 0.0295, 0.0149], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,592][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([1.5204e-04, 2.1643e-02, 1.9792e-01, 2.4932e-01, 4.8962e-03, 1.6712e-01,
        1.0391e-01, 1.5144e-01, 3.8152e-02, 3.9965e-03, 5.2697e-02, 8.7594e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,593][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0100, 0.0013, 0.1676, 0.3416, 0.0008, 0.2493, 0.0949, 0.0609, 0.0029,
        0.0013, 0.0432, 0.0263], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,594][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([1.5351e-04, 2.9347e-02, 1.7725e-01, 3.1125e-02, 1.9855e-02, 2.3797e-01,
        8.9804e-02, 2.3605e-02, 1.7720e-01, 7.1219e-02, 1.1759e-01, 2.4886e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,596][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0186, 0.0022, 0.0453, 0.4849, 0.0022, 0.1734, 0.0463, 0.1861, 0.0058,
        0.0006, 0.0174, 0.0173], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,597][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0010, 0.0160, 0.0168, 0.6533, 0.0045, 0.0388, 0.0081, 0.0971, 0.0016,
        0.0017, 0.1494, 0.0118], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,599][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0101, 0.0034, 0.0545, 0.6213, 0.0014, 0.0467, 0.0234, 0.1374, 0.0026,
        0.0009, 0.0417, 0.0566], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,600][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([8.2591e-01, 3.0508e-04, 2.1037e-03, 1.6744e-01, 3.2706e-04, 1.9011e-03,
        1.9569e-04, 6.8902e-04, 1.7281e-04, 9.6397e-05, 1.6929e-04, 6.9395e-04],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,601][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([1.7310e-02, 1.6484e-03, 2.8384e-02, 5.2591e-01, 1.3063e-03, 1.3582e-01,
        4.5255e-02, 1.5145e-01, 2.8813e-03, 3.3204e-04, 4.4007e-02, 4.5694e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,602][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([8.6753e-02, 1.0438e-04, 9.9257e-02, 2.2700e-01, 3.0514e-04, 4.8187e-01,
        2.5788e-02, 2.5697e-02, 1.2017e-03, 2.1142e-03, 3.3030e-03, 4.6613e-02],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,603][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.6629e-02, 4.5381e-03, 1.8251e-02, 8.5299e-01, 1.2307e-03, 6.4149e-02,
        1.7971e-02, 7.3151e-03, 7.4523e-04, 2.7000e-04, 8.4275e-03, 7.4825e-03],
       device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a]
[2024-07-24 10:30:40,604][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ kiss] are: tensor([0.4018, 0.0035, 0.0659, 0.0984, 0.0036, 0.0935, 0.0593, 0.0433, 0.0061,
        0.0085, 0.1297, 0.0618, 0.0246], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,604][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ kiss] are: tensor([0.0075, 0.0474, 0.0273, 0.7158, 0.0141, 0.0054, 0.0086, 0.0311, 0.0091,
        0.0100, 0.0716, 0.0154, 0.0366], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,604][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ kiss] are: tensor([4.4626e-06, 1.6416e-02, 4.2778e-01, 5.2391e-02, 4.4773e-03, 1.5618e-01,
        1.0727e-01, 5.4131e-02, 1.0873e-01, 1.6016e-02, 4.3837e-02, 3.5971e-03,
        9.1576e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,605][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ kiss] are: tensor([0.0011, 0.0029, 0.1709, 0.3309, 0.0010, 0.3015, 0.1433, 0.0154, 0.0058,
        0.0015, 0.0146, 0.0100, 0.0010], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,605][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ kiss] are: tensor([5.9507e-05, 3.4672e-02, 1.2852e-01, 2.8877e-02, 2.6284e-02, 2.0269e-01,
        5.7544e-02, 2.6201e-02, 2.3546e-01, 1.4076e-01, 8.4041e-02, 1.2095e-02,
        2.2805e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,606][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ kiss] are: tensor([0.0027, 0.0013, 0.0754, 0.3321, 0.0016, 0.1906, 0.0862, 0.2087, 0.0116,
        0.0009, 0.0289, 0.0130, 0.0471], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,606][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ kiss] are: tensor([3.9798e-04, 3.3056e-02, 8.7614e-02, 4.1652e-01, 1.3804e-02, 8.9020e-02,
        1.0019e-02, 9.1161e-02, 3.1639e-03, 7.6607e-03, 1.9632e-01, 1.0401e-02,
        4.0859e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,606][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ kiss] are: tensor([0.0007, 0.0125, 0.1396, 0.3328, 0.0127, 0.0948, 0.0423, 0.0669, 0.0214,
        0.0132, 0.0898, 0.0391, 0.1343], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,607][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ kiss] are: tensor([4.2494e-01, 6.4604e-04, 1.5832e-02, 5.2626e-01, 4.7311e-04, 1.6970e-02,
        4.7996e-04, 8.9182e-03, 8.7813e-04, 2.8811e-04, 5.8860e-04, 5.2118e-04,
        3.2036e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,608][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ kiss] are: tensor([5.4737e-03, 1.8877e-03, 6.7208e-02, 3.2646e-01, 1.2833e-03, 1.9345e-01,
        3.4224e-02, 1.2233e-01, 2.9217e-03, 3.1822e-04, 2.4355e-02, 3.3719e-02,
        1.8636e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,609][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ kiss] are: tensor([2.9190e-02, 2.7660e-05, 6.9565e-02, 2.5169e-01, 8.9966e-05, 5.9034e-01,
        2.0031e-02, 8.3069e-03, 6.5362e-04, 8.1272e-04, 1.7893e-03, 1.8356e-02,
        9.1436e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,610][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ kiss] are: tensor([4.0988e-03, 1.0305e-02, 1.1403e-02, 9.0574e-01, 1.9359e-03, 3.6046e-02,
        1.4013e-02, 4.4972e-03, 1.5008e-03, 5.3802e-04, 6.2450e-03, 3.4610e-03,
        2.1862e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss]
[2024-07-24 10:30:40,611][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1577, 0.0007, 0.1166, 0.1320, 0.0006, 0.1849, 0.0752, 0.0412, 0.0020,
        0.0025, 0.1474, 0.0756, 0.0159, 0.0478], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,612][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([1.4439e-02, 1.8809e-03, 3.6258e-03, 9.4946e-01, 6.0853e-04, 1.4051e-03,
        1.7820e-03, 9.1540e-03, 4.8530e-04, 3.0821e-04, 8.6421e-03, 4.1793e-03,
        3.3658e-03, 6.5919e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,613][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.5738e-05, 4.7720e-03, 1.8725e-01, 1.8015e-01, 1.4158e-03, 1.3829e-01,
        6.2521e-02, 5.8773e-02, 1.6790e-02, 4.4758e-03, 2.0564e-02, 3.6013e-03,
        5.6664e-03, 3.1571e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,614][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([2.8983e-03, 1.8143e-04, 4.3275e-02, 7.5343e-01, 1.2609e-04, 1.3186e-01,
        2.9816e-02, 6.9209e-03, 2.5485e-04, 2.0780e-04, 5.6607e-03, 6.3001e-03,
        6.2235e-04, 1.8437e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,615][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.1563e-05, 7.0604e-03, 9.8371e-02, 2.0262e-02, 7.4613e-03, 1.2938e-01,
        3.7398e-02, 6.7391e-03, 6.4676e-02, 4.7758e-02, 3.3003e-02, 1.2030e-02,
        6.4653e-03, 5.2931e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,616][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([9.6517e-03, 1.5795e-04, 2.9751e-02, 5.1433e-01, 1.3580e-04, 1.6805e-01,
        3.9770e-02, 7.9456e-02, 8.2087e-04, 7.7229e-05, 4.1572e-03, 9.7208e-03,
        2.1743e-02, 1.2218e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,617][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([5.7841e-04, 2.9660e-03, 7.1456e-03, 9.0880e-01, 1.0053e-03, 1.3609e-02,
        1.4071e-03, 2.4216e-02, 1.8699e-04, 4.5360e-04, 2.2271e-02, 3.3486e-03,
        1.1039e-02, 2.9724e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,618][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.0086e-03, 3.5358e-04, 4.1677e-02, 7.4919e-01, 1.6460e-04, 3.4428e-02,
        1.4456e-02, 5.7296e-02, 4.5474e-04, 1.6430e-04, 1.4060e-02, 2.0541e-02,
        2.7890e-02, 3.7320e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,619][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([4.7715e-01, 2.1112e-04, 8.0230e-03, 5.0053e-01, 5.2365e-04, 8.7706e-03,
        3.0551e-04, 1.4681e-03, 5.2390e-04, 2.9007e-04, 1.9622e-04, 4.1118e-04,
        7.5954e-04, 8.3998e-04], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,620][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.7308e-02, 1.7026e-04, 1.2633e-02, 5.7921e-01, 1.7217e-04, 7.2669e-02,
        1.6179e-02, 5.9810e-02, 2.7591e-04, 4.9127e-05, 5.6622e-03, 2.4321e-02,
        8.8461e-02, 1.2308e-01], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,621][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([8.6839e-02, 2.1831e-06, 2.6527e-02, 2.6420e-01, 1.1255e-05, 5.3091e-01,
        3.0202e-03, 4.8660e-03, 3.9844e-05, 1.5319e-04, 1.3708e-04, 1.0861e-02,
        2.0247e-03, 7.0411e-02], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,622][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([9.2775e-03, 6.0329e-05, 3.1177e-03, 9.5135e-01, 2.4464e-05, 2.8288e-02,
        3.0648e-03, 5.1605e-04, 2.0175e-05, 5.9621e-06, 8.5429e-04, 1.2615e-03,
        7.9673e-05, 2.0798e-03], device='cuda:0') for source tokens [After Kimberly and Samantha went to the office, Kimberly gave a kiss to]
[2024-07-24 10:30:40,623][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:30:40,625][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[4167],
        [  13],
        [  31],
        [   1],
        [  91],
        [   6],
        [ 411],
        [  97],
        [   4],
        [   3],
        [   6],
        [   1],
        [   4],
        [   1]], device='cuda:0')
[2024-07-24 10:30:40,626][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[4922],
        [  18],
        [  29],
        [   1],
        [ 100],
        [  12],
        [ 213],
        [ 170],
        [   3],
        [   2],
        [   9],
        [   4],
        [   6],
        [   2]], device='cuda:0')
[2024-07-24 10:30:40,628][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[27182],
        [27015],
        [27088],
        [ 1721],
        [23965],
        [19050],
        [17980],
        [13234],
        [19789],
        [19819],
        [ 8852],
        [11001],
        [12846],
        [13638]], device='cuda:0')
[2024-07-24 10:30:40,629][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[  885],
        [36500],
        [47840],
        [50183],
        [48956],
        [50182],
        [50171],
        [50179],
        [49029],
        [23097],
        [50164],
        [50173],
        [50135],
        [50181]], device='cuda:0')
[2024-07-24 10:30:40,631][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 3966],
        [32341],
        [38494],
        [50036],
        [34134],
        [44497],
        [40645],
        [46676],
        [23671],
        [23031],
        [41066],
        [41128],
        [34399],
        [39514]], device='cuda:0')
[2024-07-24 10:30:40,631][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[11679],
        [13144],
        [21574],
        [12568],
        [ 1939],
        [14018],
        [11853],
        [ 6547],
        [ 5244],
        [ 3624],
        [ 6797],
        [ 7449],
        [ 6997],
        [ 6820]], device='cuda:0')
[2024-07-24 10:30:40,632][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[42176],
        [10359],
        [11464],
        [11081],
        [10274],
        [ 9288],
        [ 8425],
        [ 8414],
        [ 4957],
        [ 3547],
        [ 6148],
        [ 6146],
        [ 5136],
        [ 6587]], device='cuda:0')
[2024-07-24 10:30:40,633][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[34663],
        [34798],
        [43631],
        [40989],
        [ 8926],
        [43718],
        [41913],
        [28889],
        [19392],
        [23110],
        [34443],
        [30657],
        [39207],
        [31148]], device='cuda:0')
[2024-07-24 10:30:40,634][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[21243],
        [11342],
        [19834],
        [48600],
        [47403],
        [48338],
        [47667],
        [46433],
        [45548],
        [38231],
        [45811],
        [45242],
        [44125],
        [46287]], device='cuda:0')
[2024-07-24 10:30:40,636][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[48764],
        [48550],
        [10305],
        [15212],
        [10281],
        [13333],
        [14124],
        [13578],
        [13938],
        [14831],
        [13049],
        [14018],
        [17993],
        [14697]], device='cuda:0')
[2024-07-24 10:30:40,637][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[34009],
        [34009],
        [34053],
        [33467],
        [34009],
        [33908],
        [34007],
        [32829],
        [34008],
        [34009],
        [33871],
        [33915],
        [33503],
        [33548]], device='cuda:0')
[2024-07-24 10:30:40,639][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[45738],
        [45738],
        [30740],
        [44699],
        [47032],
        [43127],
        [43481],
        [42115],
        [43213],
        [42859],
        [45139],
        [41645],
        [28182],
        [37642]], device='cuda:0')
[2024-07-24 10:30:40,640][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[29950],
        [29947],
        [ 8590],
        [    3],
        [ 1591],
        [11341],
        [14453],
        [ 3782],
        [ 1955],
        [ 5933],
        [ 5877],
        [ 3692],
        [ 3746],
        [ 3308]], device='cuda:0')
[2024-07-24 10:30:40,641][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[16602],
        [45273],
        [22505],
        [21488],
        [47088],
        [35316],
        [46125],
        [35063],
        [46801],
        [46540],
        [42402],
        [40955],
        [43342],
        [31179]], device='cuda:0')
[2024-07-24 10:30:40,642][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[23484],
        [29546],
        [16701],
        [17609],
        [24806],
        [16309],
        [18931],
        [22469],
        [24962],
        [24263],
        [19970],
        [17361],
        [18841],
        [18372]], device='cuda:0')
[2024-07-24 10:30:40,644][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[22117],
        [22164],
        [30474],
        [38653],
        [24385],
        [31428],
        [31699],
        [32966],
        [31257],
        [28129],
        [31969],
        [32194],
        [31776],
        [32472]], device='cuda:0')
[2024-07-24 10:30:40,645][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[31503],
        [37937],
        [31743],
        [39005],
        [35224],
        [39314],
        [38877],
        [39490],
        [37414],
        [38820],
        [40000],
        [39744],
        [40493],
        [39521]], device='cuda:0')
[2024-07-24 10:30:40,647][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[41984],
        [36845],
        [42951],
        [23859],
        [40536],
        [41226],
        [40696],
        [35522],
        [42898],
        [44120],
        [40795],
        [38082],
        [42577],
        [39327]], device='cuda:0')
[2024-07-24 10:30:40,648][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[39692],
        [38005],
        [15529],
        [11236],
        [15426],
        [10219],
        [11171],
        [10240],
        [14313],
        [14109],
        [10488],
        [11392],
        [11044],
        [10444]], device='cuda:0')
[2024-07-24 10:30:40,649][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[23368],
        [19072],
        [10836],
        [10805],
        [12638],
        [13344],
        [13535],
        [13919],
        [12851],
        [17570],
        [11124],
        [11524],
        [12588],
        [18568]], device='cuda:0')
[2024-07-24 10:30:40,651][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[31199],
        [31056],
        [ 8688],
        [11951],
        [11042],
        [14091],
        [12485],
        [12505],
        [12845],
        [12435],
        [12027],
        [10900],
        [ 9085],
        [11696]], device='cuda:0')
[2024-07-24 10:30:40,652][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[33387],
        [45498],
        [42492],
        [23527],
        [46897],
        [23752],
        [24075],
        [23899],
        [34685],
        [47487],
        [26680],
        [26571],
        [32570],
        [23143]], device='cuda:0')
[2024-07-24 10:30:40,653][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[40824],
        [34441],
        [41328],
        [31752],
        [41045],
        [35786],
        [34762],
        [32595],
        [37220],
        [37055],
        [34034],
        [34057],
        [34218],
        [32852]], device='cuda:0')
[2024-07-24 10:30:40,655][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25858],
        [25858],
        [25708],
        [24774],
        [25858],
        [25706],
        [25835],
        [23956],
        [25857],
        [25858],
        [25641],
        [25701],
        [24975],
        [25164]], device='cuda:0')
[2024-07-24 10:30:40,656][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[14373],
        [14373],
        [31294],
        [32017],
        [28439],
        [32236],
        [32496],
        [33246],
        [32430],
        [32208],
        [30802],
        [34861],
        [33039],
        [31816]], device='cuda:0')
[2024-07-24 10:30:40,658][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[21236],
        [21238],
        [36743],
        [49179],
        [41597],
        [45391],
        [44907],
        [46678],
        [45865],
        [42244],
        [46151],
        [46955],
        [47022],
        [47459]], device='cuda:0')
[2024-07-24 10:30:40,658][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[26564],
        [21646],
        [11360],
        [13460],
        [11595],
        [12020],
        [11398],
        [12378],
        [11590],
        [14381],
        [12068],
        [12343],
        [12490],
        [12540]], device='cuda:0')
[2024-07-24 10:30:40,659][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[19926],
        [22739],
        [22689],
        [28624],
        [21805],
        [25677],
        [26264],
        [27375],
        [21001],
        [17865],
        [25791],
        [25875],
        [22995],
        [26553]], device='cuda:0')
[2024-07-24 10:30:40,660][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[29670],
        [27962],
        [38271],
        [37314],
        [32368],
        [38292],
        [36756],
        [32720],
        [31922],
        [32410],
        [35609],
        [37632],
        [36426],
        [36979]], device='cuda:0')
[2024-07-24 10:30:40,661][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579],
        [22579]], device='cuda:0')
