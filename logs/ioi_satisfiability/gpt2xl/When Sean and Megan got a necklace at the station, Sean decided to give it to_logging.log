[2024-07-24 10:17:34,840][explain_satisfiability.py][line:287][INFO] ############ CASE TEXT isWhen Sean and Megan got a necklace at the station, Sean decided to give it to
[2024-07-24 10:17:34,840][explain_satisfiability.py][line:288][INFO] ############ CASE Prediction is  Megan
[2024-07-24 10:17:34,840][explain_satisfiability.py][line:289][INFO] ############ Refined Forward Graph
[2024-07-24 10:17:34,840][explain_satisfiability.py][line:290][INFO] ****** Layer 1
[2024-07-24 10:17:34,840][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 0
[2024-07-24 10:17:34,840][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 1
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 2
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 3
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 4
[2024-07-24 10:17:34,841][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 5
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 6
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 7
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,842][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 8
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 9
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 10
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 11
[2024-07-24 10:17:34,843][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 12
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 13
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 14
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 15
[2024-07-24 10:17:34,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 16
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 17
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit26']
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 18
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,845][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 19
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 20
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 21
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 22
[2024-07-24 10:17:34,846][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 23
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit26']
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 24
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 25
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 26
[2024-07-24 10:17:34,847][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 27
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 28
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 0
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,848][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 1
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit6', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 2
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit5', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,849][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 3
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit21', 'circuit22']
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 4
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 5
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit24']
[2024-07-24 10:17:34,850][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 6
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit25']
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 7
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,851][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 8
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 9
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,852][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 10
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 11
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,853][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 12
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit6', 'circuit11', 'circuit13', 'circuit16', 'circuit19', 'circuit24']
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 13
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,854][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 14
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 15
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 16
[2024-07-24 10:17:34,855][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 17
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 18
[2024-07-24 10:17:34,856][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit17', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 19
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 20
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,857][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 21
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit6', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 22
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,858][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 23
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 24
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,859][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit21', 'circuit24']
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 25
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit27']
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 26
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,860][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 27
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 28
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,861][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 0
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 1
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,862][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 2
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit6', 'circuit9', 'circuit18', 'circuit20', 'circuit26']
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit26']
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 3
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,863][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 4
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit15', 'circuit20']
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit17', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 5
[2024-07-24 10:17:34,864][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 6
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 7
[2024-07-24 10:17:34,865][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 8
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,866][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 9
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit20', 'circuit22']
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 10
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14']
[2024-07-24 10:17:34,867][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 11
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit6', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,868][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 12
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 13
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,869][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 14
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit3', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 15
[2024-07-24 10:17:34,870][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit22']
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 16
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit21']
[2024-07-24 10:17:34,871][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit18']
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 17
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 18
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,872][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit18']
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 19
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20']
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19']
[2024-07-24 10:17:34,873][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 20
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19']
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 21
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:17:34,874][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 22
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 23
[2024-07-24 10:17:34,875][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 24
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21']
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit6', 'circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:17:34,876][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 25
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 26
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,877][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 27
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,878][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 28
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 0
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,879][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 1
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,880][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 2
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit27']
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 3
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit20', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,881][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 4
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit19', 'circuit24']
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,882][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 5
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit20']
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit20']
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 6
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,883][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2']
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 7
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit5', 'circuit13', 'circuit15', 'circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:17:34,884][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 8
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit16']
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit4', 'circuit8', 'circuit10', 'circuit15', 'circuit16', 'circuit21', 'circuit25']
[2024-07-24 10:17:34,885][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 9
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit26']
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 10
[2024-07-24 10:17:34,886][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22']
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 11
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,887][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 12
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit16', 'circuit20', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:17:34,888][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit21', 'circuit24']
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 13
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit8', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,889][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 14
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit26']
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit17']
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 15
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:34,890][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 16
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24', 'circuit27']
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,891][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 17
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,892][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 18
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 19
[2024-07-24 10:17:34,893][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 20
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,894][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit18']
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 21
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,895][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24']
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 22
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23']
[2024-07-24 10:17:34,896][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 23
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit6']
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 24
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,897][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 25
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,898][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 26
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 27
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,899][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 28
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,900][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 0
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,901][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 1
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20']
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit19', 'circuit20', 'circuit24']
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,902][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 2
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit11']
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,903][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 3
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14']
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 4
[2024-07-24 10:17:34,904][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21']
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 5
[2024-07-24 10:17:34,905][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit24']
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit13']
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit16']
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 6
[2024-07-24 10:17:34,906][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 7
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,907][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 8
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,908][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit6', 'circuit8', 'circuit13']
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 9
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit20']
[2024-07-24 10:17:34,909][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17', 'circuit18']
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit20']
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 10
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit22', 'circuit25']
[2024-07-24 10:17:34,910][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 11
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18']
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,911][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 12
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit16', 'circuit27']
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,912][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 13
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit25']
[2024-07-24 10:17:34,913][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 14
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,914][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 15
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,915][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 16
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 17
[2024-07-24 10:17:34,916][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 18
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,917][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 19
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,918][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 20
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,919][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 21
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,920][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 22
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,921][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 23
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,922][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 24
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 25
[2024-07-24 10:17:34,923][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 26
[2024-07-24 10:17:34,924][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 27
[2024-07-24 10:17:34,925][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 28
[2024-07-24 10:17:34,926][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 0
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,927][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 1
[2024-07-24 10:17:34,928][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit24']
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,929][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 2
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20']
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20']
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,930][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 3
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit20', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,931][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit25']
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 4
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit20', 'circuit22']
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit19', 'circuit21']
[2024-07-24 10:17:34,932][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 5
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit27']
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit19', 'circuit21', 'circuit24', 'circuit27']
[2024-07-24 10:17:34,933][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 6
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit27']
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,934][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit19', 'circuit24']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 7
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit15', 'circuit20', 'circuit21', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,935][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit1', 'circuit20', 'circuit21', 'circuit25', 'circuit27']
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 8
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit12', 'circuit27']
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit27']
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,936][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 9
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,937][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 10
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit8', 'circuit13']
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:17:34,938][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23']
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit24']
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 11
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,939][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23']
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 12
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,940][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 13
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,941][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 14
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,942][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 15
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,943][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 16
[2024-07-24 10:17:34,944][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 17
[2024-07-24 10:17:34,945][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,946][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 18
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,947][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 19
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,948][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 20
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit23']
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,949][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 21
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,950][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 22
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,951][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 23
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,952][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 24
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,953][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 25
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,954][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 26
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,955][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 27
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,956][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 28
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,957][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 0
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,958][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 1
[2024-07-24 10:17:34,959][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,960][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 2
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,961][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 3
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit13']
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,962][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18']
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit4', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 4
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,963][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17']
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 5
[2024-07-24 10:17:34,964][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit27']
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,965][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 6
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit23']
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,966][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 7
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,967][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit20']
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 8
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit24']
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,968][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 9
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,969][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 10
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:17:34,970][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 11
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,971][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit21', 'circuit24']
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit25']
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit25']
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,972][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 12
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit5', 'circuit13', 'circuit19', 'circuit20']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit18']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,973][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 13
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,974][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit6', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 14
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,975][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 15
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:17:34,976][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,977][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 16
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,978][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 17
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,979][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 18
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,980][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 19
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,981][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 20
[2024-07-24 10:17:34,982][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,983][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 21
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,984][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 22
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,985][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 23
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,986][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 24
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,987][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 25
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,988][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 26
[2024-07-24 10:17:34,989][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,990][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 27
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,991][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 28
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,992][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 0
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:34,993][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit2', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:17:34,994][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 1
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,995][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 2
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,996][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 3
[2024-07-24 10:17:34,997][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:34,998][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 4
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:34,999][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 5
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,000][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,001][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 6
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,002][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 7
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,003][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 8
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,004][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 9
[2024-07-24 10:17:35,005][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,006][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 10
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,007][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 11
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,008][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 12
[2024-07-24 10:17:35,009][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,010][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 13
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit21', 'circuit24', 'circuit26']
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,011][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25']
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 14
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,012][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,013][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 15
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,014][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 16
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,015][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 17
[2024-07-24 10:17:35,016][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,017][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 18
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,018][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 19
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,019][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,020][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 20
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,021][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 21
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,022][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 22
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,023][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,024][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 23
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,025][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 24
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,026][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 25
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,027][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 26
[2024-07-24 10:17:35,028][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,029][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 27
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,030][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 28
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,031][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,032][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 0
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit12', 'circuit15', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,033][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit4', 'circuit13', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit7', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit26']
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 1
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,034][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,035][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 2
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,036][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 3
[2024-07-24 10:17:35,037][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,038][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 4
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,039][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 5
[2024-07-24 10:17:35,040][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,041][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 6
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,042][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,043][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 7
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,044][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 8
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,045][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 9
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,046][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,047][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 10
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,048][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 11
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,049][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,050][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 12
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,051][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 13
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,052][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,053][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 14
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,054][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 15
[2024-07-24 10:17:35,055][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,056][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 16
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,057][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,058][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 17
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,059][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 18
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,060][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,061][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 19
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,062][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 20
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,063][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 21
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,064][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,065][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 22
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,066][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 23
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,067][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,068][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 24
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,069][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 25
[2024-07-24 10:17:35,070][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,071][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 26
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,072][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,073][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 27
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,074][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 28
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,075][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,076][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 0
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22', 'circuit23']
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit7', 'circuit14']
[2024-07-24 10:17:35,077][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit13', 'circuit18', 'circuit22', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,078][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 1
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,079][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 2
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,080][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 3
[2024-07-24 10:17:35,081][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,082][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 4
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,083][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,084][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 5
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,085][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 6
[2024-07-24 10:17:35,086][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,087][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 7
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,088][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,089][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 8
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,090][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,091][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 9
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,092][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 10
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,093][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,094][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 11
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,095][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,096][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 12
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,097][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 13
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-24 10:17:35,098][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,099][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 14
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,100][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 15
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,101][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,102][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 16
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,103][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,104][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 17
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,105][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 18
[2024-07-24 10:17:35,106][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,107][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 19
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,108][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,109][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 20
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,110][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,111][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 21
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,112][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 22
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,113][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,114][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 23
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,115][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 24
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,116][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,117][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 25
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,118][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,119][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 26
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,120][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 27
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,121][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,122][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 28
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,123][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,124][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 0
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit27']
[2024-07-24 10:17:35,125][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit21']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit27']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit7', 'circuit16', 'circuit25']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit4', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit18']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit15']
[2024-07-24 10:17:35,126][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit13', 'circuit20', 'circuit21', 'circuit22', 'circuit27']
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit9', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 1
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24']
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,127][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,128][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 2
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,129][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17']
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,130][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 3
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,131][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit16', 'circuit18', 'circuit21', 'circuit24']
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 4
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,132][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,133][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit25']
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 5
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,134][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,135][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 6
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,136][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 7
[2024-07-24 10:17:35,137][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,138][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 8
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,139][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,140][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 9
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:17:35,141][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,142][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 10
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,143][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,144][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 11
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,145][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,146][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 12
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,147][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit16', 'circuit17']
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,148][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 13
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit19', 'circuit21', 'circuit27']
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit19', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,149][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 14
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,150][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,151][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 15
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,152][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,153][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 16
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,154][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,155][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 17
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,156][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 18
[2024-07-24 10:17:35,157][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,158][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 19
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,159][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,160][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 20
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,161][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,162][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 21
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,163][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,164][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 22
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,165][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,166][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 23
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,167][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit23']
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 24
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,168][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,169][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 25
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:17:35,170][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:17:35,171][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit19', 'circuit25']
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 26
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,172][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,173][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 27
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,174][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,175][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 28
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,176][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:35,177][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:17:36,178][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:36,180][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,181][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,182][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,182][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,182][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,183][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,183][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,183][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,184][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,184][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,184][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,185][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,185][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9029, 0.0971], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,186][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0011, 0.9989], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,186][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.8876, 0.1124], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,187][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.1033, 0.8967], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,187][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.4691, 0.5309], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,189][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0588, 0.9412], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,190][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.7148, 0.2852], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,192][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.9632, 0.0368], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,193][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.8715, 0.1285], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,193][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.9036, 0.0964], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,194][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.6805, 0.3195], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,194][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.7194, 0.2806], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,194][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.7638, 0.1777, 0.0585], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,195][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0042, 0.0035, 0.9923], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,195][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4078, 0.0434, 0.5489], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,195][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2577, 0.0474, 0.6949], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,196][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.6006, 0.2119, 0.1875], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,196][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.2171, 0.0100, 0.7729], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,196][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5895, 0.3855, 0.0250], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,197][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.5192, 0.2338, 0.2470], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,197][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2465, 0.0552, 0.6982], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,199][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6012, 0.1454, 0.2533], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,201][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.6323, 0.1238, 0.2440], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,202][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.5294, 0.1333, 0.3372], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,204][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.3423, 0.2095, 0.2544, 0.1937], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,204][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([7.5126e-05, 3.4419e-04, 4.4231e-05, 9.9954e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,206][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.5355, 0.1382, 0.2078, 0.1185], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,207][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([5.1360e-02, 1.9437e-02, 2.5356e-04, 9.2895e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,209][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0636, 0.0397, 0.0028, 0.8938], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,209][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([3.7891e-04, 2.3548e-05, 8.6310e-08, 9.9960e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,211][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.3486, 0.2636, 0.1838, 0.2040], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,211][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.4350, 0.1080, 0.3636, 0.0933], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,211][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.4188, 0.2481, 0.2389, 0.0941], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,212][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.5292, 0.2031, 0.2403, 0.0273], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,212][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.4381, 0.1568, 0.1747, 0.2304], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,212][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.2941, 0.1630, 0.3922, 0.1508], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,213][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.4629, 0.1290, 0.1051, 0.0970, 0.2061], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,213][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ got] are: tensor([4.9924e-04, 9.2970e-04, 9.1816e-04, 2.4733e-04, 9.9741e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,213][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.5204, 0.1086, 0.1752, 0.0799, 0.1160], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,214][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0567, 0.0115, 0.0130, 0.0093, 0.9096], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,214][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.3654, 0.0876, 0.0441, 0.0388, 0.4641], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,214][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ got] are: tensor([1.2123e-02, 3.8321e-04, 9.5187e-05, 9.9962e-06, 9.8739e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,216][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.2378, 0.4231, 0.0279, 0.2496, 0.0616], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,218][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.2942, 0.1008, 0.2428, 0.1120, 0.2502], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,219][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.3606, 0.1244, 0.3536, 0.0583, 0.1032], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,221][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.4263, 0.1432, 0.2282, 0.0872, 0.1152], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,222][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.3770, 0.1217, 0.1921, 0.0405, 0.2687], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,223][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.4479, 0.1287, 0.2168, 0.1012, 0.1055], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,225][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.4965, 0.1266, 0.0482, 0.1449, 0.1485, 0.0354], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,227][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0034, 0.0017, 0.0061, 0.0013, 0.0014, 0.9861], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,228][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.4484, 0.0701, 0.2612, 0.0444, 0.1220, 0.0540], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,229][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0702, 0.0177, 0.0381, 0.0303, 0.0996, 0.7441], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,229][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2050, 0.0597, 0.0401, 0.0640, 0.4561, 0.1751], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,229][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1229, 0.0088, 0.1343, 0.0026, 0.0326, 0.6987], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,230][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.2460, 0.2915, 0.0105, 0.3034, 0.1377, 0.0109], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,230][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1530, 0.0880, 0.1197, 0.1746, 0.2044, 0.2603], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,230][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0519, 0.0166, 0.2267, 0.0204, 0.0561, 0.6283], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,231][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.3643, 0.1096, 0.2005, 0.0846, 0.0932, 0.1477], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,231][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.3438, 0.0928, 0.2029, 0.0575, 0.0653, 0.2377], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,231][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.3487, 0.1069, 0.1769, 0.1119, 0.1228, 0.1328], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,232][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.2893, 0.1327, 0.2604, 0.0937, 0.0717, 0.1039, 0.0484],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,232][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([2.1908e-04, 1.3213e-03, 3.9601e-04, 3.1796e-04, 6.4682e-04, 2.1415e-04,
        9.9688e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,234][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.3987, 0.1235, 0.1499, 0.0533, 0.1174, 0.1272, 0.0300],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,235][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([6.8474e-03, 4.9172e-04, 1.9895e-05, 2.1505e-03, 8.8835e-04, 1.2623e-04,
        9.8948e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,237][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0684, 0.0154, 0.0017, 0.0088, 0.0142, 0.0035, 0.8880],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,238][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([3.0911e-05, 1.1588e-06, 2.4235e-09, 5.7880e-07, 1.9881e-08, 3.9287e-10,
        9.9997e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,239][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.1924, 0.2299, 0.1214, 0.2132, 0.0202, 0.0949, 0.1281],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,241][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.1374, 0.0292, 0.1388, 0.0213, 0.2615, 0.3498, 0.0619],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,243][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.3018, 0.0975, 0.1004, 0.0516, 0.0863, 0.0788, 0.2836],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,244][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.3568, 0.1274, 0.1637, 0.1200, 0.0944, 0.1252, 0.0126],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,246][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.2696, 0.0932, 0.1622, 0.0666, 0.0764, 0.0829, 0.2490],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,246][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.1623, 0.1045, 0.1584, 0.0838, 0.1466, 0.1927, 0.1517],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,246][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.2955, 0.0655, 0.0481, 0.0711, 0.1767, 0.0672, 0.2451, 0.0308],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,247][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ at] are: tensor([1.6192e-03, 3.4164e-04, 5.4911e-03, 2.4385e-04, 1.2743e-04, 1.8820e-03,
        5.3777e-05, 9.9024e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,247][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.3405, 0.0662, 0.1789, 0.0209, 0.1366, 0.0634, 0.0417, 0.1516],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,248][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ at] are: tensor([4.0012e-03, 3.5766e-04, 2.7536e-03, 1.1447e-03, 1.2691e-02, 3.7480e-02,
        2.9633e-02, 9.1194e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,248][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1275, 0.0229, 0.0194, 0.0208, 0.1897, 0.0743, 0.2423, 0.3031],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,248][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ at] are: tensor([7.7108e-02, 5.4554e-03, 1.4639e-02, 7.3871e-03, 7.9163e-03, 1.8283e-02,
        5.8928e-04, 8.6862e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,249][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1340, 0.2258, 0.0152, 0.2441, 0.0862, 0.0179, 0.2573, 0.0196],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,249][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0889, 0.0267, 0.0579, 0.0426, 0.1051, 0.2094, 0.1437, 0.3256],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,249][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0617, 0.0234, 0.3034, 0.0262, 0.0654, 0.3288, 0.0325, 0.1586],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,251][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.2696, 0.0884, 0.1691, 0.0680, 0.0877, 0.1275, 0.0524, 0.1372],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,253][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.2311, 0.0780, 0.1719, 0.0585, 0.0570, 0.1226, 0.0290, 0.2520],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,254][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.3007, 0.0875, 0.1163, 0.0863, 0.0728, 0.0614, 0.1249, 0.1501],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,256][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.3760, 0.1206, 0.0266, 0.0873, 0.0927, 0.0213, 0.2315, 0.0205, 0.0234],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,256][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.2878e-03, 2.2238e-03, 5.7317e-02, 1.6002e-04, 1.7139e-04, 4.4541e-02,
        3.6137e-05, 5.6533e-02, 8.2973e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,258][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3121, 0.0623, 0.1354, 0.0396, 0.1324, 0.0397, 0.0335, 0.2071, 0.0380],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,260][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0288, 0.0031, 0.0062, 0.0033, 0.0169, 0.0716, 0.0224, 0.1898, 0.6579],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,262][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1866, 0.0332, 0.0253, 0.0283, 0.1344, 0.0582, 0.1709, 0.2000, 0.1631],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,263][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1259, 0.0415, 0.1212, 0.0045, 0.0592, 0.2331, 0.0036, 0.0740, 0.3369],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,263][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1356, 0.2149, 0.0035, 0.3291, 0.0616, 0.0036, 0.2434, 0.0061, 0.0021],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,264][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0740, 0.0133, 0.0324, 0.0406, 0.0549, 0.0848, 0.1359, 0.2777, 0.2864],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,264][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0196, 0.0082, 0.1374, 0.0111, 0.0292, 0.3231, 0.0112, 0.0640, 0.3962],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,265][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2659, 0.0776, 0.1368, 0.0652, 0.0745, 0.1026, 0.0492, 0.1160, 0.1122],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,265][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2780, 0.0859, 0.1469, 0.0444, 0.0453, 0.1314, 0.0189, 0.0846, 0.1646],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,265][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2083, 0.0802, 0.0997, 0.0937, 0.0680, 0.1038, 0.1338, 0.0983, 0.1142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,266][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.1971, 0.1726, 0.0966, 0.0649, 0.0559, 0.0835, 0.1146, 0.0642, 0.0759,
        0.0747], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,266][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ station] are: tensor([5.2432e-04, 8.3503e-04, 3.4646e-03, 9.0934e-04, 3.4637e-04, 9.5034e-05,
        2.6181e-04, 4.2466e-03, 3.6791e-04, 9.8895e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,267][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.1977, 0.1633, 0.0696, 0.0900, 0.0712, 0.0712, 0.0303, 0.0854, 0.0603,
        0.1609], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,267][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ station] are: tensor([8.0514e-03, 1.6047e-03, 5.3207e-05, 7.6152e-04, 4.7758e-04, 1.6014e-04,
        3.4584e-03, 1.2992e-03, 8.8439e-04, 9.8325e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,269][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0425, 0.0172, 0.0034, 0.0127, 0.0208, 0.0070, 0.0242, 0.0246, 0.0170,
        0.8306], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,270][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ station] are: tensor([1.1868e-02, 3.4210e-04, 1.9762e-05, 2.1770e-04, 1.3646e-05, 4.2373e-06,
        4.4943e-04, 2.3359e-05, 3.0152e-06, 9.8706e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,272][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.1404, 0.1767, 0.0261, 0.1857, 0.0168, 0.0280, 0.2387, 0.0108, 0.0209,
        0.1559], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,273][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0424, 0.0295, 0.0243, 0.0390, 0.0898, 0.0836, 0.0091, 0.2114, 0.3188,
        0.1519], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,275][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.1969, 0.0777, 0.1377, 0.0326, 0.0545, 0.0974, 0.0431, 0.1011, 0.0949,
        0.1642], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,277][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.2200, 0.1074, 0.1149, 0.0971, 0.0676, 0.0986, 0.0854, 0.0907, 0.1037,
        0.0145], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,278][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.1298, 0.0733, 0.1168, 0.0654, 0.0382, 0.0805, 0.0444, 0.1045, 0.0802,
        0.2668], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,280][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.2027, 0.0751, 0.0924, 0.0737, 0.0711, 0.0603, 0.0963, 0.1014, 0.0432,
        0.1838], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,281][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.3679, 0.0837, 0.0136, 0.0687, 0.0954, 0.0242, 0.1917, 0.0162, 0.0435,
        0.0851, 0.0099], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,281][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [,] are: tensor([4.0970e-03, 1.7157e-04, 2.0237e-02, 1.2968e-04, 2.6255e-04, 3.2652e-04,
        5.3598e-05, 2.4373e-03, 9.4261e-04, 8.9549e-05, 9.7125e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,281][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2872, 0.0255, 0.1116, 0.0226, 0.0274, 0.0100, 0.0333, 0.0533, 0.0124,
        0.0109, 0.4058], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,282][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [,] are: tensor([3.4783e-02, 6.5652e-04, 2.7070e-03, 1.3047e-03, 9.5100e-03, 1.2266e-02,
        7.2022e-03, 5.0846e-02, 1.3061e-01, 6.7700e-02, 6.8242e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,282][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.3846, 0.0124, 0.0118, 0.0306, 0.0354, 0.0147, 0.0547, 0.0791, 0.0380,
        0.0993, 0.2394], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,283][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1198, 0.0313, 0.1738, 0.0083, 0.0377, 0.1303, 0.0160, 0.1104, 0.1226,
        0.0159, 0.2339], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,283][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.1780, 0.2058, 0.0066, 0.2564, 0.0488, 0.0077, 0.1580, 0.0106, 0.0045,
        0.1192, 0.0043], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,283][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0361, 0.0102, 0.0153, 0.0300, 0.0295, 0.0440, 0.0567, 0.1106, 0.1747,
        0.1507, 0.3423], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,284][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0292, 0.0081, 0.1524, 0.0055, 0.0155, 0.1435, 0.0066, 0.0618, 0.2192,
        0.0120, 0.3463], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,284][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.2294, 0.0747, 0.1190, 0.0567, 0.0675, 0.0828, 0.0484, 0.0956, 0.0886,
        0.0434, 0.0938], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,286][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.1832, 0.0824, 0.1407, 0.0729, 0.0615, 0.0894, 0.0399, 0.1021, 0.0869,
        0.0370, 0.1041], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,288][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.1939, 0.0733, 0.0923, 0.0708, 0.0670, 0.0565, 0.1194, 0.1018, 0.0481,
        0.0754, 0.1015], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,289][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.2208, 0.0560, 0.1322, 0.1008, 0.0280, 0.0420, 0.0703, 0.0774, 0.0587,
        0.0646, 0.1034, 0.0458], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,290][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([3.0836e-04, 5.3137e-01, 9.0610e-04, 8.7356e-04, 1.7259e-04, 8.5809e-05,
        2.6355e-04, 2.0644e-04, 1.2526e-04, 3.5621e-05, 1.2581e-05, 4.6564e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,291][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.2057, 0.1656, 0.0448, 0.0600, 0.0308, 0.0546, 0.0421, 0.0621, 0.0590,
        0.0778, 0.0617, 0.1359], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,293][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([1.1927e-03, 1.7277e-03, 1.4496e-06, 3.2996e-04, 1.8508e-05, 1.8073e-05,
        6.4161e-04, 7.8670e-05, 9.1582e-05, 4.1570e-04, 1.0272e-04, 9.9538e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,294][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0381, 0.0580, 0.0021, 0.0201, 0.0031, 0.0031, 0.0184, 0.0124, 0.0080,
        0.0116, 0.0182, 0.8069], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,295][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([3.3999e-03, 7.3276e-01, 1.3967e-07, 5.7337e-05, 2.9181e-06, 1.4364e-07,
        3.0357e-06, 2.1477e-07, 2.3377e-07, 2.3273e-07, 9.9195e-09, 2.6377e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,297][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.1354, 0.1887, 0.0428, 0.1576, 0.0135, 0.0386, 0.0668, 0.0171, 0.0494,
        0.0851, 0.0524, 0.1526], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,298][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0862, 0.0064, 0.0235, 0.0091, 0.0315, 0.0677, 0.0155, 0.1365, 0.2032,
        0.0247, 0.2745, 0.1211], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,298][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.1992, 0.0851, 0.1120, 0.1093, 0.0397, 0.0783, 0.0331, 0.0650, 0.0810,
        0.0333, 0.0919, 0.0721], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,299][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.2056, 0.0328, 0.1058, 0.1148, 0.0538, 0.0701, 0.0641, 0.0827, 0.0789,
        0.0707, 0.0966, 0.0241], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,299][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.1129, 0.2552, 0.0868, 0.0583, 0.0338, 0.0483, 0.0181, 0.0518, 0.0547,
        0.0197, 0.0545, 0.2060], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,300][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0993, 0.0693, 0.0623, 0.0596, 0.0934, 0.0560, 0.1730, 0.0823, 0.0432,
        0.0918, 0.0713, 0.0984], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,300][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.2556, 0.0955, 0.0284, 0.1150, 0.0538, 0.0255, 0.0584, 0.0300, 0.0312,
        0.0292, 0.0345, 0.1362, 0.1068], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,301][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([4.2169e-04, 4.4199e-04, 2.0352e-04, 4.0959e-04, 1.2760e-02, 4.1480e-04,
        1.9192e-04, 1.3321e-04, 9.4558e-05, 2.4166e-04, 5.3965e-05, 9.0686e-05,
        9.8454e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,301][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.2837, 0.0511, 0.0870, 0.0239, 0.1352, 0.0674, 0.0221, 0.0727, 0.0719,
        0.0450, 0.0326, 0.0415, 0.0659], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,301][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([9.1929e-04, 6.1898e-06, 8.9540e-06, 1.5689e-05, 9.8156e-05, 2.6819e-05,
        3.7537e-05, 1.0291e-04, 1.5232e-04, 5.9394e-04, 9.5944e-04, 6.1310e-04,
        9.9647e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,302][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0392, 0.0021, 0.0049, 0.0016, 0.0077, 0.0070, 0.0045, 0.0235, 0.0145,
        0.0314, 0.0263, 0.0147, 0.8226], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,303][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([1.1287e-02, 3.4411e-05, 4.4807e-05, 2.7448e-05, 2.1542e-03, 5.7195e-06,
        2.0245e-05, 1.9074e-05, 1.8325e-06, 1.0444e-05, 1.9814e-06, 1.4927e-06,
        9.8639e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,305][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.1196, 0.1658, 0.0164, 0.1446, 0.0320, 0.0206, 0.0803, 0.0145, 0.0133,
        0.0752, 0.0130, 0.2222, 0.0826], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,307][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0360, 0.0068, 0.0101, 0.0070, 0.0180, 0.0335, 0.0091, 0.0597, 0.0930,
        0.0684, 0.2417, 0.2283, 0.1884], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,308][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.1449, 0.0345, 0.1143, 0.0238, 0.0783, 0.1156, 0.0446, 0.0944, 0.1502,
        0.0396, 0.0949, 0.0403, 0.0246], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,310][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1934, 0.0674, 0.0867, 0.0436, 0.0721, 0.0658, 0.0681, 0.0686, 0.0719,
        0.0509, 0.0803, 0.0730, 0.0582], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,312][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.1781, 0.0412, 0.0813, 0.0421, 0.0751, 0.0684, 0.0413, 0.0581, 0.0683,
        0.0267, 0.0650, 0.0291, 0.2253], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,314][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.3053, 0.0514, 0.0753, 0.0446, 0.0436, 0.0407, 0.0730, 0.1010, 0.0304,
        0.0487, 0.0815, 0.0487, 0.0556], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,315][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.2012, 0.0725, 0.0196, 0.0479, 0.0659, 0.0190, 0.1358, 0.0172, 0.0261,
        0.0517, 0.0229, 0.1073, 0.1988, 0.0141], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,316][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([5.7887e-03, 3.3028e-04, 3.9632e-02, 1.6698e-04, 4.6664e-04, 1.6731e-03,
        2.2303e-05, 1.9503e-02, 1.9043e-03, 1.9527e-04, 8.6275e-03, 1.1570e-04,
        2.9689e-05, 9.2154e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,316][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2047, 0.0305, 0.1038, 0.0252, 0.0564, 0.0271, 0.0234, 0.1419, 0.0299,
        0.0165, 0.0645, 0.0250, 0.0699, 0.1812], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,316][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([6.0945e-03, 7.7714e-05, 2.8041e-04, 5.1433e-05, 8.1558e-04, 1.6076e-03,
        2.2169e-03, 3.4899e-03, 1.5014e-02, 2.9226e-03, 3.6892e-02, 6.9482e-03,
        9.8749e-02, 8.2484e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,317][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.2015e-02, 6.3910e-04, 1.3429e-03, 8.9401e-04, 1.9471e-02, 3.7131e-03,
        1.1110e-02, 7.7913e-03, 6.1797e-03, 1.3606e-02, 9.1281e-03, 7.4217e-03,
        8.0850e-01, 9.8183e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,317][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0435, 0.0160, 0.0889, 0.0018, 0.0494, 0.2118, 0.0006, 0.0207, 0.1741,
        0.0019, 0.0177, 0.0035, 0.0057, 0.3645], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,318][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0923, 0.0739, 0.0050, 0.0711, 0.0434, 0.0042, 0.0951, 0.0181, 0.0034,
        0.0766, 0.0042, 0.1086, 0.1329, 0.2713], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,318][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0272, 0.0032, 0.0057, 0.0094, 0.0096, 0.0115, 0.0202, 0.0247, 0.0410,
        0.0295, 0.1085, 0.1088, 0.3360, 0.2647], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,319][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0165, 0.0053, 0.0990, 0.0049, 0.0165, 0.1334, 0.0075, 0.0485, 0.2130,
        0.0122, 0.1784, 0.0098, 0.0075, 0.2474], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,319][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1543, 0.0541, 0.0928, 0.0429, 0.0539, 0.0753, 0.0370, 0.0776, 0.0813,
        0.0391, 0.0774, 0.0527, 0.0569, 0.1048], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,321][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1247, 0.0568, 0.1123, 0.0396, 0.0645, 0.0929, 0.0261, 0.0849, 0.0846,
        0.0282, 0.0788, 0.0436, 0.0464, 0.1166], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,323][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1649, 0.0515, 0.0767, 0.0600, 0.0607, 0.0488, 0.0818, 0.0777, 0.0370,
        0.0612, 0.0799, 0.0567, 0.0648, 0.0784], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,324][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.2231, 0.0495, 0.0278, 0.0609, 0.0631, 0.0242, 0.1142, 0.0169, 0.0279,
        0.0645, 0.0243, 0.0603, 0.1734, 0.0255, 0.0443], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,325][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ give] are: tensor([6.2516e-04, 3.0762e-04, 5.1225e-04, 3.5901e-04, 4.3849e-03, 4.6103e-04,
        2.4164e-04, 7.7518e-04, 2.1335e-04, 4.5927e-05, 1.6727e-04, 9.1817e-05,
        1.3265e-03, 5.6389e-04, 9.8992e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,327][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.1944, 0.0610, 0.0530, 0.0258, 0.0895, 0.0420, 0.0213, 0.0794, 0.0517,
        0.0161, 0.0342, 0.0593, 0.1097, 0.1121, 0.0505], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,329][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ give] are: tensor([8.9584e-04, 3.1165e-06, 4.9657e-06, 2.3225e-06, 3.2025e-05, 3.7221e-05,
        1.1770e-05, 6.5378e-05, 2.1131e-04, 8.8541e-05, 9.0878e-04, 7.1710e-04,
        8.8054e-03, 8.6061e-03, 9.7961e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,330][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0442, 0.0047, 0.0063, 0.0041, 0.0099, 0.0096, 0.0054, 0.0235, 0.0161,
        0.0123, 0.0277, 0.0279, 0.1301, 0.1225, 0.5557], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,331][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ give] are: tensor([9.2212e-03, 1.4511e-03, 5.0097e-05, 2.9705e-04, 1.4818e-03, 4.0151e-05,
        5.2019e-06, 3.3102e-05, 2.2208e-05, 3.9064e-07, 3.1397e-06, 1.2010e-04,
        1.6457e-04, 8.3867e-06, 9.8710e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,332][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.1051, 0.1305, 0.0114, 0.1204, 0.0258, 0.0127, 0.1236, 0.0107, 0.0109,
        0.1239, 0.0110, 0.2023, 0.0797, 0.0101, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,333][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0256, 0.0032, 0.0057, 0.0028, 0.0087, 0.0123, 0.0038, 0.0244, 0.0318,
        0.0154, 0.0877, 0.0796, 0.1470, 0.3147, 0.2372], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,333][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0732, 0.0221, 0.0867, 0.0271, 0.0719, 0.0711, 0.0980, 0.0630, 0.0905,
        0.0377, 0.0943, 0.0311, 0.0407, 0.1498, 0.0428], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,334][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.1280, 0.0542, 0.0740, 0.0441, 0.0596, 0.0633, 0.0616, 0.0614, 0.0639,
        0.0450, 0.0692, 0.0604, 0.0696, 0.0912, 0.0544], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,334][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.1075, 0.0358, 0.0699, 0.0306, 0.0767, 0.0609, 0.0391, 0.0673, 0.0579,
        0.0238, 0.0610, 0.0336, 0.0624, 0.0816, 0.1920], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,335][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.2538, 0.0533, 0.0868, 0.0431, 0.0503, 0.0255, 0.0500, 0.0763, 0.0196,
        0.0510, 0.0612, 0.0482, 0.0478, 0.0601, 0.0730], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,335][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.2524, 0.0488, 0.0185, 0.0750, 0.0440, 0.0115, 0.1427, 0.0149, 0.0169,
        0.0396, 0.0209, 0.0666, 0.1338, 0.0129, 0.0676, 0.0340],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,336][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ it] are: tensor([3.1317e-03, 6.1588e-04, 2.0967e-03, 8.8570e-04, 2.5781e-04, 3.3384e-03,
        1.1917e-05, 6.0135e-04, 1.0975e-02, 8.6500e-05, 7.2891e-04, 2.5062e-04,
        1.1504e-04, 3.1578e-03, 4.1402e-04, 9.7333e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,336][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.1725, 0.0228, 0.0823, 0.0343, 0.1116, 0.0250, 0.0246, 0.0678, 0.0260,
        0.0243, 0.0456, 0.0202, 0.1343, 0.0994, 0.0759, 0.0334],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,337][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ it] are: tensor([8.5237e-04, 9.6140e-06, 1.4932e-05, 1.1945e-05, 5.9171e-05, 1.0297e-04,
        3.5429e-05, 2.5968e-04, 8.7368e-04, 2.9672e-04, 3.2492e-03, 1.7290e-03,
        9.4331e-03, 1.8062e-02, 7.2820e-02, 8.9219e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,338][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0459, 0.0023, 0.0056, 0.0026, 0.0160, 0.0062, 0.0133, 0.0131, 0.0134,
        0.0158, 0.0248, 0.0160, 0.2740, 0.0924, 0.1944, 0.2642],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,340][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ it] are: tensor([2.6345e-02, 1.2317e-02, 4.1338e-03, 8.0066e-04, 8.0145e-03, 6.5092e-03,
        1.6163e-03, 1.8759e-03, 8.2229e-03, 4.4234e-04, 4.4484e-04, 1.0161e-03,
        1.0171e-03, 1.8960e-03, 5.5827e-04, 9.2479e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,341][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0885, 0.1103, 0.0045, 0.1973, 0.0333, 0.0044, 0.1025, 0.0048, 0.0029,
        0.0785, 0.0037, 0.2104, 0.1065, 0.0062, 0.0402, 0.0058],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,343][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0257, 0.0018, 0.0040, 0.0056, 0.0060, 0.0067, 0.0070, 0.0162, 0.0188,
        0.0118, 0.0606, 0.0434, 0.0689, 0.1727, 0.2946, 0.2562],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,345][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0308, 0.0064, 0.0850, 0.0127, 0.0331, 0.1310, 0.0130, 0.0405, 0.1815,
        0.0156, 0.1210, 0.0100, 0.0146, 0.1109, 0.0256, 0.1682],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,347][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.1544, 0.0510, 0.0753, 0.0450, 0.0502, 0.0574, 0.0411, 0.0599, 0.0641,
        0.0273, 0.0658, 0.0531, 0.0457, 0.0844, 0.0571, 0.0683],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,348][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.1079, 0.0444, 0.0677, 0.0402, 0.0369, 0.0659, 0.0188, 0.0500, 0.0832,
        0.0233, 0.0541, 0.0399, 0.0389, 0.0658, 0.0412, 0.2219],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,350][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.1984, 0.0536, 0.0565, 0.0492, 0.0418, 0.0344, 0.0726, 0.0632, 0.0299,
        0.0571, 0.0670, 0.0551, 0.0529, 0.0592, 0.0623, 0.0468],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,350][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1789, 0.0637, 0.0160, 0.0426, 0.0575, 0.0161, 0.1179, 0.0137, 0.0218,
        0.0441, 0.0185, 0.0952, 0.1722, 0.0114, 0.0675, 0.0491, 0.0136],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,351][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.8635e-03, 1.4691e-04, 1.7566e-02, 7.8946e-05, 2.1392e-04, 7.1490e-04,
        9.3591e-06, 8.8250e-03, 8.9387e-04, 9.7976e-05, 4.6294e-03, 6.0760e-05,
        1.4754e-05, 4.6279e-01, 4.0134e-04, 6.5253e-04, 5.0004e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,351][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1559, 0.0232, 0.0780, 0.0209, 0.0444, 0.0203, 0.0187, 0.1075, 0.0225,
        0.0132, 0.0492, 0.0202, 0.0559, 0.1374, 0.0576, 0.0280, 0.1470],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,352][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([1.6393e-03, 1.1898e-05, 4.0767e-05, 7.0633e-06, 8.5001e-05, 1.4143e-04,
        2.2940e-04, 2.4131e-04, 9.9440e-04, 2.2160e-04, 2.5217e-03, 4.4959e-04,
        6.2107e-03, 4.7906e-02, 4.0158e-01, 9.8645e-02, 4.3908e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,352][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0134, 0.0006, 0.0012, 0.0007, 0.0147, 0.0025, 0.0078, 0.0046, 0.0035,
        0.0078, 0.0050, 0.0039, 0.4300, 0.0498, 0.2331, 0.0619, 0.1596],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,352][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0267, 0.0110, 0.0608, 0.0012, 0.0354, 0.1531, 0.0004, 0.0153, 0.1242,
        0.0013, 0.0113, 0.0024, 0.0039, 0.2561, 0.0089, 0.0520, 0.2360],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,353][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0653, 0.0509, 0.0033, 0.0510, 0.0305, 0.0028, 0.0666, 0.0126, 0.0022,
        0.0536, 0.0028, 0.0778, 0.0958, 0.1928, 0.0518, 0.0073, 0.2330],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,353][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0210, 0.0017, 0.0032, 0.0052, 0.0043, 0.0045, 0.0083, 0.0080, 0.0127,
        0.0093, 0.0336, 0.0308, 0.0964, 0.0711, 0.1477, 0.2616, 0.2806],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,355][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0118, 0.0037, 0.0693, 0.0036, 0.0119, 0.0918, 0.0055, 0.0336, 0.1445,
        0.0086, 0.1246, 0.0072, 0.0054, 0.1712, 0.0139, 0.0885, 0.2048],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,357][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1204, 0.0426, 0.0715, 0.0353, 0.0432, 0.0581, 0.0304, 0.0595, 0.0623,
        0.0322, 0.0604, 0.0429, 0.0464, 0.0807, 0.0547, 0.0711, 0.0883],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,358][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0893, 0.0405, 0.0790, 0.0315, 0.0502, 0.0699, 0.0217, 0.0662, 0.0659,
        0.0245, 0.0642, 0.0380, 0.0404, 0.0970, 0.0484, 0.0750, 0.0984],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,360][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1499, 0.0445, 0.0675, 0.0523, 0.0538, 0.0429, 0.0656, 0.0655, 0.0317,
        0.0499, 0.0638, 0.0450, 0.0527, 0.0644, 0.0514, 0.0338, 0.0654],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,382][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:36,384][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,385][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,386][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,387][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,388][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,390][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,391][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,392][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,393][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,395][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,395][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,395][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,396][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.9029, 0.0971], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,396][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0011, 0.9989], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,396][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.8876, 0.1124], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,397][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.1033, 0.8967], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,397][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.4691, 0.5309], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,397][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0588, 0.9412], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,398][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.7148, 0.2852], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,398][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.9632, 0.0368], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,398][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.8715, 0.1285], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,399][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.9036, 0.0964], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,401][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.6805, 0.3195], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,402][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.7194, 0.2806], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,404][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7638, 0.1777, 0.0585], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,405][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0042, 0.0035, 0.9923], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,406][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.4078, 0.0434, 0.5489], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,408][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2577, 0.0474, 0.6949], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,410][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.6006, 0.2119, 0.1875], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,411][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.2171, 0.0100, 0.7729], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,412][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5895, 0.3855, 0.0250], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,412][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.5192, 0.2338, 0.2470], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,413][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.2465, 0.0552, 0.6982], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,413][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6012, 0.1454, 0.2533], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,413][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.6323, 0.1238, 0.2440], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,414][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.5294, 0.1333, 0.3372], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,414][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.3423, 0.2095, 0.2544, 0.1937], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,414][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([7.5126e-05, 3.4419e-04, 4.4231e-05, 9.9954e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,415][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.5355, 0.1382, 0.2078, 0.1185], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,415][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([5.1360e-02, 1.9437e-02, 2.5356e-04, 9.2895e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,415][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0636, 0.0397, 0.0028, 0.8938], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,416][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([3.7891e-04, 2.3548e-05, 8.6310e-08, 9.9960e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,417][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.3486, 0.2636, 0.1838, 0.2040], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,418][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.4350, 0.1080, 0.3636, 0.0933], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,420][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.4188, 0.2481, 0.2389, 0.0941], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,422][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.5292, 0.2031, 0.2403, 0.0273], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,423][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.4381, 0.1568, 0.1747, 0.2304], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,425][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.2941, 0.1630, 0.3922, 0.1508], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,426][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.4629, 0.1290, 0.1051, 0.0970, 0.2061], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,427][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([4.9924e-04, 9.2970e-04, 9.1816e-04, 2.4733e-04, 9.9741e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,429][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.5204, 0.1086, 0.1752, 0.0799, 0.1160], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,430][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0567, 0.0115, 0.0130, 0.0093, 0.9096], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,430][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.3654, 0.0876, 0.0441, 0.0388, 0.4641], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,430][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([1.2123e-02, 3.8321e-04, 9.5187e-05, 9.9962e-06, 9.8739e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,431][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.2378, 0.4231, 0.0279, 0.2496, 0.0616], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,431][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.2942, 0.1008, 0.2428, 0.1120, 0.2502], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,431][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.3606, 0.1244, 0.3536, 0.0583, 0.1032], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,432][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.4263, 0.1432, 0.2282, 0.0872, 0.1152], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,432][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.3770, 0.1217, 0.1921, 0.0405, 0.2687], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,432][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.4479, 0.1287, 0.2168, 0.1012, 0.1055], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,433][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.4965, 0.1266, 0.0482, 0.1449, 0.1485, 0.0354], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,433][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0034, 0.0017, 0.0061, 0.0013, 0.0014, 0.9861], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,434][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4484, 0.0701, 0.2612, 0.0444, 0.1220, 0.0540], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,436][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0702, 0.0177, 0.0381, 0.0303, 0.0996, 0.7441], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,438][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2050, 0.0597, 0.0401, 0.0640, 0.4561, 0.1751], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,439][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1229, 0.0088, 0.1343, 0.0026, 0.0326, 0.6987], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,440][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2460, 0.2915, 0.0105, 0.3034, 0.1377, 0.0109], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,442][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1530, 0.0880, 0.1197, 0.1746, 0.2044, 0.2603], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,444][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0519, 0.0166, 0.2267, 0.0204, 0.0561, 0.6283], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,445][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.3643, 0.1096, 0.2005, 0.0846, 0.0932, 0.1477], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,447][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3438, 0.0928, 0.2029, 0.0575, 0.0653, 0.2377], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,447][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.3487, 0.1069, 0.1769, 0.1119, 0.1228, 0.1328], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,448][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.2893, 0.1327, 0.2604, 0.0937, 0.0717, 0.1039, 0.0484],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,448][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([2.1908e-04, 1.3213e-03, 3.9601e-04, 3.1796e-04, 6.4682e-04, 2.1415e-04,
        9.9688e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,448][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.3987, 0.1235, 0.1499, 0.0533, 0.1174, 0.1272, 0.0300],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,449][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([6.8474e-03, 4.9172e-04, 1.9895e-05, 2.1505e-03, 8.8835e-04, 1.2623e-04,
        9.8948e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,449][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0684, 0.0154, 0.0017, 0.0088, 0.0142, 0.0035, 0.8880],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,449][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([3.0911e-05, 1.1588e-06, 2.4235e-09, 5.7880e-07, 1.9881e-08, 3.9287e-10,
        9.9997e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,450][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.1924, 0.2299, 0.1214, 0.2132, 0.0202, 0.0949, 0.1281],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,450][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.1374, 0.0292, 0.1388, 0.0213, 0.2615, 0.3498, 0.0619],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,450][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.3018, 0.0975, 0.1004, 0.0516, 0.0863, 0.0788, 0.2836],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,451][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.3568, 0.1274, 0.1637, 0.1200, 0.0944, 0.1252, 0.0126],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,453][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.2696, 0.0932, 0.1622, 0.0666, 0.0764, 0.0829, 0.2490],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,455][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.1623, 0.1045, 0.1584, 0.0838, 0.1466, 0.1927, 0.1517],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,456][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.2955, 0.0655, 0.0481, 0.0711, 0.1767, 0.0672, 0.2451, 0.0308],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,457][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([1.6192e-03, 3.4164e-04, 5.4911e-03, 2.4385e-04, 1.2743e-04, 1.8820e-03,
        5.3777e-05, 9.9024e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,459][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.3405, 0.0662, 0.1789, 0.0209, 0.1366, 0.0634, 0.0417, 0.1516],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,460][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([4.0012e-03, 3.5766e-04, 2.7536e-03, 1.1447e-03, 1.2691e-02, 3.7480e-02,
        2.9633e-02, 9.1194e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,461][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1275, 0.0229, 0.0194, 0.0208, 0.1897, 0.0743, 0.2423, 0.3031],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,462][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([7.7108e-02, 5.4554e-03, 1.4639e-02, 7.3871e-03, 7.9163e-03, 1.8283e-02,
        5.8928e-04, 8.6862e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,464][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1340, 0.2258, 0.0152, 0.2441, 0.0862, 0.0179, 0.2573, 0.0196],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,464][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0889, 0.0267, 0.0579, 0.0426, 0.1051, 0.2094, 0.1437, 0.3256],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,465][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0617, 0.0234, 0.3034, 0.0262, 0.0654, 0.3288, 0.0325, 0.1586],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,465][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.2696, 0.0884, 0.1691, 0.0680, 0.0877, 0.1275, 0.0524, 0.1372],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,465][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.2311, 0.0780, 0.1719, 0.0585, 0.0570, 0.1226, 0.0290, 0.2520],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,466][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.3007, 0.0875, 0.1163, 0.0863, 0.0728, 0.0614, 0.1249, 0.1501],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,466][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3760, 0.1206, 0.0266, 0.0873, 0.0927, 0.0213, 0.2315, 0.0205, 0.0234],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,467][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([9.2878e-03, 2.2238e-03, 5.7317e-02, 1.6002e-04, 1.7139e-04, 4.4541e-02,
        3.6137e-05, 5.6533e-02, 8.2973e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,467][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3121, 0.0623, 0.1354, 0.0396, 0.1324, 0.0397, 0.0335, 0.2071, 0.0380],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,467][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0288, 0.0031, 0.0062, 0.0033, 0.0169, 0.0716, 0.0224, 0.1898, 0.6579],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,468][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1866, 0.0332, 0.0253, 0.0283, 0.1344, 0.0582, 0.1709, 0.2000, 0.1631],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,469][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1259, 0.0415, 0.1212, 0.0045, 0.0592, 0.2331, 0.0036, 0.0740, 0.3369],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,470][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1356, 0.2149, 0.0035, 0.3291, 0.0616, 0.0036, 0.2434, 0.0061, 0.0021],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,472][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0740, 0.0133, 0.0324, 0.0406, 0.0549, 0.0848, 0.1359, 0.2777, 0.2864],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,474][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0196, 0.0082, 0.1374, 0.0111, 0.0292, 0.3231, 0.0112, 0.0640, 0.3962],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,475][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2659, 0.0776, 0.1368, 0.0652, 0.0745, 0.1026, 0.0492, 0.1160, 0.1122],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,477][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2780, 0.0859, 0.1469, 0.0444, 0.0453, 0.1314, 0.0189, 0.0846, 0.1646],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,478][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2083, 0.0802, 0.0997, 0.0937, 0.0680, 0.1038, 0.1338, 0.0983, 0.1142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,480][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.1971, 0.1726, 0.0966, 0.0649, 0.0559, 0.0835, 0.1146, 0.0642, 0.0759,
        0.0747], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,481][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([5.2432e-04, 8.3503e-04, 3.4646e-03, 9.0934e-04, 3.4637e-04, 9.5034e-05,
        2.6181e-04, 4.2466e-03, 3.6791e-04, 9.8895e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,481][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.1977, 0.1633, 0.0696, 0.0900, 0.0712, 0.0712, 0.0303, 0.0854, 0.0603,
        0.1609], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,482][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([8.0514e-03, 1.6047e-03, 5.3207e-05, 7.6152e-04, 4.7758e-04, 1.6014e-04,
        3.4584e-03, 1.2992e-03, 8.8439e-04, 9.8325e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,482][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0425, 0.0172, 0.0034, 0.0127, 0.0208, 0.0070, 0.0242, 0.0246, 0.0170,
        0.8306], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,483][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([1.1868e-02, 3.4210e-04, 1.9762e-05, 2.1770e-04, 1.3646e-05, 4.2373e-06,
        4.4943e-04, 2.3359e-05, 3.0152e-06, 9.8706e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,483][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.1404, 0.1767, 0.0261, 0.1857, 0.0168, 0.0280, 0.2387, 0.0108, 0.0209,
        0.1559], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,483][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0424, 0.0295, 0.0243, 0.0390, 0.0898, 0.0836, 0.0091, 0.2114, 0.3188,
        0.1519], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,484][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.1969, 0.0777, 0.1377, 0.0326, 0.0545, 0.0974, 0.0431, 0.1011, 0.0949,
        0.1642], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,484][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.2200, 0.1074, 0.1149, 0.0971, 0.0676, 0.0986, 0.0854, 0.0907, 0.1037,
        0.0145], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,485][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.1298, 0.0733, 0.1168, 0.0654, 0.0382, 0.0805, 0.0444, 0.1045, 0.0802,
        0.2668], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,485][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.2027, 0.0751, 0.0924, 0.0737, 0.0711, 0.0603, 0.0963, 0.1014, 0.0432,
        0.1838], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,487][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.3679, 0.0837, 0.0136, 0.0687, 0.0954, 0.0242, 0.1917, 0.0162, 0.0435,
        0.0851, 0.0099], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,488][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([4.0970e-03, 1.7157e-04, 2.0237e-02, 1.2968e-04, 2.6255e-04, 3.2652e-04,
        5.3598e-05, 2.4373e-03, 9.4261e-04, 8.9549e-05, 9.7125e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,489][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2872, 0.0255, 0.1116, 0.0226, 0.0274, 0.0100, 0.0333, 0.0533, 0.0124,
        0.0109, 0.4058], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,490][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([3.4783e-02, 6.5652e-04, 2.7070e-03, 1.3047e-03, 9.5100e-03, 1.2266e-02,
        7.2022e-03, 5.0846e-02, 1.3061e-01, 6.7700e-02, 6.8242e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,492][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.3846, 0.0124, 0.0118, 0.0306, 0.0354, 0.0147, 0.0547, 0.0791, 0.0380,
        0.0993, 0.2394], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,494][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1198, 0.0313, 0.1738, 0.0083, 0.0377, 0.1303, 0.0160, 0.1104, 0.1226,
        0.0159, 0.2339], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,495][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.1780, 0.2058, 0.0066, 0.2564, 0.0488, 0.0077, 0.1580, 0.0106, 0.0045,
        0.1192, 0.0043], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,497][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0361, 0.0102, 0.0153, 0.0300, 0.0295, 0.0440, 0.0567, 0.1106, 0.1747,
        0.1507, 0.3423], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,499][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0292, 0.0081, 0.1524, 0.0055, 0.0155, 0.1435, 0.0066, 0.0618, 0.2192,
        0.0120, 0.3463], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,499][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.2294, 0.0747, 0.1190, 0.0567, 0.0675, 0.0828, 0.0484, 0.0956, 0.0886,
        0.0434, 0.0938], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,499][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.1832, 0.0824, 0.1407, 0.0729, 0.0615, 0.0894, 0.0399, 0.1021, 0.0869,
        0.0370, 0.1041], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,500][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.1939, 0.0733, 0.0923, 0.0708, 0.0670, 0.0565, 0.1194, 0.1018, 0.0481,
        0.0754, 0.1015], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,500][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.2208, 0.0560, 0.1322, 0.1008, 0.0280, 0.0420, 0.0703, 0.0774, 0.0587,
        0.0646, 0.1034, 0.0458], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,501][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([3.0836e-04, 5.3137e-01, 9.0610e-04, 8.7356e-04, 1.7259e-04, 8.5809e-05,
        2.6355e-04, 2.0644e-04, 1.2526e-04, 3.5621e-05, 1.2581e-05, 4.6564e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,501][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.2057, 0.1656, 0.0448, 0.0600, 0.0308, 0.0546, 0.0421, 0.0621, 0.0590,
        0.0778, 0.0617, 0.1359], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,501][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([1.1927e-03, 1.7277e-03, 1.4496e-06, 3.2996e-04, 1.8508e-05, 1.8073e-05,
        6.4161e-04, 7.8670e-05, 9.1582e-05, 4.1570e-04, 1.0272e-04, 9.9538e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,502][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0381, 0.0580, 0.0021, 0.0201, 0.0031, 0.0031, 0.0184, 0.0124, 0.0080,
        0.0116, 0.0182, 0.8069], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,502][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([3.3999e-03, 7.3276e-01, 1.3967e-07, 5.7337e-05, 2.9181e-06, 1.4364e-07,
        3.0357e-06, 2.1477e-07, 2.3377e-07, 2.3273e-07, 9.9195e-09, 2.6377e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,504][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.1354, 0.1887, 0.0428, 0.1576, 0.0135, 0.0386, 0.0668, 0.0171, 0.0494,
        0.0851, 0.0524, 0.1526], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,506][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0862, 0.0064, 0.0235, 0.0091, 0.0315, 0.0677, 0.0155, 0.1365, 0.2032,
        0.0247, 0.2745, 0.1211], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,507][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.1992, 0.0851, 0.1120, 0.1093, 0.0397, 0.0783, 0.0331, 0.0650, 0.0810,
        0.0333, 0.0919, 0.0721], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,509][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.2056, 0.0328, 0.1058, 0.1148, 0.0538, 0.0701, 0.0641, 0.0827, 0.0789,
        0.0707, 0.0966, 0.0241], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,511][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.1129, 0.2552, 0.0868, 0.0583, 0.0338, 0.0483, 0.0181, 0.0518, 0.0547,
        0.0197, 0.0545, 0.2060], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,513][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0993, 0.0693, 0.0623, 0.0596, 0.0934, 0.0560, 0.1730, 0.0823, 0.0432,
        0.0918, 0.0713, 0.0984], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,514][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.2556, 0.0955, 0.0284, 0.1150, 0.0538, 0.0255, 0.0584, 0.0300, 0.0312,
        0.0292, 0.0345, 0.1362, 0.1068], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,515][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([4.2169e-04, 4.4199e-04, 2.0352e-04, 4.0959e-04, 1.2760e-02, 4.1480e-04,
        1.9192e-04, 1.3321e-04, 9.4558e-05, 2.4166e-04, 5.3965e-05, 9.0686e-05,
        9.8454e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,516][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.2837, 0.0511, 0.0870, 0.0239, 0.1352, 0.0674, 0.0221, 0.0727, 0.0719,
        0.0450, 0.0326, 0.0415, 0.0659], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,516][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([9.1929e-04, 6.1898e-06, 8.9540e-06, 1.5689e-05, 9.8156e-05, 2.6819e-05,
        3.7537e-05, 1.0291e-04, 1.5232e-04, 5.9394e-04, 9.5944e-04, 6.1310e-04,
        9.9647e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,516][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0392, 0.0021, 0.0049, 0.0016, 0.0077, 0.0070, 0.0045, 0.0235, 0.0145,
        0.0314, 0.0263, 0.0147, 0.8226], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,517][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([1.1287e-02, 3.4411e-05, 4.4807e-05, 2.7448e-05, 2.1542e-03, 5.7195e-06,
        2.0245e-05, 1.9074e-05, 1.8325e-06, 1.0444e-05, 1.9814e-06, 1.4927e-06,
        9.8639e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,517][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.1196, 0.1658, 0.0164, 0.1446, 0.0320, 0.0206, 0.0803, 0.0145, 0.0133,
        0.0752, 0.0130, 0.2222, 0.0826], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,518][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0360, 0.0068, 0.0101, 0.0070, 0.0180, 0.0335, 0.0091, 0.0597, 0.0930,
        0.0684, 0.2417, 0.2283, 0.1884], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,518][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.1449, 0.0345, 0.1143, 0.0238, 0.0783, 0.1156, 0.0446, 0.0944, 0.1502,
        0.0396, 0.0949, 0.0403, 0.0246], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,519][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.1934, 0.0674, 0.0867, 0.0436, 0.0721, 0.0658, 0.0681, 0.0686, 0.0719,
        0.0509, 0.0803, 0.0730, 0.0582], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,519][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.1781, 0.0412, 0.0813, 0.0421, 0.0751, 0.0684, 0.0413, 0.0581, 0.0683,
        0.0267, 0.0650, 0.0291, 0.2253], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,520][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.3053, 0.0514, 0.0753, 0.0446, 0.0436, 0.0407, 0.0730, 0.1010, 0.0304,
        0.0487, 0.0815, 0.0487, 0.0556], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,522][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.2012, 0.0725, 0.0196, 0.0479, 0.0659, 0.0190, 0.1358, 0.0172, 0.0261,
        0.0517, 0.0229, 0.1073, 0.1988, 0.0141], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,523][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([5.7887e-03, 3.3028e-04, 3.9632e-02, 1.6698e-04, 4.6664e-04, 1.6731e-03,
        2.2303e-05, 1.9503e-02, 1.9043e-03, 1.9527e-04, 8.6275e-03, 1.1570e-04,
        2.9689e-05, 9.2154e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,525][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.2047, 0.0305, 0.1038, 0.0252, 0.0564, 0.0271, 0.0234, 0.1419, 0.0299,
        0.0165, 0.0645, 0.0250, 0.0699, 0.1812], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,525][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([6.0945e-03, 7.7714e-05, 2.8041e-04, 5.1433e-05, 8.1558e-04, 1.6076e-03,
        2.2169e-03, 3.4899e-03, 1.5014e-02, 2.9226e-03, 3.6892e-02, 6.9482e-03,
        9.8749e-02, 8.2484e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,526][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([1.2015e-02, 6.3910e-04, 1.3429e-03, 8.9401e-04, 1.9471e-02, 3.7131e-03,
        1.1110e-02, 7.7913e-03, 6.1797e-03, 1.3606e-02, 9.1281e-03, 7.4217e-03,
        8.0850e-01, 9.8183e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,528][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0435, 0.0160, 0.0889, 0.0018, 0.0494, 0.2118, 0.0006, 0.0207, 0.1741,
        0.0019, 0.0177, 0.0035, 0.0057, 0.3645], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,530][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0923, 0.0739, 0.0050, 0.0711, 0.0434, 0.0042, 0.0951, 0.0181, 0.0034,
        0.0766, 0.0042, 0.1086, 0.1329, 0.2713], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,532][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0272, 0.0032, 0.0057, 0.0094, 0.0096, 0.0115, 0.0202, 0.0247, 0.0410,
        0.0295, 0.1085, 0.1088, 0.3360, 0.2647], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,533][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0165, 0.0053, 0.0990, 0.0049, 0.0165, 0.1334, 0.0075, 0.0485, 0.2130,
        0.0122, 0.1784, 0.0098, 0.0075, 0.2474], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,533][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1543, 0.0541, 0.0928, 0.0429, 0.0539, 0.0753, 0.0370, 0.0776, 0.0813,
        0.0391, 0.0774, 0.0527, 0.0569, 0.1048], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,534][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1247, 0.0568, 0.1123, 0.0396, 0.0645, 0.0929, 0.0261, 0.0849, 0.0846,
        0.0282, 0.0788, 0.0436, 0.0464, 0.1166], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,534][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1649, 0.0515, 0.0767, 0.0600, 0.0607, 0.0488, 0.0818, 0.0777, 0.0370,
        0.0612, 0.0799, 0.0567, 0.0648, 0.0784], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,534][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.2231, 0.0495, 0.0278, 0.0609, 0.0631, 0.0242, 0.1142, 0.0169, 0.0279,
        0.0645, 0.0243, 0.0603, 0.1734, 0.0255, 0.0443], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,535][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([6.2516e-04, 3.0762e-04, 5.1225e-04, 3.5901e-04, 4.3849e-03, 4.6103e-04,
        2.4164e-04, 7.7518e-04, 2.1335e-04, 4.5927e-05, 1.6727e-04, 9.1817e-05,
        1.3265e-03, 5.6389e-04, 9.8992e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,535][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.1944, 0.0610, 0.0530, 0.0258, 0.0895, 0.0420, 0.0213, 0.0794, 0.0517,
        0.0161, 0.0342, 0.0593, 0.1097, 0.1121, 0.0505], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,536][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([8.9584e-04, 3.1165e-06, 4.9657e-06, 2.3225e-06, 3.2025e-05, 3.7221e-05,
        1.1770e-05, 6.5378e-05, 2.1131e-04, 8.8541e-05, 9.0878e-04, 7.1710e-04,
        8.8054e-03, 8.6061e-03, 9.7961e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,536][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0442, 0.0047, 0.0063, 0.0041, 0.0099, 0.0096, 0.0054, 0.0235, 0.0161,
        0.0123, 0.0277, 0.0279, 0.1301, 0.1225, 0.5557], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,537][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([9.2212e-03, 1.4511e-03, 5.0097e-05, 2.9705e-04, 1.4818e-03, 4.0151e-05,
        5.2019e-06, 3.3102e-05, 2.2208e-05, 3.9064e-07, 3.1397e-06, 1.2010e-04,
        1.6457e-04, 8.3867e-06, 9.8710e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,539][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.1051, 0.1305, 0.0114, 0.1204, 0.0258, 0.0127, 0.1236, 0.0107, 0.0109,
        0.1239, 0.0110, 0.2023, 0.0797, 0.0101, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,540][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0256, 0.0032, 0.0057, 0.0028, 0.0087, 0.0123, 0.0038, 0.0244, 0.0318,
        0.0154, 0.0877, 0.0796, 0.1470, 0.3147, 0.2372], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,542][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0732, 0.0221, 0.0867, 0.0271, 0.0719, 0.0711, 0.0980, 0.0630, 0.0905,
        0.0377, 0.0943, 0.0311, 0.0407, 0.1498, 0.0428], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,543][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.1280, 0.0542, 0.0740, 0.0441, 0.0596, 0.0633, 0.0616, 0.0614, 0.0639,
        0.0450, 0.0692, 0.0604, 0.0696, 0.0912, 0.0544], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,545][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.1075, 0.0358, 0.0699, 0.0306, 0.0767, 0.0609, 0.0391, 0.0673, 0.0579,
        0.0238, 0.0610, 0.0336, 0.0624, 0.0816, 0.1920], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,547][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.2538, 0.0533, 0.0868, 0.0431, 0.0503, 0.0255, 0.0500, 0.0763, 0.0196,
        0.0510, 0.0612, 0.0482, 0.0478, 0.0601, 0.0730], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,549][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.2524, 0.0488, 0.0185, 0.0750, 0.0440, 0.0115, 0.1427, 0.0149, 0.0169,
        0.0396, 0.0209, 0.0666, 0.1338, 0.0129, 0.0676, 0.0340],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,550][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([3.1317e-03, 6.1588e-04, 2.0967e-03, 8.8570e-04, 2.5781e-04, 3.3384e-03,
        1.1917e-05, 6.0135e-04, 1.0975e-02, 8.6500e-05, 7.2891e-04, 2.5062e-04,
        1.1504e-04, 3.1578e-03, 4.1402e-04, 9.7333e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,550][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.1725, 0.0228, 0.0823, 0.0343, 0.1116, 0.0250, 0.0246, 0.0678, 0.0260,
        0.0243, 0.0456, 0.0202, 0.1343, 0.0994, 0.0759, 0.0334],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,551][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([8.5237e-04, 9.6140e-06, 1.4932e-05, 1.1945e-05, 5.9171e-05, 1.0297e-04,
        3.5429e-05, 2.5968e-04, 8.7368e-04, 2.9672e-04, 3.2492e-03, 1.7290e-03,
        9.4331e-03, 1.8062e-02, 7.2820e-02, 8.9219e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,551][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0459, 0.0023, 0.0056, 0.0026, 0.0160, 0.0062, 0.0133, 0.0131, 0.0134,
        0.0158, 0.0248, 0.0160, 0.2740, 0.0924, 0.1944, 0.2642],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,552][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([2.6345e-02, 1.2317e-02, 4.1338e-03, 8.0066e-04, 8.0145e-03, 6.5092e-03,
        1.6163e-03, 1.8759e-03, 8.2229e-03, 4.4234e-04, 4.4484e-04, 1.0161e-03,
        1.0171e-03, 1.8960e-03, 5.5827e-04, 9.2479e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,552][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0885, 0.1103, 0.0045, 0.1973, 0.0333, 0.0044, 0.1025, 0.0048, 0.0029,
        0.0785, 0.0037, 0.2104, 0.1065, 0.0062, 0.0402, 0.0058],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,553][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0257, 0.0018, 0.0040, 0.0056, 0.0060, 0.0067, 0.0070, 0.0162, 0.0188,
        0.0118, 0.0606, 0.0434, 0.0689, 0.1727, 0.2946, 0.2562],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,553][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0308, 0.0064, 0.0850, 0.0127, 0.0331, 0.1310, 0.0130, 0.0405, 0.1815,
        0.0156, 0.1210, 0.0100, 0.0146, 0.1109, 0.0256, 0.1682],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,553][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.1544, 0.0510, 0.0753, 0.0450, 0.0502, 0.0574, 0.0411, 0.0599, 0.0641,
        0.0273, 0.0658, 0.0531, 0.0457, 0.0844, 0.0571, 0.0683],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,554][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.1079, 0.0444, 0.0677, 0.0402, 0.0369, 0.0659, 0.0188, 0.0500, 0.0832,
        0.0233, 0.0541, 0.0399, 0.0389, 0.0658, 0.0412, 0.2219],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,556][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.1984, 0.0536, 0.0565, 0.0492, 0.0418, 0.0344, 0.0726, 0.0632, 0.0299,
        0.0571, 0.0670, 0.0551, 0.0529, 0.0592, 0.0623, 0.0468],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,558][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1789, 0.0637, 0.0160, 0.0426, 0.0575, 0.0161, 0.1179, 0.0137, 0.0218,
        0.0441, 0.0185, 0.0952, 0.1722, 0.0114, 0.0675, 0.0491, 0.0136],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,559][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([2.8635e-03, 1.4691e-04, 1.7566e-02, 7.8946e-05, 2.1392e-04, 7.1490e-04,
        9.3591e-06, 8.8250e-03, 8.9387e-04, 9.7976e-05, 4.6294e-03, 6.0760e-05,
        1.4754e-05, 4.6279e-01, 4.0134e-04, 6.5253e-04, 5.0004e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,560][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1559, 0.0232, 0.0780, 0.0209, 0.0444, 0.0203, 0.0187, 0.1075, 0.0225,
        0.0132, 0.0492, 0.0202, 0.0559, 0.1374, 0.0576, 0.0280, 0.1470],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,562][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.6393e-03, 1.1898e-05, 4.0767e-05, 7.0633e-06, 8.5001e-05, 1.4143e-04,
        2.2940e-04, 2.4131e-04, 9.9440e-04, 2.2160e-04, 2.5217e-03, 4.4959e-04,
        6.2107e-03, 4.7906e-02, 4.0158e-01, 9.8645e-02, 4.3908e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,564][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0134, 0.0006, 0.0012, 0.0007, 0.0147, 0.0025, 0.0078, 0.0046, 0.0035,
        0.0078, 0.0050, 0.0039, 0.4300, 0.0498, 0.2331, 0.0619, 0.1596],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,565][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0267, 0.0110, 0.0608, 0.0012, 0.0354, 0.1531, 0.0004, 0.0153, 0.1242,
        0.0013, 0.0113, 0.0024, 0.0039, 0.2561, 0.0089, 0.0520, 0.2360],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,567][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0653, 0.0509, 0.0033, 0.0510, 0.0305, 0.0028, 0.0666, 0.0126, 0.0022,
        0.0536, 0.0028, 0.0778, 0.0958, 0.1928, 0.0518, 0.0073, 0.2330],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,567][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0210, 0.0017, 0.0032, 0.0052, 0.0043, 0.0045, 0.0083, 0.0080, 0.0127,
        0.0093, 0.0336, 0.0308, 0.0964, 0.0711, 0.1477, 0.2616, 0.2806],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,568][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0118, 0.0037, 0.0693, 0.0036, 0.0119, 0.0918, 0.0055, 0.0336, 0.1445,
        0.0086, 0.1246, 0.0072, 0.0054, 0.1712, 0.0139, 0.0885, 0.2048],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,568][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1204, 0.0426, 0.0715, 0.0353, 0.0432, 0.0581, 0.0304, 0.0595, 0.0623,
        0.0322, 0.0604, 0.0429, 0.0464, 0.0807, 0.0547, 0.0711, 0.0883],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,569][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0893, 0.0405, 0.0790, 0.0315, 0.0502, 0.0699, 0.0217, 0.0662, 0.0659,
        0.0245, 0.0642, 0.0380, 0.0404, 0.0970, 0.0484, 0.0750, 0.0984],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,569][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1499, 0.0445, 0.0675, 0.0523, 0.0538, 0.0429, 0.0656, 0.0655, 0.0317,
        0.0499, 0.0638, 0.0450, 0.0527, 0.0644, 0.0514, 0.0338, 0.0654],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,571][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:36,572][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[41185],
        [ 4698],
        [28927],
        [    1],
        [15566],
        [15384],
        [19894],
        [32665],
        [25734],
        [ 9110],
        [ 8974],
        [ 4308],
        [31668],
        [26615],
        [16753],
        [44767],
        [29191]], device='cuda:0')
[2024-07-24 10:17:36,574][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[38652],
        [ 1097],
        [44336],
        [    1],
        [47715],
        [42097],
        [44754],
        [45396],
        [42563],
        [43302],
        [45245],
        [  616],
        [47463],
        [45217],
        [42344],
        [38028],
        [44223]], device='cuda:0')
[2024-07-24 10:17:36,575][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[16089],
        [15050],
        [13184],
        [ 7002],
        [11278],
        [10317],
        [ 8516],
        [ 6509],
        [ 5836],
        [ 8002],
        [ 6449],
        [ 9079],
        [ 7408],
        [ 6854],
        [ 7980],
        [ 7040],
        [ 8481]], device='cuda:0')
[2024-07-24 10:17:36,577][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[34004],
        [27259],
        [30549],
        [ 8656],
        [42723],
        [47996],
        [32250],
        [46909],
        [46486],
        [37423],
        [15856],
        [28887],
        [44309],
        [40489],
        [23158],
        [43321],
        [40542]], device='cuda:0')
[2024-07-24 10:17:36,578][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 2588],
        [ 2441],
        [ 7410],
        [ 1622],
        [ 3617],
        [ 5083],
        [ 3810],
        [ 7635],
        [ 7551],
        [ 1451],
        [ 8998],
        [ 2054],
        [ 6730],
        [12797],
        [11188],
        [12520],
        [14881]], device='cuda:0')
[2024-07-24 10:17:36,581][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[41768],
        [ 1414],
        [18272],
        [  122],
        [ 1984],
        [ 2862],
        [32574],
        [28503],
        [15779],
        [11572],
        [12326],
        [  489],
        [19523],
        [ 2920],
        [16082],
        [22090],
        [ 5833]], device='cuda:0')
[2024-07-24 10:17:36,582][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[19844],
        [39587],
        [22032],
        [  791],
        [21430],
        [11108],
        [27522],
        [10276],
        [ 7109],
        [42257],
        [ 8847],
        [40307],
        [33153],
        [31409],
        [19186],
        [20628],
        [22563]], device='cuda:0')
[2024-07-24 10:17:36,584][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[38320],
        [40255],
        [39545],
        [39965],
        [43119],
        [38950],
        [ 8951],
        [37780],
        [40338],
        [47316],
        [40211],
        [39326],
        [44237],
        [37702],
        [44420],
        [43459],
        [36996]], device='cuda:0')
[2024-07-24 10:17:36,585][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[47476],
        [43209],
        [39302],
        [12061],
        [ 2022],
        [ 1008],
        [ 4541],
        [ 1403],
        [  426],
        [ 7717],
        [ 2439],
        [ 9121],
        [ 7410],
        [17371],
        [12275],
        [ 3753],
        [18013]], device='cuda:0')
[2024-07-24 10:17:36,586][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[11321],
        [10766],
        [15737],
        [17071],
        [ 7129],
        [ 7765],
        [16801],
        [10804],
        [18471],
        [18848],
        [12529],
        [17266],
        [ 7366],
        [12307],
        [17378],
        [15578],
        [21908]], device='cuda:0')
[2024-07-24 10:17:36,587][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[11058],
        [ 9926],
        [25297],
        [ 8345],
        [13537],
        [23681],
        [12871],
        [21676],
        [25483],
        [ 7096],
        [33586],
        [10856],
        [18362],
        [29375],
        [16718],
        [22799],
        [26481]], device='cuda:0')
[2024-07-24 10:17:36,588][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[16924],
        [15274],
        [18639],
        [16776],
        [18771],
        [17423],
        [15047],
        [19478],
        [18398],
        [14255],
        [20316],
        [17577],
        [19282],
        [23077],
        [22899],
        [21528],
        [23755]], device='cuda:0')
[2024-07-24 10:17:36,590][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[12569],
        [35275],
        [15925],
        [35958],
        [ 6135],
        [35148],
        [24149],
        [24312],
        [39149],
        [34970],
        [29567],
        [47213],
        [20598],
        [27011],
        [19133],
        [46924],
        [30108]], device='cuda:0')
[2024-07-24 10:17:36,591][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[43672],
        [40348],
        [38623],
        [30410],
        [35406],
        [37897],
        [28784],
        [28865],
        [36407],
        [33192],
        [32056],
        [25197],
        [35471],
        [29108],
        [30834],
        [31501],
        [29090]], device='cuda:0')
[2024-07-24 10:17:36,593][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[32233],
        [ 9546],
        [35354],
        [    4],
        [39837],
        [31499],
        [31292],
        [31476],
        [39463],
        [38441],
        [13287],
        [ 9322],
        [46354],
        [41894],
        [36338],
        [48667],
        [41703]], device='cuda:0')
[2024-07-24 10:17:36,595][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[18395],
        [16802],
        [17601],
        [13950],
        [17349],
        [14656],
        [24434],
        [22322],
        [15744],
        [22437],
        [17223],
        [24600],
        [13943],
        [17186],
        [19155],
        [18460],
        [19588]], device='cuda:0')
[2024-07-24 10:17:36,596][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[27723],
        [ 6955],
        [ 3676],
        [ 9000],
        [13056],
        [ 1634],
        [22449],
        [ 2661],
        [ 2988],
        [30004],
        [11653],
        [13020],
        [31768],
        [ 6013],
        [28340],
        [15241],
        [ 7866]], device='cuda:0')
[2024-07-24 10:17:36,598][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[45656],
        [44960],
        [43767],
        [44779],
        [45901],
        [45385],
        [44617],
        [43943],
        [43723],
        [42416],
        [35132],
        [37200],
        [44451],
        [38667],
        [43184],
        [44285],
        [37855]], device='cuda:0')
[2024-07-24 10:17:36,600][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[31685],
        [44485],
        [37939],
        [32961],
        [30005],
        [28824],
        [12521],
        [32827],
        [27205],
        [24375],
        [25856],
        [38265],
        [28652],
        [36643],
        [31061],
        [30697],
        [36308]], device='cuda:0')
[2024-07-24 10:17:36,601][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[17268],
        [ 6565],
        [15234],
        [13865],
        [16660],
        [18931],
        [34684],
        [25574],
        [24945],
        [45923],
        [30572],
        [ 4355],
        [18697],
        [19072],
        [21661],
        [23068],
        [22573]], device='cuda:0')
[2024-07-24 10:17:36,603][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[20979],
        [27189],
        [40773],
        [15202],
        [24456],
        [42665],
        [25976],
        [43951],
        [41999],
        [10500],
        [44117],
        [29386],
        [26140],
        [45169],
        [35274],
        [22855],
        [45440]], device='cuda:0')
[2024-07-24 10:17:36,604][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[28431],
        [33238],
        [30553],
        [10735],
        [ 6687],
        [ 6320],
        [ 4117],
        [ 1704],
        [ 1742],
        [ 2743],
        [ 2932],
        [ 8087],
        [ 5672],
        [ 7118],
        [ 4452],
        [ 3507],
        [20584]], device='cuda:0')
[2024-07-24 10:17:36,605][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[43259],
        [43611],
        [40881],
        [39251],
        [36186],
        [27827],
        [17907],
        [31120],
        [23433],
        [23335],
        [31015],
        [24140],
        [32441],
        [25516],
        [23042],
        [25478],
        [22091]], device='cuda:0')
[2024-07-24 10:17:36,606][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[20527],
        [21423],
        [ 8871],
        [13944],
        [11481],
        [ 8302],
        [27342],
        [ 6837],
        [ 6054],
        [ 6991],
        [ 3980],
        [ 8531],
        [ 6876],
        [ 4089],
        [ 7076],
        [ 6567],
        [ 4641]], device='cuda:0')
[2024-07-24 10:17:36,607][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[45880],
        [45022],
        [36458],
        [35130],
        [37299],
        [31944],
        [33497],
        [31514],
        [29005],
        [28836],
        [25554],
        [26095],
        [27552],
        [24084],
        [26244],
        [26783],
        [24917]], device='cuda:0')
[2024-07-24 10:17:36,609][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[48846],
        [48199],
        [50003],
        [49326],
        [50069],
        [49767],
        [49282],
        [50027],
        [49668],
        [49511],
        [49982],
        [49086],
        [50079],
        [50017],
        [50089],
        [49456],
        [49981]], device='cuda:0')
[2024-07-24 10:17:36,611][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 6699],
        [26744],
        [21763],
        [28949],
        [23648],
        [18748],
        [29269],
        [28013],
        [21392],
        [25667],
        [29738],
        [31240],
        [29310],
        [30737],
        [31408],
        [28023],
        [27353]], device='cuda:0')
[2024-07-24 10:17:36,612][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 3408],
        [ 3817],
        [ 4126],
        [ 7714],
        [ 5458],
        [ 9518],
        [ 6494],
        [ 8920],
        [12359],
        [ 7668],
        [ 8835],
        [10342],
        [ 7127],
        [ 7068],
        [ 7386],
        [ 9438],
        [ 5714]], device='cuda:0')
[2024-07-24 10:17:36,614][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[15537],
        [35972],
        [11558],
        [50246],
        [ 8188],
        [14303],
        [15346],
        [15383],
        [ 7490],
        [ 9191],
        [31759],
        [36536],
        [ 2813],
        [ 6127],
        [10870],
        [ 1458],
        [ 6228]], device='cuda:0')
[2024-07-24 10:17:36,616][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841],
        [7841]], device='cuda:0')
[2024-07-24 10:17:36,641][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:36,643][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,644][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,645][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,646][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,648][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,649][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,651][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,652][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,653][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,653][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,654][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,654][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,654][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0827, 0.9173], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,655][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.6278, 0.3722], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,655][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.5622, 0.4378], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,655][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.7442, 0.2558], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,656][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.2249, 0.7751], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,656][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0057, 0.9943], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,656][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.9735, 0.0265], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,657][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.9301, 0.0699], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,657][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.9611, 0.0389], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,659][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.9975, 0.0025], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,659][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.1537, 0.8463], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,659][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0832, 0.9168], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,659][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0569, 0.7154, 0.2277], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,660][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.1056, 0.8224, 0.0720], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,660][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4000, 0.2932, 0.3068], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,660][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.5411, 0.1375, 0.3214], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,661][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1429, 0.5031, 0.3540], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,661][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0080, 0.1049, 0.8871], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,661][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.4688, 0.0846, 0.4466], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,662][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.7847, 0.1272, 0.0881], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,662][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.8369, 0.0657, 0.0974], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,662][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.7683, 0.0086, 0.2231], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,663][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1047, 0.5538, 0.3415], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,664][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.6215, 0.0039, 0.3746], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,665][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.0271, 0.5738, 0.2354, 0.1636], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,667][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.0571, 0.4121, 0.4784, 0.0524], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,668][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.3138, 0.2111, 0.2246, 0.2505], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,670][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.4726, 0.1184, 0.2919, 0.1170], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,672][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0968, 0.3544, 0.2631, 0.2858], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,673][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0119, 0.5852, 0.0579, 0.3450], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,675][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.7226, 0.0332, 0.2231, 0.0210], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,675][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.5303, 0.2337, 0.1318, 0.1042], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,676][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.7113, 0.0325, 0.2285, 0.0277], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,676][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.6572, 0.0350, 0.2877, 0.0201], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,676][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.0781, 0.3979, 0.2474, 0.2765], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,677][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0081, 0.0017, 0.0032, 0.9870], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,677][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0322, 0.2427, 0.1629, 0.1132, 0.4490], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,677][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0836, 0.4088, 0.1135, 0.2378, 0.1563], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,678][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.2491, 0.1715, 0.1818, 0.2010, 0.1965], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,678][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.4090, 0.1006, 0.2521, 0.1030, 0.1353], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,678][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0704, 0.2563, 0.1922, 0.2165, 0.2646], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,679][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ got] are: tensor([1.0665e-03, 1.2731e-01, 2.0603e-04, 1.1229e-02, 8.6019e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,680][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.5041, 0.0265, 0.2654, 0.0192, 0.1848], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,681][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.5946, 0.1332, 0.0711, 0.1270, 0.0741], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,683][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.6562, 0.0656, 0.1177, 0.0782, 0.0823], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,684][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.6194, 0.0518, 0.2466, 0.0466, 0.0357], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,686][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0573, 0.2914, 0.1806, 0.2110, 0.2597], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,687][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ got] are: tensor([2.0286e-02, 4.1786e-04, 1.0133e-02, 7.2386e-04, 9.6844e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,689][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0151, 0.1643, 0.0785, 0.0815, 0.3216, 0.3390], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,690][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.1445, 0.3468, 0.0925, 0.1420, 0.2223, 0.0518], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,692][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2073, 0.1450, 0.1539, 0.1710, 0.1677, 0.1550], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,693][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.3386, 0.0865, 0.2053, 0.0903, 0.1180, 0.1613], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,693][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0625, 0.2061, 0.1567, 0.1809, 0.2205, 0.1733], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,693][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([2.9855e-04, 5.5107e-01, 9.7847e-03, 4.0332e-01, 3.3191e-02, 2.3373e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,694][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1544, 0.0490, 0.2687, 0.0280, 0.2656, 0.2344], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,694][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.5161, 0.1123, 0.0661, 0.1085, 0.0974, 0.0996], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,694][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.4969, 0.0878, 0.0759, 0.0821, 0.0516, 0.2057], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,695][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.6065, 0.0331, 0.1789, 0.0384, 0.0265, 0.1166], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,695][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0478, 0.2440, 0.1528, 0.1844, 0.2229, 0.1481], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,696][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([3.1322e-02, 3.8083e-04, 9.0957e-03, 1.5031e-03, 3.8328e-03, 9.5387e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,696][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.0055, 0.0570, 0.0319, 0.0588, 0.2563, 0.4554, 0.1352],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,696][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0389, 0.1511, 0.1280, 0.1321, 0.2738, 0.2008, 0.0754],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,697][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.1852, 0.1214, 0.1305, 0.1442, 0.1407, 0.1307, 0.1474],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,699][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.3026, 0.0876, 0.1968, 0.0843, 0.1102, 0.1613, 0.0571],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,701][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0442, 0.1720, 0.1358, 0.1403, 0.1760, 0.1407, 0.1911],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,702][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([4.7883e-06, 6.8835e-03, 3.6774e-06, 2.8111e-02, 2.3442e-02, 1.0407e-03,
        9.4051e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,703][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.5549, 0.0106, 0.1304, 0.0091, 0.0843, 0.1903, 0.0204],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,705][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.4023, 0.1127, 0.0679, 0.0790, 0.1733, 0.1341, 0.0307],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,706][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.4373, 0.0344, 0.0960, 0.0423, 0.0778, 0.2442, 0.0679],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,708][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.3632, 0.0819, 0.1462, 0.0252, 0.0816, 0.2817, 0.0203],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,710][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.0429, 0.2084, 0.1301, 0.1501, 0.1793, 0.1221, 0.1671],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,712][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0424, 0.0018, 0.0292, 0.0019, 0.0024, 0.0085, 0.9137],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,713][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0104, 0.0870, 0.0580, 0.0576, 0.1774, 0.2691, 0.2782, 0.0623],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,715][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0260, 0.1651, 0.0604, 0.0998, 0.2154, 0.1095, 0.3064, 0.0175],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,715][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1593, 0.1075, 0.1146, 0.1261, 0.1235, 0.1150, 0.1287, 0.1252],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,715][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.2976, 0.0701, 0.1680, 0.0715, 0.0939, 0.1274, 0.0393, 0.1321],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,716][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0436, 0.1455, 0.1086, 0.1252, 0.1548, 0.1186, 0.1736, 0.1301],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,716][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ at] are: tensor([5.9887e-03, 8.0962e-02, 1.5419e-03, 2.3244e-03, 9.7292e-04, 6.0723e-04,
        8.9084e-01, 1.6762e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,717][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1046, 0.0372, 0.1806, 0.0200, 0.1659, 0.1483, 0.0612, 0.2823],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,717][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.3980, 0.0989, 0.0518, 0.0871, 0.0733, 0.0651, 0.1530, 0.0728],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,717][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.4213, 0.0922, 0.0587, 0.0733, 0.0565, 0.1634, 0.0802, 0.0544],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,718][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.5831, 0.0267, 0.1267, 0.0426, 0.0267, 0.1113, 0.0346, 0.0483],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,718][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0346, 0.1781, 0.1104, 0.1321, 0.1605, 0.1042, 0.1571, 0.1230],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,718][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ at] are: tensor([1.9348e-02, 1.2790e-03, 8.0992e-03, 1.5438e-04, 2.5582e-03, 3.6116e-03,
        1.2855e-03, 9.6366e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,719][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0107, 0.0889, 0.0420, 0.0500, 0.1306, 0.2583, 0.1977, 0.1109, 0.1109],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,721][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0433, 0.1301, 0.0803, 0.0772, 0.1702, 0.0870, 0.1543, 0.2309, 0.0267],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,723][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1406, 0.0958, 0.1021, 0.1132, 0.1104, 0.1025, 0.1160, 0.1118, 0.1076],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,724][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2417, 0.0594, 0.1440, 0.0649, 0.0843, 0.1114, 0.0357, 0.1170, 0.1416],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,726][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0375, 0.1282, 0.0925, 0.1132, 0.1359, 0.1023, 0.1613, 0.1099, 0.1191],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,727][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.9160e-05, 7.1073e-02, 1.6030e-04, 2.7605e-02, 1.1821e-03, 2.8451e-05,
        8.9945e-01, 8.5163e-08, 4.7528e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,729][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0707, 0.0356, 0.1591, 0.0189, 0.1595, 0.1218, 0.0667, 0.2462, 0.1215],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,730][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.3375, 0.0850, 0.0425, 0.0889, 0.0596, 0.0662, 0.1277, 0.0719, 0.1206],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,732][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3484, 0.0948, 0.0599, 0.0687, 0.0463, 0.1446, 0.0771, 0.0549, 0.1053],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,732][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4922, 0.0284, 0.1364, 0.0379, 0.0297, 0.0814, 0.0547, 0.0342, 0.1052],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,733][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0312, 0.1583, 0.0986, 0.1209, 0.1442, 0.0954, 0.1420, 0.1086, 0.1008],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,733][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0284, 0.0021, 0.0129, 0.0038, 0.0041, 0.0289, 0.0139, 0.0192, 0.8868],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,733][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0033, 0.0658, 0.0286, 0.0294, 0.1552, 0.2816, 0.1879, 0.0899, 0.1315,
        0.0267], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,734][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0152, 0.0894, 0.0547, 0.1026, 0.2194, 0.1418, 0.1088, 0.1943, 0.0656,
        0.0082], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,734][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.1341, 0.0838, 0.0903, 0.0992, 0.0969, 0.0903, 0.1015, 0.0994, 0.0953,
        0.1092], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,735][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.2612, 0.0550, 0.1339, 0.0520, 0.0705, 0.1025, 0.0304, 0.1043, 0.1316,
        0.0585], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,735][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0298, 0.1144, 0.0864, 0.0928, 0.1166, 0.0925, 0.1330, 0.1001, 0.1100,
        0.1245], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,735][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ station] are: tensor([2.8311e-04, 6.8643e-03, 2.9666e-04, 1.2542e-03, 1.1624e-02, 5.5826e-03,
        3.1368e-01, 5.1496e-02, 1.2910e-02, 5.9601e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,736][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.1981, 0.0082, 0.0941, 0.0066, 0.0727, 0.1253, 0.0197, 0.2626, 0.1639,
        0.0488], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,737][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.2682, 0.1181, 0.0505, 0.0563, 0.0382, 0.0566, 0.1625, 0.0779, 0.0970,
        0.0747], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,738][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.2620, 0.0522, 0.0773, 0.0377, 0.1046, 0.1494, 0.0654, 0.0649, 0.1519,
        0.0346], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,740][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.3795, 0.0428, 0.0675, 0.0379, 0.0944, 0.0913, 0.0550, 0.1059, 0.1063,
        0.0194], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,742][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0279, 0.1450, 0.0906, 0.1043, 0.1273, 0.0840, 0.1195, 0.0970, 0.0872,
        0.1171], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,742][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ station] are: tensor([5.1960e-03, 4.1028e-04, 4.1690e-03, 5.0398e-04, 8.0700e-04, 8.8584e-04,
        3.3003e-03, 1.3931e-02, 1.4935e-03, 9.6930e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,744][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0136, 0.0663, 0.0333, 0.0460, 0.0938, 0.2247, 0.1573, 0.1115, 0.1236,
        0.0504, 0.0794], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,746][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0297, 0.0851, 0.0568, 0.1456, 0.1841, 0.0619, 0.2274, 0.0973, 0.0456,
        0.0349, 0.0318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,748][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1153, 0.0773, 0.0825, 0.0911, 0.0888, 0.0826, 0.0932, 0.0900, 0.0865,
        0.1002, 0.0925], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,749][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1949, 0.0473, 0.1136, 0.0519, 0.0670, 0.0876, 0.0275, 0.0923, 0.1104,
        0.0588, 0.1486], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,750][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0311, 0.1035, 0.0754, 0.0910, 0.1103, 0.0810, 0.1211, 0.0882, 0.0931,
        0.1149, 0.0904], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,750][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [,] are: tensor([5.4179e-07, 4.9763e-04, 4.8475e-05, 1.0822e-02, 2.1876e-04, 3.8436e-06,
        8.5288e-01, 8.7910e-09, 1.1830e-04, 1.3541e-01, 1.0955e-06],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,750][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0695, 0.0210, 0.1139, 0.0135, 0.1102, 0.1024, 0.0476, 0.2012, 0.1074,
        0.0897, 0.1236], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,751][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.2490, 0.0780, 0.0307, 0.0789, 0.0550, 0.0405, 0.1472, 0.0440, 0.0747,
        0.1372, 0.0648], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,751][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.2723, 0.0771, 0.0544, 0.0607, 0.0596, 0.1200, 0.0870, 0.0414, 0.0842,
        0.0673, 0.0762], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,752][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.4114, 0.0320, 0.0995, 0.0428, 0.0270, 0.0614, 0.0498, 0.0360, 0.0809,
        0.0244, 0.1348], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,752][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0247, 0.1298, 0.0806, 0.0971, 0.1191, 0.0765, 0.1147, 0.0905, 0.0801,
        0.1123, 0.0745], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,752][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.1336, 0.0248, 0.0437, 0.0654, 0.0347, 0.0406, 0.0643, 0.0468, 0.0555,
        0.1148, 0.3758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,753][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0031, 0.0525, 0.0238, 0.0265, 0.1141, 0.1921, 0.1527, 0.0511, 0.1184,
        0.0325, 0.1164, 0.1169], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,754][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0222, 0.0218, 0.0306, 0.0544, 0.2369, 0.1591, 0.1571, 0.1309, 0.0585,
        0.0244, 0.0569, 0.0473], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,755][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.1115, 0.0693, 0.0748, 0.0830, 0.0807, 0.0744, 0.0845, 0.0825, 0.0787,
        0.0915, 0.0843, 0.0849], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,757][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.1957, 0.0429, 0.1121, 0.0444, 0.0595, 0.0830, 0.0231, 0.0871, 0.1068,
        0.0509, 0.1459, 0.0486], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,759][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0247, 0.0909, 0.0715, 0.0788, 0.0959, 0.0756, 0.1053, 0.0817, 0.0884,
        0.1033, 0.0873, 0.0966], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,760][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([2.0442e-04, 8.7962e-02, 1.7093e-03, 3.7398e-02, 2.8162e-02, 5.5881e-02,
        3.8602e-02, 6.0761e-03, 4.3062e-02, 1.8421e-02, 3.0456e-03, 6.7948e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,761][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.1600, 0.0080, 0.0789, 0.0068, 0.0633, 0.1113, 0.0273, 0.2170, 0.1442,
        0.0538, 0.1218, 0.0076], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,763][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.1423, 0.0459, 0.0355, 0.0838, 0.0829, 0.0461, 0.1113, 0.0428, 0.1071,
        0.1685, 0.1015, 0.0324], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,765][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.2644, 0.0265, 0.0574, 0.0217, 0.0592, 0.1666, 0.0563, 0.0494, 0.1017,
        0.0287, 0.1476, 0.0205], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,766][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.2113, 0.0418, 0.0617, 0.0231, 0.0460, 0.1077, 0.0471, 0.1215, 0.1041,
        0.0281, 0.1396, 0.0677], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,767][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0234, 0.1183, 0.0738, 0.0871, 0.1060, 0.0697, 0.1007, 0.0789, 0.0726,
        0.0991, 0.0677, 0.1026], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,767][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([4.0844e-06, 7.5144e-03, 5.0316e-06, 3.2148e-04, 7.3690e-06, 4.6468e-06,
        1.1735e-05, 3.6856e-05, 1.9836e-05, 1.9226e-05, 3.2998e-03, 9.8876e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,768][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0097, 0.0488, 0.0293, 0.0201, 0.1567, 0.1891, 0.0991, 0.0720, 0.0824,
        0.0183, 0.0819, 0.1325, 0.0600], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,768][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0241, 0.1008, 0.0602, 0.0578, 0.0903, 0.0700, 0.1206, 0.0313, 0.0416,
        0.0244, 0.0909, 0.2641, 0.0238], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,769][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0999, 0.0647, 0.0692, 0.0764, 0.0748, 0.0693, 0.0781, 0.0760, 0.0729,
        0.0843, 0.0779, 0.0785, 0.0781], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,769][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.1799, 0.0409, 0.1053, 0.0433, 0.0575, 0.0776, 0.0226, 0.0821, 0.0995,
        0.0496, 0.1361, 0.0477, 0.0578], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,769][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0237, 0.0845, 0.0634, 0.0699, 0.0896, 0.0668, 0.0978, 0.0755, 0.0785,
        0.0938, 0.0783, 0.0905, 0.0874], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,770][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([5.9035e-05, 1.9938e-04, 2.0788e-06, 2.0150e-05, 4.4974e-02, 9.6915e-06,
        2.1507e-03, 1.7546e-05, 8.7275e-05, 2.2539e-03, 1.9377e-06, 8.2811e-04,
        9.4940e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,770][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.1059, 0.0091, 0.0983, 0.0071, 0.0719, 0.1004, 0.0201, 0.2038, 0.1172,
        0.0470, 0.1051, 0.0086, 0.1056], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,771][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.2717, 0.0713, 0.0347, 0.0521, 0.0647, 0.0417, 0.1120, 0.0454, 0.0614,
        0.0992, 0.0695, 0.0316, 0.0447], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,773][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.2248, 0.0337, 0.0554, 0.0416, 0.0611, 0.1461, 0.0695, 0.0243, 0.1156,
        0.0412, 0.1285, 0.0321, 0.0262], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,775][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1412, 0.1591, 0.0740, 0.0628, 0.0432, 0.0532, 0.0589, 0.0509, 0.0788,
        0.0229, 0.0985, 0.1156, 0.0408], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,776][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0205, 0.1075, 0.0668, 0.0795, 0.0961, 0.0625, 0.0927, 0.0736, 0.0665,
        0.0922, 0.0608, 0.0941, 0.0872], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,777][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([6.5635e-03, 2.1596e-04, 2.7882e-03, 8.0666e-04, 3.0502e-03, 3.3150e-03,
        5.4809e-04, 1.3341e-03, 4.2445e-03, 4.7363e-03, 5.2553e-02, 1.2421e-03,
        9.1860e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,779][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0059, 0.0388, 0.0198, 0.0187, 0.1398, 0.1277, 0.0973, 0.0566, 0.0681,
        0.0229, 0.0750, 0.0809, 0.2297, 0.0187], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,781][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0201, 0.0666, 0.0431, 0.0986, 0.1112, 0.0750, 0.1352, 0.0634, 0.0595,
        0.0296, 0.0456, 0.1429, 0.1025, 0.0066], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,783][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0888, 0.0603, 0.0642, 0.0711, 0.0695, 0.0644, 0.0735, 0.0702, 0.0674,
        0.0789, 0.0720, 0.0738, 0.0728, 0.0731], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,784][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1594, 0.0377, 0.0936, 0.0416, 0.0545, 0.0708, 0.0218, 0.0752, 0.0896,
        0.0476, 0.1213, 0.0466, 0.0548, 0.0856], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,784][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0245, 0.0768, 0.0569, 0.0689, 0.0831, 0.0644, 0.0913, 0.0681, 0.0731,
        0.0894, 0.0703, 0.0853, 0.0812, 0.0668], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,785][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.3142e-07, 1.0883e-03, 3.4981e-06, 2.2968e-04, 9.6667e-05, 1.4503e-07,
        6.0244e-01, 5.6329e-08, 2.3323e-06, 7.2325e-02, 2.1777e-09, 1.5051e-02,
        3.0496e-01, 3.8002e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,785][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0324, 0.0160, 0.0838, 0.0094, 0.0851, 0.0632, 0.0369, 0.1358, 0.0649,
        0.0639, 0.0805, 0.0097, 0.1361, 0.1823], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,785][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.2227, 0.0599, 0.0350, 0.0536, 0.0345, 0.0395, 0.1198, 0.0423, 0.0619,
        0.1062, 0.0821, 0.0296, 0.0494, 0.0635], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,786][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2379, 0.0901, 0.0314, 0.0741, 0.0501, 0.1251, 0.0753, 0.0374, 0.0572,
        0.0608, 0.0536, 0.0428, 0.0281, 0.0361], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,786][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3390, 0.0223, 0.0392, 0.0235, 0.0396, 0.0878, 0.0362, 0.0284, 0.1076,
        0.0187, 0.0652, 0.0566, 0.0712, 0.0649], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,787][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0196, 0.0991, 0.0615, 0.0753, 0.0902, 0.0595, 0.0872, 0.0699, 0.0618,
        0.0861, 0.0576, 0.0884, 0.0838, 0.0600], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,787][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0342, 0.0007, 0.0258, 0.0015, 0.0024, 0.0139, 0.0033, 0.0127, 0.0047,
        0.0055, 0.2243, 0.0055, 0.0102, 0.6554], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,788][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0030, 0.0356, 0.0138, 0.0223, 0.0872, 0.0920, 0.0995, 0.0431, 0.0414,
        0.0229, 0.0641, 0.0862, 0.1790, 0.1115, 0.0984], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,790][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0154, 0.0761, 0.0264, 0.0795, 0.0708, 0.0556, 0.1401, 0.0417, 0.0722,
        0.0302, 0.0314, 0.1581, 0.1421, 0.0230, 0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,792][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0865, 0.0558, 0.0596, 0.0659, 0.0644, 0.0597, 0.0672, 0.0655, 0.0627,
        0.0725, 0.0671, 0.0676, 0.0672, 0.0678, 0.0705], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,793][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.1670, 0.0356, 0.0906, 0.0362, 0.0489, 0.0674, 0.0192, 0.0709, 0.0865,
        0.0420, 0.1190, 0.0402, 0.0494, 0.0835, 0.0435], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,794][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0200, 0.0716, 0.0528, 0.0606, 0.0749, 0.0567, 0.0825, 0.0642, 0.0665,
        0.0797, 0.0662, 0.0774, 0.0784, 0.0603, 0.0883], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,796][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ give] are: tensor([2.0523e-05, 1.9788e-04, 1.1799e-06, 4.5115e-06, 4.7980e-03, 4.4693e-06,
        2.0785e-03, 1.8548e-07, 2.3784e-05, 1.0445e-04, 8.0750e-07, 7.8678e-04,
        4.0145e-01, 1.2617e-05, 5.9052e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,798][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0498, 0.0096, 0.0721, 0.0072, 0.0585, 0.0650, 0.0224, 0.1384, 0.0735,
        0.0426, 0.0745, 0.0085, 0.0839, 0.1881, 0.1060], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,799][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.1753, 0.0715, 0.0355, 0.0569, 0.0389, 0.0430, 0.0939, 0.0438, 0.0629,
        0.0917, 0.0849, 0.0389, 0.0693, 0.0542, 0.0394], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,801][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.2621, 0.0369, 0.0368, 0.0349, 0.0677, 0.1485, 0.0466, 0.0341, 0.0844,
        0.0307, 0.0786, 0.0279, 0.0299, 0.0450, 0.0358], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,801][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.1633, 0.0257, 0.0320, 0.0153, 0.0344, 0.1116, 0.0199, 0.0507, 0.1413,
        0.0117, 0.1001, 0.0781, 0.0566, 0.1098, 0.0495], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,802][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0181, 0.0924, 0.0568, 0.0689, 0.0849, 0.0544, 0.0806, 0.0639, 0.0573,
        0.0804, 0.0532, 0.0822, 0.0777, 0.0545, 0.0748], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,802][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ give] are: tensor([1.4972e-03, 9.4607e-05, 8.1147e-04, 1.7003e-04, 3.6771e-03, 1.5961e-04,
        3.5740e-04, 1.9446e-04, 1.2730e-04, 3.6774e-04, 9.2617e-03, 2.6372e-04,
        2.9129e-04, 8.8730e-04, 9.8184e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:36,803][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0062, 0.0364, 0.0141, 0.0204, 0.0866, 0.1088, 0.0839, 0.0180, 0.0454,
        0.0176, 0.0513, 0.0876, 0.1146, 0.0406, 0.2436, 0.0248],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,803][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0207, 0.0361, 0.0398, 0.0432, 0.0953, 0.0715, 0.1087, 0.0576, 0.0294,
        0.0201, 0.0532, 0.0814, 0.1000, 0.0389, 0.1950, 0.0090],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,803][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0793, 0.0522, 0.0557, 0.0618, 0.0603, 0.0560, 0.0633, 0.0611, 0.0586,
        0.0682, 0.0626, 0.0635, 0.0630, 0.0634, 0.0659, 0.0651],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,804][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.1484, 0.0340, 0.0839, 0.0362, 0.0477, 0.0630, 0.0188, 0.0666, 0.0800,
        0.0413, 0.1083, 0.0399, 0.0478, 0.0760, 0.0421, 0.0660],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,804][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0196, 0.0665, 0.0492, 0.0595, 0.0709, 0.0541, 0.0796, 0.0589, 0.0632,
        0.0764, 0.0621, 0.0746, 0.0718, 0.0548, 0.0828, 0.0561],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,805][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ it] are: tensor([1.1041e-07, 2.8484e-05, 3.2138e-08, 2.6915e-03, 1.4424e-05, 2.1358e-07,
        9.6325e-04, 6.2969e-12, 1.2623e-05, 2.4191e-06, 1.2653e-09, 3.0535e-04,
        1.4387e-03, 1.7987e-10, 7.6274e-06, 9.9454e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,807][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0376, 0.0138, 0.0714, 0.0083, 0.0658, 0.0591, 0.0263, 0.1210, 0.0598,
        0.0481, 0.0680, 0.0077, 0.1026, 0.1577, 0.1044, 0.0485],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,808][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.2222, 0.0889, 0.0311, 0.0606, 0.0205, 0.0468, 0.0917, 0.0357, 0.0619,
        0.0780, 0.0666, 0.0323, 0.0389, 0.0470, 0.0427, 0.0350],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,810][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.2263, 0.0573, 0.0354, 0.0536, 0.0554, 0.1344, 0.0654, 0.0351, 0.0656,
        0.0421, 0.0559, 0.0308, 0.0299, 0.0370, 0.0352, 0.0405],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,811][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.2016, 0.0349, 0.0448, 0.0250, 0.0295, 0.0929, 0.0291, 0.0308, 0.1114,
        0.0142, 0.0812, 0.0740, 0.0602, 0.0734, 0.0408, 0.0560],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,812][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0187, 0.0851, 0.0538, 0.0659, 0.0774, 0.0522, 0.0766, 0.0595, 0.0547,
        0.0749, 0.0504, 0.0762, 0.0721, 0.0513, 0.0700, 0.0613],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,814][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ it] are: tensor([1.2662e-03, 4.1583e-04, 9.7461e-04, 5.3973e-04, 3.2926e-04, 2.3782e-04,
        4.8925e-04, 6.6577e-04, 1.1444e-03, 9.1488e-04, 1.6683e-02, 3.0877e-03,
        2.1306e-04, 2.9868e-03, 7.0168e-04, 9.6935e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:36,815][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0044, 0.0282, 0.0151, 0.0134, 0.1107, 0.0941, 0.0720, 0.0416, 0.0508,
        0.0167, 0.0552, 0.0587, 0.1643, 0.0135, 0.1806, 0.0693, 0.0114],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,817][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0168, 0.0544, 0.0363, 0.0826, 0.0916, 0.0626, 0.1142, 0.0526, 0.0495,
        0.0246, 0.0385, 0.1170, 0.0840, 0.0055, 0.1164, 0.0483, 0.0052],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,818][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0714, 0.0495, 0.0524, 0.0584, 0.0568, 0.0527, 0.0604, 0.0573, 0.0551,
        0.0647, 0.0588, 0.0607, 0.0596, 0.0598, 0.0624, 0.0617, 0.0583],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,819][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1376, 0.0317, 0.0777, 0.0342, 0.0449, 0.0585, 0.0177, 0.0620, 0.0739,
        0.0389, 0.0999, 0.0380, 0.0449, 0.0702, 0.0397, 0.0611, 0.0691],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,819][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0203, 0.0629, 0.0463, 0.0565, 0.0679, 0.0527, 0.0745, 0.0556, 0.0598,
        0.0728, 0.0573, 0.0698, 0.0663, 0.0545, 0.0802, 0.0523, 0.0500],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,820][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.3218e-07, 1.0750e-03, 3.6259e-06, 2.3301e-04, 1.3065e-04, 1.3839e-07,
        5.4854e-01, 6.8892e-08, 2.3778e-06, 8.2181e-02, 2.6542e-09, 1.4313e-02,
        3.3448e-01, 4.9207e-03, 4.1898e-03, 6.2405e-05, 9.8648e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,820][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0217, 0.0142, 0.0660, 0.0084, 0.0646, 0.0476, 0.0285, 0.0991, 0.0476,
        0.0494, 0.0601, 0.0083, 0.1025, 0.1264, 0.1002, 0.0419, 0.1136],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,821][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1991, 0.0513, 0.0295, 0.0404, 0.0220, 0.0298, 0.1027, 0.0383, 0.0499,
        0.0962, 0.0787, 0.0237, 0.0347, 0.0564, 0.0562, 0.0314, 0.0598],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,821][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2011, 0.0877, 0.0216, 0.0729, 0.0498, 0.1273, 0.0797, 0.0311, 0.0437,
        0.0586, 0.0420, 0.0329, 0.0246, 0.0268, 0.0287, 0.0465, 0.0250],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,821][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.2553, 0.0213, 0.0323, 0.0228, 0.0354, 0.0874, 0.0354, 0.0237, 0.1024,
        0.0165, 0.0481, 0.0531, 0.0616, 0.0552, 0.0471, 0.0447, 0.0574],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,822][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0166, 0.0815, 0.0507, 0.0623, 0.0743, 0.0493, 0.0722, 0.0577, 0.0512,
        0.0711, 0.0477, 0.0731, 0.0692, 0.0498, 0.0673, 0.0581, 0.0478],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,823][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.2678e-02, 3.8532e-04, 1.0434e-02, 9.8666e-04, 1.4800e-03, 1.0071e-02,
        1.8487e-03, 8.3786e-03, 3.3605e-03, 2.6875e-03, 1.5956e-01, 8.3575e-03,
        6.1708e-03, 3.0998e-01, 3.3969e-03, 4.6213e-03, 4.5560e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:36,847][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:36,848][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,850][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,851][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,852][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,853][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,855][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,856][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,857][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,858][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,859][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,859][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,859][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:36,868][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0812, 0.9188], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,870][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.6278, 0.3722], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,872][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.6704, 0.3296], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,874][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.4021, 0.5979], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,875][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.2341, 0.7659], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,875][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.6545, 0.3455], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,876][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.9523, 0.0477], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,876][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.9640, 0.0360], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,877][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.9970, 0.0030], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,877][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([1.5072e-05, 9.9998e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,877][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0078, 0.9922], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,877][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.1905, 0.8095], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:36,878][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0568, 0.6707, 0.2725], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,878][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.1056, 0.8224, 0.0720], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,878][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.4668, 0.2520, 0.2812], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,879][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.3240, 0.5654, 0.1105], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,879][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.1503, 0.5026, 0.3471], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,880][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1095, 0.0168, 0.8737], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,882][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2149, 0.0866, 0.6985], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,884][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.6989, 0.1148, 0.1863], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,885][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.9824, 0.0034, 0.0141], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,886][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([8.9011e-07, 9.9995e-01, 4.5344e-05], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,888][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0125, 0.4878, 0.4996], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,889][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([1.3801e-02, 8.3599e-04, 9.8536e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:36,890][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.0256, 0.5147, 0.2216, 0.2381], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,892][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.0571, 0.4121, 0.4784, 0.0524], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,893][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.3761, 0.1905, 0.2216, 0.2119], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,893][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.3549, 0.2911, 0.1441, 0.2099], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,893][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.1042, 0.3597, 0.2617, 0.2745], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,894][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.2096, 0.3442, 0.4223, 0.0239], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,894][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.5118, 0.0306, 0.4135, 0.0441], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,894][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.6339, 0.1618, 0.1369, 0.0674], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,895][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.9815, 0.0030, 0.0109, 0.0046], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,895][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([8.3734e-05, 9.9810e-01, 1.7622e-03, 5.6626e-05], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,895][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.0055, 0.3065, 0.3347, 0.3533], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,896][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0280, 0.0050, 0.1141, 0.8529], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:36,896][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0243, 0.2685, 0.1657, 0.1888, 0.3526], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,897][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0836, 0.4088, 0.1135, 0.2378, 0.1563], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,899][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.2989, 0.1551, 0.1710, 0.1783, 0.1967], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,900][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.2836, 0.2429, 0.0876, 0.2079, 0.1780], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,902][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0760, 0.2616, 0.1921, 0.2092, 0.2610], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,903][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0823, 0.0317, 0.4968, 0.0198, 0.3694], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,905][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.2071, 0.0240, 0.3602, 0.0345, 0.3742], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,907][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0685, 0.0814, 0.0872, 0.4589, 0.3040], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,908][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.9777, 0.0023, 0.0110, 0.0041, 0.0048], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,909][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([1.6845e-05, 9.9247e-01, 8.1970e-05, 8.0208e-06, 7.4261e-03],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,910][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0052, 0.1957, 0.1902, 0.2486, 0.3603], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,911][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([5.3676e-02, 9.2909e-04, 1.2373e-01, 8.4829e-05, 8.2158e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:36,911][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0162, 0.1693, 0.0936, 0.1293, 0.2411, 0.3505], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,911][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1445, 0.3468, 0.0925, 0.1420, 0.2223, 0.0518], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,912][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2498, 0.1301, 0.1508, 0.1524, 0.1715, 0.1454], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,912][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.2294, 0.2168, 0.0647, 0.2039, 0.1548, 0.1304], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,912][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0675, 0.2108, 0.1574, 0.1758, 0.2185, 0.1700], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,913][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0605, 0.0297, 0.3700, 0.0409, 0.4565, 0.0424], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,913][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0384, 0.0264, 0.2262, 0.0376, 0.4139, 0.2575], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,913][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0565, 0.0497, 0.0520, 0.3732, 0.3904, 0.0782], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,914][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.9673, 0.0018, 0.0113, 0.0039, 0.0046, 0.0110], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,915][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([3.6043e-06, 9.9532e-01, 3.7767e-05, 2.3755e-06, 4.6149e-03, 2.1079e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,916][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0041, 0.1532, 0.1554, 0.2169, 0.3143, 0.1562], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,917][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([7.0000e-03, 3.0237e-05, 6.9620e-03, 4.9681e-05, 1.2396e-04, 9.8583e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:36,919][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.0061, 0.0936, 0.0478, 0.0970, 0.1716, 0.3976, 0.1863],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,920][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0389, 0.1511, 0.1280, 0.1321, 0.2738, 0.2008, 0.0754],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,921][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.2261, 0.1127, 0.1316, 0.1274, 0.1471, 0.1169, 0.1383],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,923][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.2091, 0.2111, 0.0790, 0.1925, 0.1375, 0.0916, 0.0791],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,925][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0487, 0.1778, 0.1377, 0.1371, 0.1755, 0.1390, 0.1842],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,926][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.0459, 0.0480, 0.1904, 0.0361, 0.2080, 0.0985, 0.3730],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,928][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.3127, 0.0076, 0.1688, 0.0114, 0.1206, 0.3601, 0.0187],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,928][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.3012, 0.0689, 0.1260, 0.2154, 0.1236, 0.1584, 0.0065],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,929][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.9382, 0.0027, 0.0132, 0.0056, 0.0061, 0.0165, 0.0178],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,929][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([5.0431e-06, 9.8388e-01, 3.8848e-04, 5.8856e-05, 1.0787e-02, 1.3988e-05,
        4.8654e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,929][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.0029, 0.1224, 0.1299, 0.1649, 0.2256, 0.1015, 0.2527],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,930][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.1465, 0.0008, 0.0511, 0.0020, 0.0048, 0.0510, 0.7439],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:36,930][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0111, 0.0988, 0.0676, 0.0867, 0.1375, 0.2631, 0.2772, 0.0580],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,930][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0260, 0.1651, 0.0604, 0.0998, 0.2154, 0.1095, 0.3064, 0.0175],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,931][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.1987, 0.0993, 0.1103, 0.1171, 0.1298, 0.1057, 0.1290, 0.1102],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,931][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1991, 0.1749, 0.0559, 0.1673, 0.1421, 0.0839, 0.0843, 0.0926],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,932][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0476, 0.1504, 0.1102, 0.1229, 0.1548, 0.1175, 0.1683, 0.1283],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,933][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0291, 0.0157, 0.2612, 0.0051, 0.0909, 0.1002, 0.0508, 0.4471],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,935][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0182, 0.0170, 0.1133, 0.0226, 0.2215, 0.1210, 0.0792, 0.4072],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,936][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0924, 0.0427, 0.0565, 0.2995, 0.2742, 0.0764, 0.1008, 0.0574],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,938][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.9251, 0.0031, 0.0167, 0.0077, 0.0076, 0.0192, 0.0174, 0.0032],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,939][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([1.7805e-06, 9.9318e-01, 5.1845e-05, 6.7224e-06, 2.8285e-03, 2.0621e-05,
        2.8714e-04, 3.6275e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,940][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0044, 0.0901, 0.1021, 0.1350, 0.1966, 0.0986, 0.2156, 0.1575],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,942][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([1.0599e-03, 8.4524e-06, 2.4155e-03, 1.2277e-06, 1.4729e-04, 6.7380e-04,
        5.0900e-05, 9.9564e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:36,943][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0112, 0.0915, 0.0530, 0.0744, 0.1033, 0.2439, 0.2056, 0.0842, 0.1329],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,945][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0433, 0.1301, 0.0803, 0.0772, 0.1702, 0.0870, 0.1543, 0.2309, 0.0267],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,945][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1690, 0.0910, 0.1002, 0.1063, 0.1185, 0.0971, 0.1192, 0.0974, 0.1013],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,946][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1626, 0.1575, 0.0481, 0.1574, 0.1245, 0.0902, 0.0876, 0.0769, 0.0953],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,946][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0411, 0.1328, 0.0941, 0.1115, 0.1363, 0.1017, 0.1568, 0.1089, 0.1167],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,946][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0397, 0.0708, 0.3302, 0.0120, 0.2261, 0.0923, 0.1170, 0.0639, 0.0481],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,947][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0140, 0.0148, 0.0981, 0.0194, 0.1951, 0.0995, 0.0699, 0.3505, 0.1388],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,947][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0698, 0.0317, 0.0434, 0.2070, 0.3199, 0.0945, 0.0922, 0.0582, 0.0832],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,948][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.8374, 0.0022, 0.0122, 0.0059, 0.0054, 0.0149, 0.0121, 0.0022, 0.1077],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,948][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.9212e-06, 9.9043e-01, 4.2393e-05, 3.2740e-06, 3.5344e-03, 1.1075e-05,
        2.9567e-04, 5.5426e-03, 1.4076e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,948][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0039, 0.0809, 0.0876, 0.1280, 0.1757, 0.0917, 0.2002, 0.1276, 0.1044],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,949][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([2.3560e-02, 6.6734e-04, 1.1398e-02, 4.0089e-04, 2.7885e-04, 2.5074e-01,
        3.4325e-04, 8.4610e-03, 7.0415e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:36,950][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0043, 0.0831, 0.0419, 0.0497, 0.1086, 0.2715, 0.1986, 0.0682, 0.1473,
        0.0269], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,951][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0152, 0.0894, 0.0547, 0.1026, 0.2194, 0.1418, 0.1088, 0.1943, 0.0656,
        0.0082], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,953][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.1778, 0.0813, 0.0924, 0.0911, 0.1019, 0.0836, 0.1000, 0.0862, 0.0856,
        0.1002], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,955][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.1898, 0.1666, 0.0516, 0.1602, 0.1259, 0.0669, 0.0649, 0.0793, 0.0592,
        0.0357], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,956][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0330, 0.1194, 0.0883, 0.0915, 0.1173, 0.0921, 0.1293, 0.0993, 0.1079,
        0.1220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,957][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([2.2401e-02, 7.1715e-03, 3.9773e-02, 9.4963e-05, 4.7030e-01, 3.0209e-02,
        1.7704e-02, 4.0509e-01, 4.0195e-03, 3.2345e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,959][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0598, 0.0032, 0.0620, 0.0047, 0.0555, 0.1199, 0.0099, 0.4351, 0.2122,
        0.0377], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,960][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.5803, 0.0230, 0.0194, 0.0560, 0.0210, 0.0887, 0.1710, 0.0097, 0.0144,
        0.0165], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,962][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.8531, 0.0017, 0.0091, 0.0034, 0.0044, 0.0111, 0.0127, 0.0021, 0.0987,
        0.0037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,962][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.3424e-06, 9.8871e-01, 1.1107e-04, 2.0521e-05, 4.7023e-03, 7.1701e-06,
        2.9278e-03, 3.3719e-03, 5.2247e-05, 9.1111e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,963][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0030, 0.0805, 0.0872, 0.1116, 0.1496, 0.0698, 0.1671, 0.1169, 0.0722,
        0.1422], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,963][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([7.5204e-03, 7.0950e-06, 3.2247e-03, 7.2592e-06, 1.7022e-03, 1.7053e-03,
        8.7668e-04, 9.2586e-02, 7.2585e-04, 8.9164e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:36,964][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0116, 0.0749, 0.0420, 0.0667, 0.0820, 0.2033, 0.1742, 0.0818, 0.1314,
        0.0458, 0.0863], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,964][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0297, 0.0851, 0.0568, 0.1456, 0.1841, 0.0619, 0.2274, 0.0973, 0.0456,
        0.0349, 0.0318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,965][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1380, 0.0737, 0.0809, 0.0868, 0.0981, 0.0779, 0.0948, 0.0792, 0.0798,
        0.0961, 0.0948], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,965][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1480, 0.1347, 0.0458, 0.1348, 0.1203, 0.0738, 0.0719, 0.0783, 0.0651,
        0.0546, 0.0728], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,965][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0341, 0.1075, 0.0769, 0.0900, 0.1110, 0.0809, 0.1183, 0.0877, 0.0916,
        0.1130, 0.0890], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,966][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0258, 0.0130, 0.2767, 0.0068, 0.2832, 0.0618, 0.1074, 0.0369, 0.0365,
        0.0243, 0.1276], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,966][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0155, 0.0082, 0.0645, 0.0113, 0.1135, 0.0774, 0.0400, 0.2705, 0.1166,
        0.0990, 0.1835], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,968][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0812, 0.0247, 0.0373, 0.0959, 0.2901, 0.0556, 0.0676, 0.0454, 0.0647,
        0.1936, 0.0438], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,969][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.7945, 0.0027, 0.0149, 0.0076, 0.0065, 0.0167, 0.0150, 0.0026, 0.1188,
        0.0048, 0.0159], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,970][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([7.4747e-06, 9.7130e-01, 1.6580e-04, 5.2674e-06, 2.5664e-03, 2.1363e-05,
        2.8937e-04, 2.5214e-02, 1.8202e-04, 3.1649e-05, 2.2118e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,972][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0041, 0.0571, 0.0612, 0.0948, 0.1289, 0.0656, 0.1398, 0.1021, 0.0727,
        0.1367, 0.1370], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,973][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0772, 0.0014, 0.1338, 0.0033, 0.0026, 0.0982, 0.0035, 0.3951, 0.0306,
        0.0027, 0.2515], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:36,975][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0037, 0.0678, 0.0313, 0.0428, 0.0764, 0.1754, 0.1719, 0.0434, 0.1308,
        0.0309, 0.1065, 0.1191], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,977][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0222, 0.0218, 0.0306, 0.0544, 0.2369, 0.1591, 0.1571, 0.1309, 0.0585,
        0.0244, 0.0569, 0.0473], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,978][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.1394, 0.0659, 0.0779, 0.0779, 0.0863, 0.0703, 0.0849, 0.0711, 0.0725,
        0.0871, 0.0895, 0.0771], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,980][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.1452, 0.1198, 0.0519, 0.1140, 0.1191, 0.0701, 0.0612, 0.0713, 0.0661,
        0.0468, 0.0767, 0.0577], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,980][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0275, 0.0952, 0.0735, 0.0781, 0.0969, 0.0758, 0.1029, 0.0814, 0.0872,
        0.1017, 0.0861, 0.0938], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,981][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0341, 0.2008, 0.1870, 0.0194, 0.0568, 0.0286, 0.0116, 0.3249, 0.0104,
        0.0066, 0.0197, 0.1001], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,981][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0552, 0.0026, 0.0502, 0.0043, 0.0421, 0.1001, 0.0105, 0.3408, 0.1862,
        0.0359, 0.1685, 0.0037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,982][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0837, 0.0290, 0.0316, 0.1203, 0.1316, 0.1291, 0.2267, 0.0244, 0.0492,
        0.1028, 0.0351, 0.0365], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,982][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.8604, 0.0016, 0.0074, 0.0044, 0.0040, 0.0116, 0.0134, 0.0017, 0.0787,
        0.0059, 0.0091, 0.0016], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,982][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([6.0272e-06, 9.4972e-01, 4.2444e-04, 2.8188e-05, 9.1170e-03, 1.7236e-05,
        3.0214e-03, 8.3687e-03, 1.4646e-04, 2.1796e-04, 5.2402e-04, 2.8406e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,983][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0032, 0.0611, 0.0637, 0.0850, 0.1228, 0.0601, 0.1335, 0.0898, 0.0631,
        0.1213, 0.1292, 0.0673], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,983][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0025, 0.0991, 0.0179, 0.0026, 0.0022, 0.0032, 0.0084, 0.1005, 0.0010,
        0.0018, 0.5619, 0.1989], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:36,984][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0080, 0.0640, 0.0352, 0.0387, 0.1055, 0.1811, 0.1232, 0.0573, 0.0947,
        0.0218, 0.0837, 0.1333, 0.0536], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,985][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0241, 0.1008, 0.0602, 0.0578, 0.0903, 0.0700, 0.1206, 0.0313, 0.0416,
        0.0244, 0.0909, 0.2641, 0.0238], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,987][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.1251, 0.0625, 0.0679, 0.0708, 0.0786, 0.0635, 0.0793, 0.0662, 0.0652,
        0.0799, 0.0800, 0.0732, 0.0879], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,989][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.1415, 0.1200, 0.0414, 0.1178, 0.1029, 0.0590, 0.0624, 0.0632, 0.0553,
        0.0441, 0.0643, 0.0575, 0.0708], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,990][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0264, 0.0885, 0.0651, 0.0693, 0.0906, 0.0669, 0.0957, 0.0752, 0.0774,
        0.0926, 0.0772, 0.0881, 0.0870], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,992][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0247, 0.0087, 0.1290, 0.0020, 0.3486, 0.0745, 0.0370, 0.1151, 0.0408,
        0.1050, 0.0688, 0.0093, 0.0364], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,994][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0223, 0.0041, 0.0553, 0.0058, 0.0649, 0.0796, 0.0161, 0.2963, 0.1304,
        0.0500, 0.1586, 0.0065, 0.1103], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,996][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0400, 0.0350, 0.0350, 0.0952, 0.1628, 0.0445, 0.0620, 0.0327, 0.0447,
        0.2879, 0.0637, 0.0596, 0.0367], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,997][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([8.7276e-01, 2.0055e-03, 1.0253e-02, 4.6025e-03, 4.9694e-03, 1.0402e-02,
        9.5546e-03, 2.0162e-03, 6.9696e-02, 2.9251e-03, 8.6253e-03, 1.5542e-03,
        6.4070e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,997][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([8.7991e-07, 9.6602e-01, 4.5166e-05, 6.0165e-06, 1.5569e-03, 7.7976e-06,
        4.5076e-04, 3.1772e-03, 1.1580e-04, 2.7067e-05, 1.1481e-04, 2.4134e-02,
        4.3471e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,998][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0026, 0.0557, 0.0578, 0.0773, 0.1042, 0.0466, 0.1179, 0.0806, 0.0496,
        0.1030, 0.1054, 0.0568, 0.1425], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,998][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([7.4746e-03, 6.3239e-04, 1.0081e-02, 1.9008e-04, 2.1730e-01, 1.1427e-02,
        6.9937e-03, 7.6979e-02, 3.5542e-03, 2.0032e-01, 2.2187e-02, 1.2698e-04,
        4.4273e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:36,998][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0063, 0.0516, 0.0298, 0.0406, 0.1056, 0.1478, 0.1239, 0.0476, 0.0848,
        0.0246, 0.0778, 0.0830, 0.1410, 0.0358], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,999][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0201, 0.0666, 0.0431, 0.0986, 0.1112, 0.0750, 0.1352, 0.0634, 0.0595,
        0.0296, 0.0456, 0.1429, 0.1025, 0.0066], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:36,999][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1052, 0.0569, 0.0625, 0.0673, 0.0766, 0.0617, 0.0745, 0.0621, 0.0630,
        0.0760, 0.0746, 0.0694, 0.0846, 0.0656], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,000][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1320, 0.1161, 0.0306, 0.1276, 0.1090, 0.0565, 0.0559, 0.0638, 0.0504,
        0.0389, 0.0565, 0.0543, 0.0717, 0.0365], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,000][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0270, 0.0802, 0.0584, 0.0684, 0.0840, 0.0645, 0.0896, 0.0680, 0.0724,
        0.0882, 0.0696, 0.0832, 0.0808, 0.0658], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,001][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0044, 0.0233, 0.0869, 0.0091, 0.0549, 0.0149, 0.0354, 0.0148, 0.0159,
        0.0577, 0.0182, 0.0270, 0.0271, 0.6103], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,002][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0073, 0.0064, 0.0438, 0.0083, 0.0823, 0.0449, 0.0303, 0.1485, 0.0622,
        0.0686, 0.1083, 0.0096, 0.1384, 0.2411], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,003][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0349, 0.0180, 0.0346, 0.0728, 0.1649, 0.0590, 0.0743, 0.0335, 0.0328,
        0.2305, 0.0803, 0.0164, 0.0990, 0.0489], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,005][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.8224, 0.0031, 0.0133, 0.0084, 0.0069, 0.0166, 0.0142, 0.0026, 0.0924,
        0.0046, 0.0111, 0.0025, 0.0009, 0.0011], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,006][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.8206e-06, 9.4389e-01, 8.5817e-05, 4.7339e-06, 3.5946e-03, 2.6566e-05,
        2.8354e-04, 1.9807e-02, 1.9220e-04, 2.5554e-05, 1.4923e-04, 2.3549e-02,
        8.3625e-03, 1.9598e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,007][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0043, 0.0421, 0.0481, 0.0692, 0.0943, 0.0514, 0.0969, 0.0807, 0.0536,
        0.0936, 0.0993, 0.0489, 0.1240, 0.0936], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,009][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.1594e-02, 3.7021e-05, 3.3843e-02, 2.8512e-05, 7.4550e-04, 3.7216e-02,
        1.0326e-04, 2.6419e-01, 1.2050e-02, 3.2666e-04, 2.6799e-02, 4.2860e-05,
        2.1777e-06, 6.1303e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,011][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0036, 0.0518, 0.0217, 0.0421, 0.0725, 0.1111, 0.1183, 0.0371, 0.0596,
        0.0246, 0.0688, 0.0986, 0.1192, 0.0968, 0.0742], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,012][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0154, 0.0761, 0.0264, 0.0795, 0.0708, 0.0556, 0.1401, 0.0417, 0.0722,
        0.0302, 0.0314, 0.1581, 0.1421, 0.0230, 0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,014][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.1058, 0.0538, 0.0600, 0.0623, 0.0689, 0.0548, 0.0671, 0.0574, 0.0566,
        0.0700, 0.0699, 0.0635, 0.0798, 0.0594, 0.0706], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,014][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.1478, 0.1063, 0.0346, 0.1063, 0.0964, 0.0521, 0.0514, 0.0625, 0.0485,
        0.0353, 0.0629, 0.0488, 0.0645, 0.0357, 0.0466], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,015][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0222, 0.0751, 0.0543, 0.0603, 0.0759, 0.0569, 0.0809, 0.0642, 0.0657,
        0.0788, 0.0655, 0.0755, 0.0781, 0.0593, 0.0875], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,015][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0118, 0.0048, 0.1053, 0.0010, 0.0696, 0.0275, 0.1590, 0.0279, 0.0289,
        0.2794, 0.0928, 0.0088, 0.0955, 0.0425, 0.0452], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,016][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0090, 0.0034, 0.0320, 0.0045, 0.0489, 0.0413, 0.0164, 0.1518, 0.0642,
        0.0397, 0.0901, 0.0056, 0.0796, 0.2203, 0.1932], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,016][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0102, 0.0227, 0.0347, 0.0871, 0.0819, 0.0341, 0.0224, 0.0388, 0.0641,
        0.2455, 0.1658, 0.0463, 0.0927, 0.0279, 0.0258], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,017][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.7819, 0.0036, 0.0156, 0.0124, 0.0086, 0.0211, 0.0171, 0.0029, 0.1106,
        0.0055, 0.0133, 0.0031, 0.0010, 0.0016, 0.0017], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,017][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([3.9964e-06, 9.5350e-01, 8.4639e-05, 4.7296e-06, 2.6998e-03, 3.4168e-05,
        3.6118e-04, 8.5161e-03, 3.9432e-04, 3.4371e-05, 1.4035e-04, 2.1984e-02,
        9.7838e-03, 1.2734e-05, 2.4421e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,017][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0030, 0.0420, 0.0470, 0.0629, 0.0840, 0.0404, 0.0928, 0.0676, 0.0424,
        0.0873, 0.0866, 0.0447, 0.1147, 0.0761, 0.1085], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,018][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([5.1442e-03, 2.3372e-04, 2.2614e-02, 7.7572e-05, 2.2412e-01, 1.2058e-02,
        4.7874e-03, 1.8530e-01, 2.9403e-03, 5.4509e-02, 3.8894e-02, 4.3630e-05,
        1.5977e-02, 2.1812e-01, 2.1518e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,020][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0063, 0.0496, 0.0226, 0.0431, 0.0747, 0.1248, 0.1121, 0.0212, 0.0658,
        0.0211, 0.0632, 0.0965, 0.0825, 0.0509, 0.1292, 0.0365],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,021][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0207, 0.0361, 0.0398, 0.0432, 0.0953, 0.0715, 0.1087, 0.0576, 0.0294,
        0.0201, 0.0532, 0.0814, 0.1000, 0.0389, 0.1950, 0.0090],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,023][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0961, 0.0506, 0.0557, 0.0587, 0.0646, 0.0535, 0.0642, 0.0544, 0.0554,
        0.0652, 0.0665, 0.0607, 0.0737, 0.0558, 0.0671, 0.0577],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,024][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.1287, 0.1034, 0.0308, 0.1165, 0.0881, 0.0525, 0.0506, 0.0524, 0.0502,
        0.0365, 0.0560, 0.0501, 0.0656, 0.0301, 0.0437, 0.0448],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,026][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0217, 0.0697, 0.0507, 0.0592, 0.0719, 0.0544, 0.0782, 0.0589, 0.0625,
        0.0756, 0.0615, 0.0729, 0.0716, 0.0540, 0.0821, 0.0553],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,028][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0229, 0.0615, 0.0966, 0.0643, 0.0400, 0.0342, 0.0994, 0.0113, 0.0216,
        0.0407, 0.0643, 0.0224, 0.0692, 0.0656, 0.1157, 0.1701],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,030][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0072, 0.0041, 0.0313, 0.0060, 0.0534, 0.0368, 0.0172, 0.1263, 0.0538,
        0.0440, 0.0812, 0.0057, 0.0889, 0.1937, 0.1894, 0.0611],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,031][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0272, 0.0279, 0.0308, 0.0898, 0.1072, 0.0842, 0.0377, 0.0304, 0.0621,
        0.1690, 0.0617, 0.0151, 0.1172, 0.0376, 0.0454, 0.0568],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,032][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([7.7093e-01, 3.2371e-03, 1.4368e-02, 1.1170e-02, 7.4068e-03, 2.0075e-02,
        1.6748e-02, 3.0725e-03, 1.2475e-01, 5.4441e-03, 1.4763e-02, 3.0003e-03,
        1.1104e-03, 1.6937e-03, 1.8169e-03, 4.1314e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,032][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([2.9583e-06, 9.5132e-01, 4.9928e-05, 4.4639e-06, 3.1854e-03, 1.3842e-05,
        3.3746e-04, 6.8825e-03, 1.4311e-04, 2.5615e-05, 9.0810e-05, 2.4880e-02,
        1.0362e-02, 8.7858e-06, 1.5702e-03, 1.1185e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,033][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0032, 0.0382, 0.0473, 0.0612, 0.0753, 0.0421, 0.0855, 0.0662, 0.0438,
        0.0791, 0.0867, 0.0417, 0.1020, 0.0757, 0.0978, 0.0542],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,033][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([3.3112e-03, 1.0295e-03, 4.4070e-02, 1.3949e-04, 7.7305e-02, 7.6657e-02,
        1.7126e-03, 2.6938e-01, 3.7806e-02, 5.0738e-03, 1.1865e-01, 1.0474e-03,
        7.6501e-04, 3.2989e-01, 1.2490e-03, 3.1920e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,034][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0051, 0.0414, 0.0238, 0.0322, 0.0869, 0.1160, 0.0978, 0.0379, 0.0665,
        0.0196, 0.0605, 0.0659, 0.1138, 0.0268, 0.1069, 0.0759, 0.0228],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,034][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0168, 0.0544, 0.0363, 0.0826, 0.0916, 0.0626, 0.1142, 0.0526, 0.0495,
        0.0246, 0.0385, 0.1170, 0.0840, 0.0055, 0.1164, 0.0483, 0.0052],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,034][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0862, 0.0474, 0.0516, 0.0559, 0.0636, 0.0512, 0.0619, 0.0514, 0.0521,
        0.0629, 0.0616, 0.0577, 0.0701, 0.0543, 0.0641, 0.0544, 0.0535],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,035][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1216, 0.1002, 0.0276, 0.1082, 0.0958, 0.0487, 0.0491, 0.0566, 0.0439,
        0.0353, 0.0507, 0.0483, 0.0624, 0.0332, 0.0450, 0.0383, 0.0351],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,037][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0224, 0.0658, 0.0476, 0.0563, 0.0688, 0.0530, 0.0733, 0.0557, 0.0593,
        0.0721, 0.0569, 0.0683, 0.0662, 0.0538, 0.0796, 0.0517, 0.0491],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,038][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0055, 0.0181, 0.1019, 0.0047, 0.0454, 0.0183, 0.0118, 0.0127, 0.0140,
        0.0242, 0.0224, 0.0076, 0.0266, 0.4226, 0.0182, 0.0139, 0.2322],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,040][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0041, 0.0042, 0.0273, 0.0055, 0.0533, 0.0278, 0.0188, 0.0904, 0.0381,
        0.0417, 0.0658, 0.0063, 0.0872, 0.1451, 0.1673, 0.0505, 0.1667],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,042][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0338, 0.0213, 0.0370, 0.0794, 0.1315, 0.0621, 0.0337, 0.0332, 0.0388,
        0.1457, 0.0839, 0.0124, 0.0951, 0.0482, 0.0633, 0.0430, 0.0376],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,043][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([7.4355e-01, 4.0468e-03, 1.8468e-02, 1.5590e-02, 9.1718e-03, 2.3779e-02,
        1.8632e-02, 3.1502e-03, 1.3081e-01, 6.1221e-03, 1.5479e-02, 3.9292e-03,
        1.2173e-03, 2.0112e-03, 1.8625e-03, 4.8082e-04, 1.6963e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,044][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.1893e-06, 9.3113e-01, 9.4453e-05, 4.4446e-06, 3.9377e-03, 3.5543e-05,
        2.4359e-04, 2.7172e-02, 2.3603e-04, 2.4927e-05, 1.6603e-04, 2.3988e-02,
        9.2575e-03, 2.5750e-05, 3.0282e-03, 6.2813e-04, 2.2461e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,046][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0040, 0.0327, 0.0389, 0.0544, 0.0727, 0.0414, 0.0749, 0.0651, 0.0430,
        0.0723, 0.0777, 0.0374, 0.0955, 0.0738, 0.0961, 0.0523, 0.0678],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,047][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([6.6557e-03, 3.0125e-05, 2.2500e-02, 2.0108e-05, 9.1569e-04, 3.2021e-02,
        7.9688e-05, 2.2370e-01, 1.0631e-02, 2.8930e-04, 1.5159e-02, 3.4467e-05,
        2.9677e-06, 5.0689e-01, 5.2529e-06, 2.2629e-05, 1.8105e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,048][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:37,050][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[26443],
        [14224],
        [22082],
        [   83],
        [23452],
        [13154],
        [21200],
        [31217],
        [28030],
        [ 8108],
        [14790],
        [ 8720],
        [29559],
        [28052],
        [10976],
        [19116],
        [27100]], device='cuda:0')
[2024-07-24 10:17:37,051][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[40670],
        [ 5405],
        [30237],
        [    1],
        [19811],
        [14332],
        [24009],
        [35085],
        [26234],
        [ 3522],
        [11537],
        [ 4530],
        [30203],
        [26480],
        [13634],
        [44845],
        [27381]], device='cuda:0')
[2024-07-24 10:17:37,052][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[38748],
        [33036],
        [35467],
        [33235],
        [29419],
        [27730],
        [27823],
        [31670],
        [32633],
        [31877],
        [33232],
        [33988],
        [33331],
        [31388],
        [32732],
        [31336],
        [30578]], device='cuda:0')
[2024-07-24 10:17:37,053][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[42691],
        [39909],
        [28216],
        [34888],
        [21622],
        [28603],
        [32607],
        [25041],
        [40926],
        [40528],
        [30414],
        [40038],
        [31918],
        [30530],
        [28585],
        [34681],
        [32004]], device='cuda:0')
[2024-07-24 10:17:37,055][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[32186],
        [35207],
        [36020],
        [37442],
        [38010],
        [38159],
        [39048],
        [38971],
        [39028],
        [39392],
        [39370],
        [39471],
        [39503],
        [39520],
        [39533],
        [39567],
        [39598]], device='cuda:0')
[2024-07-24 10:17:37,056][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 9209],
        [ 7851],
        [ 8153],
        [ 6998],
        [ 6613],
        [ 7345],
        [ 7405],
        [ 7773],
        [ 9170],
        [ 8635],
        [ 9553],
        [ 9363],
        [ 9183],
        [ 9934],
        [ 9894],
        [10289],
        [10950]], device='cuda:0')
[2024-07-24 10:17:37,058][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[18363],
        [18082],
        [16179],
        [15298],
        [14698],
        [14523],
        [14583],
        [14271],
        [14123],
        [14583],
        [14836],
        [14774],
        [14471],
        [14480],
        [14104],
        [13935],
        [13929]], device='cuda:0')
[2024-07-24 10:17:37,059][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[36739],
        [30636],
        [19953],
        [22723],
        [42493],
        [22153],
        [41753],
        [41962],
        [41642],
        [24252],
        [39932],
        [24930],
        [45251],
        [45019],
        [47056],
        [10227],
        [45119]], device='cuda:0')
[2024-07-24 10:17:37,061][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[41644],
        [41580],
        [38568],
        [40138],
        [37434],
        [30908],
        [36981],
        [28366],
        [27644],
        [29672],
        [27486],
        [28762],
        [27538],
        [25746],
        [24939],
        [24787],
        [24101]], device='cuda:0')
[2024-07-24 10:17:37,063][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[18935],
        [16635],
        [17082],
        [ 2832],
        [ 1261],
        [ 2337],
        [ 3356],
        [ 1157],
        [ 1814],
        [ 1987],
        [ 1019],
        [ 1283],
        [ 1881],
        [ 1768],
        [ 2093],
        [ 1919],
        [ 2346]], device='cuda:0')
[2024-07-24 10:17:37,065][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 3636],
        [ 3177],
        [ 4516],
        [ 9564],
        [ 7959],
        [11587],
        [14366],
        [13523],
        [15521],
        [14704],
        [15116],
        [14248],
        [13980],
        [11466],
        [11560],
        [10286],
        [10167]], device='cuda:0')
[2024-07-24 10:17:37,066][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[ 9240],
        [ 9276],
        [ 6054],
        [ 7122],
        [10416],
        [ 8683],
        [ 9634],
        [10973],
        [12123],
        [17543],
        [11612],
        [16853],
        [23608],
        [14831],
        [17241],
        [15014],
        [13135]], device='cuda:0')
[2024-07-24 10:17:37,067][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[ 2122],
        [ 8994],
        [ 9726],
        [10028],
        [ 9663],
        [ 9481],
        [ 7800],
        [ 7748],
        [ 7700],
        [ 7476],
        [ 7613],
        [ 7978],
        [ 7908],
        [ 7894],
        [ 7919],
        [ 7813],
        [ 7856]], device='cuda:0')
[2024-07-24 10:17:37,068][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[14918],
        [39113],
        [16406],
        [44574],
        [32458],
        [18504],
        [44234],
        [19795],
        [22139],
        [32890],
        [23830],
        [11526],
        [41648],
        [25298],
        [14406],
        [31211],
        [22692]], device='cuda:0')
[2024-07-24 10:17:37,069][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[28569],
        [48186],
        [31123],
        [45350],
        [43401],
        [40882],
        [25609],
        [43861],
        [31724],
        [40112],
        [22336],
        [48627],
        [34340],
        [29445],
        [35521],
        [43210],
        [27674]], device='cuda:0')
[2024-07-24 10:17:37,070][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[21087],
        [28526],
        [27604],
        [27265],
        [22563],
        [22027],
        [23736],
        [25175],
        [25270],
        [24936],
        [24325],
        [25106],
        [25214],
        [24147],
        [21543],
        [20203],
        [20890]], device='cuda:0')
[2024-07-24 10:17:37,072][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 1328],
        [ 1113],
        [16739],
        [ 5900],
        [ 5575],
        [ 2029],
        [ 1258],
        [ 3452],
        [ 2426],
        [ 1418],
        [ 2711],
        [ 1176],
        [ 4709],
        [ 2509],
        [ 2429],
        [ 1365],
        [ 1485]], device='cuda:0')
[2024-07-24 10:17:37,074][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[29780],
        [29852],
        [30460],
        [28198],
        [27988],
        [28414],
        [27162],
        [27358],
        [27696],
        [27396],
        [27617],
        [27823],
        [28405],
        [28366],
        [28277],
        [28321],
        [28253]], device='cuda:0')
[2024-07-24 10:17:37,075][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[31388],
        [42803],
        [41318],
        [45970],
        [39521],
        [35096],
        [33851],
        [29466],
        [26050],
        [29013],
        [23302],
        [22274],
        [22794],
        [21998],
        [22488],
        [21090],
        [19824]], device='cuda:0')
[2024-07-24 10:17:37,077][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[24553],
        [22270],
        [26187],
        [26582],
        [25492],
        [27296],
        [26451],
        [27188],
        [28626],
        [28564],
        [28701],
        [28455],
        [27976],
        [28609],
        [28150],
        [28194],
        [28612]], device='cuda:0')
[2024-07-24 10:17:37,079][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[16168],
        [16810],
        [ 6482],
        [12065],
        [12946],
        [15729],
        [16540],
        [13109],
        [ 9899],
        [21541],
        [12870],
        [14276],
        [18016],
        [20746],
        [30151],
        [19401],
        [17706]], device='cuda:0')
[2024-07-24 10:17:37,080][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 5570],
        [ 5440],
        [11119],
        [ 8825],
        [14958],
        [18532],
        [14498],
        [20798],
        [21218],
        [21687],
        [22333],
        [22415],
        [22526],
        [23613],
        [24615],
        [24792],
        [25586]], device='cuda:0')
[2024-07-24 10:17:37,082][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[24459],
        [24531],
        [22979],
        [25993],
        [33540],
        [34212],
        [38481],
        [43114],
        [38222],
        [42894],
        [40251],
        [47237],
        [42095],
        [43623],
        [36226],
        [41874],
        [39172]], device='cuda:0')
[2024-07-24 10:17:37,084][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25053],
        [24950],
        [24678],
        [24634],
        [24576],
        [24445],
        [23901],
        [23658],
        [24313],
        [24396],
        [23980],
        [24222],
        [24258],
        [23890],
        [23585],
        [23734],
        [23433]], device='cuda:0')
[2024-07-24 10:17:37,085][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[5093],
        [ 789],
        [ 789],
        [ 798],
        [ 752],
        [ 766],
        [ 740],
        [ 766],
        [ 753],
        [ 763],
        [ 698],
        [ 770],
        [ 810],
        [ 730],
        [ 764],
        [ 775],
        [ 709]], device='cuda:0')
[2024-07-24 10:17:37,086][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[30378],
        [12875],
        [21973],
        [22021],
        [26111],
        [27215],
        [30603],
        [32723],
        [32815],
        [34561],
        [36159],
        [35372],
        [37167],
        [37144],
        [38424],
        [38583],
        [38443]], device='cuda:0')
[2024-07-24 10:17:37,087][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[38271],
        [24066],
        [30129],
        [16361],
        [32586],
        [43916],
        [ 8705],
        [43779],
        [41961],
        [16194],
        [39856],
        [32076],
        [18316],
        [40839],
        [29940],
        [38151],
        [39658]], device='cuda:0')
[2024-07-24 10:17:37,088][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[15877],
        [26307],
        [19113],
        [20567],
        [17629],
        [15789],
        [20495],
        [12851],
        [14454],
        [12952],
        [15870],
        [13734],
        [12478],
        [12259],
        [11625],
        [15128],
        [16172]], device='cuda:0')
[2024-07-24 10:17:37,090][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[  575],
        [ 2928],
        [11770],
        [ 7971],
        [ 5659],
        [ 6462],
        [14582],
        [ 3072],
        [ 8908],
        [ 6048],
        [13408],
        [ 1087],
        [ 4987],
        [ 8839],
        [ 2552],
        [ 1421],
        [11604]], device='cuda:0')
[2024-07-24 10:17:37,091][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595],
        [29595]], device='cuda:0')
[2024-07-24 10:17:37,120][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:37,120][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,121][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,121][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,121][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,122][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,122][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,123][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,124][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,124][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,124][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,125][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,125][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,126][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.4964, 0.5036], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,128][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.9379, 0.0621], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,129][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.4759, 0.5241], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,131][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.2529, 0.7471], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,132][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.6110, 0.3890], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,134][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.2012, 0.7988], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,136][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.2255, 0.7745], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,137][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.5835, 0.4165], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,138][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.7315, 0.2685], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,138][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.5326, 0.4674], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,138][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0063, 0.9937], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,138][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.5636, 0.4364], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,139][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3159, 0.4170, 0.2671], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,139][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.8886, 0.0567, 0.0547], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,139][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0239, 0.9552, 0.0209], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,140][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.1620, 0.5344, 0.3036], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,140][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.4490, 0.2830, 0.2680], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,140][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0511, 0.2404, 0.7085], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,141][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0526, 0.3951, 0.5523], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,142][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3407, 0.3849, 0.2744], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,143][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2868, 0.3150, 0.3982], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,145][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.2576, 0.5203, 0.2220], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,146][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ and] are: tensor([5.8401e-04, 6.3472e-01, 3.6469e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,147][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2263, 0.2680, 0.5057], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,149][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.1963, 0.3077, 0.2525, 0.2435], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,151][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.8117, 0.0609, 0.0565, 0.0709], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,152][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.0394, 0.7867, 0.1607, 0.0131], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,154][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.0748, 0.3276, 0.2508, 0.3468], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,154][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.1711, 0.4438, 0.3141, 0.0711], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,155][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0066, 0.0487, 0.1630, 0.7817], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,155][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.2648, 0.2616, 0.2733, 0.2004], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,155][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.2965, 0.2012, 0.2161, 0.2862], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,156][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.1756, 0.1953, 0.5251, 0.1040], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,156][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.1693, 0.3692, 0.2410, 0.2206], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,156][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.0024, 0.3787, 0.2404, 0.3786], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,157][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.3061, 0.1704, 0.3401, 0.1834], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,157][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.1615, 0.2390, 0.1925, 0.2199, 0.1872], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,158][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.7789, 0.0645, 0.0567, 0.0718, 0.0280], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,158][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0277, 0.7555, 0.1131, 0.0685, 0.0352], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,159][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0773, 0.2226, 0.1839, 0.2744, 0.2418], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,160][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.3659, 0.1481, 0.3142, 0.0744, 0.0975], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,162][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0036, 0.0327, 0.0940, 0.4359, 0.4337], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,163][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1286, 0.1697, 0.2478, 0.1327, 0.3212], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,165][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.2231, 0.1749, 0.1815, 0.2546, 0.1658], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,166][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.1531, 0.1486, 0.2931, 0.1534, 0.2517], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,168][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.1168, 0.3002, 0.1459, 0.2522, 0.1849], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,170][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0023, 0.2022, 0.1507, 0.2264, 0.4185], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,171][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.1816, 0.1482, 0.3748, 0.1501, 0.1452], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,172][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1609, 0.1797, 0.1618, 0.1910, 0.1752, 0.1314], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,172][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.7678, 0.0563, 0.0498, 0.0643, 0.0229, 0.0389], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,173][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0644, 0.4975, 0.1711, 0.0129, 0.2510, 0.0032], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,173][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0686, 0.1662, 0.1407, 0.2286, 0.2075, 0.1884], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,173][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1936, 0.1210, 0.2393, 0.1412, 0.2002, 0.1046], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,174][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0034, 0.0274, 0.0718, 0.3488, 0.3264, 0.2222], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,174][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0957, 0.1438, 0.2195, 0.1038, 0.2890, 0.1483], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,174][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1708, 0.1666, 0.1291, 0.2329, 0.1432, 0.1574], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,175][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0860, 0.0911, 0.1970, 0.1402, 0.3772, 0.1085], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,175][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1382, 0.2377, 0.1202, 0.2012, 0.1927, 0.1101], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,175][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0015, 0.1289, 0.0930, 0.1457, 0.2835, 0.3474], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,176][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1387, 0.1251, 0.3254, 0.1228, 0.1569, 0.1312], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,178][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.1016, 0.1476, 0.1312, 0.1653, 0.1725, 0.1073, 0.1745],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,180][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.5849, 0.0751, 0.0618, 0.0789, 0.0381, 0.0588, 0.1025],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,181][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.0291, 0.6542, 0.0982, 0.0709, 0.0989, 0.0413, 0.0075],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,183][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.0300, 0.0980, 0.0712, 0.1854, 0.2169, 0.2384, 0.1601],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,184][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.2591, 0.0715, 0.1914, 0.1467, 0.1798, 0.1312, 0.0203],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,186][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([2.8519e-04, 3.8128e-03, 1.2777e-02, 9.0121e-02, 8.1496e-02, 5.7880e-02,
        7.5363e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,187][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.1996, 0.0916, 0.1317, 0.0972, 0.2052, 0.1141, 0.1606],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,188][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.1676, 0.1329, 0.1392, 0.1679, 0.1291, 0.1324, 0.1310],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,189][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.0916, 0.0681, 0.1890, 0.0750, 0.3276, 0.1422, 0.1065],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,190][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.1025, 0.1767, 0.0923, 0.1672, 0.1933, 0.1208, 0.1472],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,190][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.0068, 0.1110, 0.0948, 0.1325, 0.2411, 0.2800, 0.1339],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,190][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.1562, 0.1371, 0.1580, 0.1295, 0.1434, 0.0959, 0.1799],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,191][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0935, 0.1373, 0.0939, 0.1402, 0.1242, 0.0931, 0.1644, 0.1533],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,191][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.6014, 0.0597, 0.0544, 0.0682, 0.0303, 0.0457, 0.0894, 0.0509],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,191][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0429, 0.5334, 0.0961, 0.0560, 0.1345, 0.0499, 0.0813, 0.0060],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,192][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0433, 0.1083, 0.0732, 0.1681, 0.1451, 0.1749, 0.2236, 0.0635],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,192][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1409, 0.0701, 0.2313, 0.0900, 0.1728, 0.1864, 0.0500, 0.0584],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,193][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0010, 0.0075, 0.0209, 0.0810, 0.0877, 0.0573, 0.6466, 0.0981],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,193][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0485, 0.0834, 0.1161, 0.0740, 0.1827, 0.0970, 0.1208, 0.2775],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,195][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1383, 0.1106, 0.0943, 0.1566, 0.1058, 0.1084, 0.1177, 0.1683],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,197][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0433, 0.0373, 0.1070, 0.0933, 0.3139, 0.1647, 0.0940, 0.1464],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,198][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0905, 0.1674, 0.0862, 0.1574, 0.1566, 0.1073, 0.1687, 0.0660],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,200][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0020, 0.0836, 0.0607, 0.0983, 0.1830, 0.2180, 0.0988, 0.2557],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,201][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0927, 0.0825, 0.2204, 0.0794, 0.1033, 0.1039, 0.1123, 0.2055],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,203][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0922, 0.1150, 0.0911, 0.1247, 0.1049, 0.0783, 0.1376, 0.1798, 0.0766],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,205][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5560, 0.0564, 0.0519, 0.0639, 0.0278, 0.0431, 0.0811, 0.0470, 0.0729],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,206][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0418, 0.3600, 0.1073, 0.0133, 0.3112, 0.0129, 0.0383, 0.1131, 0.0021],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,207][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0426, 0.0878, 0.0778, 0.1194, 0.1237, 0.1529, 0.1935, 0.1099, 0.0925],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,207][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1398, 0.1189, 0.1568, 0.1764, 0.0834, 0.1121, 0.0749, 0.0768, 0.0609],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,207][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([5.2057e-04, 4.5831e-03, 1.4549e-02, 6.8499e-02, 7.2547e-02, 4.9910e-02,
        5.9797e-01, 8.5145e-02, 1.0627e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,208][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0309, 0.0571, 0.0895, 0.0579, 0.1457, 0.0802, 0.0939, 0.2334, 0.2115],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,208][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1132, 0.0958, 0.0827, 0.1310, 0.0846, 0.0935, 0.1070, 0.1506, 0.1417],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,209][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0335, 0.0605, 0.0995, 0.0658, 0.1651, 0.0815, 0.1266, 0.3294, 0.0382],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,209][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1092, 0.1547, 0.0828, 0.1287, 0.1356, 0.0753, 0.1616, 0.0841, 0.0679],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,209][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0015, 0.0534, 0.0380, 0.0687, 0.1339, 0.1525, 0.0718, 0.1920, 0.2881],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,210][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0843, 0.0703, 0.2162, 0.0711, 0.1049, 0.0881, 0.0821, 0.2008, 0.0822],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,210][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0672, 0.0965, 0.0696, 0.1103, 0.0944, 0.0641, 0.1513, 0.1572, 0.0703,
        0.1192], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,211][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.4988, 0.0499, 0.0481, 0.0600, 0.0281, 0.0428, 0.0804, 0.0484, 0.0713,
        0.0722], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,213][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0127, 0.5868, 0.0540, 0.0202, 0.1243, 0.0327, 0.0522, 0.0845, 0.0205,
        0.0123], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,215][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0147, 0.0677, 0.0516, 0.1197, 0.1457, 0.1527, 0.1856, 0.0838, 0.0958,
        0.0827], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,216][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0862, 0.0910, 0.1610, 0.0621, 0.1548, 0.1372, 0.0729, 0.0757, 0.1396,
        0.0194], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,217][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ station] are: tensor([3.6115e-04, 3.7125e-03, 1.3033e-02, 5.6299e-02, 6.2744e-02, 4.2644e-02,
        5.4459e-01, 7.9521e-02, 1.0457e-01, 9.2529e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,218][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0315, 0.0348, 0.0427, 0.0417, 0.0715, 0.0446, 0.0799, 0.1449, 0.1519,
        0.3565], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,220][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.1113, 0.0879, 0.0862, 0.1124, 0.0870, 0.0826, 0.0936, 0.1288, 0.1157,
        0.0945], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,222][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0394, 0.0474, 0.0725, 0.0333, 0.1202, 0.0890, 0.1106, 0.3351, 0.0584,
        0.0940], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,224][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0557, 0.1350, 0.0729, 0.1151, 0.1277, 0.1016, 0.1435, 0.0804, 0.0820,
        0.0862], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,225][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0040, 0.0485, 0.0379, 0.0620, 0.1088, 0.1264, 0.0660, 0.1584, 0.2276,
        0.1603], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,227][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0938, 0.0899, 0.1414, 0.0830, 0.0888, 0.0737, 0.1243, 0.1140, 0.0926,
        0.0985], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,228][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0673, 0.0851, 0.0688, 0.1025, 0.0950, 0.0690, 0.1068, 0.1297, 0.0675,
        0.1252, 0.0832], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,229][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.4956, 0.0473, 0.0443, 0.0540, 0.0264, 0.0379, 0.0714, 0.0444, 0.0619,
        0.0636, 0.0532], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,229][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0319, 0.2415, 0.0759, 0.0528, 0.2775, 0.0525, 0.0790, 0.0528, 0.0208,
        0.1133, 0.0019], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,230][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0356, 0.0731, 0.0553, 0.1104, 0.1201, 0.1187, 0.1548, 0.0755, 0.0922,
        0.1173, 0.0469], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,230][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1151, 0.0425, 0.0919, 0.0865, 0.0855, 0.1532, 0.0764, 0.0561, 0.1459,
        0.0726, 0.0742], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,231][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0005, 0.0041, 0.0121, 0.0479, 0.0597, 0.0393, 0.4859, 0.0770, 0.0933,
        0.1052, 0.0749], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,231][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0222, 0.0318, 0.0463, 0.0347, 0.0775, 0.0436, 0.0531, 0.1240, 0.1131,
        0.2859, 0.1676], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,231][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0911, 0.0858, 0.0682, 0.1106, 0.0690, 0.0752, 0.0815, 0.1199, 0.1103,
        0.0825, 0.1058], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,232][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0425, 0.0305, 0.0665, 0.0464, 0.1265, 0.0911, 0.1241, 0.2384, 0.0614,
        0.1114, 0.0613], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,232][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0727, 0.1199, 0.0660, 0.1095, 0.1071, 0.0811, 0.1261, 0.0716, 0.0709,
        0.1142, 0.0609], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,233][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0011, 0.0403, 0.0278, 0.0499, 0.0870, 0.0991, 0.0493, 0.1220, 0.1690,
        0.1220, 0.2324], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,234][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0687, 0.0593, 0.1749, 0.0666, 0.0839, 0.0708, 0.0833, 0.1384, 0.0805,
        0.0726, 0.1012], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,236][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0503, 0.0599, 0.0530, 0.0888, 0.0778, 0.0570, 0.1087, 0.1278, 0.0572,
        0.1140, 0.0729, 0.1325], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,237][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.3841, 0.0524, 0.0486, 0.0569, 0.0313, 0.0427, 0.0762, 0.0492, 0.0647,
        0.0677, 0.0592, 0.0669], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,239][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0573, 0.0514, 0.0472, 0.0518, 0.2886, 0.0545, 0.1074, 0.0157, 0.0401,
        0.2478, 0.0077, 0.0305], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,240][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.0115, 0.0405, 0.0342, 0.0848, 0.1180, 0.1273, 0.1499, 0.0800, 0.0810,
        0.0924, 0.0374, 0.1430], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,241][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0444, 0.0432, 0.0992, 0.0521, 0.0364, 0.0988, 0.1624, 0.0364, 0.0817,
        0.1955, 0.1000, 0.0501], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,243][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([1.2094e-04, 1.4120e-03, 5.5087e-03, 3.1782e-02, 3.2666e-02, 2.1876e-02,
        3.1891e-01, 4.5520e-02, 5.8969e-02, 6.0794e-02, 5.4827e-02, 3.6761e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,244][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0425, 0.0241, 0.0262, 0.0345, 0.0451, 0.0302, 0.0545, 0.0928, 0.0962,
        0.2018, 0.1276, 0.2243], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,246][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0890, 0.0677, 0.0716, 0.0895, 0.0676, 0.0701, 0.0652, 0.1073, 0.1005,
        0.0740, 0.1080, 0.0893], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,246][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0305, 0.0146, 0.0632, 0.0199, 0.1184, 0.0861, 0.0687, 0.2396, 0.0728,
        0.1504, 0.0794, 0.0563], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,247][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0483, 0.0646, 0.0602, 0.0898, 0.1185, 0.0788, 0.1303, 0.0747, 0.0732,
        0.1158, 0.0679, 0.0778], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,247][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0006, 0.0428, 0.0295, 0.0460, 0.0817, 0.0990, 0.0505, 0.1119, 0.1620,
        0.1054, 0.1891, 0.0816], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,248][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0881, 0.0496, 0.1083, 0.0691, 0.0837, 0.0680, 0.1114, 0.0958, 0.0809,
        0.1046, 0.0867, 0.0537], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,248][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0441, 0.0673, 0.0545, 0.0826, 0.0678, 0.0512, 0.0695, 0.1182, 0.0540,
        0.1107, 0.0760, 0.1487, 0.0554], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,248][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.3990, 0.0450, 0.0428, 0.0514, 0.0276, 0.0377, 0.0696, 0.0447, 0.0588,
        0.0618, 0.0530, 0.0623, 0.0462], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,249][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0217, 0.2927, 0.1032, 0.0295, 0.1170, 0.0294, 0.0254, 0.0162, 0.0471,
        0.0640, 0.0167, 0.2179, 0.0190], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,249][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0229, 0.0532, 0.0391, 0.0941, 0.0683, 0.1024, 0.1086, 0.0643, 0.0769,
        0.1013, 0.0401, 0.1594, 0.0695], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,250][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.1607, 0.0318, 0.1136, 0.0410, 0.1146, 0.1014, 0.0259, 0.0520, 0.1155,
        0.0479, 0.1264, 0.0406, 0.0285], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,251][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([2.5777e-04, 1.8761e-03, 6.5389e-03, 2.9111e-02, 3.1844e-02, 2.3192e-02,
        3.0443e-01, 4.4218e-02, 5.7232e-02, 6.4859e-02, 4.9327e-02, 3.6124e-01,
        2.5869e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,252][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0180, 0.0205, 0.0286, 0.0243, 0.0500, 0.0293, 0.0375, 0.0811, 0.0774,
        0.1761, 0.1133, 0.1712, 0.1726], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,254][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0809, 0.0634, 0.0645, 0.0814, 0.0548, 0.0622, 0.0607, 0.1029, 0.0917,
        0.0689, 0.1137, 0.0901, 0.0649], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,256][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0294, 0.0462, 0.0430, 0.0384, 0.0766, 0.0606, 0.0561, 0.1223, 0.0483,
        0.0776, 0.0751, 0.1528, 0.1736], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,257][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0494, 0.1130, 0.0526, 0.0860, 0.0807, 0.0689, 0.0961, 0.0606, 0.0579,
        0.0819, 0.0569, 0.1219, 0.0741], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,259][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0013, 0.0358, 0.0282, 0.0441, 0.0751, 0.0866, 0.0436, 0.0974, 0.1383,
        0.0899, 0.1704, 0.0773, 0.1121], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,261][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0715, 0.0553, 0.1201, 0.0545, 0.0633, 0.0508, 0.0862, 0.1084, 0.0754,
        0.1028, 0.0969, 0.0530, 0.0618], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,262][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0530, 0.0624, 0.0495, 0.0688, 0.0645, 0.0463, 0.0897, 0.1005, 0.0448,
        0.0875, 0.0628, 0.1317, 0.0661, 0.0724], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,264][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.4556, 0.0349, 0.0359, 0.0426, 0.0205, 0.0308, 0.0594, 0.0359, 0.0525,
        0.0545, 0.0446, 0.0538, 0.0371, 0.0419], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,264][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0356, 0.1881, 0.0581, 0.0222, 0.1155, 0.0324, 0.0844, 0.0208, 0.0229,
        0.0769, 0.0083, 0.1462, 0.1878, 0.0010], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,265][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0315, 0.0538, 0.0396, 0.0764, 0.0649, 0.0863, 0.1037, 0.0474, 0.0694,
        0.1008, 0.0397, 0.1604, 0.1003, 0.0257], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,265][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0472, 0.0506, 0.0933, 0.0483, 0.1051, 0.0766, 0.0385, 0.0487, 0.0557,
        0.0456, 0.0932, 0.0518, 0.2025, 0.0429], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,265][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.2012e-04, 1.6478e-03, 5.6806e-03, 2.8040e-02, 2.8042e-02, 1.8173e-02,
        2.7713e-01, 3.7331e-02, 4.7266e-02, 5.8903e-02, 4.6726e-02, 3.5537e-01,
        2.6414e-02, 6.9050e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,266][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0228, 0.0146, 0.0219, 0.0229, 0.0427, 0.0250, 0.0333, 0.0696, 0.0668,
        0.1480, 0.0944, 0.1463, 0.1407, 0.1508], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,266][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0750, 0.0633, 0.0508, 0.0841, 0.0560, 0.0563, 0.0616, 0.0960, 0.0843,
        0.0627, 0.0872, 0.0815, 0.0707, 0.0703], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,267][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0157, 0.0188, 0.0467, 0.0252, 0.1101, 0.0457, 0.0983, 0.0998, 0.0320,
        0.0818, 0.0543, 0.0631, 0.2828, 0.0256], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,267][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0626, 0.0896, 0.0456, 0.0946, 0.0809, 0.0573, 0.0978, 0.0427, 0.0514,
        0.0907, 0.0467, 0.1078, 0.1054, 0.0270], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,268][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0023, 0.0200, 0.0148, 0.0306, 0.0519, 0.0516, 0.0297, 0.0707, 0.0945,
        0.0818, 0.1420, 0.0685, 0.1099, 0.2318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,270][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0435, 0.0553, 0.1287, 0.0536, 0.0735, 0.0588, 0.0539, 0.1273, 0.0648,
        0.0446, 0.0863, 0.0479, 0.0654, 0.0963], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,272][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0413, 0.0595, 0.0430, 0.0626, 0.0576, 0.0425, 0.0755, 0.0888, 0.0451,
        0.0830, 0.0541, 0.1309, 0.0861, 0.0854, 0.0444], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,273][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.3242, 0.0428, 0.0400, 0.0484, 0.0282, 0.0375, 0.0644, 0.0433, 0.0556,
        0.0563, 0.0486, 0.0556, 0.0434, 0.0502, 0.0614], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,275][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0233, 0.1971, 0.0501, 0.0393, 0.0576, 0.0215, 0.0594, 0.0153, 0.0237,
        0.0650, 0.0109, 0.1444, 0.2229, 0.0629, 0.0067], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,276][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0195, 0.0497, 0.0348, 0.0573, 0.0673, 0.0910, 0.1083, 0.0513, 0.0765,
        0.0924, 0.0331, 0.1442, 0.0972, 0.0262, 0.0510], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,278][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0975, 0.0952, 0.1312, 0.0256, 0.0556, 0.0738, 0.0370, 0.0432, 0.0785,
        0.0154, 0.1413, 0.0924, 0.0598, 0.0496, 0.0040], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,279][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ give] are: tensor([2.8668e-04, 1.9789e-03, 6.6566e-03, 2.5105e-02, 2.7714e-02, 1.9005e-02,
        2.7271e-01, 3.9679e-02, 5.1228e-02, 5.8870e-02, 4.8917e-02, 3.3051e-01,
        2.8222e-02, 6.4680e-02, 2.4439e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,281][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0229, 0.0152, 0.0190, 0.0212, 0.0345, 0.0211, 0.0309, 0.0585, 0.0593,
        0.1243, 0.0821, 0.1322, 0.1172, 0.1338, 0.1278], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,281][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0674, 0.0568, 0.0548, 0.0719, 0.0524, 0.0571, 0.0506, 0.0882, 0.0791,
        0.0568, 0.0857, 0.0726, 0.0679, 0.0699, 0.0688], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,282][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0231, 0.0372, 0.0331, 0.0212, 0.0406, 0.0387, 0.0633, 0.0712, 0.0376,
        0.0602, 0.0447, 0.1029, 0.3054, 0.0619, 0.0589], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,282][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0493, 0.0938, 0.0409, 0.0747, 0.0688, 0.0597, 0.0961, 0.0579, 0.0521,
        0.0770, 0.0450, 0.1131, 0.0935, 0.0291, 0.0490], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,283][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0010, 0.0293, 0.0205, 0.0361, 0.0557, 0.0629, 0.0344, 0.0714, 0.0992,
        0.0679, 0.1235, 0.0586, 0.0833, 0.1934, 0.0628], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,283][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0535, 0.0435, 0.1003, 0.0415, 0.0603, 0.0526, 0.0635, 0.1024, 0.0688,
        0.0604, 0.0778, 0.0396, 0.0635, 0.1094, 0.0630], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,284][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0372, 0.0553, 0.0434, 0.0631, 0.0617, 0.0403, 0.0719, 0.0876, 0.0395,
        0.0769, 0.0571, 0.1179, 0.0696, 0.0773, 0.0682, 0.0330],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,284][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.3568, 0.0368, 0.0351, 0.0423, 0.0224, 0.0318, 0.0579, 0.0368, 0.0501,
        0.0514, 0.0434, 0.0499, 0.0376, 0.0437, 0.0536, 0.0504],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,285][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0286, 0.0619, 0.0727, 0.0201, 0.1151, 0.0276, 0.0449, 0.0099, 0.0087,
        0.1371, 0.0064, 0.0410, 0.2832, 0.0239, 0.1186, 0.0003],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,285][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0177, 0.0397, 0.0333, 0.0542, 0.0657, 0.0766, 0.1026, 0.0551, 0.0583,
        0.0802, 0.0330, 0.1244, 0.0953, 0.0356, 0.0871, 0.0413],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,287][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0672, 0.0302, 0.1166, 0.0867, 0.0511, 0.0998, 0.0474, 0.0424, 0.0643,
        0.0515, 0.1150, 0.0424, 0.1019, 0.0586, 0.0169, 0.0080],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,288][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ it] are: tensor([2.0563e-04, 1.6698e-03, 5.9745e-03, 2.6481e-02, 2.7820e-02, 1.8131e-02,
        2.7182e-01, 3.4870e-02, 4.6707e-02, 5.4201e-02, 4.5486e-02, 3.1123e-01,
        2.6754e-02, 6.6636e-02, 2.6918e-02, 3.5094e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,290][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0102, 0.0132, 0.0173, 0.0186, 0.0316, 0.0195, 0.0273, 0.0546, 0.0540,
        0.1100, 0.0753, 0.1143, 0.1026, 0.1158, 0.1122, 0.1234],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,292][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0712, 0.0500, 0.0483, 0.0665, 0.0467, 0.0496, 0.0507, 0.0843, 0.0722,
        0.0536, 0.0798, 0.0668, 0.0579, 0.0614, 0.0585, 0.0824],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,293][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0157, 0.0172, 0.0385, 0.0195, 0.0599, 0.0426, 0.0459, 0.1349, 0.0285,
        0.0628, 0.0500, 0.0563, 0.2101, 0.0665, 0.1377, 0.0139],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,295][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0418, 0.0774, 0.0461, 0.0722, 0.0767, 0.0588, 0.0892, 0.0454, 0.0476,
        0.0749, 0.0488, 0.0860, 0.0889, 0.0307, 0.0730, 0.0425],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,297][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0007, 0.0227, 0.0173, 0.0291, 0.0477, 0.0546, 0.0280, 0.0654, 0.0916,
        0.0629, 0.1187, 0.0525, 0.0771, 0.1919, 0.0587, 0.0811],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,298][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0553, 0.0397, 0.1128, 0.0409, 0.0500, 0.0551, 0.0450, 0.1148, 0.0563,
        0.0421, 0.0850, 0.0388, 0.0516, 0.1067, 0.0631, 0.0428],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,299][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0460, 0.0530, 0.0426, 0.0577, 0.0550, 0.0393, 0.0764, 0.0867, 0.0374,
        0.0749, 0.0533, 0.1109, 0.0567, 0.0620, 0.0556, 0.0354, 0.0571],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,299][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.4350, 0.0292, 0.0296, 0.0361, 0.0155, 0.0251, 0.0502, 0.0289, 0.0450,
        0.0459, 0.0362, 0.0435, 0.0292, 0.0328, 0.0407, 0.0424, 0.0345],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,300][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0327, 0.1633, 0.0500, 0.0214, 0.1027, 0.0299, 0.0821, 0.0208, 0.0235,
        0.0693, 0.0078, 0.1280, 0.1606, 0.0010, 0.0894, 0.0173, 0.0005],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,300][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0260, 0.0450, 0.0334, 0.0643, 0.0537, 0.0716, 0.0869, 0.0390, 0.0583,
        0.0836, 0.0331, 0.1316, 0.0826, 0.0212, 0.0967, 0.0516, 0.0214],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,301][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0434, 0.0409, 0.0782, 0.0387, 0.0908, 0.0668, 0.0307, 0.0443, 0.0494,
        0.0403, 0.0863, 0.0424, 0.1930, 0.0391, 0.0321, 0.0552, 0.0283],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,301][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.3907e-04, 1.7769e-03, 6.2480e-03, 2.3820e-02, 2.7574e-02, 1.6473e-02,
        2.4827e-01, 3.5833e-02, 4.4597e-02, 5.4564e-02, 4.3439e-02, 3.0359e-01,
        2.6155e-02, 6.3156e-02, 2.6795e-02, 3.6158e-02, 4.1311e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,302][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0173, 0.0108, 0.0153, 0.0173, 0.0298, 0.0180, 0.0249, 0.0484, 0.0474,
        0.0989, 0.0642, 0.0998, 0.0925, 0.1009, 0.0997, 0.1095, 0.1053],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,302][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0590, 0.0534, 0.0421, 0.0690, 0.0452, 0.0454, 0.0495, 0.0771, 0.0670,
        0.0501, 0.0692, 0.0645, 0.0561, 0.0553, 0.0562, 0.0823, 0.0584],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,304][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0115, 0.0143, 0.0337, 0.0186, 0.0845, 0.0315, 0.0774, 0.0731, 0.0222,
        0.0641, 0.0396, 0.0472, 0.2077, 0.0181, 0.2180, 0.0228, 0.0157],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,306][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0556, 0.0766, 0.0399, 0.0792, 0.0711, 0.0501, 0.0848, 0.0366, 0.0446,
        0.0761, 0.0398, 0.0906, 0.0889, 0.0236, 0.0691, 0.0526, 0.0208],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,307][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0019, 0.0149, 0.0112, 0.0238, 0.0379, 0.0367, 0.0221, 0.0507, 0.0664,
        0.0577, 0.0994, 0.0484, 0.0774, 0.1604, 0.0560, 0.0750, 0.1603],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,309][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0380, 0.0424, 0.1070, 0.0419, 0.0556, 0.0476, 0.0429, 0.1029, 0.0533,
        0.0363, 0.0700, 0.0380, 0.0521, 0.0825, 0.0557, 0.0535, 0.0800],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,337][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:37,339][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,340][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,341][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,342][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,343][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,345][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,346][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,347][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,349][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,349][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,350][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,350][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,350][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.5239, 0.4761], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,351][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.9632, 0.0368], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,351][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0115, 0.9885], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,351][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.2235, 0.7765], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,352][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.7618, 0.2382], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,352][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([9.4973e-04, 9.9905e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,352][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0736, 0.9264], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,353][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([1.5468e-04, 9.9985e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,353][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.6972, 0.3028], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,354][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.9182, 0.0818], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,355][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0600, 0.9400], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,357][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0325, 0.9675], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,358][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3349, 0.4309, 0.2342], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,360][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9337, 0.0343, 0.0320], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,361][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([4.5817e-05, 2.5645e-01, 7.4350e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,363][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.1486, 0.5081, 0.3433], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,364][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.5829, 0.2642, 0.1529], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,366][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0007, 0.6165, 0.3828], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,367][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0082, 0.4626, 0.5292], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,367][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([5.0412e-05, 9.9242e-01, 7.5343e-03], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,367][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.3315, 0.3240, 0.3445], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,368][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.7934, 0.1173, 0.0893], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,368][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0536, 0.5996, 0.3468], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,368][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0505, 0.6811, 0.2684], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,369][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.2095, 0.3276, 0.2466, 0.2163], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,369][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.7898, 0.0520, 0.0456, 0.1126], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,369][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([6.7684e-07, 5.4146e-03, 1.8867e-01, 8.0592e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,370][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.0706, 0.2897, 0.2229, 0.4167], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,370][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.5376, 0.2089, 0.1480, 0.1054], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,370][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([7.3174e-05, 2.1511e-01, 3.1668e-01, 4.6814e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,371][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.0518, 0.3004, 0.2482, 0.3997], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,372][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([6.1232e-04, 9.6560e-01, 2.3800e-02, 9.9824e-03], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,374][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.2297, 0.2230, 0.4103, 0.1370], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,376][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.7569, 0.0986, 0.0791, 0.0654], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,377][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.0228, 0.3612, 0.2047, 0.4114], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,379][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0352, 0.4920, 0.2029, 0.2699], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,380][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.1791, 0.2572, 0.1896, 0.2030, 0.1711], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,382][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.8339, 0.0418, 0.0314, 0.0834, 0.0095], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,383][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([2.0504e-07, 3.5707e-04, 1.7326e-02, 7.4224e-01, 2.4008e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,384][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0667, 0.2110, 0.1576, 0.3144, 0.2503], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,384][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.5545, 0.2042, 0.1300, 0.0715, 0.0398], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,385][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([1.9926e-05, 3.2468e-02, 5.0542e-02, 1.0520e-01, 8.1177e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,385][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0353, 0.1854, 0.2523, 0.2731, 0.2539], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,385][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([2.3299e-04, 9.6076e-01, 1.3188e-02, 6.8325e-03, 1.8986e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,386][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.1679, 0.1472, 0.2291, 0.1686, 0.2871], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,386][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.7037, 0.0985, 0.0741, 0.0646, 0.0591], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,386][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0099, 0.1746, 0.1006, 0.2170, 0.4979], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,387][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0079, 0.2410, 0.1247, 0.1550, 0.4714], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,387][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1713, 0.1886, 0.1606, 0.1799, 0.1667, 0.1329], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,387][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.7989, 0.0424, 0.0311, 0.0845, 0.0103, 0.0329], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,388][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([6.8516e-08, 7.5210e-05, 5.3926e-03, 1.4193e-01, 5.9181e-01, 2.6079e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,390][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0545, 0.1718, 0.1251, 0.2597, 0.2251, 0.1637], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,391][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.5815, 0.1847, 0.1088, 0.0636, 0.0359, 0.0255], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,392][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([1.4975e-05, 1.6813e-02, 2.9793e-02, 5.9675e-02, 6.9602e-01, 1.9768e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,394][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0173, 0.1430, 0.2144, 0.2106, 0.2255, 0.1891], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,395][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([3.5446e-04, 9.4024e-01, 1.5261e-02, 8.5004e-03, 2.4025e-02, 1.1623e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,396][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1092, 0.1016, 0.1581, 0.1411, 0.3593, 0.1307], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,398][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.7049, 0.0806, 0.0629, 0.0538, 0.0498, 0.0479], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,400][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0120, 0.1442, 0.0862, 0.1781, 0.3565, 0.2230], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,401][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0030, 0.1261, 0.0727, 0.0816, 0.2925, 0.4241], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,402][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.1111, 0.1566, 0.1315, 0.1566, 0.1677, 0.1072, 0.1692],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,402][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.4346, 0.0701, 0.0528, 0.1105, 0.0262, 0.0665, 0.2394],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,402][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([1.0125e-08, 1.7998e-05, 1.1428e-03, 3.7437e-02, 8.5517e-02, 4.6687e-01,
        4.0902e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,403][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0246, 0.1083, 0.0854, 0.1960, 0.1693, 0.1423, 0.2741],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,403][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.5423, 0.1536, 0.1088, 0.0707, 0.0446, 0.0307, 0.0493],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,403][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([2.7611e-06, 6.4624e-03, 1.3088e-02, 1.9869e-02, 2.7072e-01, 1.4611e-01,
        5.4375e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,404][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0563, 0.1000, 0.1273, 0.2134, 0.1579, 0.1327, 0.2124],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,404][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0018, 0.8174, 0.0417, 0.0250, 0.0589, 0.0360, 0.0193],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,405][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.1108, 0.0741, 0.1463, 0.0811, 0.3184, 0.1568, 0.1125],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,405][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.6729, 0.0774, 0.0616, 0.0523, 0.0489, 0.0473, 0.0395],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,406][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.0054, 0.1073, 0.0608, 0.1233, 0.3256, 0.1708, 0.2068],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,408][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0114, 0.1656, 0.0672, 0.0997, 0.2366, 0.3007, 0.1187],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,409][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0966, 0.1503, 0.0903, 0.1346, 0.1168, 0.0939, 0.1685, 0.1491],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,411][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.5525, 0.0440, 0.0360, 0.0824, 0.0148, 0.0370, 0.1894, 0.0439],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,412][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([9.7244e-09, 1.2460e-06, 7.5892e-05, 3.8969e-03, 6.7804e-03, 4.8265e-02,
        1.7365e-01, 7.6734e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,413][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0352, 0.1064, 0.0753, 0.1522, 0.1378, 0.1153, 0.2602, 0.1176],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,415][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.5550, 0.1477, 0.1014, 0.0685, 0.0389, 0.0300, 0.0367, 0.0217],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,416][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([5.3193e-06, 4.6802e-03, 8.3153e-03, 1.2677e-02, 1.3456e-01, 7.1343e-02,
        6.5811e-01, 1.1031e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,418][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0114, 0.0770, 0.1066, 0.1623, 0.1373, 0.1205, 0.1508, 0.2341],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,419][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([3.8539e-04, 9.1527e-01, 1.9616e-02, 1.0533e-02, 2.8229e-02, 1.4005e-02,
        6.1520e-03, 5.8136e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,419][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0614, 0.0451, 0.0931, 0.0942, 0.2806, 0.1631, 0.1026, 0.1599],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,419][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.6364, 0.0719, 0.0587, 0.0506, 0.0466, 0.0451, 0.0391, 0.0516],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,420][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0076, 0.0984, 0.0569, 0.1208, 0.2506, 0.1550, 0.1880, 0.1228],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,420][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0044, 0.1078, 0.0504, 0.0733, 0.2011, 0.2637, 0.0774, 0.2220],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,421][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0946, 0.1229, 0.0869, 0.1186, 0.0974, 0.0791, 0.1387, 0.1847, 0.0771],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,421][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.5296, 0.0391, 0.0317, 0.0733, 0.0137, 0.0353, 0.1768, 0.0421, 0.0584],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,421][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([3.0366e-10, 4.6306e-08, 3.2524e-06, 1.1950e-04, 3.0119e-04, 5.7845e-04,
        9.1411e-03, 7.8734e-01, 2.0252e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,422][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0306, 0.0921, 0.0681, 0.1333, 0.1188, 0.1026, 0.2271, 0.1168, 0.1104],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,422][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.5025, 0.1450, 0.1033, 0.0755, 0.0439, 0.0348, 0.0401, 0.0241, 0.0307],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,423][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([1.6086e-06, 2.4681e-03, 5.0911e-03, 7.9846e-03, 1.0501e-01, 4.5129e-02,
        5.6239e-01, 1.2123e-01, 1.5070e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,425][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0060, 0.0516, 0.0862, 0.1207, 0.1127, 0.1036, 0.1186, 0.2151, 0.1855],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,426][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([7.4527e-04, 8.4994e-01, 2.9718e-02, 1.8863e-02, 4.3722e-02, 2.5101e-02,
        1.1380e-02, 1.2342e-02, 8.1883e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,428][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0494, 0.0649, 0.0877, 0.0710, 0.1768, 0.0972, 0.1204, 0.2776, 0.0550],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,429][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.6003, 0.0650, 0.0544, 0.0468, 0.0441, 0.0426, 0.0371, 0.0493, 0.0606],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,431][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0078, 0.0897, 0.0554, 0.1091, 0.2241, 0.1415, 0.1678, 0.1102, 0.0944],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,432][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0023, 0.0663, 0.0410, 0.0504, 0.1492, 0.2072, 0.0452, 0.1818, 0.2567],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,432][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0723, 0.1028, 0.0662, 0.1056, 0.0867, 0.0618, 0.1596, 0.1607, 0.0708,
        0.1136], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,433][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.4276, 0.0336, 0.0311, 0.0709, 0.0153, 0.0402, 0.1757, 0.0464, 0.0647,
        0.0945], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,433][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([8.0678e-11, 1.1258e-08, 7.9533e-07, 2.8534e-05, 6.3341e-05, 3.0376e-04,
        3.5799e-03, 1.5705e-01, 3.5850e-01, 4.8047e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,434][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0234, 0.0743, 0.0573, 0.1152, 0.1051, 0.0862, 0.2026, 0.1060, 0.1109,
        0.1189], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,434][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.4030, 0.1220, 0.0969, 0.0786, 0.0497, 0.0366, 0.0536, 0.0305, 0.0378,
        0.0915], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,435][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([6.9208e-07, 1.0308e-03, 2.1419e-03, 3.8305e-03, 3.7698e-02, 1.9872e-02,
        1.9848e-01, 5.3013e-02, 8.0790e-02, 6.0315e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,437][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0091, 0.0389, 0.0509, 0.1104, 0.0678, 0.0645, 0.1170, 0.1449, 0.1401,
        0.2563], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,438][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0018, 0.7665, 0.0351, 0.0318, 0.0489, 0.0325, 0.0203, 0.0159, 0.0139,
        0.0333], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,439][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0522, 0.0489, 0.0647, 0.0422, 0.1343, 0.0995, 0.1061, 0.2679, 0.0734,
        0.1107], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,439][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.5271, 0.0638, 0.0553, 0.0476, 0.0454, 0.0441, 0.0385, 0.0510, 0.0617,
        0.0654], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,439][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0044, 0.0792, 0.0439, 0.0907, 0.2246, 0.1229, 0.1484, 0.0927, 0.0782,
        0.1149], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,440][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0037, 0.0706, 0.0326, 0.0456, 0.1274, 0.1645, 0.0580, 0.1592, 0.2269,
        0.1115], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,440][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0701, 0.0903, 0.0665, 0.1001, 0.0899, 0.0716, 0.1080, 0.1336, 0.0705,
        0.1188, 0.0807], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,441][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.4725, 0.0331, 0.0289, 0.0636, 0.0131, 0.0306, 0.1477, 0.0402, 0.0503,
        0.0785, 0.0415], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,441][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([3.9463e-11, 2.1802e-09, 1.6964e-07, 1.7619e-05, 1.4637e-05, 7.4964e-05,
        8.6362e-04, 7.1772e-03, 7.2522e-02, 2.6492e-01, 6.5441e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,442][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0282, 0.0756, 0.0555, 0.1094, 0.0989, 0.0820, 0.1686, 0.0923, 0.0958,
        0.1044, 0.0893], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,442][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.4030, 0.1429, 0.1056, 0.0829, 0.0455, 0.0379, 0.0388, 0.0242, 0.0299,
        0.0655, 0.0241], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,443][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([7.3565e-07, 8.0951e-04, 1.6896e-03, 2.8132e-03, 3.6474e-02, 1.5659e-02,
        1.3594e-01, 3.8082e-02, 6.5423e-02, 6.3088e-01, 7.2236e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,444][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0038, 0.0314, 0.0484, 0.0807, 0.0668, 0.0618, 0.0762, 0.1257, 0.1105,
        0.2030, 0.1916], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,446][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([7.1442e-04, 8.4723e-01, 2.8804e-02, 1.7129e-02, 3.7154e-02, 2.0437e-02,
        9.7038e-03, 9.5118e-03, 6.4944e-03, 1.5607e-02, 7.2137e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,447][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0527, 0.0346, 0.0588, 0.0516, 0.1254, 0.0937, 0.1115, 0.2027, 0.0710,
        0.1200, 0.0780], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,449][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.5607, 0.0566, 0.0463, 0.0401, 0.0374, 0.0358, 0.0301, 0.0410, 0.0509,
        0.0548, 0.0461], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,450][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0074, 0.0758, 0.0469, 0.0943, 0.1808, 0.1139, 0.1384, 0.0901, 0.0786,
        0.1125, 0.0612], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,452][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0029, 0.0583, 0.0337, 0.0458, 0.1154, 0.1501, 0.0416, 0.1302, 0.1802,
        0.0871, 0.1548], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,454][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0558, 0.0610, 0.0508, 0.0858, 0.0720, 0.0571, 0.1136, 0.1342, 0.0592,
        0.1111, 0.0708, 0.1285], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,455][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.3554, 0.0388, 0.0343, 0.0678, 0.0165, 0.0371, 0.1512, 0.0453, 0.0585,
        0.0852, 0.0489, 0.0611], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,456][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([8.3445e-11, 7.1583e-11, 1.5533e-08, 7.5145e-07, 1.3578e-06, 3.9603e-06,
        7.7203e-05, 1.2459e-04, 3.8718e-03, 1.0221e-02, 1.0894e-01, 8.7676e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,456][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.0181, 0.0567, 0.0473, 0.0861, 0.0840, 0.0710, 0.1649, 0.0873, 0.0895,
        0.1020, 0.0907, 0.1023], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,457][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.3296, 0.1156, 0.0955, 0.0786, 0.0496, 0.0381, 0.0506, 0.0293, 0.0362,
        0.0895, 0.0307, 0.0568], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,457][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([1.3495e-06, 5.3313e-04, 1.4088e-03, 2.2831e-03, 2.5268e-02, 1.2089e-02,
        1.0662e-01, 2.9493e-02, 4.2033e-02, 4.1894e-01, 9.0975e-02, 2.7035e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,457][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0105, 0.0274, 0.0304, 0.0969, 0.0424, 0.0424, 0.0844, 0.0884, 0.0898,
        0.1507, 0.1386, 0.1983], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,458][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0018, 0.7349, 0.0405, 0.0301, 0.0473, 0.0329, 0.0182, 0.0188, 0.0141,
        0.0266, 0.0161, 0.0187], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,458][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.0415, 0.0200, 0.0541, 0.0263, 0.1279, 0.0884, 0.0682, 0.1944, 0.0770,
        0.1459, 0.0881, 0.0682], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,459][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.4577, 0.0642, 0.0528, 0.0464, 0.0441, 0.0420, 0.0363, 0.0467, 0.0556,
        0.0593, 0.0513, 0.0436], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,459][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0044, 0.0668, 0.0383, 0.0771, 0.1828, 0.1047, 0.1249, 0.0794, 0.0680,
        0.1015, 0.0529, 0.0993], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,460][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0042, 0.0561, 0.0252, 0.0359, 0.0989, 0.1259, 0.0467, 0.1245, 0.1718,
        0.0802, 0.1355, 0.0950], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,462][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0489, 0.0708, 0.0546, 0.0800, 0.0632, 0.0516, 0.0676, 0.1270, 0.0559,
        0.1098, 0.0761, 0.1492, 0.0453], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,463][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.3654, 0.0392, 0.0332, 0.0660, 0.0161, 0.0340, 0.1381, 0.0424, 0.0504,
        0.0775, 0.0427, 0.0566, 0.0383], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,464][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([1.2760e-12, 5.0669e-12, 7.1099e-10, 1.7462e-08, 1.0502e-07, 2.2794e-07,
        2.4893e-06, 5.7165e-05, 2.3239e-04, 9.0228e-04, 1.8526e-02, 8.1624e-01,
        1.6403e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,466][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0238, 0.0564, 0.0423, 0.0852, 0.0732, 0.0623, 0.1355, 0.0723, 0.0735,
        0.0909, 0.0816, 0.1036, 0.0995], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,468][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.4014, 0.1236, 0.0947, 0.0596, 0.0379, 0.0278, 0.0307, 0.0220, 0.0277,
        0.0637, 0.0229, 0.0381, 0.0499], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,469][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([6.3875e-07, 2.9801e-04, 4.8743e-04, 1.1765e-03, 1.1733e-02, 5.3438e-03,
        5.4481e-02, 1.3940e-02, 2.3536e-02, 2.3921e-01, 4.1100e-02, 2.4123e-01,
        3.6746e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,471][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0051, 0.0221, 0.0293, 0.0601, 0.0432, 0.0415, 0.0568, 0.0824, 0.0771,
        0.1377, 0.1351, 0.1723, 0.1374], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,472][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([5.4856e-04, 8.7948e-01, 2.1605e-02, 1.4537e-02, 2.7688e-02, 1.4591e-02,
        6.7286e-03, 5.8937e-03, 4.1240e-03, 1.0293e-02, 4.5455e-03, 6.1475e-03,
        3.8231e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,473][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0367, 0.0433, 0.0393, 0.0420, 0.0853, 0.0651, 0.0581, 0.1154, 0.0532,
        0.0800, 0.0763, 0.1353, 0.1700], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,473][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.4574, 0.0593, 0.0475, 0.0419, 0.0388, 0.0374, 0.0320, 0.0421, 0.0505,
        0.0539, 0.0464, 0.0407, 0.0520], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,474][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0039, 0.0583, 0.0348, 0.0709, 0.1546, 0.0928, 0.1109, 0.0722, 0.0603,
        0.0899, 0.0499, 0.0875, 0.1140], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,474][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0040, 0.0497, 0.0259, 0.0336, 0.0859, 0.1173, 0.0405, 0.1168, 0.1591,
        0.0730, 0.1357, 0.0879, 0.0707], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,475][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0582, 0.0676, 0.0492, 0.0669, 0.0596, 0.0478, 0.0945, 0.1070, 0.0466,
        0.0834, 0.0617, 0.1341, 0.0563, 0.0672], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,475][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.4604, 0.0216, 0.0213, 0.0487, 0.0093, 0.0237, 0.1266, 0.0314, 0.0407,
        0.0711, 0.0346, 0.0478, 0.0335, 0.0291], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,475][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.9794e-12, 1.5494e-12, 1.0223e-10, 4.8940e-09, 9.9904e-09, 5.0877e-08,
        4.9097e-07, 4.4480e-06, 4.6691e-05, 9.7261e-05, 1.6245e-03, 8.4117e-02,
        1.1151e-01, 8.0260e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,476][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0235, 0.0544, 0.0401, 0.0763, 0.0669, 0.0593, 0.1276, 0.0686, 0.0710,
        0.0819, 0.0727, 0.0892, 0.0975, 0.0709], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,476][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.3902, 0.1183, 0.0867, 0.0636, 0.0419, 0.0333, 0.0350, 0.0201, 0.0258,
        0.0584, 0.0215, 0.0330, 0.0512, 0.0211], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,477][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([5.0235e-07, 2.6079e-04, 6.0570e-04, 7.3831e-04, 9.1370e-03, 3.7820e-03,
        3.7046e-02, 1.0279e-02, 1.5391e-02, 1.5518e-01, 2.9376e-02, 1.3824e-01,
        3.6911e-01, 2.3086e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,479][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0054, 0.0157, 0.0251, 0.0596, 0.0388, 0.0381, 0.0525, 0.0758, 0.0707,
        0.1171, 0.1198, 0.1544, 0.1205, 0.1066], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,481][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0018, 0.7350, 0.0421, 0.0313, 0.0456, 0.0320, 0.0159, 0.0160, 0.0116,
        0.0190, 0.0124, 0.0143, 0.0116, 0.0115], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,482][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0226, 0.0221, 0.0440, 0.0286, 0.1041, 0.0525, 0.0794, 0.0994, 0.0412,
        0.0822, 0.0664, 0.0701, 0.2449, 0.0425], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,484][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.5739, 0.0397, 0.0347, 0.0295, 0.0274, 0.0258, 0.0213, 0.0307, 0.0389,
        0.0422, 0.0345, 0.0309, 0.0415, 0.0290], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,485][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0057, 0.0569, 0.0375, 0.0681, 0.1398, 0.0892, 0.1022, 0.0701, 0.0608,
        0.0852, 0.0500, 0.0850, 0.1057, 0.0439], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,487][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0009, 0.0317, 0.0215, 0.0265, 0.0739, 0.1058, 0.0191, 0.0877, 0.1257,
        0.0516, 0.1071, 0.0658, 0.0630, 0.2198], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,489][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0470, 0.0654, 0.0429, 0.0611, 0.0533, 0.0432, 0.0775, 0.0929, 0.0476,
        0.0799, 0.0528, 0.1328, 0.0796, 0.0856, 0.0384], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,490][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.3044, 0.0387, 0.0333, 0.0631, 0.0179, 0.0341, 0.1214, 0.0417, 0.0491,
        0.0714, 0.0417, 0.0538, 0.0392, 0.0382, 0.0523], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,490][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([1.1542e-13, 4.5333e-14, 4.2954e-12, 2.7859e-10, 1.0106e-10, 1.1859e-09,
        2.6667e-08, 2.3282e-07, 1.2888e-06, 3.3095e-06, 9.2962e-05, 2.5148e-03,
        4.1489e-03, 7.9136e-01, 2.0187e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,491][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0204, 0.0468, 0.0377, 0.0642, 0.0606, 0.0546, 0.1179, 0.0615, 0.0667,
        0.0756, 0.0669, 0.0782, 0.0895, 0.0682, 0.0912], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,491][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.3622, 0.1169, 0.0870, 0.0636, 0.0377, 0.0300, 0.0317, 0.0218, 0.0273,
        0.0610, 0.0231, 0.0352, 0.0495, 0.0259, 0.0271], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,492][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([2.7370e-07, 1.3755e-04, 2.8015e-04, 3.3323e-04, 3.3526e-03, 1.9399e-03,
        1.7738e-02, 5.0407e-03, 7.4101e-03, 7.7645e-02, 1.5831e-02, 7.9543e-02,
        1.6709e-01, 1.3995e-01, 4.8372e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,492][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0075, 0.0171, 0.0229, 0.0585, 0.0332, 0.0330, 0.0512, 0.0658, 0.0630,
        0.1052, 0.1074, 0.1402, 0.1045, 0.0969, 0.0938], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,493][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0011, 0.8152, 0.0359, 0.0233, 0.0362, 0.0221, 0.0103, 0.0091, 0.0068,
        0.0111, 0.0068, 0.0083, 0.0055, 0.0052, 0.0031], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,493][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0291, 0.0340, 0.0309, 0.0255, 0.0515, 0.0448, 0.0597, 0.0732, 0.0418,
        0.0650, 0.0516, 0.0945, 0.2516, 0.0677, 0.0791], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,494][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.4115, 0.0552, 0.0450, 0.0403, 0.0367, 0.0349, 0.0297, 0.0388, 0.0463,
        0.0496, 0.0422, 0.0368, 0.0486, 0.0388, 0.0455], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,495][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0037, 0.0502, 0.0306, 0.0609, 0.1359, 0.0805, 0.0946, 0.0640, 0.0523,
        0.0772, 0.0426, 0.0756, 0.1029, 0.0353, 0.0939], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,496][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0021, 0.0344, 0.0168, 0.0229, 0.0651, 0.0865, 0.0279, 0.0798, 0.1151,
        0.0527, 0.0909, 0.0618, 0.0536, 0.2190, 0.0714], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,498][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0393, 0.0597, 0.0426, 0.0617, 0.0590, 0.0414, 0.0746, 0.0923, 0.0412,
        0.0739, 0.0568, 0.1206, 0.0636, 0.0771, 0.0659, 0.0302],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,500][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.3220, 0.0321, 0.0283, 0.0562, 0.0139, 0.0292, 0.1190, 0.0362, 0.0441,
        0.0675, 0.0374, 0.0490, 0.0356, 0.0337, 0.0474, 0.0483],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,500][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([2.9380e-14, 9.4979e-15, 8.8472e-13, 4.5208e-11, 7.8993e-11, 4.0884e-10,
        5.4837e-09, 5.8809e-08, 3.0976e-07, 1.0569e-06, 1.1906e-05, 3.7703e-04,
        8.1057e-04, 6.9380e-02, 8.2235e-01, 1.0707e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,502][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0182, 0.0431, 0.0324, 0.0608, 0.0554, 0.0483, 0.1035, 0.0582, 0.0603,
        0.0663, 0.0608, 0.0712, 0.0780, 0.0678, 0.0905, 0.0853],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,504][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.3561, 0.1107, 0.0872, 0.0606, 0.0405, 0.0304, 0.0310, 0.0211, 0.0254,
        0.0564, 0.0225, 0.0340, 0.0511, 0.0235, 0.0267, 0.0228],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,505][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([1.6873e-07, 5.4984e-05, 1.5914e-04, 1.5799e-04, 1.8247e-03, 8.5537e-04,
        8.7504e-03, 1.9263e-03, 3.0678e-03, 3.2023e-02, 8.1729e-03, 3.1672e-02,
        7.2553e-02, 7.3213e-02, 4.0344e-01, 3.6213e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,507][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0030, 0.0146, 0.0217, 0.0484, 0.0325, 0.0321, 0.0455, 0.0639, 0.0615,
        0.0960, 0.1013, 0.1217, 0.0964, 0.0886, 0.0905, 0.0823],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,507][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0013, 0.7754, 0.0372, 0.0253, 0.0395, 0.0265, 0.0119, 0.0120, 0.0090,
        0.0152, 0.0093, 0.0105, 0.0076, 0.0077, 0.0043, 0.0072],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,508][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0224, 0.0199, 0.0342, 0.0229, 0.0670, 0.0483, 0.0469, 0.1129, 0.0343,
        0.0635, 0.0540, 0.0594, 0.1823, 0.0711, 0.1378, 0.0231],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,508][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.4119, 0.0514, 0.0425, 0.0372, 0.0346, 0.0326, 0.0275, 0.0359, 0.0433,
        0.0462, 0.0396, 0.0337, 0.0460, 0.0361, 0.0444, 0.0371],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,509][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0039, 0.0477, 0.0299, 0.0575, 0.1231, 0.0774, 0.0906, 0.0620, 0.0518,
        0.0731, 0.0408, 0.0724, 0.0940, 0.0357, 0.0846, 0.0555],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,509][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0012, 0.0258, 0.0162, 0.0192, 0.0551, 0.0818, 0.0179, 0.0759, 0.1056,
        0.0393, 0.0866, 0.0500, 0.0467, 0.1991, 0.0613, 0.1182],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,510][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0512, 0.0585, 0.0427, 0.0575, 0.0516, 0.0414, 0.0816, 0.0929, 0.0396,
        0.0721, 0.0527, 0.1145, 0.0488, 0.0579, 0.0508, 0.0328, 0.0533],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,510][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.4278, 0.0186, 0.0177, 0.0435, 0.0072, 0.0205, 0.1207, 0.0259, 0.0356,
        0.0622, 0.0282, 0.0398, 0.0282, 0.0232, 0.0358, 0.0398, 0.0253],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,511][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([2.5112e-14, 2.5473e-15, 1.3435e-13, 7.0898e-12, 9.5580e-12, 4.6024e-11,
        6.2021e-10, 2.9944e-09, 3.2214e-08, 6.1903e-08, 9.6408e-07, 5.3246e-05,
        6.2834e-05, 4.5156e-04, 8.1599e-02, 1.6513e-01, 7.5271e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,512][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0200, 0.0427, 0.0321, 0.0572, 0.0514, 0.0463, 0.0958, 0.0535, 0.0548,
        0.0613, 0.0557, 0.0652, 0.0709, 0.0533, 0.0835, 0.0859, 0.0703],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,514][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.3140, 0.1289, 0.0899, 0.0659, 0.0471, 0.0361, 0.0328, 0.0201, 0.0253,
        0.0552, 0.0202, 0.0307, 0.0489, 0.0193, 0.0252, 0.0229, 0.0172],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,515][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.6698e-07, 4.6761e-05, 1.3687e-04, 1.2338e-04, 1.5888e-03, 6.6985e-04,
        6.1132e-03, 1.7594e-03, 2.5020e-03, 2.5140e-02, 5.7043e-03, 2.1380e-02,
        5.5032e-02, 3.8503e-02, 2.6276e-01, 3.1763e-01, 2.6092e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,517][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0047, 0.0127, 0.0200, 0.0487, 0.0315, 0.0311, 0.0428, 0.0602, 0.0570,
        0.0874, 0.0929, 0.1166, 0.0895, 0.0817, 0.0817, 0.0764, 0.0652],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,518][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0022, 0.6810, 0.0437, 0.0357, 0.0445, 0.0347, 0.0182, 0.0176, 0.0133,
        0.0194, 0.0139, 0.0155, 0.0123, 0.0130, 0.0074, 0.0121, 0.0152],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,520][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0172, 0.0168, 0.0325, 0.0212, 0.0808, 0.0378, 0.0633, 0.0750, 0.0300,
        0.0645, 0.0489, 0.0530, 0.1803, 0.0312, 0.1862, 0.0337, 0.0276],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,521][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.4993, 0.0395, 0.0342, 0.0294, 0.0267, 0.0249, 0.0208, 0.0289, 0.0363,
        0.0393, 0.0321, 0.0288, 0.0386, 0.0275, 0.0374, 0.0306, 0.0257],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,523][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0048, 0.0470, 0.0313, 0.0566, 0.1164, 0.0743, 0.0848, 0.0584, 0.0506,
        0.0705, 0.0413, 0.0700, 0.0879, 0.0366, 0.0816, 0.0550, 0.0329],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,524][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0006, 0.0209, 0.0147, 0.0175, 0.0493, 0.0727, 0.0122, 0.0594, 0.0862,
        0.0334, 0.0715, 0.0433, 0.0421, 0.1502, 0.0480, 0.0999, 0.1781],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,526][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:37,527][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[14937],
        [ 8419],
        [12967],
        [  677],
        [13875],
        [ 7587],
        [20448],
        [25206],
        [21726],
        [ 3516],
        [ 8733],
        [ 8235],
        [16401],
        [19713],
        [ 7273],
        [14893],
        [19860]], device='cuda:0')
[2024-07-24 10:17:37,528][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[24720],
        [16225],
        [18617],
        [ 1154],
        [18384],
        [10802],
        [21213],
        [28582],
        [25236],
        [ 4970],
        [12139],
        [10714],
        [25629],
        [25125],
        [ 8119],
        [16066],
        [24244]], device='cuda:0')
[2024-07-24 10:17:37,529][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[11746],
        [11853],
        [12495],
        [10917],
        [ 8576],
        [ 9159],
        [ 8384],
        [ 8826],
        [ 9486],
        [ 9024],
        [ 9416],
        [ 9814],
        [10019],
        [10057],
        [ 9861],
        [ 9524],
        [ 9788]], device='cuda:0')
[2024-07-24 10:17:37,531][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[32292],
        [33268],
        [33785],
        [32897],
        [33102],
        [33544],
        [34546],
        [34919],
        [35200],
        [35466],
        [35327],
        [35570],
        [35173],
        [35024],
        [35361],
        [35667],
        [35522]], device='cuda:0')
[2024-07-24 10:17:37,532][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[48220],
        [46761],
        [39533],
        [40896],
        [40865],
        [44078],
        [42011],
        [43358],
        [46101],
        [43845],
        [45626],
        [44989],
        [43859],
        [43881],
        [42148],
        [39530],
        [42721]], device='cuda:0')
[2024-07-24 10:17:37,534][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[13505],
        [ 6443],
        [ 8519],
        [ 9803],
        [12500],
        [13684],
        [12907],
        [11545],
        [11391],
        [10613],
        [ 9955],
        [10131],
        [ 9371],
        [ 9314],
        [ 9698],
        [10084],
        [ 9936]], device='cuda:0')
[2024-07-24 10:17:37,535][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 9516],
        [ 8996],
        [ 8466],
        [ 7847],
        [ 7723],
        [ 7069],
        [ 6818],
        [ 6774],
        [ 6246],
        [ 7096],
        [ 6701],
        [ 5341],
        [ 8291],
        [10983],
        [ 8791],
        [ 8364],
        [10626]], device='cuda:0')
[2024-07-24 10:17:37,537][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[1080],
        [ 749],
        [1475],
        [2612],
        [2344],
        [2078],
        [2268],
        [2170],
        [2079],
        [2146],
        [2158],
        [2053],
        [2070],
        [2037],
        [2017],
        [1960],
        [1925]], device='cuda:0')
[2024-07-24 10:17:37,539][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[45579],
        [20405],
        [33232],
        [33198],
        [32664],
        [33294],
        [33241],
        [34118],
        [36145],
        [32863],
        [35992],
        [39572],
        [38688],
        [37659],
        [36249],
        [36663],
        [36360]], device='cuda:0')
[2024-07-24 10:17:37,540][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[37959],
        [20105],
        [19585],
        [10111],
        [ 9796],
        [ 9786],
        [11107],
        [10364],
        [10357],
        [12246],
        [11849],
        [11932],
        [12384],
        [12458],
        [13855],
        [13901],
        [13906]], device='cuda:0')
[2024-07-24 10:17:37,542][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[34779],
        [33217],
        [36353],
        [36457],
        [32571],
        [33349],
        [35258],
        [36060],
        [37330],
        [39004],
        [38838],
        [39870],
        [37061],
        [36909],
        [38065],
        [40504],
        [40470]], device='cuda:0')
[2024-07-24 10:17:37,543][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[43498],
        [43612],
        [43960],
        [44312],
        [44948],
        [44737],
        [45574],
        [45382],
        [45322],
        [45598],
        [45589],
        [45663],
        [45465],
        [45651],
        [45530],
        [45634],
        [45631]], device='cuda:0')
[2024-07-24 10:17:37,544][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[35388],
        [ 6544],
        [ 9662],
        [ 6393],
        [ 8148],
        [ 6646],
        [ 5984],
        [ 5879],
        [ 4898],
        [ 5655],
        [ 6379],
        [ 6442],
        [ 7100],
        [ 7771],
        [ 7760],
        [ 7957],
        [ 8461]], device='cuda:0')
[2024-07-24 10:17:37,545][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[40259],
        [17126],
        [28556],
        [26823],
        [29292],
        [31305],
        [34528],
        [36590],
        [38310],
        [37908],
        [38630],
        [37926],
        [37933],
        [37526],
        [39476],
        [39279],
        [39027]], device='cuda:0')
[2024-07-24 10:17:37,546][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[11822],
        [38479],
        [30521],
        [19475],
        [46731],
        [42512],
        [46320],
        [45712],
        [39042],
        [46055],
        [43241],
        [45177],
        [37656],
        [35065],
        [41357],
        [44630],
        [40001]], device='cuda:0')
[2024-07-24 10:17:37,548][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[42976],
        [38476],
        [37863],
        [36731],
        [36818],
        [38096],
        [37067],
        [36165],
        [36450],
        [35231],
        [36500],
        [35758],
        [36038],
        [36573],
        [36521],
        [36424],
        [36678]], device='cuda:0')
[2024-07-24 10:17:37,550][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[13918],
        [14928],
        [14485],
        [15077],
        [14896],
        [14326],
        [10446],
        [10370],
        [ 9812],
        [ 8777],
        [ 8942],
        [ 8089],
        [ 7905],
        [ 7909],
        [ 6790],
        [ 6307],
        [ 6628]], device='cuda:0')
[2024-07-24 10:17:37,551][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[24689],
        [23052],
        [17375],
        [21484],
        [20392],
        [ 7151],
        [ 6249],
        [42122],
        [43664],
        [ 7991],
        [11048],
        [22551],
        [19824],
        [31514],
        [31166],
        [19188],
        [36456]], device='cuda:0')
[2024-07-24 10:17:37,553][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[21602],
        [27353],
        [27869],
        [29171],
        [28021],
        [28097],
        [28506],
        [28788],
        [28903],
        [29033],
        [28967],
        [28983],
        [28539],
        [28666],
        [29121],
        [29130],
        [29231]], device='cuda:0')
[2024-07-24 10:17:37,555][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[28795],
        [33793],
        [35841],
        [34991],
        [35112],
        [34542],
        [33453],
        [33095],
        [31976],
        [30364],
        [30716],
        [28215],
        [29452],
        [28929],
        [28145],
        [27781],
        [27862]], device='cuda:0')
[2024-07-24 10:17:37,556][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[12253],
        [23874],
        [21658],
        [18960],
        [20935],
        [23275],
        [23461],
        [24086],
        [26650],
        [20664],
        [20171],
        [21277],
        [20881],
        [21022],
        [22465],
        [23522],
        [23665]], device='cuda:0')
[2024-07-24 10:17:37,558][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 5844],
        [ 4010],
        [ 1787],
        [ 3052],
        [ 2609],
        [ 3525],
        [ 8096],
        [ 6547],
        [ 7247],
        [10005],
        [ 7790],
        [ 5596],
        [ 5971],
        [ 6145],
        [ 6156],
        [ 6426],
        [ 6454]], device='cuda:0')
[2024-07-24 10:17:37,560][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[14680],
        [15301],
        [15308],
        [15397],
        [15562],
        [15820],
        [17112],
        [16092],
        [16861],
        [17909],
        [16918],
        [18378],
        [16589],
        [18558],
        [17364],
        [18028],
        [19657]], device='cuda:0')
[2024-07-24 10:17:37,561][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[41281],
        [42159],
        [42952],
        [43197],
        [41201],
        [40704],
        [40926],
        [40469],
        [41029],
        [40616],
        [40826],
        [40536],
        [39665],
        [38190],
        [37732],
        [37600],
        [36990]], device='cuda:0')
[2024-07-24 10:17:37,562][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[23787],
        [23740],
        [23505],
        [23348],
        [23140],
        [23126],
        [23019],
        [22949],
        [22919],
        [22712],
        [22756],
        [22429],
        [22404],
        [22679],
        [22269],
        [22325],
        [22561]], device='cuda:0')
[2024-07-24 10:17:37,563][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[8181],
        [6772],
        [7481],
        [5704],
        [6237],
        [6251],
        [5874],
        [6405],
        [6234],
        [6218],
        [6411],
        [6495],
        [6568],
        [6910],
        [7050],
        [6882],
        [7117]], device='cuda:0')
[2024-07-24 10:17:37,564][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 6859],
        [ 9945],
        [17744],
        [20021],
        [15176],
        [18616],
        [18670],
        [19571],
        [19648],
        [20018],
        [20466],
        [20004],
        [19642],
        [17945],
        [17710],
        [18551],
        [17157]], device='cuda:0')
[2024-07-24 10:17:37,566][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[13351],
        [ 9823],
        [11305],
        [11017],
        [12569],
        [15099],
        [15416],
        [ 9363],
        [ 8922],
        [15193],
        [15075],
        [12745],
        [13495],
        [11438],
        [11484],
        [13575],
        [10725]], device='cuda:0')
[2024-07-24 10:17:37,567][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[15416],
        [ 5306],
        [ 5892],
        [ 7681],
        [  547],
        [  912],
        [  761],
        [  580],
        [ 1197],
        [ 1519],
        [  911],
        [ 2688],
        [ 1901],
        [ 1885],
        [ 1471],
        [ 1887],
        [ 1545]], device='cuda:0')
[2024-07-24 10:17:37,569][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352],
        [32352]], device='cuda:0')
[2024-07-24 10:17:37,600][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:37,602][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,603][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,604][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,605][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,606][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,608][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,609][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,610][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,611][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,612][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,612][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,612][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,612][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9781, 0.0219], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,613][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.5016, 0.4984], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,613][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.3296, 0.6704], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,613][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.8147, 0.1853], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,614][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([9.9937e-01, 6.3085e-04], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,614][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.5751, 0.4249], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,614][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.2362, 0.7638], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,615][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([5.4706e-05, 9.9995e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,615][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.4160, 0.5840], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,616][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.6177, 0.3823], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,618][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.8939, 0.1061], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,619][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.1525, 0.8475], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,620][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.9159e-01, 8.3935e-03, 1.6582e-05], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,622][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.3255, 0.3311, 0.3435], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,623][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.1797, 0.3603, 0.4600], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,625][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.6621, 0.1688, 0.1691], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,626][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ and] are: tensor([9.9949e-01, 4.0598e-04, 1.0652e-04], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,627][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.4290, 0.4420, 0.1290], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,629][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2030, 0.7960, 0.0010], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,629][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ and] are: tensor([8.4864e-06, 3.9424e-01, 6.0575e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,629][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2386, 0.3494, 0.4121], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,630][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.2615, 0.3027, 0.4358], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,630][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.7018, 0.2464, 0.0518], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,630][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0897, 0.4168, 0.4935], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,631][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([9.8607e-01, 3.8010e-03, 2.0939e-05, 1.0110e-02], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,631][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.2416, 0.2370, 0.2492, 0.2722], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,631][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.1270, 0.2752, 0.3831, 0.2147], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,632][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.5013, 0.1633, 0.2086, 0.1267], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,632][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([9.9928e-01, 4.9404e-04, 1.2485e-04, 9.8102e-05], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,632][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.3704, 0.3495, 0.1059, 0.1742], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,633][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.0142, 0.8542, 0.0185, 0.1131], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,634][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([3.5157e-06, 9.7601e-02, 1.7803e-01, 7.2436e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,636][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.1257, 0.2624, 0.4150, 0.1969], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,638][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.1107, 0.3039, 0.4346, 0.1508], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,639][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.6252, 0.2328, 0.0558, 0.0862], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,641][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0748, 0.2290, 0.3727, 0.3235], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,641][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ got] are: tensor([9.6024e-01, 1.3621e-02, 4.7190e-04, 1.0931e-02, 1.4737e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,643][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.1929, 0.1940, 0.2020, 0.2215, 0.1896], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,645][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0951, 0.2167, 0.3221, 0.2139, 0.1522], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,646][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.5226, 0.1364, 0.1525, 0.1094, 0.0791], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,646][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ got] are: tensor([9.9881e-01, 5.9722e-04, 2.0303e-04, 3.3240e-04, 5.9191e-05],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,647][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.2929, 0.2963, 0.1023, 0.1844, 0.1241], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,647][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0189, 0.5757, 0.0098, 0.3736, 0.0219], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,647][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ got] are: tensor([1.0269e-06, 3.3898e-02, 8.7194e-02, 4.9780e-01, 3.8111e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,648][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.1070, 0.1975, 0.3661, 0.1563, 0.1731], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,648][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.4610, 0.0250, 0.1991, 0.1342, 0.1807], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,648][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.7893, 0.0801, 0.0198, 0.0322, 0.0787], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,649][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0600, 0.1330, 0.2641, 0.2672, 0.2758], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,649][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([9.4319e-01, 2.5468e-02, 7.5536e-04, 1.9846e-02, 8.3434e-03, 2.4011e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,650][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.1614, 0.1642, 0.1694, 0.1872, 0.1597, 0.1581], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,650][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0826, 0.1681, 0.2402, 0.1682, 0.1416, 0.1993], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,652][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.4231, 0.1199, 0.1307, 0.1041, 0.0963, 0.1258], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,653][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([9.9746e-01, 8.3823e-04, 6.1474e-04, 8.2278e-04, 1.8733e-04, 8.0951e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,655][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.2433, 0.3026, 0.0924, 0.1726, 0.1022, 0.0869], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,656][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0262, 0.5263, 0.0369, 0.3059, 0.1029, 0.0018], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,657][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([9.3945e-07, 5.1334e-02, 7.5157e-02, 4.0017e-01, 3.6741e-01, 1.0593e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,659][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0995, 0.1767, 0.2555, 0.1696, 0.1968, 0.1019], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,660][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1904, 0.0299, 0.1252, 0.0517, 0.5963, 0.0065], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,662][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.6938, 0.0840, 0.0203, 0.0352, 0.0905, 0.0762], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,663][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0325, 0.1414, 0.2022, 0.2790, 0.1949, 0.1499], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,664][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([8.3229e-01, 4.1223e-02, 8.5209e-04, 3.0525e-02, 7.7010e-03, 5.5039e-04,
        8.6863e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,664][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.1390, 0.1397, 0.1466, 0.1601, 0.1381, 0.1369, 0.1395],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,665][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.0597, 0.1380, 0.2283, 0.1303, 0.1125, 0.2519, 0.0793],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,665][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.3487, 0.1126, 0.1304, 0.0958, 0.0794, 0.1673, 0.0658],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,665][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([9.9827e-01, 6.9235e-04, 2.8698e-04, 3.8975e-04, 1.4848e-04, 4.9889e-05,
        1.6044e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,666][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.1980, 0.1967, 0.0846, 0.1494, 0.1358, 0.1181, 0.1174],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,666][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.1039, 0.3525, 0.0565, 0.2297, 0.0855, 0.1618, 0.0101],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,666][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([2.0872e-06, 3.2171e-02, 5.3152e-02, 2.9638e-01, 3.1616e-01, 8.1082e-02,
        2.2106e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,667][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.0675, 0.1527, 0.2395, 0.1400, 0.2064, 0.1130, 0.0810],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,667][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.0503, 0.2046, 0.2764, 0.0729, 0.0971, 0.2071, 0.0917],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,668][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.6198, 0.0878, 0.0273, 0.0376, 0.0996, 0.0811, 0.0467],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,669][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0183, 0.1099, 0.1439, 0.2215, 0.2022, 0.1351, 0.1691],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,671][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ at] are: tensor([9.4579e-01, 1.5448e-02, 1.4398e-04, 4.7701e-03, 2.7897e-03, 5.2225e-04,
        2.8543e-02, 1.9967e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,672][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.1209, 0.1221, 0.1272, 0.1396, 0.1206, 0.1196, 0.1228, 0.1270],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,673][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0621, 0.1240, 0.1845, 0.1191, 0.0974, 0.2046, 0.0814, 0.1268],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,675][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.3399, 0.1057, 0.1085, 0.0869, 0.0803, 0.1105, 0.0635, 0.1047],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,676][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ at] are: tensor([9.9847e-01, 4.6649e-04, 2.3947e-04, 3.1673e-04, 1.2796e-04, 2.9268e-05,
        2.9182e-04, 6.0728e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,678][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.2172, 0.2191, 0.0701, 0.1299, 0.0871, 0.0752, 0.1161, 0.0853],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,679][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ at] are: tensor([3.2323e-02, 1.0816e-01, 1.5849e-02, 6.5463e-01, 1.2496e-01, 2.4444e-02,
        3.9158e-02, 4.8042e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,680][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ at] are: tensor([1.8809e-06, 2.5534e-02, 5.1599e-02, 2.5999e-01, 2.6211e-01, 8.7160e-02,
        2.6115e-01, 5.2458e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,681][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0638, 0.1180, 0.1605, 0.1400, 0.1222, 0.0879, 0.1020, 0.2056],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,681][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0437, 0.0239, 0.2668, 0.0554, 0.3286, 0.0976, 0.1433, 0.0407],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,681][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.6661, 0.0598, 0.0168, 0.0246, 0.0723, 0.0550, 0.0364, 0.0689],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,682][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0162, 0.0623, 0.1259, 0.1389, 0.1609, 0.1446, 0.1781, 0.1731],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,682][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.4174e-01, 1.9720e-02, 1.7169e-04, 5.9808e-03, 2.8722e-03, 4.7220e-04,
        2.7453e-02, 7.7471e-04, 8.2008e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,683][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1068, 0.1083, 0.1132, 0.1242, 0.1070, 0.1062, 0.1088, 0.1127, 0.1126],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,683][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0493, 0.1074, 0.1464, 0.0976, 0.0961, 0.1455, 0.0740, 0.1403, 0.1433],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,683][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3350, 0.0900, 0.0889, 0.0760, 0.0674, 0.0867, 0.0605, 0.1000, 0.0956],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,684][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([9.9789e-01, 6.5123e-04, 3.2973e-04, 4.6800e-04, 1.7892e-04, 5.3151e-05,
        2.2750e-04, 1.0160e-04, 1.0029e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,684][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2076, 0.1924, 0.0665, 0.1153, 0.0786, 0.0703, 0.1038, 0.0775, 0.0882],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,685][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0070, 0.4413, 0.0181, 0.3760, 0.0397, 0.0103, 0.0899, 0.0159, 0.0017],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,686][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([2.6046e-06, 2.4218e-02, 4.3240e-02, 2.3227e-01, 2.4905e-01, 7.3843e-02,
        2.4003e-01, 6.0994e-02, 7.6350e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,688][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0579, 0.1030, 0.1518, 0.1074, 0.1262, 0.0774, 0.0790, 0.2422, 0.0552],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,689][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1508, 0.0347, 0.1122, 0.0779, 0.2895, 0.0046, 0.2893, 0.0347, 0.0062],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,691][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.7053, 0.0438, 0.0103, 0.0170, 0.0536, 0.0391, 0.0257, 0.0420, 0.0633],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,692][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0118, 0.0627, 0.0967, 0.1120, 0.1038, 0.1113, 0.1838, 0.2100, 0.1078],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,693][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ station] are: tensor([9.8278e-01, 4.9113e-03, 3.0764e-05, 3.1386e-03, 4.7026e-04, 5.9256e-05,
        5.5716e-03, 9.0379e-05, 7.4395e-05, 2.8736e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,695][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0960, 0.0979, 0.1029, 0.1128, 0.0974, 0.0966, 0.0983, 0.1030, 0.1031,
        0.0920], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,697][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0333, 0.0942, 0.1384, 0.0901, 0.0704, 0.1561, 0.0510, 0.1351, 0.1748,
        0.0564], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,698][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.2653, 0.0916, 0.0898, 0.0644, 0.0576, 0.1063, 0.0499, 0.1043, 0.1077,
        0.0632], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,698][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ station] are: tensor([9.9866e-01, 3.2146e-04, 1.4404e-04, 1.8648e-04, 7.7799e-05, 1.5185e-05,
        2.1367e-04, 5.5614e-05, 5.6282e-05, 2.6527e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,699][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.1938, 0.1466, 0.0605, 0.0970, 0.0755, 0.0699, 0.0978, 0.0788, 0.0871,
        0.0929], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,699][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0361, 0.0382, 0.0286, 0.1667, 0.1072, 0.0423, 0.0579, 0.4890, 0.0301,
        0.0040], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,699][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ station] are: tensor([8.8518e-07, 2.1238e-02, 4.3697e-02, 1.8668e-01, 2.2977e-01, 5.3868e-02,
        1.8536e-01, 4.2928e-02, 5.6291e-02, 1.8017e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,700][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0387, 0.1007, 0.1428, 0.0827, 0.1216, 0.0705, 0.0618, 0.2617, 0.0531,
        0.0665], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,700][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0241, 0.0935, 0.0791, 0.0281, 0.0482, 0.0299, 0.0358, 0.6031, 0.0218,
        0.0366], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,701][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.6854, 0.0322, 0.0084, 0.0150, 0.0456, 0.0271, 0.0229, 0.0336, 0.0485,
        0.0812], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,701][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0094, 0.0569, 0.0801, 0.1229, 0.0987, 0.0900, 0.1212, 0.2066, 0.1179,
        0.0963], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,701][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [,] are: tensor([8.7910e-01, 3.3460e-02, 7.9457e-04, 1.4418e-02, 7.9746e-03, 1.6882e-03,
        4.8311e-02, 2.1140e-03, 2.0649e-03, 7.9403e-03, 2.1304e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,702][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0883, 0.0898, 0.0935, 0.1030, 0.0888, 0.0877, 0.0897, 0.0934, 0.0932,
        0.0837, 0.0890], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,704][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0441, 0.0866, 0.1247, 0.0812, 0.0716, 0.1305, 0.0484, 0.1183, 0.1450,
        0.0567, 0.0929], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,706][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.2967, 0.0661, 0.0722, 0.0588, 0.0503, 0.0843, 0.0464, 0.0734, 0.0874,
        0.0620, 0.1024], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,707][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [,] are: tensor([9.9557e-01, 1.0594e-03, 5.6962e-04, 7.8110e-04, 2.2344e-04, 8.1724e-05,
        3.3495e-04, 1.6434e-04, 1.3898e-04, 8.3896e-04, 2.3816e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,708][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1718, 0.1422, 0.0515, 0.0950, 0.0758, 0.0692, 0.0894, 0.0721, 0.0797,
        0.0839, 0.0693], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,709][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [,] are: tensor([8.8650e-03, 1.5871e-02, 7.9637e-04, 9.5249e-02, 1.0102e-02, 2.0689e-02,
        2.1703e-03, 2.3844e-03, 8.3013e-01, 1.3617e-02, 1.2350e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,710][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [,] are: tensor([6.9659e-07, 1.8180e-02, 3.5166e-02, 1.8098e-01, 2.1811e-01, 5.8034e-02,
        1.5646e-01, 3.5826e-02, 4.4020e-02, 1.9249e-01, 6.0729e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,712][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0472, 0.0763, 0.1086, 0.0889, 0.1042, 0.0722, 0.0522, 0.1730, 0.0548,
        0.0829, 0.1395], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,714][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1063, 0.0294, 0.0814, 0.0464, 0.0267, 0.0235, 0.2670, 0.0434, 0.0306,
        0.3307, 0.0145], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,715][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.6636, 0.0347, 0.0089, 0.0148, 0.0411, 0.0290, 0.0180, 0.0312, 0.0440,
        0.0664, 0.0481], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,716][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0134, 0.0502, 0.0784, 0.0941, 0.0825, 0.0933, 0.1080, 0.1672, 0.1166,
        0.1034, 0.0928], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,716][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([9.2992e-01, 2.9911e-02, 9.2327e-05, 6.5056e-03, 1.7571e-03, 2.8834e-04,
        2.5799e-02, 3.9120e-04, 4.3995e-04, 3.5951e-03, 3.4898e-04, 9.5455e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,716][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0823, 0.0807, 0.0853, 0.0929, 0.0804, 0.0799, 0.0808, 0.0856, 0.0853,
        0.0759, 0.0809, 0.0900], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,717][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0331, 0.0714, 0.1147, 0.0660, 0.0544, 0.1283, 0.0401, 0.1169, 0.1487,
        0.0442, 0.1000, 0.0824], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,717][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.2289, 0.0534, 0.0713, 0.0553, 0.0485, 0.0806, 0.0492, 0.0825, 0.0821,
        0.0667, 0.1127, 0.0688], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,718][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([9.9911e-01, 2.2713e-04, 8.7593e-05, 1.1253e-04, 2.5806e-05, 1.0689e-05,
        1.2294e-04, 2.1795e-05, 2.7707e-05, 1.4156e-04, 7.2512e-05, 3.7482e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,718][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.1411, 0.1104, 0.0499, 0.0822, 0.0761, 0.0732, 0.0814, 0.0752, 0.0812,
        0.0770, 0.0691, 0.0830], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,718][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0277, 0.0632, 0.0482, 0.2644, 0.0902, 0.0162, 0.1737, 0.0710, 0.0134,
        0.1032, 0.0461, 0.0826], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,719][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([3.3643e-07, 7.4048e-03, 2.5569e-02, 1.1444e-01, 1.5945e-01, 3.5671e-02,
        1.2420e-01, 2.7929e-02, 3.7454e-02, 1.5740e-01, 6.2181e-02, 2.4829e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,721][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0322, 0.0530, 0.1147, 0.0661, 0.0893, 0.0569, 0.0471, 0.2083, 0.0456,
        0.0697, 0.1515, 0.0656], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,722][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0397, 0.0229, 0.1650, 0.1157, 0.0419, 0.1507, 0.0592, 0.0173, 0.0919,
        0.0851, 0.1806, 0.0300], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,724][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.4213, 0.0614, 0.0171, 0.0250, 0.0663, 0.0472, 0.0302, 0.0443, 0.0719,
        0.1050, 0.0803, 0.0298], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,725][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0081, 0.0378, 0.0558, 0.0818, 0.0840, 0.0720, 0.1086, 0.1619, 0.1088,
        0.1215, 0.0736, 0.0861], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,727][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([9.2904e-01, 1.1753e-02, 2.7113e-04, 7.6159e-03, 2.4086e-03, 4.1160e-04,
        1.9804e-02, 9.8445e-04, 8.8398e-04, 6.2387e-03, 8.2539e-04, 1.0228e-03,
        1.8737e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,728][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0753, 0.0744, 0.0784, 0.0854, 0.0740, 0.0736, 0.0748, 0.0785, 0.0783,
        0.0700, 0.0746, 0.0829, 0.0798], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,730][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0346, 0.0715, 0.0998, 0.0646, 0.0538, 0.1180, 0.0412, 0.1066, 0.1343,
        0.0501, 0.0840, 0.0863, 0.0553], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,731][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.2175, 0.0668, 0.0688, 0.0529, 0.0406, 0.0742, 0.0431, 0.0772, 0.0766,
        0.0537, 0.1008, 0.0855, 0.0421], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,733][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([9.9785e-01, 5.5679e-04, 2.3007e-04, 2.5619e-04, 7.4584e-05, 2.6064e-05,
        1.8170e-04, 5.3773e-05, 6.3894e-05, 4.3998e-04, 1.3450e-04, 9.0001e-05,
        3.9355e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,735][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.1331, 0.1040, 0.0475, 0.0793, 0.0710, 0.0655, 0.0693, 0.0683, 0.0719,
        0.0724, 0.0617, 0.0788, 0.0772], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,736][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0105, 0.0834, 0.0057, 0.1078, 0.0122, 0.1087, 0.1673, 0.0792, 0.1219,
        0.1192, 0.0165, 0.1607, 0.0070], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,737][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([9.3635e-07, 9.3621e-03, 2.4402e-02, 1.0872e-01, 1.3148e-01, 3.5802e-02,
        9.4485e-02, 2.6094e-02, 3.4415e-02, 1.2826e-01, 5.4326e-02, 1.7613e-01,
        1.7651e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,738][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0419, 0.0702, 0.1056, 0.0580, 0.0755, 0.0537, 0.0448, 0.1741, 0.0403,
        0.0750, 0.1348, 0.0755, 0.0506], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,738][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1706, 0.0787, 0.0745, 0.1480, 0.0982, 0.0254, 0.1024, 0.0470, 0.0164,
        0.0234, 0.0948, 0.1140, 0.0065], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,739][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.6175, 0.0349, 0.0093, 0.0143, 0.0365, 0.0276, 0.0191, 0.0261, 0.0407,
        0.0623, 0.0429, 0.0168, 0.0520], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,739][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0175, 0.0393, 0.0643, 0.0772, 0.0669, 0.0812, 0.1062, 0.1490, 0.0974,
        0.0846, 0.0773, 0.0957, 0.0433], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,739][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.6759, 0.0922, 0.0036, 0.0479, 0.0212, 0.0039, 0.0613, 0.0064, 0.0064,
        0.0252, 0.0055, 0.0124, 0.0270, 0.0111], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,740][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0683, 0.0694, 0.0727, 0.0791, 0.0686, 0.0679, 0.0695, 0.0722, 0.0720,
        0.0652, 0.0691, 0.0769, 0.0740, 0.0751], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,740][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0297, 0.0688, 0.0948, 0.0701, 0.0592, 0.1016, 0.0437, 0.0860, 0.1149,
        0.0485, 0.0787, 0.0774, 0.0674, 0.0592], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,741][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.2201, 0.0540, 0.0603, 0.0477, 0.0475, 0.0669, 0.0406, 0.0594, 0.0692,
        0.0556, 0.0907, 0.0677, 0.0560, 0.0644], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,741][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.9455e-01, 1.4208e-03, 7.2600e-04, 6.7503e-04, 2.1632e-04, 8.2431e-05,
        3.6194e-04, 1.6060e-04, 1.5345e-04, 7.2337e-04, 3.0881e-04, 2.4424e-04,
        1.2254e-04, 2.5483e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,742][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.1535, 0.1216, 0.0418, 0.0786, 0.0570, 0.0492, 0.0686, 0.0541, 0.0595,
        0.0655, 0.0548, 0.0704, 0.0726, 0.0529], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,744][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([3.1603e-03, 8.7250e-02, 4.4053e-03, 4.0047e-01, 1.3369e-02, 1.7428e-02,
        2.1054e-03, 1.8582e-03, 4.8225e-02, 6.5970e-02, 1.9886e-02, 1.0914e-01,
        2.2636e-01, 3.7563e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,745][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.1931e-06, 2.0337e-02, 2.5065e-02, 1.2290e-01, 1.1460e-01, 3.3288e-02,
        9.6108e-02, 2.2646e-02, 2.6888e-02, 1.3076e-01, 4.3832e-02, 1.8094e-01,
        1.6233e-01, 2.0311e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,746][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0304, 0.0592, 0.0984, 0.0542, 0.0749, 0.0492, 0.0418, 0.1177, 0.0440,
        0.0747, 0.1425, 0.0769, 0.0704, 0.0656], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,748][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0261, 0.0363, 0.0639, 0.0327, 0.0970, 0.0173, 0.1632, 0.0048, 0.0198,
        0.1369, 0.0315, 0.0441, 0.3237, 0.0029], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,750][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.6160, 0.0231, 0.0061, 0.0113, 0.0301, 0.0211, 0.0138, 0.0257, 0.0308,
        0.0511, 0.0392, 0.0157, 0.0520, 0.0640], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,752][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0066, 0.0399, 0.0614, 0.0589, 0.0678, 0.0643, 0.0981, 0.1244, 0.0847,
        0.0783, 0.0800, 0.1001, 0.0731, 0.0626], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,753][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ give] are: tensor([8.6564e-01, 2.0718e-02, 5.5533e-04, 1.7110e-02, 8.9054e-03, 9.0432e-04,
        4.8524e-02, 1.9215e-03, 1.2482e-03, 8.5604e-03, 1.3953e-03, 2.5812e-03,
        8.1746e-03, 2.8125e-03, 1.0952e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,754][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0635, 0.0648, 0.0679, 0.0737, 0.0641, 0.0635, 0.0647, 0.0673, 0.0670,
        0.0608, 0.0642, 0.0714, 0.0689, 0.0699, 0.0685], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,755][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0290, 0.0627, 0.0885, 0.0530, 0.0449, 0.1083, 0.0335, 0.0992, 0.1203,
        0.0374, 0.0739, 0.0737, 0.0543, 0.0801, 0.0411], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,755][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.2122, 0.0603, 0.0614, 0.0407, 0.0344, 0.0605, 0.0318, 0.0661, 0.0613,
        0.0453, 0.0870, 0.0794, 0.0433, 0.0729, 0.0434], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,756][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ give] are: tensor([9.9548e-01, 1.2748e-03, 3.8882e-04, 4.7818e-04, 1.5846e-04, 6.3106e-05,
        3.1070e-04, 1.1298e-04, 1.3905e-04, 4.8714e-04, 2.1503e-04, 1.6059e-04,
        1.0789e-04, 3.2510e-04, 2.9930e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,756][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.1115, 0.0793, 0.0392, 0.0686, 0.0705, 0.0635, 0.0598, 0.0628, 0.0647,
        0.0645, 0.0527, 0.0687, 0.0751, 0.0549, 0.0641], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,757][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0078, 0.0645, 0.0129, 0.0430, 0.0047, 0.0358, 0.0016, 0.3317, 0.0565,
        0.0436, 0.0238, 0.1325, 0.0978, 0.1432, 0.0006], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,757][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ give] are: tensor([2.6702e-06, 8.5677e-03, 1.9773e-02, 7.9374e-02, 8.1920e-02, 2.9191e-02,
        8.7997e-02, 2.2314e-02, 2.9397e-02, 1.2123e-01, 4.6032e-02, 1.5671e-01,
        1.8132e-01, 2.5074e-02, 1.1110e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,757][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0328, 0.0639, 0.0827, 0.0453, 0.0746, 0.0395, 0.0362, 0.1392, 0.0321,
        0.0645, 0.1124, 0.0741, 0.0791, 0.0740, 0.0496], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,758][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.1051, 0.0204, 0.0351, 0.0691, 0.0660, 0.0347, 0.1560, 0.0161, 0.0340,
        0.0502, 0.0657, 0.0327, 0.1000, 0.1978, 0.0171], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,758][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.3751, 0.0446, 0.0127, 0.0184, 0.0501, 0.0362, 0.0229, 0.0341, 0.0540,
        0.0738, 0.0574, 0.0245, 0.0713, 0.0860, 0.0389], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,759][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0097, 0.0334, 0.0559, 0.0600, 0.0588, 0.0570, 0.0807, 0.1551, 0.0836,
        0.0750, 0.0670, 0.0874, 0.0701, 0.0685, 0.0377], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,761][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.7664, 0.0440, 0.0015, 0.0281, 0.0140, 0.0024, 0.0598, 0.0031, 0.0040,
        0.0235, 0.0041, 0.0075, 0.0168, 0.0098, 0.0086, 0.0063],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,763][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0593, 0.0604, 0.0640, 0.0693, 0.0602, 0.0599, 0.0606, 0.0636, 0.0634,
        0.0569, 0.0603, 0.0671, 0.0645, 0.0660, 0.0648, 0.0597],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,764][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0275, 0.0646, 0.0859, 0.0547, 0.0503, 0.0858, 0.0358, 0.0911, 0.0944,
        0.0398, 0.0663, 0.0702, 0.0599, 0.0706, 0.0533, 0.0496],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,765][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.1507, 0.0551, 0.0576, 0.0457, 0.0450, 0.0621, 0.0381, 0.0606, 0.0604,
        0.0552, 0.0828, 0.0742, 0.0489, 0.0684, 0.0575, 0.0377],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,767][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ it] are: tensor([9.9782e-01, 4.2883e-04, 1.5910e-04, 1.8763e-04, 5.5608e-05, 1.9000e-05,
        1.9087e-04, 3.4445e-05, 5.3817e-05, 4.4410e-04, 1.0606e-04, 8.3661e-05,
        4.0228e-05, 1.2900e-04, 2.1146e-04, 3.7733e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,769][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.1263, 0.0867, 0.0358, 0.0642, 0.0558, 0.0504, 0.0584, 0.0519, 0.0567,
        0.0607, 0.0484, 0.0640, 0.0664, 0.0515, 0.0642, 0.0584],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,770][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0058, 0.1824, 0.0065, 0.1941, 0.0047, 0.0111, 0.0124, 0.0333, 0.0037,
        0.2142, 0.0211, 0.2030, 0.0689, 0.0322, 0.0033, 0.0033],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,771][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ it] are: tensor([6.7459e-07, 1.0613e-02, 1.3572e-02, 6.2669e-02, 9.0539e-02, 2.8549e-02,
        6.5114e-02, 2.2209e-02, 2.8170e-02, 1.0078e-01, 4.8002e-02, 1.7959e-01,
        1.7404e-01, 2.4339e-02, 1.0060e-01, 5.1213e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,772][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0251, 0.0518, 0.0748, 0.0479, 0.0653, 0.0390, 0.0369, 0.1380, 0.0285,
        0.0541, 0.1129, 0.0674, 0.0707, 0.0750, 0.0902, 0.0225],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,773][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0989, 0.0121, 0.0255, 0.0264, 0.0479, 0.0054, 0.0863, 0.0226, 0.0053,
        0.1003, 0.0189, 0.0192, 0.1644, 0.0569, 0.3072, 0.0028],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,773][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.4459, 0.0354, 0.0094, 0.0153, 0.0358, 0.0293, 0.0194, 0.0331, 0.0440,
        0.0626, 0.0486, 0.0179, 0.0531, 0.0754, 0.0314, 0.0433],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,774][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0083, 0.0398, 0.0593, 0.0552, 0.0570, 0.0665, 0.1073, 0.1235, 0.0674,
        0.0736, 0.0703, 0.0931, 0.0422, 0.0709, 0.0504, 0.0152],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,774][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.6705, 0.0724, 0.0032, 0.0567, 0.0182, 0.0030, 0.0469, 0.0055, 0.0055,
        0.0257, 0.0051, 0.0141, 0.0362, 0.0103, 0.0117, 0.0045, 0.0106],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,775][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0563, 0.0572, 0.0599, 0.0652, 0.0566, 0.0561, 0.0573, 0.0595, 0.0594,
        0.0536, 0.0569, 0.0634, 0.0610, 0.0619, 0.0609, 0.0563, 0.0586],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,775][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0242, 0.0586, 0.0796, 0.0586, 0.0494, 0.0857, 0.0368, 0.0722, 0.0974,
        0.0407, 0.0658, 0.0664, 0.0559, 0.0488, 0.0533, 0.0629, 0.0437],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,775][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1803, 0.0464, 0.0513, 0.0411, 0.0407, 0.0580, 0.0357, 0.0505, 0.0597,
        0.0490, 0.0777, 0.0590, 0.0484, 0.0549, 0.0518, 0.0465, 0.0489],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,776][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.9456e-01, 1.3158e-03, 5.6780e-04, 6.1100e-04, 1.8339e-04, 6.9022e-05,
        3.5443e-04, 1.1835e-04, 1.3040e-04, 5.8056e-04, 2.4094e-04, 2.0029e-04,
        1.0793e-04, 2.3091e-04, 3.9485e-04, 9.3858e-05, 2.4271e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,778][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.1402, 0.1087, 0.0341, 0.0645, 0.0455, 0.0388, 0.0600, 0.0434, 0.0485,
        0.0562, 0.0455, 0.0585, 0.0601, 0.0436, 0.0595, 0.0528, 0.0403],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,779][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([2.7957e-03, 8.2588e-02, 3.3611e-03, 3.6530e-01, 1.1847e-02, 1.6115e-02,
        1.7788e-03, 1.5102e-03, 3.9462e-02, 5.5023e-02, 1.6496e-02, 1.0430e-01,
        1.8775e-01, 3.6216e-04, 2.3755e-03, 1.0855e-01, 3.8248e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,780][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([6.3235e-07, 1.6141e-02, 1.8574e-02, 9.5446e-02, 9.2314e-02, 2.7475e-02,
        7.3157e-02, 1.8630e-02, 2.3384e-02, 1.0790e-01, 3.6565e-02, 1.6042e-01,
        1.4587e-01, 1.7800e-02, 9.2928e-02, 4.7377e-02, 2.6018e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,781][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0253, 0.0506, 0.0813, 0.0457, 0.0657, 0.0420, 0.0361, 0.0982, 0.0373,
        0.0650, 0.1184, 0.0663, 0.0587, 0.0548, 0.0707, 0.0342, 0.0498],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,783][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0183, 0.0268, 0.0428, 0.0256, 0.0655, 0.0126, 0.1108, 0.0035, 0.0149,
        0.0948, 0.0212, 0.0327, 0.1998, 0.0018, 0.2986, 0.0288, 0.0015],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,785][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.4977, 0.0244, 0.0067, 0.0118, 0.0307, 0.0223, 0.0140, 0.0254, 0.0313,
        0.0506, 0.0394, 0.0166, 0.0510, 0.0636, 0.0282, 0.0371, 0.0493],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,787][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0061, 0.0357, 0.0535, 0.0502, 0.0587, 0.0575, 0.0891, 0.1099, 0.0748,
        0.0683, 0.0692, 0.0866, 0.0628, 0.0530, 0.0623, 0.0185, 0.0440],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,819][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:37,820][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,821][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,823][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,823][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,823][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,824][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,824][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,824][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,825][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,825][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,825][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,825][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:37,826][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.9585, 0.0415], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,826][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.6404, 0.3596], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,826][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0070, 0.9930], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,827][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.0980, 0.9020], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,829][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.5700, 0.4300], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,831][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.4945, 0.5055], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,832][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.3675, 0.6325], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,833][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.5228, 0.4772], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,835][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.1983, 0.8017], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,837][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.4948, 0.5052], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,838][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.8192, 0.1808], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,840][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.2191, 0.7809], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:37,840][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.8555, 0.0346, 0.1100], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,841][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.4105, 0.2596, 0.3299], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,841][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0048, 0.3307, 0.6646], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,841][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0237, 0.6422, 0.3341], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,842][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.3542, 0.3227, 0.3231], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,842][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.2638, 0.6972, 0.0390], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,842][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2361, 0.1919, 0.5719], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,843][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.3572, 0.3228, 0.3200], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,843][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0785, 0.5666, 0.3549], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,843][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.1797, 0.2335, 0.5868], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,844][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.7166, 0.1252, 0.1581], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,844][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1448, 0.4123, 0.4429], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:37,846][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.8682, 0.0138, 0.0517, 0.0662], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,848][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.2556, 0.1534, 0.2697, 0.3213], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,849][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.0019, 0.1676, 0.3928, 0.4377], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,850][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.0292, 0.3734, 0.4401, 0.1574], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,852][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.2528, 0.2597, 0.2391, 0.2484], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,854][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.1709, 0.2940, 0.0952, 0.4400], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,855][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.1568, 0.0988, 0.4310, 0.3133], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,857][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.2760, 0.2411, 0.2370, 0.2459], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,858][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.0546, 0.3408, 0.4329, 0.1718], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,858][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.0991, 0.1810, 0.4949, 0.2249], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,858][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.5898, 0.1048, 0.1296, 0.1757], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,859][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.1054, 0.2311, 0.3557, 0.3077], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:37,859][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.8852, 0.0134, 0.0575, 0.0226, 0.0213], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,859][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.2021, 0.1277, 0.2135, 0.2851, 0.1716], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,860][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0007, 0.1165, 0.2522, 0.3187, 0.3119], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,860][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0224, 0.4018, 0.2945, 0.2148, 0.0665], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,860][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.1886, 0.2246, 0.1935, 0.2086, 0.1848], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,861][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.1001, 0.3424, 0.0556, 0.3371, 0.1647], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,861][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0681, 0.0473, 0.1710, 0.1578, 0.5559], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,862][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.2203, 0.1982, 0.1959, 0.2077, 0.1778], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,864][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0364, 0.2447, 0.4313, 0.1424, 0.1453], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,865][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.1347, 0.0595, 0.3156, 0.1990, 0.2912], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,867][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.5061, 0.0890, 0.1092, 0.1474, 0.1483], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,868][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0956, 0.1258, 0.2517, 0.2457, 0.2812], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:37,870][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.7382, 0.0208, 0.0628, 0.0387, 0.0186, 0.1209], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,872][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1709, 0.1142, 0.1619, 0.2481, 0.1584, 0.1465], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,873][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0003, 0.0992, 0.2176, 0.3006, 0.2794, 0.1029], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,875][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0106, 0.3226, 0.1740, 0.2073, 0.1645, 0.1210], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,875][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1763, 0.1922, 0.1640, 0.1683, 0.1500, 0.1491], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,875][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0905, 0.4174, 0.0288, 0.3138, 0.1053, 0.0442], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,876][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0399, 0.0314, 0.1035, 0.1002, 0.4057, 0.3193], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,876][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1877, 0.1694, 0.1661, 0.1782, 0.1514, 0.1472], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,877][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0293, 0.2524, 0.2566, 0.1957, 0.2110, 0.0551], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,877][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1190, 0.0726, 0.2636, 0.1291, 0.3536, 0.0622], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,877][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.4192, 0.0785, 0.0997, 0.1341, 0.1348, 0.1336], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,878][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0575, 0.1407, 0.1940, 0.2597, 0.2081, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:37,878][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.8433, 0.0123, 0.0415, 0.0185, 0.0098, 0.0486, 0.0260],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,878][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.1366, 0.0984, 0.1511, 0.2109, 0.1451, 0.1305, 0.1274],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,879][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([2.1626e-04, 8.1929e-02, 1.7480e-01, 2.5415e-01, 2.9865e-01, 1.1829e-01,
        7.1958e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,881][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0104, 0.2460, 0.1874, 0.1154, 0.0609, 0.3699, 0.0099],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,882][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.1560, 0.1463, 0.1292, 0.1360, 0.1204, 0.1156, 0.1965],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,884][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.0371, 0.0825, 0.0376, 0.3700, 0.2372, 0.0678, 0.1678],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,885][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0445, 0.0223, 0.0989, 0.0640, 0.2942, 0.3927, 0.0833],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,887][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.1623, 0.1448, 0.1435, 0.1523, 0.1324, 0.1283, 0.1363],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,889][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.0266, 0.1963, 0.2419, 0.1469, 0.2523, 0.0853, 0.0506],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,890][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.0409, 0.0950, 0.2513, 0.1207, 0.1621, 0.1458, 0.1842],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,892][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.3498, 0.0750, 0.0906, 0.1295, 0.1268, 0.1244, 0.1040],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,892][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0286, 0.1156, 0.1485, 0.2158, 0.2054, 0.1340, 0.1520],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:37,893][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.6986, 0.0188, 0.0678, 0.0282, 0.0165, 0.0794, 0.0207, 0.0700],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,893][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.1338, 0.0872, 0.1291, 0.2039, 0.1359, 0.1255, 0.1126, 0.0719],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,893][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0008, 0.0858, 0.1714, 0.2061, 0.2373, 0.0947, 0.0621, 0.1419],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,894][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0049, 0.3576, 0.1518, 0.1397, 0.1197, 0.1331, 0.0183, 0.0750],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,894][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1238, 0.1308, 0.1186, 0.1231, 0.1157, 0.1132, 0.1661, 0.1087],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,895][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0947, 0.2409, 0.0238, 0.2947, 0.1299, 0.0429, 0.1480, 0.0250],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,895][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0436, 0.0189, 0.0614, 0.0662, 0.2448, 0.1932, 0.1112, 0.2607],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,895][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1412, 0.1260, 0.1244, 0.1330, 0.1146, 0.1112, 0.1193, 0.1302],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,896][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0165, 0.1515, 0.1402, 0.1800, 0.1103, 0.0569, 0.1049, 0.2398],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,897][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0375, 0.0489, 0.2930, 0.0969, 0.2073, 0.1410, 0.1109, 0.0647],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,899][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.3491, 0.0621, 0.0782, 0.1050, 0.1080, 0.1056, 0.0834, 0.1085],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,900][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0272, 0.0627, 0.1241, 0.1345, 0.1751, 0.1429, 0.1680, 0.1655],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:37,902][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.6176, 0.0202, 0.0635, 0.0384, 0.0177, 0.0853, 0.0243, 0.0371, 0.0959],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,903][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.1140, 0.0795, 0.1123, 0.1700, 0.1184, 0.0999, 0.1097, 0.0673, 0.1291],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,905][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0004, 0.0741, 0.1579, 0.1968, 0.2112, 0.0780, 0.0517, 0.1369, 0.0930],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,906][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0058, 0.3166, 0.1099, 0.1637, 0.1011, 0.0763, 0.0296, 0.1027, 0.0943],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,908][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1128, 0.1176, 0.1060, 0.1111, 0.1026, 0.0997, 0.1546, 0.0966, 0.0991],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,910][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0848, 0.2595, 0.0185, 0.3157, 0.0582, 0.0284, 0.1558, 0.0308, 0.0482],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,910][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0197, 0.0161, 0.0518, 0.0521, 0.2054, 0.1659, 0.0859, 0.2442, 0.1590],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,910][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1255, 0.1128, 0.1106, 0.1191, 0.1015, 0.0984, 0.1057, 0.1155, 0.1107],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,911][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0109, 0.1316, 0.1277, 0.1169, 0.1182, 0.0430, 0.0663, 0.3602, 0.0252],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,911][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0817, 0.0663, 0.1959, 0.1302, 0.1935, 0.0455, 0.1850, 0.0592, 0.0426],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,912][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3112, 0.0577, 0.0717, 0.0984, 0.0980, 0.0959, 0.0772, 0.0972, 0.0927],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,912][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0209, 0.0629, 0.0924, 0.1076, 0.1116, 0.1076, 0.1754, 0.2056, 0.1161],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:37,912][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.7701, 0.0111, 0.0418, 0.0167, 0.0075, 0.0506, 0.0105, 0.0261, 0.0568,
        0.0087], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,913][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.1029, 0.0659, 0.1129, 0.1431, 0.1073, 0.1041, 0.0902, 0.0703, 0.1401,
        0.0631], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,913][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0003, 0.0590, 0.1212, 0.1572, 0.2098, 0.0959, 0.0595, 0.1344, 0.1079,
        0.0548], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,914][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0080, 0.2827, 0.1146, 0.0747, 0.0490, 0.1598, 0.0133, 0.1266, 0.1451,
        0.0262], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,916][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.1011, 0.1068, 0.0985, 0.1035, 0.0895, 0.0873, 0.1367, 0.0875, 0.0924,
        0.0966], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,918][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0578, 0.0989, 0.0224, 0.2509, 0.2044, 0.0459, 0.1134, 0.0327, 0.1096,
        0.0640], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,919][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0237, 0.0122, 0.0530, 0.0338, 0.1632, 0.1851, 0.0543, 0.2674, 0.1726,
        0.0348], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,920][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.1128, 0.1010, 0.1000, 0.1061, 0.0919, 0.0891, 0.0948, 0.1041, 0.0994,
        0.1009], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,922][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0130, 0.1203, 0.1226, 0.0728, 0.1095, 0.0433, 0.0402, 0.4140, 0.0331,
        0.0312], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,924][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0375, 0.0593, 0.1567, 0.0964, 0.1172, 0.0697, 0.1044, 0.1512, 0.0550,
        0.1525], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,925][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.2654, 0.0553, 0.0676, 0.0941, 0.0907, 0.0915, 0.0751, 0.0939, 0.0879,
        0.0785], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,927][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0138, 0.0561, 0.0792, 0.1183, 0.0975, 0.0858, 0.1100, 0.2137, 0.1273,
        0.0983], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:37,927][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.5356, 0.0189, 0.0659, 0.0373, 0.0172, 0.0769, 0.0236, 0.0417, 0.0781,
        0.0126, 0.0921], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,928][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.1015, 0.0662, 0.0978, 0.1473, 0.0920, 0.0901, 0.0777, 0.0540, 0.1200,
        0.0577, 0.0959], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,928][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0002, 0.0589, 0.1299, 0.1665, 0.1796, 0.0602, 0.0357, 0.1083, 0.0768,
        0.0460, 0.1378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,929][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0070, 0.1404, 0.0917, 0.1289, 0.0722, 0.1675, 0.0208, 0.0613, 0.1868,
        0.0555, 0.0679], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,929][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0961, 0.0963, 0.0876, 0.0921, 0.0872, 0.0836, 0.1225, 0.0795, 0.0830,
        0.0899, 0.0822], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,929][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0970, 0.1640, 0.0176, 0.3325, 0.0777, 0.0320, 0.0962, 0.0192, 0.0925,
        0.0510, 0.0202], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,930][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0343, 0.0131, 0.0432, 0.0362, 0.1545, 0.1281, 0.0597, 0.1929, 0.1253,
        0.0455, 0.1671], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,930][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1019, 0.0920, 0.0907, 0.0979, 0.0836, 0.0810, 0.0869, 0.0951, 0.0909,
        0.0933, 0.0866], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,931][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0117, 0.1019, 0.0896, 0.1079, 0.1143, 0.0556, 0.0382, 0.2444, 0.0383,
        0.0598, 0.1384], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,932][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0716, 0.0512, 0.1484, 0.0884, 0.0665, 0.0717, 0.1434, 0.0554, 0.0663,
        0.1657, 0.0714], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,933][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.2982, 0.0465, 0.0584, 0.0785, 0.0787, 0.0761, 0.0607, 0.0762, 0.0732,
        0.0652, 0.0883], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,935][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0228, 0.0482, 0.0754, 0.0871, 0.0878, 0.0885, 0.0993, 0.1589, 0.1206,
        0.1102, 0.1011], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:37,937][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.6485, 0.0263, 0.0468, 0.0215, 0.0091, 0.0469, 0.0143, 0.0280, 0.0567,
        0.0070, 0.0745, 0.0204], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,938][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0849, 0.0559, 0.0855, 0.1222, 0.0859, 0.0847, 0.0826, 0.0678, 0.1082,
        0.0597, 0.0910, 0.0716], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,940][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0002, 0.0411, 0.1026, 0.1201, 0.1509, 0.0591, 0.0353, 0.0986, 0.0717,
        0.0336, 0.1492, 0.1376], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,941][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.0049, 0.0815, 0.1072, 0.0973, 0.0517, 0.1308, 0.0253, 0.1165, 0.1212,
        0.0598, 0.1181, 0.0856], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,943][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0764, 0.0971, 0.0836, 0.0898, 0.0864, 0.0805, 0.1065, 0.0768, 0.0770,
        0.0820, 0.0750, 0.0689], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,944][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0427, 0.0329, 0.0378, 0.1285, 0.2236, 0.0443, 0.0908, 0.0318, 0.0748,
        0.2198, 0.0496, 0.0235], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,945][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0162, 0.0069, 0.0368, 0.0244, 0.1131, 0.1480, 0.0378, 0.2181, 0.1343,
        0.0269, 0.1760, 0.0616], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,945][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0952, 0.0848, 0.0831, 0.0884, 0.0765, 0.0742, 0.0791, 0.0867, 0.0830,
        0.0848, 0.0794, 0.0849], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,945][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.0091, 0.0479, 0.1020, 0.0637, 0.0826, 0.0348, 0.0331, 0.3238, 0.0282,
        0.0448, 0.1651, 0.0647], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,946][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0273, 0.0266, 0.1466, 0.0989, 0.0681, 0.0951, 0.0784, 0.0319, 0.0730,
        0.1250, 0.1681, 0.0610], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,946][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.2534, 0.0463, 0.0562, 0.0770, 0.0755, 0.0730, 0.0578, 0.0740, 0.0702,
        0.0623, 0.0835, 0.0708], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,947][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0123, 0.0375, 0.0530, 0.0783, 0.0800, 0.0675, 0.0988, 0.1593, 0.1159,
        0.1271, 0.0818, 0.0885], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:37,947][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.6246, 0.0101, 0.0468, 0.0204, 0.0142, 0.0580, 0.0127, 0.0296, 0.0604,
        0.0079, 0.0697, 0.0087, 0.0369], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,948][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0881, 0.0493, 0.0895, 0.1118, 0.0748, 0.0853, 0.0675, 0.0517, 0.1218,
        0.0541, 0.0890, 0.0614, 0.0557], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,948][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0002, 0.0421, 0.0843, 0.1192, 0.1394, 0.0550, 0.0398, 0.0874, 0.0640,
        0.0373, 0.1290, 0.1577, 0.0444], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,950][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0069, 0.1468, 0.0928, 0.0775, 0.0299, 0.0876, 0.0187, 0.1017, 0.0917,
        0.0385, 0.0932, 0.1976, 0.0173], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,952][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0744, 0.0812, 0.0722, 0.0774, 0.0760, 0.0740, 0.1037, 0.0685, 0.0734,
        0.0767, 0.0726, 0.0654, 0.0844], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,954][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0515, 0.0970, 0.0325, 0.0784, 0.0784, 0.0282, 0.0686, 0.0378, 0.0833,
        0.1516, 0.0353, 0.0702, 0.1872], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,955][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0225, 0.0074, 0.0262, 0.0224, 0.0798, 0.0971, 0.0358, 0.1291, 0.0873,
        0.0271, 0.1311, 0.0533, 0.2809], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,957][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0868, 0.0779, 0.0767, 0.0824, 0.0709, 0.0686, 0.0736, 0.0802, 0.0763,
        0.0782, 0.0729, 0.0786, 0.0768], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,959][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0161, 0.0841, 0.0947, 0.0502, 0.0657, 0.0342, 0.0316, 0.2620, 0.0236,
        0.0555, 0.1504, 0.0896, 0.0423], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,960][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0511, 0.0556, 0.1033, 0.1147, 0.0991, 0.0468, 0.1114, 0.0380, 0.0328,
        0.0811, 0.1033, 0.1290, 0.0338], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,961][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.2331, 0.0424, 0.0537, 0.0713, 0.0710, 0.0696, 0.0545, 0.0707, 0.0651,
        0.0585, 0.0790, 0.0627, 0.0684], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,962][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0266, 0.0381, 0.0611, 0.0722, 0.0653, 0.0776, 0.0929, 0.1449, 0.1011,
        0.0868, 0.0887, 0.0977, 0.0471], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:37,962][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.5077, 0.0177, 0.0514, 0.0257, 0.0156, 0.0671, 0.0175, 0.0358, 0.0643,
        0.0114, 0.0745, 0.0146, 0.0241, 0.0725], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,963][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0752, 0.0542, 0.0748, 0.1081, 0.0794, 0.0720, 0.0737, 0.0482, 0.0875,
        0.0462, 0.0780, 0.0631, 0.0729, 0.0667], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,963][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0002, 0.0432, 0.0909, 0.1099, 0.1385, 0.0508, 0.0319, 0.0806, 0.0605,
        0.0324, 0.1317, 0.1327, 0.0492, 0.0475], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,964][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0051, 0.1518, 0.0761, 0.0917, 0.0946, 0.0964, 0.0194, 0.0396, 0.1025,
        0.0591, 0.0603, 0.1355, 0.0529, 0.0148], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,964][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0760, 0.0765, 0.0702, 0.0732, 0.0682, 0.0665, 0.0958, 0.0646, 0.0687,
        0.0728, 0.0682, 0.0592, 0.0765, 0.0637], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,964][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0775, 0.1252, 0.0125, 0.1490, 0.0647, 0.0364, 0.0688, 0.0162, 0.0843,
        0.0280, 0.0293, 0.1454, 0.1555, 0.0073], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,965][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0099, 0.0041, 0.0154, 0.0168, 0.0690, 0.0488, 0.0327, 0.0802, 0.0477,
        0.0252, 0.0958, 0.0471, 0.2813, 0.2261], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,966][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0794, 0.0719, 0.0711, 0.0771, 0.0665, 0.0642, 0.0689, 0.0751, 0.0712,
        0.0732, 0.0680, 0.0737, 0.0724, 0.0671], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,968][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0056, 0.0741, 0.0906, 0.0461, 0.0687, 0.0300, 0.0282, 0.1367, 0.0303,
        0.0546, 0.1976, 0.1023, 0.0892, 0.0461], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,969][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0363, 0.0515, 0.1630, 0.0697, 0.1013, 0.0534, 0.0756, 0.0198, 0.0466,
        0.0846, 0.0739, 0.0786, 0.1005, 0.0452], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,971][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.2297, 0.0391, 0.0477, 0.0651, 0.0647, 0.0617, 0.0489, 0.0624, 0.0579,
        0.0516, 0.0700, 0.0579, 0.0615, 0.0819], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,972][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0114, 0.0382, 0.0574, 0.0517, 0.0686, 0.0590, 0.0902, 0.1147, 0.0869,
        0.0790, 0.0870, 0.0965, 0.0839, 0.0755], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:37,974][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.5411, 0.0107, 0.0499, 0.0226, 0.0146, 0.0633, 0.0149, 0.0289, 0.0531,
        0.0079, 0.0707, 0.0091, 0.0160, 0.0649, 0.0323], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,976][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0677, 0.0476, 0.0722, 0.0895, 0.0706, 0.0693, 0.0610, 0.0502, 0.0869,
        0.0423, 0.0737, 0.0549, 0.0685, 0.0676, 0.0779], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,977][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0001, 0.0341, 0.0770, 0.1013, 0.1297, 0.0512, 0.0287, 0.0773, 0.0592,
        0.0287, 0.1302, 0.1277, 0.0391, 0.0475, 0.0681], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,978][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0037, 0.2081, 0.0916, 0.0469, 0.0233, 0.0578, 0.0078, 0.0771, 0.0583,
        0.0293, 0.0707, 0.2507, 0.0240, 0.0396, 0.0111], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,979][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0708, 0.0709, 0.0639, 0.0687, 0.0662, 0.0613, 0.0873, 0.0595, 0.0621,
        0.0695, 0.0620, 0.0570, 0.0767, 0.0615, 0.0626], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,979][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0319, 0.0684, 0.0241, 0.0768, 0.1282, 0.0274, 0.1277, 0.0204, 0.0412,
        0.0754, 0.0236, 0.0533, 0.2071, 0.0131, 0.0814], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,980][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0252, 0.0047, 0.0155, 0.0139, 0.0432, 0.0442, 0.0211, 0.0577, 0.0367,
        0.0165, 0.0607, 0.0302, 0.1476, 0.1492, 0.3337], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,980][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0751, 0.0679, 0.0670, 0.0717, 0.0620, 0.0601, 0.0642, 0.0700, 0.0667,
        0.0684, 0.0640, 0.0689, 0.0674, 0.0635, 0.0631], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,981][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0097, 0.0742, 0.0627, 0.0331, 0.0719, 0.0211, 0.0241, 0.1982, 0.0174,
        0.0491, 0.1249, 0.0999, 0.1187, 0.0666, 0.0283], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,981][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0382, 0.0286, 0.0782, 0.0720, 0.0715, 0.0460, 0.0785, 0.0220, 0.0385,
        0.0767, 0.0762, 0.0673, 0.0814, 0.1750, 0.0500], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,982][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.1881, 0.0388, 0.0463, 0.0619, 0.0608, 0.0590, 0.0483, 0.0596, 0.0561,
        0.0499, 0.0674, 0.0558, 0.0596, 0.0785, 0.0698], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,982][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0158, 0.0319, 0.0516, 0.0546, 0.0581, 0.0514, 0.0710, 0.1426, 0.0829,
        0.0746, 0.0720, 0.0859, 0.0776, 0.0847, 0.0454], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:37,983][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.5232, 0.0113, 0.0470, 0.0258, 0.0114, 0.0635, 0.0140, 0.0234, 0.0686,
        0.0081, 0.0663, 0.0100, 0.0179, 0.0498, 0.0147, 0.0451],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,985][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0625, 0.0430, 0.0689, 0.0874, 0.0636, 0.0583, 0.0563, 0.0424, 0.0834,
        0.0408, 0.0699, 0.0529, 0.0564, 0.0614, 0.0758, 0.0769],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,986][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([1.1560e-04, 3.2804e-02, 7.7896e-02, 9.8411e-02, 1.2538e-01, 4.6998e-02,
        2.7199e-02, 7.3717e-02, 5.3734e-02, 2.8604e-02, 1.1744e-01, 1.3305e-01,
        4.0619e-02, 4.6235e-02, 6.8914e-02, 2.8886e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,988][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0050, 0.1547, 0.0747, 0.0649, 0.0690, 0.0712, 0.0172, 0.0532, 0.0568,
        0.0569, 0.0586, 0.1830, 0.0401, 0.0345, 0.0417, 0.0186],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,989][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0642, 0.0765, 0.0654, 0.0670, 0.0646, 0.0609, 0.0809, 0.0589, 0.0587,
        0.0636, 0.0586, 0.0506, 0.0673, 0.0583, 0.0556, 0.0489],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,991][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0495, 0.1019, 0.0218, 0.2122, 0.0318, 0.0302, 0.0563, 0.0186, 0.0899,
        0.0525, 0.0246, 0.0997, 0.0962, 0.0077, 0.0373, 0.0696],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,993][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0103, 0.0039, 0.0124, 0.0102, 0.0449, 0.0356, 0.0166, 0.0528, 0.0323,
        0.0131, 0.0488, 0.0243, 0.1372, 0.1634, 0.3581, 0.0361],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,994][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0710, 0.0641, 0.0631, 0.0677, 0.0583, 0.0564, 0.0603, 0.0658, 0.0627,
        0.0645, 0.0600, 0.0646, 0.0635, 0.0596, 0.0593, 0.0589],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,996][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0072, 0.0617, 0.0581, 0.0425, 0.0555, 0.0223, 0.0273, 0.2022, 0.0137,
        0.0336, 0.1320, 0.0847, 0.0900, 0.0706, 0.0861, 0.0126],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,996][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0408, 0.0258, 0.0691, 0.0589, 0.0699, 0.0321, 0.0872, 0.0299, 0.0254,
        0.0947, 0.0587, 0.0607, 0.0836, 0.1234, 0.1198, 0.0198],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,996][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.1743, 0.0351, 0.0419, 0.0587, 0.0569, 0.0554, 0.0458, 0.0573, 0.0536,
        0.0478, 0.0652, 0.0543, 0.0555, 0.0753, 0.0660, 0.0568],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,997][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0123, 0.0365, 0.0530, 0.0492, 0.0552, 0.0602, 0.0930, 0.1131, 0.0666,
        0.0755, 0.0761, 0.0929, 0.0467, 0.0893, 0.0611, 0.0194],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:37,997][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.4386, 0.0161, 0.0466, 0.0235, 0.0141, 0.0597, 0.0155, 0.0325, 0.0581,
        0.0105, 0.0681, 0.0134, 0.0223, 0.0659, 0.0195, 0.0275, 0.0682],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,998][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0614, 0.0426, 0.0608, 0.0876, 0.0651, 0.0583, 0.0596, 0.0392, 0.0707,
        0.0368, 0.0627, 0.0505, 0.0593, 0.0538, 0.0740, 0.0674, 0.0498],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,998][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0002, 0.0357, 0.0781, 0.0943, 0.1172, 0.0436, 0.0261, 0.0677, 0.0524,
        0.0278, 0.1208, 0.1215, 0.0425, 0.0409, 0.0653, 0.0299, 0.0361],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,999][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0043, 0.1319, 0.0642, 0.0815, 0.0792, 0.0882, 0.0178, 0.0335, 0.0942,
        0.0548, 0.0525, 0.1229, 0.0473, 0.0123, 0.0349, 0.0721, 0.0084],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:37,999][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0618, 0.0703, 0.0628, 0.0639, 0.0581, 0.0561, 0.0777, 0.0561, 0.0575,
        0.0606, 0.0579, 0.0488, 0.0651, 0.0542, 0.0533, 0.0476, 0.0481],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,001][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0665, 0.1147, 0.0103, 0.1323, 0.0483, 0.0277, 0.0583, 0.0133, 0.0671,
        0.0251, 0.0231, 0.1321, 0.1164, 0.0055, 0.0546, 0.0992, 0.0056],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,003][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0078, 0.0022, 0.0082, 0.0080, 0.0332, 0.0235, 0.0155, 0.0379, 0.0227,
        0.0120, 0.0459, 0.0211, 0.1280, 0.1013, 0.3485, 0.0361, 0.1481],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,004][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0658, 0.0601, 0.0594, 0.0642, 0.0555, 0.0536, 0.0573, 0.0625, 0.0594,
        0.0611, 0.0569, 0.0616, 0.0605, 0.0562, 0.0565, 0.0560, 0.0533],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,006][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0048, 0.0667, 0.0757, 0.0393, 0.0631, 0.0273, 0.0256, 0.1148, 0.0270,
        0.0500, 0.1714, 0.0926, 0.0739, 0.0396, 0.0619, 0.0362, 0.0302],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,008][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0302, 0.0433, 0.1276, 0.0605, 0.0830, 0.0442, 0.0585, 0.0164, 0.0390,
        0.0698, 0.0577, 0.0667, 0.0752, 0.0355, 0.1189, 0.0402, 0.0335],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,010][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1799, 0.0328, 0.0394, 0.0540, 0.0532, 0.0509, 0.0417, 0.0522, 0.0490,
        0.0435, 0.0590, 0.0493, 0.0511, 0.0684, 0.0604, 0.0515, 0.0637],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,011][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0107, 0.0334, 0.0488, 0.0433, 0.0573, 0.0507, 0.0775, 0.0984, 0.0743,
        0.0661, 0.0723, 0.0816, 0.0695, 0.0625, 0.0733, 0.0240, 0.0563],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,013][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:38,014][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[13614],
        [ 4200],
        [ 8364],
        [ 1190],
        [ 8424],
        [ 4340],
        [22404],
        [21866],
        [ 9563],
        [ 2708],
        [ 3815],
        [ 4310],
        [10585],
        [14566],
        [ 1978],
        [ 5425],
        [13973]], device='cuda:0')
[2024-07-24 10:17:38,015][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[14215],
        [ 6902],
        [ 7557],
        [ 2876],
        [ 9281],
        [ 3828],
        [18175],
        [19621],
        [13534],
        [ 1162],
        [ 4262],
        [ 6673],
        [10538],
        [13879],
        [ 3933],
        [ 7211],
        [13312]], device='cuda:0')
[2024-07-24 10:17:38,016][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[11813],
        [13749],
        [12525],
        [12850],
        [14674],
        [16278],
        [21563],
        [14845],
        [15328],
        [12764],
        [18828],
        [16394],
        [15417],
        [29534],
        [18884],
        [24194],
        [29157]], device='cuda:0')
[2024-07-24 10:17:38,017][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[3845],
        [4030],
        [4560],
        [4481],
        [4701],
        [4896],
        [4886],
        [4923],
        [4845],
        [4851],
        [4839],
        [4876],
        [4922],
        [4996],
        [4953],
        [4980],
        [5023]], device='cuda:0')
[2024-07-24 10:17:38,019][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[8863],
        [4936],
        [5376],
        [4757],
        [4892],
        [5035],
        [4940],
        [5574],
        [5616],
        [5509],
        [6039],
        [5997],
        [5767],
        [5714],
        [5953],
        [5973],
        [5934]], device='cuda:0')
[2024-07-24 10:17:38,021][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 7184],
        [ 8741],
        [10376],
        [11702],
        [10646],
        [11697],
        [11979],
        [11652],
        [11876],
        [11966],
        [11671],
        [12223],
        [12038],
        [11512],
        [11714],
        [11580],
        [11368]], device='cuda:0')
[2024-07-24 10:17:38,022][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[38133],
        [38141],
        [38143],
        [38143],
        [38150],
        [38171],
        [38166],
        [38162],
        [38169],
        [38157],
        [38215],
        [38146],
        [38177],
        [38242],
        [38228],
        [38188],
        [38243]], device='cuda:0')
[2024-07-24 10:17:38,024][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 7626],
        [ 3845],
        [ 3916],
        [ 3996],
        [ 5755],
        [ 6280],
        [ 8401],
        [ 7258],
        [ 7626],
        [ 9547],
        [ 9563],
        [10054],
        [10700],
        [ 9875],
        [11989],
        [12046],
        [11133]], device='cuda:0')
[2024-07-24 10:17:38,026][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[14423],
        [30322],
        [31167],
        [36472],
        [38518],
        [38329],
        [33047],
        [38225],
        [40724],
        [38241],
        [43908],
        [45680],
        [44011],
        [43091],
        [39964],
        [42690],
        [43030]], device='cuda:0')
[2024-07-24 10:17:38,027][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[44242],
        [44145],
        [41167],
        [42732],
        [41491],
        [40839],
        [39488],
        [38674],
        [38434],
        [38551],
        [38467],
        [37485],
        [37591],
        [37680],
        [36299],
        [36035],
        [36407]], device='cuda:0')
[2024-07-24 10:17:38,029][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[8778],
        [4436],
        [4005],
        [3587],
        [3581],
        [4351],
        [4727],
        [5732],
        [5975],
        [5950],
        [5698],
        [5720],
        [5633],
        [5571],
        [5783],
        [5898],
        [5749]], device='cuda:0')
[2024-07-24 10:17:38,031][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[44263],
        [45042],
        [46854],
        [46184],
        [47701],
        [48091],
        [48339],
        [48468],
        [47239],
        [43085],
        [45335],
        [48422],
        [47383],
        [46966],
        [48858],
        [49107],
        [48983]], device='cuda:0')
[2024-07-24 10:17:38,032][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[14343],
        [ 9775],
        [ 8354],
        [ 6979],
        [ 5862],
        [ 4211],
        [ 3223],
        [ 2934],
        [ 3221],
        [ 3365],
        [ 3485],
        [ 3252],
        [ 3516],
        [ 3423],
        [ 3456],
        [ 3522],
        [ 3527]], device='cuda:0')
[2024-07-24 10:17:38,033][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[4574],
        [1399],
        [1850],
        [1272],
        [1475],
        [1860],
        [2521],
        [3395],
        [3494],
        [2962],
        [2960],
        [2848],
        [2799],
        [2717],
        [2767],
        [2886],
        [2774]], device='cuda:0')
[2024-07-24 10:17:38,034][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[29187],
        [14652],
        [17723],
        [14119],
        [30547],
        [38195],
        [46519],
        [38306],
        [16330],
        [39796],
        [20031],
        [14730],
        [16153],
        [39983],
        [ 8812],
        [23562],
        [35539]], device='cuda:0')
[2024-07-24 10:17:38,035][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[28782],
        [28589],
        [29518],
        [28628],
        [29363],
        [29321],
        [29345],
        [29663],
        [29165],
        [29107],
        [28628],
        [28362],
        [28849],
        [27853],
        [28042],
        [28030],
        [27178]], device='cuda:0')
[2024-07-24 10:17:38,037][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[16779],
        [20134],
        [19238],
        [20688],
        [22182],
        [22701],
        [24182],
        [23917],
        [22840],
        [22998],
        [22438],
        [23459],
        [23249],
        [23766],
        [23607],
        [23951],
        [23997]], device='cuda:0')
[2024-07-24 10:17:38,038][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[48153],
        [43593],
        [45717],
        [44949],
        [46020],
        [46168],
        [46269],
        [46159],
        [46228],
        [46122],
        [46488],
        [46090],
        [45886],
        [46000],
        [45834],
        [45702],
        [45700]], device='cuda:0')
[2024-07-24 10:17:38,040][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[40641],
        [35543],
        [40534],
        [44038],
        [43985],
        [44689],
        [45282],
        [44463],
        [44551],
        [44688],
        [45584],
        [45745],
        [44979],
        [45457],
        [44623],
        [45391],
        [44787]], device='cuda:0')
[2024-07-24 10:17:38,041][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[31746],
        [23006],
        [14644],
        [14560],
        [14613],
        [14735],
        [13745],
        [13755],
        [13652],
        [13111],
        [13066],
        [13570],
        [14049],
        [14398],
        [14950],
        [15383],
        [15895]], device='cuda:0')
[2024-07-24 10:17:38,043][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 7226],
        [13246],
        [13449],
        [ 9233],
        [11831],
        [11466],
        [ 9328],
        [10614],
        [ 9830],
        [14600],
        [12040],
        [26754],
        [23811],
        [12792],
        [19857],
        [11206],
        [10734]], device='cuda:0')
[2024-07-24 10:17:38,045][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[3485],
        [4577],
        [2522],
        [3332],
        [3692],
        [4098],
        [4424],
        [3452],
        [3658],
        [3570],
        [3673],
        [3725],
        [3876],
        [3690],
        [3329],
        [3374],
        [3423]], device='cuda:0')
[2024-07-24 10:17:38,047][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 9704],
        [10669],
        [11248],
        [11452],
        [11423],
        [11477],
        [11374],
        [11388],
        [11423],
        [11536],
        [11655],
        [11855],
        [11909],
        [11994],
        [11982],
        [11996],
        [12047]], device='cuda:0')
[2024-07-24 10:17:38,048][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[14179],
        [27086],
        [30151],
        [32786],
        [31818],
        [30154],
        [29325],
        [29991],
        [28540],
        [27751],
        [28858],
        [27437],
        [27599],
        [27492],
        [26310],
        [25614],
        [26198]], device='cuda:0')
[2024-07-24 10:17:38,049][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[41464],
        [45167],
        [43138],
        [43951],
        [42755],
        [41546],
        [42256],
        [41174],
        [42235],
        [41321],
        [42653],
        [42706],
        [43196],
        [41916],
        [41347],
        [40986],
        [41859]], device='cuda:0')
[2024-07-24 10:17:38,050][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[34464],
        [36621],
        [36370],
        [35376],
        [34698],
        [34316],
        [34348],
        [34150],
        [34180],
        [33948],
        [33848],
        [33820],
        [33746],
        [33525],
        [33137],
        [32871],
        [32852]], device='cuda:0')
[2024-07-24 10:17:38,051][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[41626],
        [43990],
        [43433],
        [43570],
        [43453],
        [44133],
        [43417],
        [43833],
        [44043],
        [44284],
        [43937],
        [43854],
        [44052],
        [44088],
        [44170],
        [43862],
        [43951]], device='cuda:0')
[2024-07-24 10:17:38,052][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 8937],
        [ 9430],
        [10065],
        [ 9208],
        [ 9505],
        [ 9386],
        [ 8770],
        [ 9319],
        [ 9299],
        [ 8809],
        [ 8575],
        [ 7295],
        [ 7825],
        [ 9090],
        [ 8770],
        [ 9736],
        [ 9632]], device='cuda:0')
[2024-07-24 10:17:38,054][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[18841],
        [21370],
        [16031],
        [13746],
        [ 9049],
        [ 5600],
        [ 9977],
        [10362],
        [13946],
        [12338],
        [11421],
        [10021],
        [14354],
        [ 5316],
        [12018],
        [ 8803],
        [ 7076]], device='cuda:0')
[2024-07-24 10:17:38,056][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165],
        [24165]], device='cuda:0')
[2024-07-24 10:17:38,092][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:38,094][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,095][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,096][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,097][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,099][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,100][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,101][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,103][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,104][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,104][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,104][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,105][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,105][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.4151, 0.5849], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,105][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.2561, 0.7439], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,106][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.8251, 0.1749], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,106][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.9979, 0.0021], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,106][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0646, 0.9354], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,107][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0860, 0.9140], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,107][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.2358, 0.7642], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,107][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.1216, 0.8784], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,108][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.8831, 0.1169], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,110][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.3332, 0.6668], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,111][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.7721, 0.2279], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,112][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([9.9999e-01, 8.0044e-06], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,114][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.1141, 0.0740, 0.8119], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,115][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.2340, 0.3557, 0.4103], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,116][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.6161, 0.2370, 0.1469], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,118][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.9931, 0.0033, 0.0036], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,120][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0344, 0.4597, 0.5058], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,121][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0058, 0.0165, 0.9777], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,122][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1297, 0.4390, 0.4313], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,122][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0439, 0.3206, 0.6355], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,122][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.8662, 0.0827, 0.0511], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,123][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.2126, 0.3062, 0.4812], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,123][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.6154, 0.1862, 0.1984], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,123][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ and] are: tensor([9.9934e-01, 6.4955e-04, 1.0203e-05], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,124][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.0159, 0.0223, 0.9495, 0.0124], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,124][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.1196, 0.0718, 0.6210, 0.1875], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,124][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.5095, 0.1126, 0.0861, 0.2918], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,125][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.9909, 0.0012, 0.0042, 0.0037], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,125][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0214, 0.3139, 0.3471, 0.3177], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,126][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0120, 0.0059, 0.4754, 0.5067], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,128][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.0901, 0.3026, 0.2970, 0.3103], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,129][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0277, 0.1668, 0.3047, 0.5008], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,131][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.7683, 0.1039, 0.0712, 0.0567], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,132][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.0983, 0.1752, 0.2638, 0.4627], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,134][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.5257, 0.1554, 0.1625, 0.1564], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,135][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([9.9995e-01, 3.2733e-06, 5.0810e-05, 4.0534e-10], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,136][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0114, 0.0254, 0.7423, 0.2018, 0.0192], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,138][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.1677, 0.0350, 0.4402, 0.0903, 0.2668], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,139][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.4371, 0.1036, 0.0755, 0.2160, 0.1679], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,139][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.9453, 0.0026, 0.0050, 0.0125, 0.0346], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,139][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0143, 0.2447, 0.2690, 0.2463, 0.2257], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,140][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0039, 0.0032, 0.2656, 0.3283, 0.3992], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,140][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0619, 0.2321, 0.2299, 0.2422, 0.2339], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,140][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0189, 0.1110, 0.2324, 0.3331, 0.3047], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,141][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.6733, 0.0969, 0.0695, 0.0635, 0.0968], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,141][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0633, 0.1348, 0.2272, 0.3914, 0.1833], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,141][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.4663, 0.1345, 0.1372, 0.1296, 0.1324], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,142][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ got] are: tensor([9.9998e-01, 1.1524e-05, 7.8805e-06, 6.5223e-08, 4.6442e-08],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,142][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2195, 0.0059, 0.1965, 0.2286, 0.1676, 0.1819], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,143][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0113, 0.0181, 0.1527, 0.2146, 0.5494, 0.0538], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,145][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2947, 0.1444, 0.0811, 0.2096, 0.2023, 0.0679], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,147][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.9037, 0.0020, 0.0048, 0.0109, 0.0393, 0.0393], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,148][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0110, 0.1995, 0.2198, 0.2009, 0.1843, 0.1845], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,149][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([4.7447e-03, 3.5792e-04, 7.3354e-03, 3.0308e-02, 3.4408e-02, 9.2285e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,151][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0534, 0.1882, 0.1859, 0.1955, 0.1888, 0.1882], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,152][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0153, 0.0866, 0.1762, 0.2553, 0.2251, 0.2414], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,154][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.7020, 0.0633, 0.0418, 0.0404, 0.0702, 0.0823], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,155][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0574, 0.1023, 0.1608, 0.2883, 0.1964, 0.1947], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,156][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.4159, 0.1200, 0.1201, 0.1116, 0.1140, 0.1184], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,156][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([9.9996e-01, 2.6165e-05, 7.7112e-06, 1.6477e-07, 1.4659e-06, 5.5293e-08],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,157][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([2.9385e-03, 1.7006e-03, 1.7989e-01, 1.4867e-02, 7.8492e-03, 7.9259e-01,
        1.6248e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,157][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0281, 0.0165, 0.1333, 0.1137, 0.2566, 0.2164, 0.2354],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,157][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.2931, 0.1300, 0.0691, 0.1761, 0.2357, 0.0647, 0.0313],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,158][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.9367, 0.0016, 0.0024, 0.0035, 0.0271, 0.0270, 0.0017],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,158][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0100, 0.1677, 0.1831, 0.1683, 0.1548, 0.1554, 0.1608],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,158][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([1.8306e-02, 2.3974e-04, 2.6424e-03, 1.3166e-02, 1.3287e-02, 2.7967e-01,
        6.7269e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,159][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.0456, 0.1561, 0.1543, 0.1618, 0.1578, 0.1576, 0.1668],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,159][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.0147, 0.0763, 0.1341, 0.2140, 0.1789, 0.1976, 0.1843],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,160][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.6104, 0.0708, 0.0451, 0.0429, 0.0687, 0.0912, 0.0709],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,161][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.0421, 0.0766, 0.1233, 0.2515, 0.1485, 0.2303, 0.1278],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,163][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.3647, 0.1046, 0.1055, 0.0982, 0.1010, 0.1048, 0.1211],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,164][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([9.9951e-01, 2.2152e-04, 2.0333e-04, 9.3375e-07, 3.7388e-05, 2.5568e-05,
        1.1976e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,165][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ at] are: tensor([7.3716e-03, 1.0469e-03, 3.0033e-02, 6.2982e-02, 2.3186e-02, 8.7455e-01,
        2.2575e-04, 6.0454e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,166][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0237, 0.0181, 0.0885, 0.2315, 0.2631, 0.1371, 0.1921, 0.0459],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,168][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.2315, 0.1553, 0.0809, 0.2117, 0.1529, 0.0508, 0.0473, 0.0696],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,169][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ at] are: tensor([9.5271e-01, 5.0442e-04, 7.3474e-04, 2.5281e-03, 9.6080e-03, 9.0301e-03,
        6.2009e-03, 1.8679e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,170][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0082, 0.1450, 0.1590, 0.1459, 0.1337, 0.1340, 0.1381, 0.1361],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,171][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ at] are: tensor([4.0629e-03, 5.2543e-05, 4.5148e-04, 2.5747e-03, 4.1696e-03, 9.9487e-02,
        5.9348e-01, 2.9573e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,173][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0386, 0.1341, 0.1326, 0.1391, 0.1349, 0.1349, 0.1438, 0.1421],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,173][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0157, 0.0656, 0.1176, 0.1702, 0.1616, 0.1719, 0.1471, 0.1502],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,174][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.5274, 0.0636, 0.0487, 0.0341, 0.0618, 0.0953, 0.0722, 0.0968],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,174][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0491, 0.0697, 0.1112, 0.1948, 0.1469, 0.1789, 0.1328, 0.1165],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,174][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.3322, 0.0920, 0.0943, 0.0881, 0.0902, 0.0929, 0.1090, 0.1011],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,175][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ at] are: tensor([9.9990e-01, 7.2447e-05, 4.7137e-06, 5.7295e-07, 2.7232e-06, 1.8046e-06,
        2.0005e-05, 8.4346e-07], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,175][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0691, 0.0025, 0.1327, 0.2160, 0.1190, 0.1778, 0.0012, 0.1592, 0.1224],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,176][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0078, 0.0093, 0.0513, 0.1107, 0.4091, 0.0313, 0.2984, 0.0719, 0.0102],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,176][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2825, 0.1260, 0.0625, 0.1537, 0.1415, 0.0550, 0.0310, 0.0649, 0.0830],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,176][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.7932, 0.0009, 0.0021, 0.0049, 0.0219, 0.0182, 0.0092, 0.0605, 0.0892],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,177][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0068, 0.1281, 0.1401, 0.1286, 0.1175, 0.1178, 0.1211, 0.1194, 0.1205],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,177][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([3.1410e-03, 1.9591e-05, 1.0823e-04, 1.0166e-03, 8.8015e-04, 3.1175e-02,
        1.5040e-01, 1.3761e-01, 6.7565e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,179][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0345, 0.1178, 0.1161, 0.1222, 0.1183, 0.1180, 0.1259, 0.1245, 0.1226],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,181][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0117, 0.0576, 0.1012, 0.1563, 0.1361, 0.1445, 0.1238, 0.1248, 0.1438],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,182][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5423, 0.0524, 0.0369, 0.0315, 0.0598, 0.0730, 0.0633, 0.0809, 0.0600],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,184][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0331, 0.0625, 0.1012, 0.1770, 0.1308, 0.1337, 0.1089, 0.1338, 0.1190],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,186][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2872, 0.0850, 0.0860, 0.0808, 0.0815, 0.0852, 0.1011, 0.0917, 0.1014],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,187][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.9998e-01, 6.3275e-06, 2.9917e-06, 5.2646e-08, 8.8479e-07, 6.9288e-08,
        9.0878e-07, 6.6268e-06, 1.7899e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,188][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ station] are: tensor([1.8131e-03, 5.4711e-03, 6.2961e-02, 2.7781e-02, 7.2009e-03, 6.8622e-01,
        5.1281e-04, 2.9749e-02, 1.7745e-01, 8.4630e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,189][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0194, 0.0085, 0.0494, 0.0292, 0.1939, 0.1493, 0.1695, 0.1769, 0.0665,
        0.1373], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,190][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.2393, 0.0857, 0.0640, 0.1726, 0.1531, 0.0523, 0.0180, 0.0614, 0.0939,
        0.0597], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,191][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.8973, 0.0030, 0.0018, 0.0041, 0.0213, 0.0151, 0.0024, 0.0152, 0.0386,
        0.0012], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,191][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0063, 0.1149, 0.1260, 0.1158, 0.1055, 0.1057, 0.1093, 0.1076, 0.1083,
        0.1006], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,192][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ station] are: tensor([1.1857e-03, 2.2212e-05, 2.1119e-04, 1.2808e-03, 1.1023e-03, 3.1617e-02,
        8.9078e-02, 1.0744e-01, 7.1229e-01, 5.5771e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,192][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0306, 0.1049, 0.1037, 0.1086, 0.1056, 0.1054, 0.1124, 0.1112, 0.1096,
        0.1078], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,192][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0104, 0.0470, 0.0842, 0.1312, 0.1148, 0.1241, 0.1055, 0.1084, 0.1224,
        0.1520], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,193][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.4581, 0.0537, 0.0434, 0.0287, 0.0537, 0.0768, 0.0653, 0.0821, 0.0622,
        0.0761], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,193][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0289, 0.0454, 0.0756, 0.1774, 0.1003, 0.1367, 0.0929, 0.1159, 0.1348,
        0.0922], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,194][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.2503, 0.0770, 0.0789, 0.0736, 0.0753, 0.0787, 0.0928, 0.0852, 0.0932,
        0.0951], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,194][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ station] are: tensor([9.9992e-01, 1.9893e-05, 7.9730e-06, 4.8900e-08, 6.3188e-07, 1.8459e-06,
        4.4492e-07, 8.4306e-06, 4.1788e-05, 5.3096e-07], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,195][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [,] are: tensor([6.2019e-03, 7.3100e-04, 7.2818e-03, 3.2535e-02, 1.6197e-02, 3.8592e-01,
        2.8380e-04, 6.8336e-03, 3.0665e-01, 3.7365e-03, 2.3363e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,196][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0075, 0.0063, 0.0272, 0.0870, 0.1158, 0.1107, 0.1652, 0.1150, 0.0521,
        0.3081, 0.0052], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,198][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1848, 0.1279, 0.0649, 0.1408, 0.1079, 0.0480, 0.0268, 0.0628, 0.0690,
        0.0799, 0.0872], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,199][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [,] are: tensor([8.7861e-01, 5.2347e-04, 7.2019e-04, 1.8215e-03, 1.0589e-02, 8.0071e-03,
        4.3503e-03, 1.8888e-02, 3.6036e-02, 2.5489e-03, 3.7904e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,201][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0055, 0.1041, 0.1147, 0.1047, 0.0959, 0.0959, 0.0988, 0.0974, 0.0982,
        0.0915, 0.0934], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,202][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [,] are: tensor([9.5466e-04, 3.8794e-06, 2.0957e-05, 1.5407e-04, 1.6202e-04, 7.7556e-03,
        2.9217e-02, 3.0679e-02, 1.9693e-01, 2.3971e-02, 7.1015e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,204][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0277, 0.0949, 0.0934, 0.0984, 0.0951, 0.0949, 0.1014, 0.1003, 0.0986,
        0.0975, 0.0977], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,205][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0076, 0.0396, 0.0732, 0.1072, 0.0960, 0.1028, 0.0843, 0.0882, 0.1015,
        0.1150, 0.1846], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,207][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.5081, 0.0439, 0.0308, 0.0294, 0.0431, 0.0650, 0.0503, 0.0677, 0.0537,
        0.0705, 0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,208][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0432, 0.0513, 0.0767, 0.1448, 0.0945, 0.1094, 0.0936, 0.0935, 0.1126,
        0.0875, 0.0929], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,208][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.2420, 0.0691, 0.0711, 0.0660, 0.0677, 0.0706, 0.0817, 0.0761, 0.0839,
        0.0850, 0.0868], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,208][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [,] are: tensor([9.9939e-01, 2.4990e-04, 2.3869e-06, 5.8602e-07, 4.8503e-06, 3.8979e-06,
        2.3395e-05, 1.3204e-05, 2.3374e-04, 7.3244e-05, 1.3775e-07],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,209][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([1.7067e-03, 7.4127e-04, 3.5091e-02, 1.1945e-02, 1.9656e-03, 2.8230e-01,
        2.7139e-04, 2.2444e-02, 6.0591e-02, 1.3584e-03, 5.7586e-01, 5.7303e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,209][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0086, 0.0059, 0.0376, 0.0458, 0.0553, 0.0644, 0.3816, 0.1230, 0.0342,
        0.1899, 0.0209, 0.0328], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,210][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.2580, 0.0491, 0.0379, 0.1619, 0.0894, 0.0340, 0.0496, 0.0490, 0.0484,
        0.0832, 0.0652, 0.0742], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,210][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([8.6043e-01, 2.2016e-04, 9.1086e-04, 7.2089e-04, 1.0720e-02, 1.3400e-02,
        7.0043e-04, 2.8954e-02, 3.7751e-02, 3.9439e-04, 4.2417e-02, 3.3778e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,210][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0052, 0.0948, 0.1052, 0.0966, 0.0882, 0.0882, 0.0904, 0.0895, 0.0900,
        0.0839, 0.0859, 0.0821], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,211][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([9.5243e-04, 9.3147e-06, 9.4274e-05, 4.1818e-04, 4.8070e-04, 1.3790e-02,
        2.2887e-02, 3.0677e-02, 1.3759e-01, 2.9126e-02, 6.7021e-01, 9.3763e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,211][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0245, 0.0859, 0.0852, 0.0891, 0.0866, 0.0866, 0.0928, 0.0917, 0.0903,
        0.0888, 0.0893, 0.0892], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,213][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0075, 0.0368, 0.0607, 0.0988, 0.0819, 0.0866, 0.0718, 0.0764, 0.0886,
        0.1008, 0.1558, 0.1343], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,215][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.3739, 0.0464, 0.0382, 0.0352, 0.0521, 0.0691, 0.0558, 0.0690, 0.0602,
        0.0827, 0.0427, 0.0748], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,216][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0209, 0.0380, 0.0733, 0.1414, 0.0813, 0.1083, 0.0782, 0.0907, 0.1073,
        0.0838, 0.0964, 0.0804], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,218][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.2189, 0.0651, 0.0663, 0.0609, 0.0626, 0.0652, 0.0752, 0.0700, 0.0772,
        0.0780, 0.0793, 0.0814], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,219][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([9.9996e-01, 4.0608e-07, 8.0954e-06, 8.4762e-09, 3.8574e-07, 5.7585e-07,
        8.9144e-07, 4.6631e-06, 1.8564e-05, 1.8089e-06, 5.9222e-07, 9.3127e-08],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,220][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([8.5454e-04, 5.5794e-04, 3.8808e-02, 3.4122e-03, 1.1254e-03, 1.0867e-01,
        2.1909e-04, 1.1251e-01, 5.3396e-02, 9.3314e-04, 6.7342e-01, 5.8387e-03,
        2.6150e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,222][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0138, 0.0022, 0.0587, 0.0097, 0.1356, 0.0367, 0.1589, 0.2954, 0.0356,
        0.1369, 0.0519, 0.0246, 0.0400], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,223][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.3424, 0.0473, 0.0502, 0.0849, 0.0647, 0.0406, 0.0197, 0.0451, 0.0728,
        0.0447, 0.0719, 0.0824, 0.0335], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,225][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.8272, 0.0011, 0.0013, 0.0039, 0.0093, 0.0106, 0.0035, 0.0210, 0.0326,
        0.0018, 0.0477, 0.0188, 0.0214], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,225][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0048, 0.0881, 0.0977, 0.0896, 0.0818, 0.0814, 0.0837, 0.0828, 0.0832,
        0.0775, 0.0796, 0.0760, 0.0739], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,225][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([9.2785e-04, 6.8621e-06, 3.2152e-05, 3.5599e-04, 1.4551e-04, 6.0664e-03,
        1.4417e-02, 1.1817e-02, 8.7555e-02, 1.4745e-02, 6.2408e-01, 9.8894e-02,
        1.4096e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,226][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0223, 0.0791, 0.0781, 0.0820, 0.0793, 0.0794, 0.0849, 0.0839, 0.0825,
        0.0816, 0.0818, 0.0824, 0.0828], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,226][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0053, 0.0304, 0.0567, 0.0852, 0.0752, 0.0795, 0.0632, 0.0671, 0.0760,
        0.0852, 0.1323, 0.1092, 0.1347], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,227][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.3951, 0.0409, 0.0320, 0.0267, 0.0458, 0.0608, 0.0470, 0.0638, 0.0506,
        0.0684, 0.0358, 0.0751, 0.0579], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,227][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0264, 0.0496, 0.0749, 0.1080, 0.0712, 0.0832, 0.0612, 0.0989, 0.0901,
        0.0709, 0.0816, 0.0826, 0.1014], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,228][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.2007, 0.0571, 0.0603, 0.0557, 0.0575, 0.0600, 0.0707, 0.0655, 0.0726,
        0.0738, 0.0761, 0.0771, 0.0728], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,228][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([9.9992e-01, 1.1921e-05, 7.4114e-06, 5.6440e-08, 1.6838e-07, 7.1674e-07,
        2.4128e-06, 1.3634e-05, 3.0688e-05, 3.3621e-06, 3.7350e-07, 3.3276e-06,
        2.6658e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,229][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([5.7077e-03, 4.0292e-04, 7.6900e-03, 2.1289e-02, 1.6243e-02, 3.4111e-01,
        9.4631e-05, 1.9772e-03, 2.9945e-01, 2.1779e-03, 2.8186e-01, 5.2037e-03,
        5.6310e-03, 1.1167e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,230][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0046, 0.0077, 0.0151, 0.0199, 0.1817, 0.0449, 0.1515, 0.0631, 0.0197,
        0.2417, 0.0106, 0.0461, 0.1868, 0.0064], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,232][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1868, 0.0613, 0.0439, 0.1266, 0.0947, 0.0391, 0.0213, 0.0469, 0.0534,
        0.0564, 0.0702, 0.1040, 0.0584, 0.0369], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,233][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([9.1041e-01, 1.7237e-04, 2.6187e-04, 7.8624e-04, 3.3031e-03, 2.5310e-03,
        2.3983e-03, 6.8321e-03, 1.2317e-02, 1.3558e-03, 2.0436e-02, 7.2045e-03,
        1.5540e-02, 1.6451e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,235][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0043, 0.0821, 0.0904, 0.0828, 0.0758, 0.0758, 0.0777, 0.0768, 0.0775,
        0.0718, 0.0736, 0.0702, 0.0684, 0.0727], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,236][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([4.9442e-04, 1.4482e-06, 6.6100e-06, 5.8819e-05, 5.6264e-05, 1.7814e-03,
        9.9659e-03, 8.5278e-03, 5.5094e-02, 6.1422e-03, 5.6328e-01, 7.0380e-02,
        1.3302e-01, 1.5119e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,238][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0211, 0.0731, 0.0722, 0.0761, 0.0735, 0.0734, 0.0785, 0.0775, 0.0763,
        0.0755, 0.0757, 0.0766, 0.0768, 0.0739], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,240][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0058, 0.0280, 0.0481, 0.0734, 0.0643, 0.0678, 0.0595, 0.0590, 0.0673,
        0.0783, 0.1207, 0.0965, 0.1133, 0.1180], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,241][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.3544, 0.0414, 0.0297, 0.0257, 0.0412, 0.0561, 0.0437, 0.0618, 0.0464,
        0.0623, 0.0350, 0.0697, 0.0562, 0.0764], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,243][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0197, 0.0334, 0.0573, 0.1004, 0.0644, 0.0902, 0.0608, 0.0772, 0.0808,
        0.0686, 0.0721, 0.0621, 0.1276, 0.0855], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,245][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1986, 0.0528, 0.0556, 0.0512, 0.0527, 0.0554, 0.0647, 0.0596, 0.0664,
        0.0678, 0.0688, 0.0703, 0.0655, 0.0705], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,246][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([9.9883e-01, 1.9935e-04, 4.5141e-06, 9.8002e-07, 6.7337e-06, 1.9258e-06,
        4.1325e-05, 5.9910e-06, 1.2548e-04, 8.0567e-05, 7.1462e-07, 3.9047e-05,
        6.6689e-04, 3.9676e-07], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,247][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0019, 0.0047, 0.0282, 0.0112, 0.0089, 0.3119, 0.0004, 0.0128, 0.1448,
        0.0008, 0.2692, 0.0118, 0.0012, 0.1887, 0.0034], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,248][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0187, 0.0039, 0.0314, 0.0168, 0.1063, 0.0310, 0.1094, 0.0972, 0.0168,
        0.1149, 0.0203, 0.0347, 0.0692, 0.0711, 0.2583], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,248][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.1825, 0.0787, 0.0403, 0.0856, 0.0791, 0.0342, 0.0224, 0.0414, 0.0590,
        0.0612, 0.0663, 0.1099, 0.0536, 0.0391, 0.0467], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,249][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ give] are: tensor([7.8996e-01, 3.7493e-04, 6.2667e-04, 1.9363e-03, 5.9303e-03, 5.8757e-03,
        2.9935e-03, 1.2186e-02, 1.8963e-02, 1.6497e-03, 3.4784e-02, 1.0500e-02,
        2.0992e-02, 3.7957e-02, 5.5271e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,249][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0037, 0.0770, 0.0853, 0.0781, 0.0711, 0.0709, 0.0725, 0.0719, 0.0724,
        0.0669, 0.0688, 0.0653, 0.0637, 0.0678, 0.0646], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,249][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ give] are: tensor([3.9034e-04, 3.8566e-06, 1.8573e-05, 1.4104e-04, 9.0887e-05, 2.5253e-03,
        6.9130e-03, 7.5405e-03, 3.1867e-02, 4.3716e-03, 4.6989e-01, 5.2045e-02,
        1.4212e-01, 2.0942e-01, 7.2665e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,250][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0192, 0.0679, 0.0670, 0.0705, 0.0682, 0.0684, 0.0730, 0.0723, 0.0712,
        0.0704, 0.0705, 0.0711, 0.0714, 0.0690, 0.0701], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,250][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0048, 0.0224, 0.0429, 0.0634, 0.0588, 0.0630, 0.0509, 0.0520, 0.0600,
        0.0690, 0.1088, 0.0817, 0.1025, 0.1058, 0.1140], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,251][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.2802, 0.0379, 0.0280, 0.0230, 0.0432, 0.0510, 0.0426, 0.0646, 0.0437,
        0.0622, 0.0329, 0.0647, 0.0581, 0.0773, 0.0905], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,252][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0161, 0.0227, 0.0547, 0.0848, 0.0635, 0.0760, 0.0531, 0.0785, 0.0773,
        0.0554, 0.0741, 0.0436, 0.1415, 0.0926, 0.0662], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,254][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.1767, 0.0500, 0.0520, 0.0481, 0.0501, 0.0524, 0.0607, 0.0569, 0.0632,
        0.0637, 0.0651, 0.0655, 0.0620, 0.0664, 0.0669], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,255][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ give] are: tensor([9.9974e-01, 2.0994e-05, 1.3319e-05, 1.1681e-07, 8.1812e-07, 9.9703e-07,
        3.5876e-06, 1.5798e-05, 5.2965e-05, 9.9817e-06, 1.0532e-06, 4.8946e-06,
        1.1640e-04, 3.0045e-06, 1.3151e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,256][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ it] are: tensor([3.3142e-03, 1.2181e-03, 6.2996e-03, 5.8877e-03, 1.8000e-02, 2.3590e-02,
        2.3977e-04, 2.0302e-02, 6.6161e-03, 2.6349e-03, 3.7759e-01, 1.1462e-02,
        1.1056e-02, 4.6777e-01, 3.8240e-02, 5.7811e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,257][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0034, 0.0014, 0.0141, 0.0127, 0.0414, 0.0082, 0.0350, 0.0389, 0.0046,
        0.0505, 0.0113, 0.0182, 0.1273, 0.0197, 0.5916, 0.0215],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,259][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.1846, 0.0507, 0.0354, 0.1095, 0.0666, 0.0271, 0.0209, 0.0417, 0.0518,
        0.0451, 0.0623, 0.0908, 0.0378, 0.0295, 0.0415, 0.1049],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,260][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ it] are: tensor([7.7291e-01, 2.7916e-04, 3.4842e-04, 8.3968e-04, 6.2477e-03, 5.0997e-03,
        2.3706e-03, 9.8212e-03, 2.1882e-02, 1.4191e-03, 2.5105e-02, 7.4913e-03,
        2.5738e-02, 2.8735e-02, 8.3348e-02, 8.3616e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,262][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0034, 0.0720, 0.0801, 0.0727, 0.0667, 0.0664, 0.0684, 0.0675, 0.0680,
        0.0630, 0.0647, 0.0613, 0.0600, 0.0638, 0.0610, 0.0610],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,263][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ it] are: tensor([9.1615e-04, 3.6624e-06, 1.0525e-05, 1.2715e-04, 9.3966e-05, 2.0237e-03,
        6.2338e-03, 6.9934e-03, 3.5658e-02, 4.1676e-03, 2.4094e-01, 5.3383e-02,
        1.2950e-01, 1.2029e-01, 9.7534e-02, 3.0212e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,264][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0181, 0.0638, 0.0627, 0.0660, 0.0637, 0.0637, 0.0682, 0.0674, 0.0662,
        0.0656, 0.0656, 0.0665, 0.0667, 0.0643, 0.0658, 0.0658],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,265][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0048, 0.0212, 0.0384, 0.0566, 0.0517, 0.0545, 0.0447, 0.0472, 0.0536,
        0.0604, 0.0931, 0.0741, 0.0871, 0.0946, 0.0971, 0.1210],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,265][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.2897, 0.0332, 0.0259, 0.0259, 0.0391, 0.0485, 0.0390, 0.0504, 0.0411,
        0.0550, 0.0294, 0.0607, 0.0574, 0.0678, 0.0981, 0.0388],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,266][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0143, 0.0261, 0.0491, 0.1005, 0.0616, 0.0766, 0.0443, 0.0652, 0.0709,
        0.0544, 0.0611, 0.0505, 0.1016, 0.0817, 0.0672, 0.0748],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,266][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.1681, 0.0478, 0.0493, 0.0457, 0.0471, 0.0491, 0.0569, 0.0528, 0.0583,
        0.0594, 0.0601, 0.0614, 0.0580, 0.0613, 0.0621, 0.0628],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,267][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ it] are: tensor([9.9990e-01, 3.0153e-06, 8.4079e-07, 9.2226e-09, 4.4424e-07, 1.9455e-08,
        2.7321e-07, 3.0045e-06, 5.6245e-07, 1.5462e-06, 5.6255e-08, 5.6294e-07,
        2.5392e-05, 1.2910e-07, 5.9958e-05, 4.4976e-07], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,267][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([3.7396e-03, 2.4721e-04, 4.7566e-03, 1.2740e-02, 9.5525e-03, 2.2339e-01,
        5.0414e-05, 1.5923e-03, 1.9794e-01, 1.0407e-03, 1.8303e-01, 3.2095e-03,
        3.2607e-03, 8.1210e-03, 1.0693e-01, 2.3455e-01, 5.8499e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,268][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0036, 0.0049, 0.0095, 0.0134, 0.1334, 0.0282, 0.1098, 0.0425, 0.0124,
        0.1690, 0.0065, 0.0323, 0.1225, 0.0042, 0.2418, 0.0607, 0.0052],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,268][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1458, 0.0498, 0.0386, 0.1029, 0.0795, 0.0326, 0.0175, 0.0412, 0.0453,
        0.0473, 0.0608, 0.0838, 0.0483, 0.0317, 0.0439, 0.0991, 0.0320],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,269][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([8.6964e-01, 1.5496e-04, 2.1494e-04, 6.4068e-04, 2.9696e-03, 2.1219e-03,
        1.8749e-03, 5.5231e-03, 1.0070e-02, 1.0905e-03, 1.5967e-02, 5.7641e-03,
        1.2895e-02, 1.3139e-02, 3.3911e-02, 6.2873e-03, 1.7735e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,270][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0035, 0.0678, 0.0748, 0.0685, 0.0628, 0.0626, 0.0642, 0.0635, 0.0640,
        0.0593, 0.0608, 0.0578, 0.0565, 0.0600, 0.0573, 0.0573, 0.0593],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,272][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.5597e-04, 6.4013e-07, 2.2424e-06, 2.5009e-05, 2.3948e-05, 7.1506e-04,
        3.6651e-03, 3.2900e-03, 2.0098e-02, 2.2058e-03, 2.2592e-01, 2.7766e-02,
        5.0687e-02, 5.9909e-02, 5.8030e-02, 4.3450e-01, 1.1290e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,273][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0171, 0.0598, 0.0588, 0.0621, 0.0599, 0.0598, 0.0640, 0.0632, 0.0623,
        0.0616, 0.0617, 0.0625, 0.0627, 0.0603, 0.0619, 0.0621, 0.0601],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,275][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0042, 0.0201, 0.0337, 0.0522, 0.0454, 0.0481, 0.0421, 0.0418, 0.0480,
        0.0558, 0.0847, 0.0676, 0.0789, 0.0836, 0.0880, 0.1107, 0.0950],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,277][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2889, 0.0341, 0.0245, 0.0214, 0.0340, 0.0458, 0.0359, 0.0511, 0.0382,
        0.0513, 0.0290, 0.0572, 0.0464, 0.0624, 0.0800, 0.0350, 0.0648],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,279][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0155, 0.0269, 0.0464, 0.0789, 0.0517, 0.0732, 0.0478, 0.0632, 0.0656,
        0.0553, 0.0586, 0.0505, 0.1013, 0.0694, 0.0572, 0.0727, 0.0658],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,280][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1608, 0.0438, 0.0460, 0.0426, 0.0436, 0.0459, 0.0534, 0.0494, 0.0548,
        0.0561, 0.0567, 0.0580, 0.0539, 0.0579, 0.0580, 0.0593, 0.0598],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,281][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([9.9816e-01, 1.4741e-04, 3.7674e-06, 9.1904e-07, 6.8147e-06, 2.7156e-06,
        3.7130e-05, 5.7142e-06, 1.3434e-04, 6.3278e-05, 6.6311e-07, 3.1135e-05,
        5.1254e-04, 4.1168e-07, 3.4354e-04, 5.5370e-04, 5.8323e-07],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,315][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:38,316][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,316][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,316][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,317][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,317][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,317][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,318][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,318][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,318][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,318][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,319][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,319][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,320][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.5388, 0.4612], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,321][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0881, 0.9119], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,323][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.6382, 0.3618], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,325][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.1625, 0.8375], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,326][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.4487, 0.5513], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,327][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.1821, 0.8179], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,329][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.4416, 0.5584], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,330][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.1289, 0.8711], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,332][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.8648, 0.1352], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,333][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.8686, 0.1314], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,333][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.3328, 0.6672], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,333][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.4447, 0.5553], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,334][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.2476, 0.1528, 0.5996], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,334][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0522, 0.4243, 0.5236], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,334][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.2868, 0.4219, 0.2913], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,335][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0143, 0.0434, 0.9423], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,335][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0835, 0.0745, 0.8419], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,335][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0254, 0.0289, 0.9457], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,336][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2807, 0.1420, 0.5774], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,336][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0940, 0.4637, 0.4422], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,336][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7487, 0.1375, 0.1138], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,337][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6388, 0.0296, 0.3316], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,339][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.3079, 0.4460, 0.2461], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,340][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0025, 0.1149, 0.8826], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,342][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.1568, 0.0870, 0.5935, 0.1627], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,343][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.0246, 0.2558, 0.3500, 0.3696], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,345][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.3112, 0.0765, 0.0784, 0.5339], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,347][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.0098, 0.0146, 0.4582, 0.5174], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,348][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0840, 0.0126, 0.0815, 0.8219], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,350][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.0168, 0.0077, 0.5926, 0.3829], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,350][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.1739, 0.0741, 0.3967, 0.3552], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,351][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0992, 0.3071, 0.2865, 0.3072], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,351][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.2523, 0.5574, 0.1439, 0.0464], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,351][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.3145, 0.0260, 0.3397, 0.3198], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,352][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.2861, 0.3441, 0.1741, 0.1957], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,352][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([2.3161e-07, 2.7982e-08, 5.8331e-01, 4.1669e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,352][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0459, 0.0569, 0.4573, 0.3451, 0.0949], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,353][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0176, 0.1735, 0.2349, 0.2485, 0.3255], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,353][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.3536, 0.0882, 0.0941, 0.2417, 0.2224], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,353][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0013, 0.0064, 0.2200, 0.4869, 0.2855], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,354][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.1637, 0.0518, 0.1670, 0.0856, 0.5318], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,355][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0109, 0.0051, 0.3196, 0.2668, 0.3976], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,356][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0826, 0.0421, 0.2178, 0.2880, 0.3695], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,358][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0807, 0.2484, 0.2373, 0.2269, 0.2067], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,359][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.4085, 0.1140, 0.0940, 0.0524, 0.3311], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,361][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.1758, 0.0158, 0.2013, 0.3036, 0.3035], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,362][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.1508, 0.2980, 0.1547, 0.1142, 0.2823], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,363][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([2.7314e-09, 3.6747e-08, 1.1190e-04, 9.7084e-01, 2.9052e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,365][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1060, 0.0383, 0.2903, 0.2554, 0.2271, 0.0828], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,367][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0149, 0.1360, 0.1813, 0.1840, 0.2407, 0.2430], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,368][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1703, 0.1649, 0.0771, 0.1771, 0.3447, 0.0659], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,368][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([2.2284e-03, 7.3946e-04, 1.7497e-02, 4.0925e-02, 6.4030e-02, 8.7458e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,369][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0271, 0.0717, 0.0918, 0.0927, 0.0654, 0.6513], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,369][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0218, 0.0010, 0.0194, 0.0457, 0.0698, 0.8422], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,369][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0614, 0.0252, 0.1368, 0.1605, 0.2738, 0.3423], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,370][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0601, 0.1914, 0.1556, 0.2033, 0.1512, 0.2383], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,370][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1556, 0.0742, 0.0416, 0.0302, 0.4603, 0.2381], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,370][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2558, 0.0053, 0.0435, 0.0736, 0.1423, 0.4795], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,371][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.1296, 0.2745, 0.1343, 0.0985, 0.1881, 0.1750], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,371][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.7741e-10, 8.5644e-13, 2.2691e-11, 5.0256e-06, 1.7026e-05, 9.9998e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,371][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.0710, 0.0316, 0.3612, 0.1310, 0.1019, 0.2536, 0.0497],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,372][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0122, 0.1010, 0.1363, 0.1407, 0.1946, 0.1960, 0.2192],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,374][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.1093, 0.0778, 0.0358, 0.1229, 0.5344, 0.0923, 0.0274],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,375][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0067, 0.0006, 0.0056, 0.0191, 0.0391, 0.3336, 0.5954],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,377][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0104, 0.0079, 0.0168, 0.0294, 0.0618, 0.0172, 0.8566],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,378][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([2.0238e-02, 3.6418e-04, 7.9754e-03, 1.7513e-02, 2.7018e-02, 4.2870e-01,
        4.9819e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,379][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0366, 0.0275, 0.1180, 0.1180, 0.2380, 0.3265, 0.1354],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,381][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0681, 0.1647, 0.1606, 0.1344, 0.1188, 0.1774, 0.1761],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,383][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.1342, 0.2159, 0.0205, 0.0474, 0.1679, 0.2825, 0.1316],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,384][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.1164, 0.0022, 0.0285, 0.0281, 0.0816, 0.4280, 0.3153],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,385][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.1251, 0.1825, 0.1322, 0.0882, 0.1443, 0.1660, 0.1617],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,385][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([3.2042e-10, 8.6294e-13, 6.2696e-12, 4.7533e-07, 2.6210e-07, 7.2182e-01,
        2.7818e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,386][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0721, 0.0269, 0.1783, 0.2255, 0.1481, 0.1846, 0.1289, 0.0356],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,386][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0112, 0.0907, 0.1179, 0.1168, 0.1540, 0.1588, 0.1854, 0.1652],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,387][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.1509, 0.1484, 0.1035, 0.1434, 0.1042, 0.0347, 0.1642, 0.1508],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,387][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([1.0024e-03, 1.4726e-04, 1.0407e-03, 4.3691e-03, 1.0657e-02, 1.3942e-01,
        4.6429e-01, 3.7907e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,387][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0542, 0.0181, 0.1624, 0.1220, 0.0436, 0.0680, 0.2572, 0.2746],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,388][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([1.2730e-02, 1.2688e-04, 1.8689e-03, 3.7352e-03, 1.0962e-02, 1.7277e-01,
        4.9304e-01, 3.0476e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,388][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0508, 0.0103, 0.0578, 0.0600, 0.1129, 0.1442, 0.0949, 0.4691],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,389][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0294, 0.1617, 0.1169, 0.1475, 0.1102, 0.1584, 0.1598, 0.1160],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,389][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.1175, 0.0351, 0.0363, 0.0058, 0.1550, 0.3538, 0.1989, 0.0976],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,391][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0574, 0.0010, 0.0116, 0.0137, 0.0567, 0.2370, 0.2688, 0.3538],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,393][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.1046, 0.2565, 0.0873, 0.0532, 0.0981, 0.1112, 0.1108, 0.1784],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,394][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([1.1068e-10, 1.7849e-15, 6.3012e-16, 1.2245e-09, 7.2432e-10, 4.6831e-04,
        3.6248e-01, 6.3706e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,395][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0479, 0.0219, 0.1769, 0.1585, 0.1643, 0.0547, 0.1721, 0.1472, 0.0564],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,396][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0086, 0.0757, 0.1013, 0.1009, 0.1324, 0.1350, 0.1651, 0.1436, 0.1375],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,398][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.2220, 0.1409, 0.0447, 0.0950, 0.1540, 0.0413, 0.0782, 0.1274, 0.0965],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,399][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([9.1139e-04, 2.9720e-05, 2.2084e-04, 1.1380e-03, 2.1560e-03, 2.2565e-02,
        1.1865e-01, 1.7756e-01, 6.7677e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,401][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0626, 0.0787, 0.0993, 0.0464, 0.0222, 0.0297, 0.4702, 0.0461, 0.1447],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,402][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([1.3034e-02, 5.3345e-05, 4.7574e-04, 1.4559e-03, 2.4105e-03, 4.8411e-02,
        9.5231e-02, 1.5874e-01, 6.8019e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,402][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0321, 0.0078, 0.0428, 0.0437, 0.0823, 0.0992, 0.0665, 0.3430, 0.2827],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,403][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0302, 0.1325, 0.0836, 0.1488, 0.0962, 0.1469, 0.1429, 0.0915, 0.1275],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,403][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0914, 0.0431, 0.0283, 0.0076, 0.2854, 0.1578, 0.2488, 0.1038, 0.0337],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,404][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0732, 0.0006, 0.0049, 0.0071, 0.0207, 0.0778, 0.0850, 0.2211, 0.5095],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,404][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1044, 0.1524, 0.0845, 0.0659, 0.1296, 0.1081, 0.1071, 0.1598, 0.0882],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,404][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([2.9687e-12, 4.1881e-19, 7.3007e-19, 8.2243e-14, 1.7210e-13, 1.1896e-07,
        3.4732e-06, 1.2549e-02, 9.8745e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,405][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0189, 0.0243, 0.2381, 0.1296, 0.0653, 0.1907, 0.0461, 0.1012, 0.1204,
        0.0654], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,405][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0086, 0.0630, 0.0813, 0.0866, 0.1189, 0.1214, 0.1371, 0.1296, 0.1280,
        0.1254], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,406][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0388, 0.0400, 0.0536, 0.0745, 0.1776, 0.1291, 0.0215, 0.1646, 0.2861,
        0.0142], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,406][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([1.2116e-03, 1.2172e-04, 4.0543e-04, 3.1003e-03, 2.9424e-03, 2.6841e-02,
        6.0629e-02, 9.1585e-02, 6.9690e-01, 1.1627e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,407][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0179, 0.0179, 0.1785, 0.0302, 0.0082, 0.0157, 0.2053, 0.0651, 0.0440,
        0.4173], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,408][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([2.8901e-03, 3.1321e-05, 4.4675e-04, 1.2965e-03, 1.5442e-03, 3.5703e-02,
        5.1802e-02, 9.7508e-02, 7.4098e-01, 6.7799e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,410][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0146, 0.0103, 0.0400, 0.0458, 0.0752, 0.1083, 0.0573, 0.3180, 0.2738,
        0.0567], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,411][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0431, 0.1153, 0.0972, 0.1011, 0.0814, 0.1254, 0.1197, 0.0887, 0.1122,
        0.1158], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,413][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0763, 0.0477, 0.0492, 0.0013, 0.1051, 0.1842, 0.4051, 0.1052, 0.0240,
        0.0018], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,414][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([2.6082e-02, 3.9777e-04, 3.6061e-03, 5.6398e-03, 1.4649e-02, 7.2356e-02,
        8.5738e-02, 2.0313e-01, 5.0162e-01, 8.6784e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,416][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0866, 0.1435, 0.0765, 0.0434, 0.1230, 0.1004, 0.0578, 0.1033, 0.0688,
        0.1968], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,417][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([4.4816e-14, 8.0077e-20, 1.4384e-19, 2.7820e-14, 5.6770e-15, 9.8463e-08,
        2.6511e-06, 6.6232e-05, 9.9992e-01, 6.4546e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,418][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0294, 0.0156, 0.0934, 0.1031, 0.0886, 0.0813, 0.1044, 0.0617, 0.0799,
        0.2051, 0.1374], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,420][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0080, 0.0595, 0.0783, 0.0762, 0.1010, 0.1066, 0.1203, 0.1119, 0.1090,
        0.1129, 0.1164], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,420][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1025, 0.1340, 0.0566, 0.0684, 0.0963, 0.0395, 0.0596, 0.1783, 0.0866,
        0.1023, 0.0760], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,420][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([5.3127e-04, 1.0639e-05, 4.4326e-05, 3.5138e-04, 5.8566e-04, 9.2617e-03,
        4.8127e-02, 5.4144e-02, 3.2531e-01, 9.5858e-02, 4.6578e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,421][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0198, 0.0054, 0.1480, 0.0145, 0.0095, 0.0066, 0.1726, 0.0230, 0.0163,
        0.2368, 0.3474], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,421][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([3.0401e-03, 9.3418e-06, 8.4486e-05, 1.7554e-04, 3.4405e-04, 1.1434e-02,
        1.7236e-02, 2.8376e-02, 2.1558e-01, 2.9596e-02, 6.9412e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,422][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0252, 0.0050, 0.0277, 0.0291, 0.0506, 0.0651, 0.0419, 0.2248, 0.1926,
        0.0553, 0.2829], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,422][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0367, 0.1183, 0.0985, 0.0953, 0.0703, 0.1018, 0.1043, 0.0801, 0.0892,
        0.0914, 0.1142], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,422][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.1889, 0.0457, 0.0279, 0.0241, 0.0969, 0.2565, 0.1456, 0.0815, 0.0603,
        0.0204, 0.0521], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,423][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([2.9178e-02, 2.2764e-04, 2.3616e-03, 3.3496e-03, 1.0685e-02, 4.9492e-02,
        5.2104e-02, 1.4300e-01, 3.4390e-01, 7.0919e-02, 2.9478e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,423][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0825, 0.1203, 0.0616, 0.0402, 0.0788, 0.0874, 0.0731, 0.1090, 0.0596,
        0.1455, 0.1420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,424][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([2.1342e-12, 1.9840e-19, 1.0881e-21, 7.7412e-15, 1.2411e-14, 1.9296e-08,
        5.2329e-05, 5.8223e-04, 5.9945e-01, 2.9306e-03, 3.9698e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,426][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0162, 0.0117, 0.1602, 0.0829, 0.0338, 0.1268, 0.0476, 0.0898, 0.0717,
        0.1032, 0.2356, 0.0205], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,427][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0048, 0.0470, 0.0677, 0.0669, 0.0939, 0.0968, 0.1079, 0.1015, 0.1002,
        0.1024, 0.1105, 0.1002], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,429][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0591, 0.0144, 0.0285, 0.1317, 0.0882, 0.0350, 0.2558, 0.1226, 0.0396,
        0.1383, 0.0635, 0.0233], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,430][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([9.5717e-04, 2.2421e-05, 1.4991e-04, 6.8851e-04, 1.3562e-03, 1.3015e-02,
        2.5377e-02, 4.8438e-02, 1.5426e-01, 5.3179e-02, 5.4223e-01, 1.6033e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,432][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0856, 0.0844, 0.0172, 0.2536, 0.0066, 0.0260, 0.0697, 0.0381, 0.0126,
        0.0198, 0.1545, 0.2320], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,432][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([1.6727e-03, 8.7799e-06, 1.5820e-04, 2.5731e-04, 5.0052e-04, 1.4191e-02,
        1.5086e-02, 2.6747e-02, 1.4740e-01, 3.3717e-02, 6.7838e-01, 8.1877e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,434][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0115, 0.0033, 0.0215, 0.0266, 0.0468, 0.0586, 0.0382, 0.2229, 0.1820,
        0.0385, 0.2935, 0.0567], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,436][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0253, 0.0917, 0.0789, 0.0838, 0.0639, 0.0982, 0.0939, 0.0734, 0.0996,
        0.1023, 0.1005, 0.0884], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,437][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.1790, 0.0170, 0.0392, 0.0623, 0.2303, 0.1877, 0.1070, 0.0337, 0.0503,
        0.0415, 0.0329, 0.0191], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,438][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([2.3303e-02, 1.8320e-04, 2.6025e-03, 2.8638e-03, 5.6181e-03, 3.7641e-02,
        3.3515e-02, 7.4995e-02, 2.6853e-01, 6.5046e-02, 4.3435e-01, 5.1354e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,438][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0492, 0.1055, 0.0489, 0.0418, 0.0763, 0.0744, 0.0548, 0.0750, 0.0631,
        0.1669, 0.1327, 0.1115], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,438][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([2.4016e-12, 9.5387e-23, 3.7529e-21, 5.2898e-16, 1.0335e-14, 2.3096e-09,
        3.1736e-06, 8.2178e-06, 9.4671e-03, 2.0543e-05, 9.8956e-01, 9.3733e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,439][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0102, 0.0103, 0.1647, 0.0688, 0.0298, 0.0914, 0.0475, 0.1505, 0.0690,
        0.0800, 0.2444, 0.0231, 0.0102], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,439][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0049, 0.0430, 0.0601, 0.0599, 0.0804, 0.0847, 0.1000, 0.0894, 0.0884,
        0.0935, 0.0992, 0.0929, 0.1036], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,440][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.1731, 0.0156, 0.0740, 0.0321, 0.0264, 0.0622, 0.0414, 0.1149, 0.2229,
        0.0497, 0.1108, 0.0471, 0.0299], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,440][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([3.4917e-04, 2.5551e-05, 1.0957e-04, 8.7406e-04, 4.9130e-04, 7.2655e-03,
        3.5064e-02, 4.1140e-02, 1.2353e-01, 5.1648e-02, 4.1356e-01, 2.2465e-01,
        1.0130e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,440][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0125, 0.0259, 0.0602, 0.0102, 0.0549, 0.0061, 0.0216, 0.0297, 0.0188,
        0.0442, 0.1714, 0.0461, 0.4984], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,441][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([1.9989e-03, 1.0265e-05, 8.7781e-05, 3.6821e-04, 2.2232e-04, 7.6160e-03,
        7.3723e-03, 9.3237e-03, 9.0711e-02, 1.5879e-02, 6.4729e-01, 9.3067e-02,
        1.2605e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,443][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0082, 0.0045, 0.0207, 0.0275, 0.0423, 0.0569, 0.0352, 0.2020, 0.1613,
        0.0389, 0.2645, 0.0782, 0.0598], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,445][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0344, 0.0868, 0.0807, 0.0734, 0.0653, 0.0906, 0.0981, 0.0649, 0.0785,
        0.0901, 0.0961, 0.0720, 0.0692], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,446][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0659, 0.0429, 0.0463, 0.0075, 0.2147, 0.2504, 0.0649, 0.0584, 0.0567,
        0.0158, 0.0434, 0.0580, 0.0749], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,448][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0227, 0.0004, 0.0016, 0.0055, 0.0065, 0.0393, 0.0553, 0.0683, 0.2343,
        0.0583, 0.3365, 0.0912, 0.0800], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,450][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0632, 0.1060, 0.0504, 0.0458, 0.0703, 0.0595, 0.0535, 0.0843, 0.0493,
        0.1054, 0.0946, 0.1004, 0.1173], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,451][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([1.1069e-11, 1.7361e-19, 3.7240e-20, 1.5000e-13, 3.5819e-15, 3.5892e-09,
        3.9024e-06, 2.6955e-05, 1.2197e-02, 2.8287e-05, 3.5640e-01, 5.8322e-01,
        4.8126e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,452][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0261, 0.0112, 0.0823, 0.1157, 0.0847, 0.0730, 0.0688, 0.0289, 0.0822,
        0.1616, 0.1579, 0.0203, 0.0499, 0.0374], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,454][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0053, 0.0424, 0.0564, 0.0552, 0.0736, 0.0761, 0.0893, 0.0803, 0.0783,
        0.0844, 0.0863, 0.0834, 0.0937, 0.0955], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,455][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1448, 0.0340, 0.0296, 0.0955, 0.0592, 0.0290, 0.0643, 0.0970, 0.0525,
        0.0616, 0.0555, 0.0619, 0.1443, 0.0707], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,455][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([4.5958e-04, 3.1772e-06, 1.1421e-05, 1.0639e-04, 1.8245e-04, 2.3268e-03,
        1.7136e-02, 1.6351e-02, 8.8733e-02, 2.5634e-02, 2.9722e-01, 8.6801e-02,
        1.0878e-01, 3.5625e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,455][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0335, 0.0102, 0.1431, 0.0155, 0.0254, 0.0400, 0.1195, 0.0281, 0.0357,
        0.0797, 0.3704, 0.0199, 0.0150, 0.0641], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,456][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.3901e-03, 2.6530e-06, 1.6386e-05, 5.0129e-05, 8.9694e-05, 2.1305e-03,
        5.2562e-03, 6.4410e-03, 4.5731e-02, 6.5798e-03, 5.0108e-01, 6.7342e-02,
        1.3636e-01, 2.2654e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,456][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0192, 0.0037, 0.0199, 0.0196, 0.0364, 0.0428, 0.0296, 0.1575, 0.1329,
        0.0386, 0.2119, 0.0569, 0.0586, 0.1725], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,457][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0207, 0.0885, 0.0695, 0.0777, 0.0611, 0.0783, 0.0855, 0.0644, 0.0681,
        0.0690, 0.0787, 0.0751, 0.0680, 0.0954], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,457][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.1015, 0.0281, 0.0254, 0.0060, 0.1196, 0.1325, 0.0791, 0.0676, 0.0268,
        0.0103, 0.0411, 0.0408, 0.2785, 0.0426], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,458][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([4.7682e-02, 1.3382e-04, 6.7722e-04, 1.3929e-03, 3.2945e-03, 1.5232e-02,
        2.1180e-02, 3.9740e-02, 1.0222e-01, 4.0272e-02, 1.6300e-01, 4.7838e-02,
        1.4312e-01, 3.7422e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,458][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0601, 0.0788, 0.0518, 0.0263, 0.0686, 0.0709, 0.0445, 0.0907, 0.0494,
        0.1049, 0.0946, 0.0817, 0.1060, 0.0717], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,459][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([7.3959e-13, 1.5474e-22, 3.8903e-26, 7.1948e-18, 1.5091e-17, 4.6841e-12,
        1.3046e-07, 1.3438e-07, 2.0138e-04, 1.6490e-06, 3.1498e-03, 2.0748e-02,
        1.3274e-01, 8.4315e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,460][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0098, 0.0177, 0.1094, 0.0721, 0.0622, 0.0956, 0.0578, 0.0673, 0.0836,
        0.0604, 0.1652, 0.0263, 0.0244, 0.1304, 0.0177], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,462][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0045, 0.0350, 0.0485, 0.0483, 0.0639, 0.0675, 0.0796, 0.0722, 0.0709,
        0.0748, 0.0791, 0.0735, 0.0832, 0.0887, 0.1102], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,464][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.1058, 0.0694, 0.0290, 0.0370, 0.0284, 0.0248, 0.0675, 0.0621, 0.1056,
        0.0681, 0.0657, 0.1060, 0.0774, 0.1023, 0.0507], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,465][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([1.8922e-04, 7.6117e-06, 3.5959e-05, 2.9916e-04, 2.1733e-04, 2.5453e-03,
        1.3785e-02, 1.4155e-02, 3.7913e-02, 2.2397e-02, 1.7110e-01, 6.4485e-02,
        7.2180e-02, 4.1734e-01, 1.8336e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,467][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0209, 0.0145, 0.0809, 0.0189, 0.0429, 0.0702, 0.0424, 0.0139, 0.0767,
        0.0749, 0.3008, 0.0346, 0.0217, 0.1382, 0.0484], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,468][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([9.5276e-04, 4.6790e-06, 2.8565e-05, 9.9249e-05, 8.5958e-05, 2.1351e-03,
        2.9941e-03, 4.9258e-03, 2.1369e-02, 3.0688e-03, 3.8918e-01, 4.1388e-02,
        1.1914e-01, 3.4064e-01, 7.3992e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,470][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0126, 0.0029, 0.0154, 0.0174, 0.0278, 0.0376, 0.0237, 0.1366, 0.1158,
        0.0295, 0.1815, 0.0518, 0.0482, 0.1557, 0.1436], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,471][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0211, 0.0830, 0.0720, 0.0577, 0.0537, 0.0844, 0.0775, 0.0578, 0.0718,
        0.0659, 0.0799, 0.0696, 0.0508, 0.0895, 0.0653], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,472][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0589, 0.0138, 0.0065, 0.0027, 0.2023, 0.0373, 0.0591, 0.0770, 0.0077,
        0.0094, 0.0101, 0.0254, 0.4357, 0.0425, 0.0116], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,472][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([9.3031e-03, 6.5459e-05, 4.0539e-04, 6.3768e-04, 1.5737e-03, 1.0659e-02,
        2.2295e-02, 2.0383e-02, 5.9968e-02, 2.1266e-02, 9.4718e-02, 2.0365e-02,
        1.2760e-01, 4.1556e-01, 1.9520e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,473][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0796, 0.0767, 0.0482, 0.0295, 0.0740, 0.0714, 0.0492, 0.0748, 0.0464,
        0.0897, 0.0790, 0.0756, 0.0829, 0.0591, 0.0638], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,473][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([2.5658e-14, 2.9409e-22, 9.1232e-23, 1.1142e-16, 9.8979e-17, 7.7594e-12,
        4.0853e-09, 7.1363e-08, 4.3212e-05, 1.2370e-07, 7.8348e-04, 6.7003e-04,
        1.6978e-02, 9.7612e-01, 5.4078e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,473][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0116, 0.0106, 0.0567, 0.0516, 0.0820, 0.0287, 0.0537, 0.0720, 0.0241,
        0.1451, 0.1563, 0.0238, 0.0608, 0.1375, 0.0546, 0.0309],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,474][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0040, 0.0324, 0.0447, 0.0447, 0.0601, 0.0625, 0.0720, 0.0668, 0.0655,
        0.0671, 0.0723, 0.0680, 0.0777, 0.0812, 0.1042, 0.0768],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,474][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.1172, 0.0350, 0.0241, 0.1089, 0.0329, 0.0132, 0.0719, 0.0819, 0.0724,
        0.0351, 0.0396, 0.0569, 0.0415, 0.0339, 0.0579, 0.1775],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,475][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([1.9312e-04, 1.9960e-06, 7.2823e-06, 5.0135e-05, 7.7798e-05, 1.2166e-03,
        9.2339e-03, 4.7417e-03, 2.7702e-02, 1.6123e-02, 7.0826e-02, 2.2252e-02,
        4.7163e-02, 1.3337e-01, 1.2768e-01, 5.3936e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,475][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0379, 0.0065, 0.0691, 0.0262, 0.0284, 0.0029, 0.1414, 0.0057, 0.0435,
        0.1456, 0.1561, 0.0143, 0.1465, 0.0102, 0.0026, 0.1629],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,476][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([2.2906e-03, 3.0807e-06, 1.4565e-05, 7.2687e-05, 1.0874e-04, 1.7143e-03,
        2.0309e-03, 4.2880e-03, 2.6503e-02, 2.5441e-03, 2.1409e-01, 3.5801e-02,
        1.0537e-01, 1.7726e-01, 9.7597e-02, 3.3031e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,478][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0067, 0.0027, 0.0141, 0.0168, 0.0261, 0.0358, 0.0231, 0.1337, 0.1038,
        0.0267, 0.1663, 0.0448, 0.0421, 0.1442, 0.1549, 0.0582],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,480][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0214, 0.0704, 0.0502, 0.0724, 0.0538, 0.0774, 0.0747, 0.0470, 0.0671,
        0.0705, 0.0632, 0.0673, 0.0517, 0.0735, 0.0515, 0.0879],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,481][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0732, 0.0232, 0.0235, 0.0116, 0.0936, 0.0647, 0.0385, 0.0343, 0.0210,
        0.0059, 0.0195, 0.0255, 0.3962, 0.0458, 0.0931, 0.0303],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,482][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([1.0729e-02, 5.8391e-05, 2.9224e-04, 7.3439e-04, 1.4011e-03, 5.3311e-03,
        4.8963e-03, 1.3466e-02, 3.2619e-02, 6.1103e-03, 5.7812e-02, 2.0460e-02,
        4.7036e-02, 2.1269e-01, 1.7175e-01, 4.1461e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,484][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0473, 0.0698, 0.0373, 0.0245, 0.0610, 0.0558, 0.0390, 0.0753, 0.0417,
        0.0951, 0.0940, 0.0829, 0.1157, 0.0702, 0.0576, 0.0328],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,485][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([4.4608e-13, 2.5755e-23, 4.3142e-25, 1.1650e-18, 2.3942e-17, 7.5690e-13,
        2.9738e-10, 1.4118e-08, 2.1064e-06, 2.4204e-09, 1.2302e-04, 7.5408e-05,
        2.3172e-03, 6.1805e-02, 7.4483e-02, 8.6119e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,487][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0227, 0.0090, 0.0631, 0.0902, 0.0700, 0.0558, 0.0541, 0.0258, 0.0652,
        0.1179, 0.1245, 0.0164, 0.0422, 0.0322, 0.0687, 0.1098, 0.0326],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,489][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0042, 0.0323, 0.0428, 0.0422, 0.0556, 0.0582, 0.0677, 0.0612, 0.0601,
        0.0636, 0.0660, 0.0635, 0.0709, 0.0731, 0.0918, 0.0707, 0.0760],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,489][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0999, 0.0211, 0.0200, 0.0645, 0.0398, 0.0214, 0.0515, 0.0730, 0.0403,
        0.0474, 0.0396, 0.0384, 0.1060, 0.0535, 0.0918, 0.1363, 0.0556],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,489][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.4409e-04, 5.4849e-07, 1.6413e-06, 1.6119e-05, 3.6773e-05, 4.4248e-04,
        3.0703e-03, 3.0060e-03, 1.6505e-02, 4.6909e-03, 5.4652e-02, 1.4620e-02,
        2.0913e-02, 6.7095e-02, 6.7415e-02, 5.9223e-01, 1.5516e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,490][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0333, 0.0092, 0.1345, 0.0140, 0.0233, 0.0352, 0.1001, 0.0263, 0.0340,
        0.0714, 0.3406, 0.0181, 0.0139, 0.0676, 0.0011, 0.0037, 0.0737],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,490][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([8.8684e-04, 6.7575e-07, 3.0558e-06, 1.2417e-05, 2.4172e-05, 5.4925e-04,
        1.2428e-03, 1.5780e-03, 1.1336e-02, 1.4374e-03, 1.4624e-01, 1.7108e-02,
        3.5453e-02, 6.0795e-02, 5.6519e-02, 5.0821e-01, 1.5860e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,491][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0160, 0.0026, 0.0141, 0.0136, 0.0245, 0.0290, 0.0208, 0.1032, 0.0865,
        0.0278, 0.1386, 0.0384, 0.0395, 0.1150, 0.1192, 0.0598, 0.1511],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,491][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0177, 0.0715, 0.0550, 0.0628, 0.0491, 0.0633, 0.0681, 0.0511, 0.0555,
        0.0553, 0.0626, 0.0598, 0.0545, 0.0756, 0.0513, 0.0711, 0.0758],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,492][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0890, 0.0244, 0.0210, 0.0055, 0.1087, 0.1005, 0.0742, 0.0606, 0.0220,
        0.0101, 0.0370, 0.0362, 0.2577, 0.0398, 0.0312, 0.0428, 0.0391],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,492][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.4753e-02, 3.0417e-05, 1.5369e-04, 2.9781e-04, 8.1164e-04, 3.2316e-03,
        5.3097e-03, 8.4154e-03, 2.1995e-02, 7.9788e-03, 3.4040e-02, 1.0079e-02,
        3.3156e-02, 9.4894e-02, 1.5058e-01, 4.3796e-01, 1.7631e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,494][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0543, 0.0620, 0.0458, 0.0235, 0.0572, 0.0622, 0.0364, 0.0792, 0.0437,
        0.0877, 0.0814, 0.0665, 0.0906, 0.0634, 0.0493, 0.0282, 0.0687],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,495][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([2.7463e-15, 3.4415e-26, 7.0206e-30, 1.3965e-21, 1.2591e-20, 4.1101e-15,
        4.0805e-11, 7.8587e-11, 1.1887e-07, 5.2542e-10, 1.3210e-06, 4.6939e-06,
        5.7436e-05, 6.4692e-04, 2.1059e-03, 9.3544e-01, 6.1739e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,496][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:38,499][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[13270],
        [ 2093],
        [ 2627],
        [  617],
        [ 1002],
        [ 1363],
        [ 1314],
        [ 2132],
        [ 1079],
        [  164],
        [  374],
        [  742],
        [  800],
        [  835],
        [  463],
        [  365],
        [  671]], device='cuda:0')
[2024-07-24 10:17:38,500][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[13200],
        [ 3998],
        [ 7014],
        [ 1916],
        [ 6958],
        [ 3180],
        [16827],
        [16800],
        [ 7302],
        [ 1506],
        [ 3245],
        [ 3414],
        [ 6155],
        [10532],
        [ 1071],
        [ 3208],
        [ 9189]], device='cuda:0')
[2024-07-24 10:17:38,502][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[18959],
        [28278],
        [ 6286],
        [ 5823],
        [ 4861],
        [ 7911],
        [17733],
        [20251],
        [ 9156],
        [18341],
        [13279],
        [10587],
        [ 9704],
        [12575],
        [11609],
        [ 9022],
        [17516]], device='cuda:0')
[2024-07-24 10:17:38,504][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[9929],
        [7107],
        [4356],
        [2315],
        [2022],
        [1461],
        [1853],
        [1447],
        [1992],
        [1693],
        [1768],
        [2688],
        [1616],
        [2144],
        [1523],
        [1969],
        [2000]], device='cuda:0')
[2024-07-24 10:17:38,505][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[10933],
        [22233],
        [20573],
        [27908],
        [26694],
        [27870],
        [27847],
        [29027],
        [24944],
        [23580],
        [23053],
        [24577],
        [20726],
        [24061],
        [24021],
        [22263],
        [22210]], device='cuda:0')
[2024-07-24 10:17:38,506][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[  603],
        [  617],
        [  666],
        [  693],
        [ 1407],
        [ 2681],
        [ 1470],
        [ 1101],
        [10175],
        [ 2605],
        [ 3168],
        [ 4275],
        [ 6423],
        [ 1808],
        [ 9699],
        [11234],
        [ 3325]], device='cuda:0')
[2024-07-24 10:17:38,507][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[6993],
        [5782],
        [4891],
        [4600],
        [4511],
        [4214],
        [4273],
        [4233],
        [4128],
        [4134],
        [4068],
        [3996],
        [4020],
        [3985],
        [3983],
        [3985],
        [3939]], device='cuda:0')
[2024-07-24 10:17:38,508][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 7412],
        [ 8139],
        [ 6584],
        [11903],
        [ 4244],
        [ 2955],
        [  947],
        [  486],
        [ 4492],
        [ 5419],
        [ 2092],
        [ 1837],
        [ 2151],
        [ 1969],
        [ 2015],
        [ 2110],
        [ 1825]], device='cuda:0')
[2024-07-24 10:17:38,509][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[48227],
        [48940],
        [49061],
        [49107],
        [49262],
        [49341],
        [49333],
        [49377],
        [49369],
        [49385],
        [49390],
        [49429],
        [49446],
        [49443],
        [49478],
        [49490],
        [49498]], device='cuda:0')
[2024-07-24 10:17:38,511][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[5238],
        [2990],
        [2019],
        [2371],
        [2061],
        [1918],
        [1945],
        [1917],
        [1906],
        [1890],
        [1844],
        [1842],
        [1802],
        [1760],
        [1721],
        [1716],
        [1694]], device='cuda:0')
[2024-07-24 10:17:38,513][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[12468],
        [12567],
        [13438],
        [12741],
        [13598],
        [12953],
        [13155],
        [14092],
        [13644],
        [15402],
        [15337],
        [15663],
        [14956],
        [14938],
        [14806],
        [14439],
        [14657]], device='cuda:0')
[2024-07-24 10:17:38,514][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[23834],
        [17566],
        [18431],
        [17718],
        [18082],
        [18901],
        [19610],
        [19263],
        [18853],
        [19642],
        [19738],
        [19489],
        [19424],
        [19590],
        [20002],
        [20137],
        [20154]], device='cuda:0')
[2024-07-24 10:17:38,516][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[24675],
        [22720],
        [21489],
        [20427],
        [19234],
        [17976],
        [18163],
        [17518],
        [16954],
        [16676],
        [16640],
        [16454],
        [15810],
        [15381],
        [14805],
        [14611],
        [14318]], device='cuda:0')
[2024-07-24 10:17:38,518][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[14642],
        [14642],
        [14652],
        [14640],
        [14642],
        [14642],
        [14643],
        [14642],
        [14642],
        [14641],
        [14643],
        [14640],
        [14642],
        [14646],
        [14640],
        [14641],
        [14647]], device='cuda:0')
[2024-07-24 10:17:38,520][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 8439],
        [ 1682],
        [ 6687],
        [ 1476],
        [ 1940],
        [ 9366],
        [13370],
        [ 6536],
        [11447],
        [11379],
        [10180],
        [ 3269],
        [11635],
        [11950],
        [20757],
        [ 5304],
        [ 9600]], device='cuda:0')
[2024-07-24 10:17:38,521][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[35012],
        [32049],
        [25399],
        [25516],
        [26064],
        [26904],
        [28443],
        [29821],
        [30758],
        [32577],
        [30246],
        [33341],
        [34631],
        [30336],
        [33632],
        [31052],
        [29052]], device='cuda:0')
[2024-07-24 10:17:38,523][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[18256],
        [30724],
        [32035],
        [30347],
        [30728],
        [30814],
        [29204],
        [29042],
        [28713],
        [28309],
        [28236],
        [27603],
        [27362],
        [27383],
        [26716],
        [26179],
        [26347]], device='cuda:0')
[2024-07-24 10:17:38,524][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[3464],
        [ 517],
        [ 500],
        [2988],
        [ 995],
        [1230],
        [1606],
        [ 344],
        [ 970],
        [2351],
        [ 571],
        [ 509],
        [1207],
        [ 798],
        [ 902],
        [2571],
        [1932]], device='cuda:0')
[2024-07-24 10:17:38,525][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[23350],
        [15108],
        [15284],
        [13161],
        [15453],
        [10176],
        [ 3408],
        [ 2937],
        [ 5947],
        [ 7210],
        [ 7876],
        [10098],
        [11648],
        [11401],
        [11133],
        [11752],
        [11667]], device='cuda:0')
[2024-07-24 10:17:38,526][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[11054],
        [  309],
        [26748],
        [ 6937],
        [ 9862],
        [17844],
        [  343],
        [ 2320],
        [  547],
        [ 5325],
        [ 3842],
        [  708],
        [ 1397],
        [10403],
        [14132],
        [ 3358],
        [11440]], device='cuda:0')
[2024-07-24 10:17:38,527][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[33646],
        [11295],
        [16960],
        [11662],
        [ 7253],
        [28444],
        [18991],
        [11918],
        [16078],
        [17394],
        [28694],
        [27484],
        [26136],
        [22941],
        [18895],
        [16010],
        [16580]], device='cuda:0')
[2024-07-24 10:17:38,529][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[12493],
        [11050],
        [13262],
        [14104],
        [14264],
        [15365],
        [15774],
        [15161],
        [16173],
        [16426],
        [16550],
        [16713],
        [16770],
        [16642],
        [16595],
        [16745],
        [16806]], device='cuda:0')
[2024-07-24 10:17:38,530][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[14329],
        [ 5088],
        [ 5974],
        [ 5208],
        [ 5638],
        [ 5365],
        [ 5558],
        [ 5584],
        [ 5706],
        [ 6550],
        [ 6330],
        [ 6372],
        [ 6585],
        [ 6404],
        [ 6075],
        [ 5976],
        [ 5976]], device='cuda:0')
[2024-07-24 10:17:38,532][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 2794],
        [ 3494],
        [ 4894],
        [15057],
        [14016],
        [12430],
        [11714],
        [ 9461],
        [12213],
        [11467],
        [ 8478],
        [10091],
        [10538],
        [11906],
        [13933],
        [12593],
        [11367]], device='cuda:0')
[2024-07-24 10:17:38,534][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 4734],
        [ 5821],
        [ 9708],
        [18978],
        [12251],
        [10252],
        [ 8725],
        [ 7690],
        [11706],
        [11458],
        [ 9961],
        [ 9541],
        [ 9399],
        [ 8722],
        [ 8452],
        [12712],
        [12643]], device='cuda:0')
[2024-07-24 10:17:38,535][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[2835],
        [4251],
        [2020],
        [1650],
        [3712],
        [2463],
        [1980],
        [1478],
        [1676],
        [2154],
        [1772],
        [1973],
        [2458],
        [2084],
        [2107],
        [2248],
        [1865]], device='cuda:0')
[2024-07-24 10:17:38,537][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[42530],
        [34033],
        [24504],
        [16439],
        [ 9548],
        [33431],
        [30992],
        [17377],
        [35862],
        [35988],
        [26732],
        [15639],
        [14693],
        [22295],
        [21306],
        [19039],
        [17874]], device='cuda:0')
[2024-07-24 10:17:38,539][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[38109],
        [47051],
        [40696],
        [40840],
        [41268],
        [35093],
        [44408],
        [46323],
        [42941],
        [39390],
        [41664],
        [43605],
        [41380],
        [39596],
        [39345],
        [39705],
        [39207]], device='cuda:0')
[2024-07-24 10:17:38,540][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[46085],
        [49648],
        [49581],
        [47291],
        [49502],
        [48353],
        [46795],
        [47025],
        [47237],
        [38670],
        [45306],
        [48497],
        [45794],
        [44903],
        [46494],
        [47541],
        [46071]], device='cuda:0')
[2024-07-24 10:17:38,542][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965],
        [4965]], device='cuda:0')
[2024-07-24 10:17:38,582][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:38,583][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,583][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,583][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,584][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,584][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,584][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,585][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,587][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,588][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,589][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,590][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,591][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,592][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0683, 0.9317], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,593][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.8134, 0.1866], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,595][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.1868, 0.8132], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,597][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.9370, 0.0630], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,598][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0033, 0.9967], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,599][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.1236, 0.8764], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,599][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.2563, 0.7437], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,599][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0573, 0.9427], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,600][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.1010, 0.8990], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,600][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.1392, 0.8608], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,600][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.7744, 0.2256], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,601][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.6855, 0.3145], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,601][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0381, 0.6651, 0.2968], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,601][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.9796, 0.0163, 0.0042], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,601][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.2944, 0.1437, 0.5619], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,602][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.9114, 0.0713, 0.0172], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,603][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0011, 0.4780, 0.5209], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,604][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0639, 0.4317, 0.5044], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,606][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0383, 0.3669, 0.5948], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,607][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0280, 0.2301, 0.7419], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,609][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0504, 0.3943, 0.5553], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,609][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0727, 0.4483, 0.4790], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,611][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.9485, 0.0479, 0.0035], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,613][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1418, 0.0249, 0.8333], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,614][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.0177, 0.2893, 0.1778, 0.5152], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,616][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.7441, 0.1697, 0.0223, 0.0639], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,616][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.1154, 0.0161, 0.2911, 0.5774], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,616][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.6781, 0.1213, 0.1356, 0.0650], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,617][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0008, 0.2958, 0.3302, 0.3732], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,617][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0232, 0.2193, 0.3075, 0.4501], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,617][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.0432, 0.0554, 0.6969, 0.2045], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,618][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0170, 0.0082, 0.0381, 0.9367], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,618][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.0291, 0.2203, 0.3381, 0.4124], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,618][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.0522, 0.3093, 0.3215, 0.3170], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,619][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.6155, 0.0388, 0.0043, 0.3415], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,619][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.2485, 0.2012, 0.1495, 0.4008], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,619][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0145, 0.2019, 0.1625, 0.4822, 0.1389], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,620][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ got] are: tensor([9.7035e-01, 8.4911e-03, 6.3827e-04, 1.2123e-02, 8.3942e-03],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,622][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0136, 0.0137, 0.0641, 0.6802, 0.2284], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,623][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.6132, 0.0653, 0.1108, 0.1669, 0.0438], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,625][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0006, 0.2166, 0.2360, 0.2726, 0.2741], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,626][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0154, 0.1231, 0.1748, 0.3660, 0.3206], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,627][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0177, 0.0599, 0.3283, 0.3972, 0.1969], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,629][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0087, 0.0048, 0.0326, 0.3965, 0.5574], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,631][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0221, 0.1423, 0.2083, 0.2843, 0.3430], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,632][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0372, 0.2168, 0.2294, 0.2507, 0.2659], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,633][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.8050, 0.0722, 0.0120, 0.0922, 0.0187], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,633][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.1491, 0.0414, 0.1006, 0.0369, 0.6721], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,634][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0134, 0.2026, 0.1341, 0.3715, 0.1403, 0.1382], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,634][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.9256, 0.0258, 0.0010, 0.0260, 0.0180, 0.0038], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,635][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0467, 0.0031, 0.0067, 0.1697, 0.0722, 0.7016], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,635][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.8952, 0.0139, 0.0079, 0.0333, 0.0161, 0.0336], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,635][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0003, 0.1726, 0.1863, 0.2162, 0.2216, 0.2029], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,636][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0178, 0.1032, 0.1415, 0.3000, 0.3025, 0.1350], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,636][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0340, 0.0152, 0.0410, 0.0723, 0.1353, 0.7021], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,636][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0022, 0.0012, 0.0057, 0.1828, 0.2211, 0.5870], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,637][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0186, 0.1173, 0.1784, 0.2269, 0.2802, 0.1786], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,637][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0299, 0.1786, 0.1880, 0.1996, 0.2129, 0.1909], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,639][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.8578, 0.0984, 0.0034, 0.0244, 0.0081, 0.0078], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,641][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0845, 0.0426, 0.0645, 0.1364, 0.0774, 0.5945], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,642][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.0077, 0.1626, 0.1063, 0.3549, 0.1337, 0.1234, 0.1112],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,643][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([9.1741e-01, 5.3479e-02, 2.6664e-04, 1.4988e-02, 4.5825e-03, 1.3159e-03,
        7.9569e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,644][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.0772, 0.0021, 0.0026, 0.0513, 0.0330, 0.4919, 0.3419],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,646][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.1677, 0.0095, 0.0146, 0.0190, 0.0171, 0.7606, 0.0115],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,648][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0004, 0.1394, 0.1541, 0.1711, 0.1775, 0.1679, 0.1896],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,649][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.0055, 0.0618, 0.0832, 0.2036, 0.2663, 0.1090, 0.2707],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,651][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.0185, 0.0040, 0.0200, 0.0202, 0.0605, 0.5100, 0.3667],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,651][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([2.1817e-02, 5.8843e-04, 1.5505e-03, 2.2143e-02, 1.0447e-01, 1.1539e-01,
        7.3404e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,651][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.0179, 0.0906, 0.1355, 0.1753, 0.2297, 0.1528, 0.1983],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,652][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.0253, 0.1473, 0.1563, 0.1595, 0.1796, 0.1630, 0.1689],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,652][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.3720, 0.0121, 0.0011, 0.0220, 0.0053, 0.0082, 0.5793],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,653][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.1257, 0.0780, 0.0621, 0.1348, 0.0501, 0.1199, 0.4294],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,653][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0080, 0.1401, 0.0817, 0.3456, 0.1145, 0.1280, 0.1342, 0.0480],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,653][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.8195, 0.0420, 0.0014, 0.0475, 0.0104, 0.0041, 0.0677, 0.0074],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,654][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ at] are: tensor([1.7576e-02, 4.5275e-04, 7.9050e-04, 3.0348e-02, 7.2063e-03, 1.4905e-01,
        2.6592e-01, 5.2865e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,654][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.6163, 0.0388, 0.0207, 0.0512, 0.0395, 0.0942, 0.0671, 0.0722],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,654][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0003, 0.1203, 0.1278, 0.1486, 0.1527, 0.1408, 0.1709, 0.1385],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,656][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0105, 0.0521, 0.0794, 0.2049, 0.2100, 0.1113, 0.2768, 0.0551],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,657][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0278, 0.0032, 0.0050, 0.0139, 0.0295, 0.1553, 0.4228, 0.3425],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,658][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ at] are: tensor([1.0476e-03, 2.5967e-04, 5.3499e-04, 4.9317e-02, 8.9581e-02, 8.1687e-02,
        6.3354e-01, 1.4403e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,660][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0150, 0.0763, 0.1162, 0.1502, 0.1814, 0.1206, 0.1777, 0.1626],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,661][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0195, 0.1285, 0.1319, 0.1431, 0.1539, 0.1376, 0.1587, 0.1268],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,663][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.8155, 0.0663, 0.0033, 0.0199, 0.0061, 0.0038, 0.0789, 0.0062],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,665][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0732, 0.0184, 0.0743, 0.0179, 0.0451, 0.1169, 0.0323, 0.6217],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,666][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0081, 0.1461, 0.0915, 0.2915, 0.1031, 0.1159, 0.1303, 0.0628, 0.0508],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,667][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.0332e-01, 3.7575e-02, 3.1848e-04, 1.8527e-02, 4.4750e-03, 8.6621e-04,
        2.8454e-02, 3.9058e-03, 2.5591e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,668][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.1967e-02, 1.4298e-04, 1.4965e-04, 6.4359e-03, 1.9905e-03, 3.1019e-02,
        6.0384e-02, 3.6174e-01, 5.1617e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,669][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.7520, 0.0166, 0.0077, 0.0283, 0.0064, 0.0359, 0.0265, 0.1124, 0.0142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,669][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0002, 0.1027, 0.1111, 0.1302, 0.1331, 0.1223, 0.1501, 0.1208, 0.1294],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,669][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0122, 0.0623, 0.0875, 0.1651, 0.1840, 0.0980, 0.2396, 0.0849, 0.0665],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,670][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0205, 0.0009, 0.0015, 0.0027, 0.0087, 0.0439, 0.0936, 0.3064, 0.5218],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,670][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([7.3865e-04, 7.0325e-05, 2.2039e-04, 1.0228e-02, 1.9455e-02, 2.2630e-02,
        4.6119e-01, 1.5410e-01, 3.3136e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,671][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0123, 0.0678, 0.1018, 0.1294, 0.1559, 0.1030, 0.1548, 0.1465, 0.1284],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,671][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0171, 0.1126, 0.1177, 0.1298, 0.1353, 0.1205, 0.1404, 0.1111, 0.1155],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,671][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.8123, 0.0331, 0.0026, 0.0136, 0.0025, 0.0031, 0.1283, 0.0018, 0.0028],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,672][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0405, 0.0114, 0.0250, 0.0283, 0.0394, 0.1196, 0.0314, 0.0458, 0.6586],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,672][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0071, 0.1353, 0.0785, 0.2995, 0.1095, 0.1155, 0.0953, 0.0534, 0.0471,
        0.0587], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,674][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ station] are: tensor([9.1688e-01, 3.5703e-02, 5.7398e-04, 1.6846e-02, 1.1453e-02, 1.0298e-03,
        1.0623e-02, 2.2741e-03, 1.6477e-03, 2.9744e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,675][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ station] are: tensor([7.8397e-03, 2.1987e-04, 1.6507e-04, 5.5624e-03, 4.0847e-03, 4.7748e-02,
        4.9539e-02, 1.7484e-01, 6.2327e-01, 8.6736e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,676][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.1727, 0.0137, 0.0242, 0.0144, 0.0162, 0.3988, 0.0226, 0.1298, 0.2013,
        0.0064], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,678][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0003, 0.0893, 0.1012, 0.1133, 0.1189, 0.1116, 0.1248, 0.1074, 0.1173,
        0.1160], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,679][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0058, 0.0506, 0.0786, 0.1281, 0.1418, 0.1019, 0.1825, 0.0712, 0.0664,
        0.1730], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,681][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0126, 0.0010, 0.0020, 0.0032, 0.0092, 0.0624, 0.0777, 0.1215, 0.6054,
        0.1049], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,682][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ station] are: tensor([3.3806e-03, 9.2184e-05, 3.4736e-04, 6.2991e-03, 1.6942e-02, 3.1380e-02,
        3.0763e-01, 1.5922e-01, 2.6423e-01, 2.1048e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,684][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0098, 0.0545, 0.0810, 0.1084, 0.1412, 0.0900, 0.1278, 0.1311, 0.1183,
        0.1379], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,685][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0175, 0.1020, 0.1047, 0.1130, 0.1187, 0.1065, 0.1232, 0.0992, 0.1037,
        0.1115], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,686][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.1101, 0.0217, 0.0013, 0.0255, 0.0039, 0.0035, 0.0110, 0.0047, 0.0045,
        0.8138], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,686][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0837, 0.0240, 0.0536, 0.0184, 0.3550, 0.0452, 0.0503, 0.1154, 0.1085,
        0.1459], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,686][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0069, 0.1416, 0.0594, 0.2798, 0.0950, 0.0982, 0.1144, 0.0476, 0.0431,
        0.0585, 0.0555], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,687][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [,] are: tensor([9.4453e-01, 1.7266e-02, 7.2680e-05, 2.5727e-02, 8.7019e-04, 3.3221e-04,
        8.6361e-03, 6.0775e-04, 6.5437e-04, 1.0810e-03, 2.2647e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,687][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [,] are: tensor([3.1993e-02, 7.1618e-05, 5.5021e-05, 3.9534e-03, 1.1215e-03, 1.7891e-02,
        3.4027e-02, 1.5359e-01, 4.1296e-01, 1.0319e-01, 2.4114e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,688][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.6745, 0.0060, 0.0016, 0.0108, 0.0098, 0.0784, 0.0115, 0.0380, 0.0950,
        0.0168, 0.0577], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,688][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0002, 0.0811, 0.0869, 0.1008, 0.1032, 0.0974, 0.1161, 0.0955, 0.1030,
        0.1077, 0.1081], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,688][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0095, 0.0478, 0.0626, 0.1401, 0.1436, 0.0775, 0.1817, 0.0573, 0.0557,
        0.1765, 0.0477], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,689][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [,] are: tensor([1.9163e-02, 4.7320e-04, 2.5374e-04, 1.1288e-03, 3.8317e-03, 2.6900e-02,
        4.5548e-02, 8.8730e-02, 3.0618e-01, 1.4682e-01, 3.6097e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,689][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [,] are: tensor([4.2466e-04, 1.8753e-05, 6.6603e-05, 4.1251e-03, 2.7820e-03, 7.1182e-03,
        1.4606e-01, 4.1272e-02, 1.0536e-01, 7.5267e-02, 6.1751e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,690][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0101, 0.0510, 0.0767, 0.0987, 0.1193, 0.0791, 0.1136, 0.1115, 0.1010,
        0.1278, 0.1113], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,692][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0144, 0.0892, 0.0935, 0.1015, 0.1066, 0.0963, 0.1098, 0.0883, 0.0931,
        0.1051, 0.1023], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,694][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.5794, 0.0221, 0.0017, 0.0174, 0.0018, 0.0021, 0.0360, 0.0026, 0.0031,
        0.3321, 0.0017], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,695][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0809, 0.0219, 0.2286, 0.0355, 0.0842, 0.0749, 0.0438, 0.0500, 0.0890,
        0.0600, 0.2312], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,696][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0075, 0.1078, 0.0738, 0.2438, 0.0973, 0.0968, 0.0803, 0.0532, 0.0455,
        0.0524, 0.0647, 0.0768], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,698][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.7008, 0.1021, 0.0279, 0.0540, 0.0463, 0.0033, 0.0237, 0.0065, 0.0035,
        0.0267, 0.0034, 0.0018], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,699][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([2.2075e-02, 1.2018e-04, 2.0883e-04, 3.8390e-03, 2.3104e-03, 1.5170e-02,
        2.5211e-02, 6.3800e-02, 1.3855e-01, 4.1349e-02, 1.9911e-01, 4.8825e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,701][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.2676, 0.0123, 0.0178, 0.0194, 0.0103, 0.0607, 0.0156, 0.1923, 0.0602,
        0.0167, 0.2817, 0.0454], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,702][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0002, 0.0708, 0.0801, 0.0911, 0.0953, 0.0884, 0.1051, 0.0860, 0.0925,
        0.0976, 0.1015, 0.0914], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,703][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0049, 0.0247, 0.0464, 0.1308, 0.1489, 0.0665, 0.1631, 0.0496, 0.0380,
        0.2039, 0.0455, 0.0777], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,703][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([2.8903e-02, 1.9690e-04, 2.5041e-04, 1.6143e-03, 2.8356e-03, 2.2127e-02,
        4.4669e-02, 3.9041e-02, 1.6497e-01, 7.0367e-02, 5.6988e-01, 5.5142e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,704][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([1.0035e-02, 1.4770e-04, 2.6723e-04, 1.8391e-02, 3.7308e-03, 8.9430e-03,
        7.4174e-02, 3.2152e-02, 3.9822e-02, 1.4523e-01, 2.1556e-01, 4.5156e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,704][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0060, 0.0393, 0.0671, 0.0841, 0.1137, 0.0669, 0.1111, 0.0994, 0.0907,
        0.1212, 0.1093, 0.0910], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,705][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0142, 0.0744, 0.0850, 0.0897, 0.0965, 0.0899, 0.0983, 0.0849, 0.0880,
        0.0994, 0.0959, 0.0838], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,705][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.2867, 0.2098, 0.0033, 0.0266, 0.0057, 0.0041, 0.0308, 0.0036, 0.0020,
        0.3541, 0.0041, 0.0692], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,705][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.1354, 0.0542, 0.0474, 0.0902, 0.0275, 0.1005, 0.0242, 0.1030, 0.1974,
        0.1002, 0.0731, 0.0469], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,706][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0084, 0.0941, 0.0644, 0.1918, 0.0731, 0.0820, 0.0842, 0.0444, 0.0412,
        0.0486, 0.0574, 0.0740, 0.1364], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,706][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.7615, 0.0343, 0.0067, 0.0419, 0.0260, 0.0034, 0.0858, 0.0059, 0.0057,
        0.0164, 0.0029, 0.0028, 0.0066], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,707][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([4.6372e-03, 4.6979e-05, 5.7875e-05, 1.3886e-03, 6.7663e-04, 7.2171e-03,
        3.1905e-02, 2.5968e-02, 9.0461e-02, 2.6149e-02, 2.7524e-01, 3.0002e-01,
        2.3624e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,709][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.4387, 0.0096, 0.0156, 0.0132, 0.0048, 0.0750, 0.0168, 0.1807, 0.0610,
        0.0190, 0.1146, 0.0357, 0.0153], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,711][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0002, 0.0658, 0.0715, 0.0838, 0.0843, 0.0804, 0.0944, 0.0774, 0.0829,
        0.0863, 0.0904, 0.0837, 0.0989], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,712][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0068, 0.0361, 0.0543, 0.1109, 0.1013, 0.0704, 0.1832, 0.0515, 0.0443,
        0.1425, 0.0422, 0.0821, 0.0744], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,713][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([9.2663e-03, 4.8626e-04, 3.9820e-04, 2.0081e-03, 2.7160e-03, 1.6157e-02,
        4.5333e-02, 3.6341e-02, 1.6737e-01, 5.8893e-02, 4.4861e-01, 1.1670e-01,
        9.5713e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,714][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([2.9725e-03, 3.0052e-05, 1.8322e-05, 2.1707e-03, 6.8547e-04, 2.1032e-03,
        5.7503e-02, 6.9475e-03, 1.3149e-02, 3.9760e-02, 8.0724e-02, 1.3169e-01,
        6.6225e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,715][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0066, 0.0391, 0.0584, 0.0795, 0.0966, 0.0620, 0.0978, 0.0888, 0.0802,
        0.1048, 0.0954, 0.0918, 0.0990], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,717][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0146, 0.0732, 0.0758, 0.0832, 0.0889, 0.0783, 0.0909, 0.0732, 0.0749,
        0.0884, 0.0844, 0.0831, 0.0911], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,719][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.4932, 0.0149, 0.0028, 0.1310, 0.0144, 0.0061, 0.0274, 0.0025, 0.0062,
        0.0553, 0.0021, 0.0108, 0.2334], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,720][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0268, 0.0091, 0.0651, 0.0136, 0.1986, 0.0938, 0.0255, 0.0957, 0.0994,
        0.0094, 0.0718, 0.0121, 0.2790], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,721][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0061, 0.0935, 0.0501, 0.2071, 0.0660, 0.0833, 0.0863, 0.0306, 0.0345,
        0.0461, 0.0451, 0.0679, 0.1319, 0.0515], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,721][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([8.9042e-01, 2.5274e-02, 5.6475e-04, 3.2042e-02, 1.3496e-02, 1.3306e-03,
        1.9649e-02, 1.3550e-03, 3.7712e-03, 4.3581e-03, 1.6009e-03, 1.5179e-03,
        2.5846e-03, 2.0366e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,722][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([9.3613e-03, 9.4037e-06, 4.7538e-06, 5.0076e-04, 1.6735e-04, 1.8087e-03,
        5.5911e-03, 1.7124e-02, 4.0394e-02, 1.6176e-02, 4.5881e-02, 3.3428e-01,
        4.0771e-01, 1.2100e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,722][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.4752, 0.0081, 0.0050, 0.0063, 0.0038, 0.0509, 0.0134, 0.0796, 0.0126,
        0.0200, 0.2147, 0.0469, 0.0203, 0.0433], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,722][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0002, 0.0604, 0.0643, 0.0749, 0.0768, 0.0711, 0.0859, 0.0705, 0.0747,
        0.0787, 0.0805, 0.0754, 0.0901, 0.0965], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,723][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0059, 0.0384, 0.0478, 0.1129, 0.1014, 0.0614, 0.1512, 0.0413, 0.0460,
        0.1353, 0.0393, 0.0881, 0.0937, 0.0372], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,723][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([1.5030e-02, 1.9671e-04, 9.2869e-05, 4.9183e-04, 1.6778e-03, 7.1280e-03,
        2.3573e-02, 3.1278e-02, 1.0141e-01, 4.4407e-02, 2.2678e-01, 7.5592e-02,
        2.1043e-01, 2.6192e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,724][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([8.4358e-05, 1.7572e-06, 1.4351e-06, 2.2999e-04, 2.2621e-04, 1.9083e-04,
        1.0798e-02, 1.3949e-03, 2.8903e-03, 9.4803e-03, 3.1704e-02, 2.5946e-02,
        7.2212e-01, 1.9493e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,725][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0064, 0.0387, 0.0570, 0.0758, 0.0879, 0.0570, 0.0892, 0.0786, 0.0740,
        0.0961, 0.0873, 0.0848, 0.0918, 0.0755], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,726][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0110, 0.0680, 0.0705, 0.0775, 0.0813, 0.0725, 0.0846, 0.0682, 0.0698,
        0.0800, 0.0776, 0.0775, 0.0861, 0.0752], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,728][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([5.0480e-01, 3.9684e-02, 1.5545e-03, 1.4015e-02, 4.4123e-03, 2.4890e-03,
        5.0169e-02, 2.4683e-03, 2.2554e-03, 3.5412e-01, 1.4052e-03, 9.4223e-03,
        1.2921e-02, 2.8154e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,729][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0381, 0.0052, 0.0616, 0.0042, 0.0594, 0.0346, 0.0073, 0.0364, 0.0308,
        0.0153, 0.0525, 0.0034, 0.0054, 0.6459], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,731][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0051, 0.0986, 0.0575, 0.2265, 0.0607, 0.0776, 0.0664, 0.0377, 0.0328,
        0.0359, 0.0465, 0.0618, 0.1067, 0.0513, 0.0349], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,732][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ give] are: tensor([9.0549e-01, 2.6521e-02, 4.4010e-04, 1.9506e-02, 4.4593e-03, 1.5580e-03,
        1.2097e-02, 1.1303e-03, 1.3500e-03, 5.6037e-03, 1.1657e-03, 4.4595e-03,
        8.0289e-03, 4.5012e-03, 3.6927e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,733][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ give] are: tensor([5.0782e-03, 2.4013e-05, 1.6373e-05, 7.3368e-04, 2.6124e-04, 3.5032e-03,
        1.2239e-02, 1.7754e-02, 4.4535e-02, 2.1989e-02, 6.2182e-02, 2.1772e-01,
        2.6181e-01, 2.0059e-01, 1.5157e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,735][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.2562, 0.0185, 0.0205, 0.0097, 0.0041, 0.0430, 0.0271, 0.0742, 0.0605,
        0.0268, 0.2339, 0.0505, 0.0285, 0.1335, 0.0131], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,736][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0002, 0.0544, 0.0590, 0.0679, 0.0691, 0.0647, 0.0773, 0.0640, 0.0679,
        0.0710, 0.0736, 0.0679, 0.0806, 0.0880, 0.0945], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,737][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0039, 0.0335, 0.0425, 0.0922, 0.1039, 0.0567, 0.1366, 0.0518, 0.0437,
        0.1297, 0.0365, 0.0748, 0.0913, 0.0402, 0.0628], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,738][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ give] are: tensor([4.1570e-03, 3.3535e-04, 4.7913e-04, 1.2838e-03, 2.0367e-03, 6.4725e-03,
        1.5648e-02, 2.2818e-02, 5.4624e-02, 3.9089e-02, 1.5730e-01, 4.5279e-02,
        1.7176e-01, 3.8718e-01, 9.1531e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,738][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ give] are: tensor([9.0490e-04, 1.2957e-05, 4.9541e-06, 8.1534e-04, 2.1749e-04, 7.0505e-04,
        1.7252e-02, 2.9374e-03, 5.8253e-03, 9.8260e-03, 3.4186e-02, 4.3826e-02,
        3.0109e-01, 4.7421e-01, 1.0818e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,739][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0067, 0.0358, 0.0485, 0.0682, 0.0773, 0.0522, 0.0803, 0.0738, 0.0661,
        0.0848, 0.0781, 0.0776, 0.0857, 0.0738, 0.0911], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,739][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0114, 0.0632, 0.0643, 0.0676, 0.0752, 0.0672, 0.0757, 0.0634, 0.0646,
        0.0740, 0.0714, 0.0716, 0.0826, 0.0704, 0.0774], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,740][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.2961, 0.0222, 0.0033, 0.0316, 0.0012, 0.0053, 0.3672, 0.0022, 0.0063,
        0.2197, 0.0050, 0.0084, 0.0258, 0.0007, 0.0051], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,740][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0411, 0.0098, 0.0991, 0.0241, 0.0820, 0.1077, 0.0344, 0.0459, 0.0989,
        0.0127, 0.0684, 0.0097, 0.0186, 0.1218, 0.2259], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,741][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0062, 0.0900, 0.0535, 0.1658, 0.0625, 0.0708, 0.0708, 0.0399, 0.0316,
        0.0442, 0.0537, 0.0640, 0.1084, 0.0583, 0.0447, 0.0357],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,741][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.7109, 0.0892, 0.0020, 0.0277, 0.0218, 0.0056, 0.0341, 0.0057, 0.0041,
        0.0189, 0.0069, 0.0226, 0.0083, 0.0085, 0.0290, 0.0048],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,742][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ it] are: tensor([3.4444e-03, 8.5948e-06, 3.1617e-06, 5.9753e-04, 6.6961e-05, 5.0165e-04,
        2.4262e-03, 4.2853e-03, 1.6323e-02, 6.4415e-03, 2.5151e-02, 1.6900e-01,
        1.6995e-01, 9.6567e-02, 2.4461e-01, 2.6063e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,744][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.5036, 0.0076, 0.0182, 0.0145, 0.0058, 0.0190, 0.0125, 0.1137, 0.0119,
        0.0146, 0.1111, 0.0280, 0.0320, 0.0609, 0.0315, 0.0151],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,745][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0002, 0.0486, 0.0543, 0.0618, 0.0636, 0.0597, 0.0701, 0.0588, 0.0622,
        0.0651, 0.0671, 0.0623, 0.0738, 0.0805, 0.0869, 0.0850],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,747][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0065, 0.0355, 0.0484, 0.0894, 0.0954, 0.0526, 0.1342, 0.0432, 0.0354,
        0.1062, 0.0396, 0.0750, 0.0744, 0.0441, 0.0880, 0.0321],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,748][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ it] are: tensor([9.0063e-03, 1.7121e-04, 6.7248e-05, 3.0352e-04, 1.2652e-03, 4.1633e-03,
        1.3603e-02, 2.5952e-02, 4.1933e-02, 3.7733e-02, 8.6922e-02, 5.0721e-02,
        1.0496e-01, 1.7882e-01, 1.9340e-01, 2.5097e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,749][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ it] are: tensor([2.7032e-04, 2.5939e-06, 2.8952e-06, 5.7450e-04, 1.3363e-04, 1.4590e-04,
        5.0725e-03, 1.9790e-03, 3.2419e-03, 4.0892e-03, 2.7279e-02, 3.3708e-02,
        2.4147e-01, 3.2386e-01, 1.7523e-01, 1.8294e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,751][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0055, 0.0304, 0.0455, 0.0599, 0.0731, 0.0458, 0.0749, 0.0688, 0.0608,
        0.0808, 0.0735, 0.0678, 0.0798, 0.0675, 0.0903, 0.0756],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,753][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0101, 0.0573, 0.0613, 0.0669, 0.0705, 0.0626, 0.0722, 0.0575, 0.0599,
        0.0685, 0.0670, 0.0661, 0.0769, 0.0646, 0.0726, 0.0658],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,754][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.3970, 0.0463, 0.0115, 0.0098, 0.0075, 0.0137, 0.0832, 0.0088, 0.0112,
        0.2917, 0.0075, 0.0110, 0.0825, 0.0022, 0.0039, 0.0122],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,756][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0583, 0.0113, 0.0132, 0.0161, 0.0236, 0.0162, 0.0289, 0.0352, 0.1406,
        0.0377, 0.0184, 0.0145, 0.0410, 0.0622, 0.0128, 0.4701],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,758][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0053, 0.0817, 0.0434, 0.1788, 0.0577, 0.0723, 0.0755, 0.0266, 0.0297,
        0.0405, 0.0402, 0.0587, 0.1128, 0.0442, 0.0397, 0.0457, 0.0473],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,759][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([8.2457e-01, 3.8368e-02, 6.7607e-04, 4.0082e-02, 1.6926e-02, 2.0051e-03,
        2.4689e-02, 1.8572e-03, 4.9596e-03, 8.3301e-03, 2.2046e-03, 2.2225e-03,
        3.4086e-03, 2.7996e-03, 1.7696e-02, 3.1804e-03, 6.0234e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,760][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([2.7872e-03, 2.1257e-06, 1.2695e-06, 1.1962e-04, 4.9730e-05, 4.0558e-04,
        1.6472e-03, 3.8253e-03, 1.0782e-02, 3.7203e-03, 1.0944e-02, 7.8395e-02,
        9.1829e-02, 3.2317e-02, 1.0830e-01, 5.7384e-01, 8.1043e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,760][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.3989, 0.0081, 0.0053, 0.0065, 0.0032, 0.0503, 0.0131, 0.0766, 0.0113,
        0.0197, 0.2138, 0.0447, 0.0169, 0.0421, 0.0240, 0.0340, 0.0314],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,761][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0001, 0.0464, 0.0494, 0.0576, 0.0590, 0.0546, 0.0649, 0.0539, 0.0571,
        0.0595, 0.0615, 0.0582, 0.0687, 0.0737, 0.0808, 0.0798, 0.0748],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,761][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0048, 0.0315, 0.0405, 0.0945, 0.0860, 0.0528, 0.1269, 0.0355, 0.0392,
        0.1141, 0.0333, 0.0736, 0.0790, 0.0318, 0.0804, 0.0461, 0.0301],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,762][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([8.0174e-03, 6.9912e-05, 3.1265e-05, 1.6268e-04, 7.0262e-04, 3.1725e-03,
        8.5791e-03, 1.1035e-02, 4.1014e-02, 1.5275e-02, 8.0831e-02, 2.7906e-02,
        6.1780e-02, 1.0327e-01, 1.2005e-01, 3.0309e-01, 2.1501e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,762][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([9.2428e-05, 1.1604e-06, 9.4446e-07, 1.6884e-04, 1.5334e-04, 9.8661e-05,
        5.9789e-03, 7.6845e-04, 1.3938e-03, 5.5707e-03, 1.5559e-02, 1.3923e-02,
        3.3216e-01, 1.0792e-01, 1.1816e-01, 1.5948e-01, 2.3857e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,763][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0051, 0.0303, 0.0445, 0.0588, 0.0683, 0.0443, 0.0698, 0.0614, 0.0577,
        0.0755, 0.0683, 0.0660, 0.0722, 0.0593, 0.0811, 0.0763, 0.0611],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,763][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0086, 0.0549, 0.0569, 0.0630, 0.0659, 0.0586, 0.0687, 0.0551, 0.0564,
        0.0648, 0.0628, 0.0627, 0.0701, 0.0606, 0.0685, 0.0634, 0.0592],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,764][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([4.5020e-01, 5.1413e-02, 1.8543e-03, 1.7184e-02, 4.4785e-03, 2.7012e-03,
        4.7947e-02, 2.6615e-03, 2.7287e-03, 3.8342e-01, 1.7210e-03, 1.1894e-02,
        1.3138e-02, 3.0849e-04, 2.7387e-03, 5.2631e-03, 3.4144e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,765][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0254, 0.0031, 0.0389, 0.0026, 0.0365, 0.0213, 0.0044, 0.0228, 0.0182,
        0.0084, 0.0340, 0.0021, 0.0033, 0.3929, 0.0108, 0.0028, 0.3725],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,807][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:38,809][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,810][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,811][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,812][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,813][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,813][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,813][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,814][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,814][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,814][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,814][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,815][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:38,815][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.8432, 0.1568], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,815][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.7572, 0.2428], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,816][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.1650, 0.8350], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,816][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.3825, 0.6175], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,817][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.2406, 0.7594], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,819][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.9493, 0.0507], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,820][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.2563, 0.7437], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,822][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.2030, 0.7970], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,823][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.5022, 0.4978], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,825][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.9338, 0.0662], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,827][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.8873, 0.1127], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,828][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.8888, 0.1112], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:38,829][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([9.8699e-01, 1.2765e-02, 2.4246e-04], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,830][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9448, 0.0418, 0.0133], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,830][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.1871, 0.2037, 0.6092], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,830][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0410, 0.0378, 0.9212], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,831][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0105, 0.0145, 0.9750], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,831][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9624, 0.0293, 0.0082], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,831][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0383, 0.3669, 0.5948], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,832][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.1575, 0.2195, 0.6230], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,832][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.4486, 0.1099, 0.4415], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,832][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.9749, 0.0209, 0.0042], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,833][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9767, 0.0185, 0.0047], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,833][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.9656, 0.0213, 0.0131], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:38,833][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.7732, 0.1740, 0.0143, 0.0384], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,834][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.7181, 0.1920, 0.0270, 0.0629], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,836][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.0742, 0.0388, 0.4440, 0.4430], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,838][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.0342, 0.0241, 0.5739, 0.3679], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,839][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0078, 0.0056, 0.6636, 0.3230], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,841][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.8191, 0.1187, 0.0557, 0.0065], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,842][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.0432, 0.0554, 0.6969, 0.2045], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,844][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0550, 0.0362, 0.2129, 0.6960], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,846][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.2121, 0.0639, 0.5399, 0.1842], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,847][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.7901, 0.1616, 0.0353, 0.0130], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,847][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.8588, 0.0512, 0.0382, 0.0517], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,848][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.8261, 0.0695, 0.0474, 0.0570], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:38,848][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([9.5008e-01, 2.2414e-02, 9.1148e-04, 1.3574e-02, 1.3023e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,848][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.8958, 0.0288, 0.0035, 0.0274, 0.0445], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,849][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0191, 0.0325, 0.1031, 0.4585, 0.3868], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,849][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0111, 0.0096, 0.3146, 0.3851, 0.2796], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,849][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0089, 0.0044, 0.2154, 0.2332, 0.5381], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,850][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.8679, 0.0558, 0.0307, 0.0146, 0.0310], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,850][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0177, 0.0599, 0.3283, 0.3972, 0.1969], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,851][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0767, 0.0174, 0.0923, 0.1292, 0.6844], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,851][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.2293, 0.0355, 0.1034, 0.3412, 0.2906], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,853][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.8924, 0.0382, 0.0111, 0.0207, 0.0377], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,855][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.8913, 0.0589, 0.0133, 0.0219, 0.0146], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,856][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.8528, 0.0434, 0.0176, 0.0264, 0.0597], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:38,857][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([9.5166e-01, 2.1438e-02, 4.3279e-04, 6.5831e-03, 1.1227e-02, 8.6569e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,858][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.7783, 0.0721, 0.0038, 0.0564, 0.0668, 0.0226], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,860][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0457, 0.0053, 0.0070, 0.0671, 0.0818, 0.7931], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,862][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0407, 0.0051, 0.0786, 0.1334, 0.1419, 0.6003], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,863][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0101, 0.0013, 0.0110, 0.0417, 0.1088, 0.8271], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,864][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.8874, 0.0473, 0.0164, 0.0096, 0.0284, 0.0109], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,865][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0340, 0.0152, 0.0410, 0.0723, 0.1353, 0.7021], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,865][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0362, 0.0029, 0.0081, 0.0311, 0.1588, 0.7628], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,866][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1961, 0.0145, 0.0209, 0.0628, 0.1450, 0.5606], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,866][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.9037, 0.0287, 0.0061, 0.0093, 0.0230, 0.0291], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,866][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.9538, 0.0171, 0.0046, 0.0060, 0.0089, 0.0095], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,867][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.8750, 0.0245, 0.0062, 0.0200, 0.0143, 0.0600], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:38,867][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.8121, 0.0663, 0.0021, 0.0260, 0.0200, 0.0226, 0.0510],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,867][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.8077, 0.1117, 0.0009, 0.0314, 0.0132, 0.0064, 0.0287],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,868][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.0564, 0.0038, 0.0034, 0.0287, 0.0443, 0.5636, 0.2998],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,868][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0319, 0.0037, 0.0461, 0.0789, 0.0950, 0.3446, 0.3998],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,869][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([1.5680e-02, 5.1964e-04, 3.4810e-03, 1.3450e-02, 4.5684e-02, 3.2020e-01,
        6.0098e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,870][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([9.2677e-01, 4.2835e-02, 7.8801e-04, 1.0871e-02, 1.0507e-02, 5.3096e-03,
        2.9162e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,871][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0185, 0.0040, 0.0200, 0.0202, 0.0605, 0.5100, 0.3667],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,873][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0960, 0.0018, 0.0032, 0.0095, 0.1103, 0.2685, 0.5107],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,874][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.3230, 0.0106, 0.0069, 0.0436, 0.0910, 0.4130, 0.1119],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,875][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.8154, 0.0496, 0.0113, 0.0156, 0.0500, 0.0450, 0.0130],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,877][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.8442, 0.0119, 0.0124, 0.0118, 0.0183, 0.0276, 0.0738],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,879][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.8381, 0.0317, 0.0090, 0.0193, 0.0216, 0.0459, 0.0343],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:38,880][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([8.9077e-01, 2.8457e-02, 7.7349e-04, 1.6517e-02, 9.4641e-03, 1.3228e-02,
        2.4267e-02, 1.6521e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,881][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.5815, 0.1003, 0.0040, 0.0669, 0.0221, 0.0180, 0.1699, 0.0373],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,882][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0187, 0.0011, 0.0011, 0.0153, 0.0118, 0.1940, 0.2358, 0.5221],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,882][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0159, 0.0018, 0.0229, 0.0465, 0.0481, 0.1891, 0.3663, 0.3094],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,883][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([3.0420e-03, 1.3207e-04, 4.5514e-04, 3.7890e-03, 1.0782e-02, 9.1010e-02,
        4.0578e-01, 4.8501e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,883][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.8206, 0.0582, 0.0089, 0.0201, 0.0251, 0.0208, 0.0338, 0.0125],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,883][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0278, 0.0032, 0.0050, 0.0139, 0.0295, 0.1553, 0.4228, 0.3425],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,884][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0205, 0.0009, 0.0011, 0.0132, 0.0956, 0.1660, 0.4807, 0.2219],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,884][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.1338, 0.0057, 0.0084, 0.0366, 0.0452, 0.2962, 0.2017, 0.2723],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,885][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.8598, 0.0260, 0.0049, 0.0079, 0.0189, 0.0294, 0.0279, 0.0253],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,885][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.8517, 0.0222, 0.0062, 0.0131, 0.0117, 0.0225, 0.0237, 0.0490],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,885][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.8622, 0.0200, 0.0040, 0.0101, 0.0127, 0.0263, 0.0160, 0.0487],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:38,886][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([9.3067e-01, 1.9586e-02, 3.2605e-04, 5.3668e-03, 7.9197e-03, 4.6851e-03,
        1.3208e-02, 1.1336e-02, 6.9069e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,888][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.7582, 0.0897, 0.0009, 0.0289, 0.0116, 0.0038, 0.0727, 0.0212, 0.0130],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,889][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([1.7772e-02, 3.4810e-04, 2.2281e-04, 3.4275e-03, 3.5194e-03, 4.2163e-02,
        5.8295e-02, 3.0710e-01, 5.6715e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,891][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0199, 0.0010, 0.0080, 0.0241, 0.0201, 0.0754, 0.1787, 0.1507, 0.5220],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,892][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([4.1123e-03, 4.5193e-05, 1.1303e-04, 1.0314e-03, 3.7482e-03, 3.0750e-02,
        1.6271e-01, 3.0868e-01, 4.8881e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,893][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.9212, 0.0309, 0.0020, 0.0057, 0.0083, 0.0040, 0.0178, 0.0062, 0.0039],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,894][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0205, 0.0009, 0.0015, 0.0027, 0.0087, 0.0439, 0.0936, 0.3064, 0.5218],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,896][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0447, 0.0005, 0.0005, 0.0032, 0.0258, 0.0592, 0.2898, 0.2226, 0.3537],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,898][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2058, 0.0038, 0.0022, 0.0133, 0.0246, 0.1162, 0.1332, 0.2630, 0.2379],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,899][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.9155, 0.0153, 0.0022, 0.0033, 0.0109, 0.0132, 0.0127, 0.0131, 0.0138],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,900][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.9483, 0.0062, 0.0022, 0.0033, 0.0030, 0.0049, 0.0062, 0.0163, 0.0096],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,900][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.8813, 0.0142, 0.0023, 0.0075, 0.0078, 0.0185, 0.0117, 0.0213, 0.0355],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:38,900][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.7335, 0.0444, 0.0018, 0.0242, 0.0233, 0.0166, 0.0390, 0.0357, 0.0319,
        0.0495], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,901][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.7837, 0.0813, 0.0014, 0.0341, 0.0294, 0.0054, 0.0327, 0.0141, 0.0095,
        0.0084], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,901][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([5.4155e-03, 4.2575e-04, 2.3862e-04, 2.9260e-03, 5.5799e-03, 5.7572e-02,
        4.3560e-02, 1.5902e-01, 6.2928e-01, 9.5982e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,902][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0118, 0.0010, 0.0079, 0.0196, 0.0168, 0.0672, 0.1155, 0.1050, 0.4860,
        0.1694], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,902][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([2.4887e-03, 4.3580e-05, 2.2245e-04, 1.2739e-03, 3.7810e-03, 3.6718e-02,
        9.5917e-02, 2.4622e-01, 4.1498e-01, 1.9835e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,902][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.7739, 0.0371, 0.0054, 0.0122, 0.0494, 0.0104, 0.0485, 0.0234, 0.0337,
        0.0058], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,903][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0126, 0.0010, 0.0020, 0.0032, 0.0092, 0.0624, 0.0777, 0.1215, 0.6054,
        0.1049], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,904][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([1.7544e-02, 2.6256e-04, 6.5944e-04, 2.1438e-03, 1.8505e-02, 7.4893e-02,
        1.6737e-01, 1.9991e-01, 3.3292e-01, 1.8579e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,905][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0438, 0.0024, 0.0016, 0.0155, 0.0276, 0.1174, 0.0883, 0.1678, 0.4612,
        0.0744], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,907][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.7319, 0.0273, 0.0086, 0.0082, 0.0298, 0.0411, 0.0428, 0.0372, 0.0492,
        0.0239], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,908][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.8954, 0.0096, 0.0056, 0.0058, 0.0149, 0.0095, 0.0067, 0.0210, 0.0128,
        0.0185], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,910][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.8022, 0.0175, 0.0045, 0.0097, 0.0157, 0.0276, 0.0165, 0.0302, 0.0430,
        0.0332], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:38,911][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([9.3806e-01, 1.3324e-02, 1.1895e-04, 1.1287e-02, 4.2150e-03, 4.3577e-03,
        8.7065e-03, 6.2120e-03, 4.5113e-03, 6.9889e-03, 2.2186e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,912][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([8.3284e-01, 5.5314e-02, 3.5105e-04, 4.9126e-02, 3.3513e-03, 2.3223e-03,
        3.7511e-02, 6.1637e-03, 5.5430e-03, 5.1766e-03, 2.2993e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,913][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([1.8237e-02, 1.6828e-04, 8.7142e-05, 1.9910e-03, 2.1111e-03, 2.5215e-02,
        3.3556e-02, 1.4222e-01, 4.3972e-01, 1.1020e-01, 2.2650e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,914][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([6.2206e-03, 2.1304e-04, 1.1083e-03, 3.9945e-03, 3.7930e-03, 1.6245e-02,
        4.0822e-02, 3.3156e-02, 1.2406e-01, 8.8500e-02, 6.8189e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,915][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([1.3590e-03, 7.5115e-06, 1.7629e-05, 1.6078e-04, 6.2784e-04, 9.2711e-03,
        3.5608e-02, 6.6302e-02, 1.9447e-01, 8.5060e-02, 6.0712e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,917][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.8932, 0.0280, 0.0053, 0.0093, 0.0111, 0.0068, 0.0164, 0.0084, 0.0076,
        0.0088, 0.0052], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,917][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([1.9163e-02, 4.7320e-04, 2.5374e-04, 1.1288e-03, 3.8317e-03, 2.6900e-02,
        4.5548e-02, 8.8730e-02, 3.0618e-01, 1.4682e-01, 3.6097e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,917][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([2.8027e-02, 1.5710e-04, 2.5808e-04, 1.8177e-03, 7.7956e-03, 2.9489e-02,
        1.4062e-01, 9.3619e-02, 1.7320e-01, 1.0703e-01, 4.1799e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,918][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.2215, 0.0025, 0.0013, 0.0147, 0.0181, 0.0925, 0.0719, 0.1283, 0.2201,
        0.0885, 0.1407], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,918][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.9037, 0.0134, 0.0017, 0.0028, 0.0081, 0.0101, 0.0080, 0.0107, 0.0117,
        0.0128, 0.0170], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,919][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.9091, 0.0066, 0.0019, 0.0036, 0.0041, 0.0054, 0.0065, 0.0176, 0.0101,
        0.0107, 0.0243], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,919][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.8150, 0.0111, 0.0024, 0.0066, 0.0083, 0.0167, 0.0089, 0.0169, 0.0189,
        0.0222, 0.0730], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:38,919][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.7663, 0.0602, 0.0047, 0.0224, 0.0244, 0.0171, 0.0193, 0.0136, 0.0127,
        0.0344, 0.0127, 0.0121], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,920][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.5788, 0.1246, 0.0409, 0.0619, 0.0734, 0.0085, 0.0345, 0.0154, 0.0102,
        0.0370, 0.0111, 0.0037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,920][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([1.8067e-02, 3.0411e-04, 3.4432e-04, 2.8432e-03, 4.7701e-03, 2.7724e-02,
        3.2461e-02, 8.1721e-02, 1.9841e-01, 6.0372e-02, 2.2009e-01, 3.5289e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,921][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([1.1163e-02, 3.5003e-04, 1.5542e-03, 6.5612e-03, 4.9417e-03, 1.8132e-02,
        3.4053e-02, 4.4026e-02, 1.1218e-01, 7.8219e-02, 4.9183e-01, 1.9699e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,922][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([2.5547e-03, 8.2441e-06, 8.8511e-05, 2.7458e-04, 1.9195e-03, 1.0100e-02,
        3.7023e-02, 5.9037e-02, 9.8218e-02, 7.2736e-02, 6.1049e-01, 1.0755e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,924][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.6612, 0.0231, 0.0740, 0.0198, 0.0919, 0.0089, 0.0450, 0.0140, 0.0174,
        0.0248, 0.0177, 0.0021], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,925][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([2.8903e-02, 1.9690e-04, 2.5041e-04, 1.6143e-03, 2.8356e-03, 2.2127e-02,
        4.4669e-02, 3.9041e-02, 1.6497e-01, 7.0367e-02, 5.6988e-01, 5.5142e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,927][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0350, 0.0005, 0.0008, 0.0068, 0.0082, 0.0376, 0.0652, 0.0703, 0.1225,
        0.1506, 0.3011, 0.2015], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,927][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.1410, 0.0021, 0.0066, 0.0116, 0.0428, 0.0523, 0.0715, 0.0738, 0.1663,
        0.1135, 0.2564, 0.0621], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,929][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.8054, 0.0245, 0.0061, 0.0154, 0.0137, 0.0174, 0.0218, 0.0141, 0.0191,
        0.0165, 0.0211, 0.0250], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,931][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.8566, 0.0279, 0.0050, 0.0037, 0.0072, 0.0046, 0.0052, 0.0119, 0.0111,
        0.0114, 0.0321, 0.0233], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,933][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.7474, 0.0188, 0.0080, 0.0153, 0.0144, 0.0312, 0.0093, 0.0180, 0.0262,
        0.0277, 0.0423, 0.0414], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:38,934][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.6809, 0.0346, 0.0030, 0.0156, 0.0211, 0.0271, 0.0429, 0.0275, 0.0281,
        0.0353, 0.0310, 0.0342, 0.0185], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,935][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.4626, 0.0673, 0.0173, 0.0643, 0.0755, 0.0144, 0.1616, 0.0295, 0.0297,
        0.0394, 0.0166, 0.0071, 0.0147], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,935][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([3.4960e-03, 9.9176e-05, 8.8295e-05, 8.9238e-04, 1.2608e-03, 1.2579e-02,
        3.4828e-02, 3.7731e-02, 1.3641e-01, 4.0799e-02, 2.7488e-01, 2.0575e-01,
        2.5118e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,935][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([5.5455e-03, 2.0238e-04, 8.5365e-04, 3.9935e-03, 2.5479e-03, 1.2136e-02,
        2.7306e-02, 2.0864e-02, 8.0540e-02, 5.3259e-02, 4.6258e-01, 1.8606e-01,
        1.4411e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,936][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([1.3156e-03, 8.5466e-06, 3.4492e-05, 2.6310e-04, 4.5395e-04, 5.9646e-03,
        2.2397e-02, 2.4950e-02, 7.3074e-02, 4.3046e-02, 3.8045e-01, 1.5180e-01,
        2.9623e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,936][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.6018, 0.0386, 0.0167, 0.0235, 0.0685, 0.0149, 0.1317, 0.0206, 0.0242,
        0.0202, 0.0258, 0.0106, 0.0032], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,937][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([9.2663e-03, 4.8626e-04, 3.9820e-04, 2.0081e-03, 2.7160e-03, 1.6157e-02,
        4.5333e-02, 3.6341e-02, 1.6737e-01, 5.8893e-02, 4.4861e-01, 1.1670e-01,
        9.5713e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,937][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([2.9506e-02, 2.3928e-04, 1.6980e-04, 1.4982e-03, 3.5401e-03, 2.0493e-02,
        7.5713e-02, 3.3202e-02, 7.1765e-02, 7.9241e-02, 1.7343e-01, 1.0770e-01,
        4.0350e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,937][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0831, 0.0010, 0.0009, 0.0061, 0.0132, 0.0394, 0.0779, 0.0704, 0.1386,
        0.0643, 0.3068, 0.1320, 0.0663], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,938][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.6872, 0.0353, 0.0056, 0.0081, 0.0165, 0.0322, 0.0293, 0.0253, 0.0398,
        0.0295, 0.0415, 0.0362, 0.0134], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,940][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.7575, 0.0410, 0.0058, 0.0148, 0.0070, 0.0111, 0.0084, 0.0211, 0.0164,
        0.0119, 0.0567, 0.0312, 0.0172], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,942][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.6990, 0.0158, 0.0068, 0.0102, 0.0158, 0.0344, 0.0095, 0.0284, 0.0273,
        0.0162, 0.0592, 0.0330, 0.0443], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:38,943][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.3322e-01, 8.6335e-03, 1.5864e-04, 5.7816e-03, 4.7916e-03, 3.8347e-03,
        7.6533e-03, 3.6467e-03, 6.1494e-03, 6.8247e-03, 6.7679e-03, 4.8633e-03,
        2.9772e-03, 4.6984e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,944][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.6090, 0.0810, 0.0024, 0.0576, 0.0485, 0.0088, 0.0809, 0.0118, 0.0309,
        0.0202, 0.0159, 0.0063, 0.0085, 0.0181], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,945][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([7.3852e-03, 2.5643e-05, 8.8883e-06, 3.1583e-04, 3.9512e-04, 3.9200e-03,
        7.4022e-03, 2.5368e-02, 6.7940e-02, 2.4069e-02, 6.1714e-02, 2.1084e-01,
        3.8106e-01, 2.0956e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,946][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([5.1637e-03, 7.5188e-05, 3.1374e-04, 1.3138e-03, 1.4686e-03, 5.3955e-03,
        1.7733e-02, 1.2907e-02, 4.6368e-02, 3.1205e-02, 3.2332e-01, 1.2595e-01,
        2.2964e-01, 1.9915e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,948][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([8.8210e-04, 1.6840e-06, 2.5441e-06, 3.8600e-05, 1.4960e-04, 1.1041e-03,
        6.7605e-03, 8.2209e-03, 2.4279e-02, 1.3398e-02, 1.7147e-01, 8.9119e-02,
        2.8905e-01, 3.9553e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,949][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.8885, 0.0211, 0.0045, 0.0054, 0.0119, 0.0062, 0.0147, 0.0047, 0.0097,
        0.0056, 0.0098, 0.0091, 0.0047, 0.0041], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,950][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([1.5030e-02, 1.9671e-04, 9.2869e-05, 4.9183e-04, 1.6778e-03, 7.1280e-03,
        2.3573e-02, 3.1278e-02, 1.0141e-01, 4.4407e-02, 2.2678e-01, 7.5592e-02,
        2.1043e-01, 2.6192e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,951][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([8.4087e-03, 5.3159e-05, 2.6650e-05, 3.1174e-04, 1.8686e-03, 4.4804e-03,
        2.9153e-02, 1.3380e-02, 2.7956e-02, 3.7187e-02, 1.0377e-01, 3.6347e-02,
        4.0796e-01, 3.2910e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,952][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.1261, 0.0006, 0.0004, 0.0034, 0.0064, 0.0290, 0.0243, 0.0391, 0.1056,
        0.0267, 0.1831, 0.1347, 0.1332, 0.1875], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,952][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.8372, 0.0148, 0.0014, 0.0027, 0.0085, 0.0103, 0.0114, 0.0122, 0.0127,
        0.0172, 0.0184, 0.0174, 0.0197, 0.0159], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,953][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.9302, 0.0061, 0.0010, 0.0022, 0.0026, 0.0037, 0.0041, 0.0087, 0.0056,
        0.0051, 0.0137, 0.0042, 0.0027, 0.0098], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,953][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.8098, 0.0088, 0.0019, 0.0047, 0.0069, 0.0134, 0.0083, 0.0129, 0.0128,
        0.0148, 0.0382, 0.0189, 0.0167, 0.0318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:38,953][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.6126, 0.0439, 0.0009, 0.0220, 0.0292, 0.0225, 0.0193, 0.0205, 0.0183,
        0.0292, 0.0244, 0.0346, 0.0165, 0.0521, 0.0539], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,954][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.6258, 0.0778, 0.0030, 0.0461, 0.0213, 0.0109, 0.0451, 0.0098, 0.0110,
        0.0225, 0.0109, 0.0177, 0.0344, 0.0424, 0.0213], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,954][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([2.2993e-03, 4.8426e-05, 3.0251e-05, 4.2412e-04, 5.0444e-04, 6.5369e-03,
        1.1904e-02, 2.5251e-02, 6.9241e-02, 2.9120e-02, 8.0187e-02, 1.3016e-01,
        2.3448e-01, 2.9278e-01, 1.1703e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,955][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([4.0581e-03, 1.2554e-04, 4.6984e-04, 2.1688e-03, 1.6716e-03, 7.5810e-03,
        1.2068e-02, 9.9842e-03, 5.3643e-02, 2.6652e-02, 2.4342e-01, 9.9098e-02,
        1.6950e-01, 2.5112e-01, 1.1844e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,955][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([5.9629e-04, 4.4772e-06, 1.1897e-05, 1.1031e-04, 1.8205e-04, 1.6642e-03,
        5.8859e-03, 8.3321e-03, 2.5362e-02, 1.2834e-02, 1.2827e-01, 6.2699e-02,
        2.0771e-01, 3.8979e-01, 1.5655e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,957][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.7215, 0.0368, 0.0035, 0.0152, 0.0129, 0.0102, 0.0439, 0.0137, 0.0106,
        0.0164, 0.0171, 0.0405, 0.0215, 0.0267, 0.0095], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,959][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([4.1570e-03, 3.3535e-04, 4.7913e-04, 1.2838e-03, 2.0367e-03, 6.4725e-03,
        1.5648e-02, 2.2818e-02, 5.4624e-02, 3.9089e-02, 1.5730e-01, 4.5279e-02,
        1.7176e-01, 3.8718e-01, 9.1531e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,960][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([7.6304e-03, 1.1775e-04, 5.2546e-05, 6.4021e-04, 1.3066e-03, 7.8391e-03,
        2.6505e-02, 1.6745e-02, 3.2889e-02, 2.3105e-02, 9.0098e-02, 4.5987e-02,
        1.6408e-01, 5.2470e-01, 5.8297e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,961][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([6.6983e-02, 8.9927e-04, 2.9069e-04, 4.5243e-03, 3.8895e-03, 2.8472e-02,
        2.0195e-02, 3.4259e-02, 5.4597e-02, 4.0808e-02, 1.0836e-01, 1.4874e-01,
        1.2186e-01, 2.9574e-01, 7.0384e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,962][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.6054, 0.0264, 0.0035, 0.0065, 0.0173, 0.0230, 0.0199, 0.0281, 0.0297,
        0.0424, 0.0433, 0.0344, 0.0447, 0.0455, 0.0299], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,964][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.7055, 0.0286, 0.0051, 0.0103, 0.0089, 0.0108, 0.0192, 0.0223, 0.0161,
        0.0146, 0.0454, 0.0282, 0.0210, 0.0404, 0.0235], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,966][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.5809, 0.0228, 0.0072, 0.0158, 0.0163, 0.0296, 0.0138, 0.0242, 0.0232,
        0.0191, 0.0674, 0.0432, 0.0343, 0.0406, 0.0616], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:38,967][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([8.7591e-01, 1.2279e-02, 8.2062e-04, 1.0215e-02, 8.2615e-03, 8.9313e-03,
        8.4744e-03, 8.0587e-03, 6.3586e-03, 9.5545e-03, 9.7623e-03, 1.1293e-02,
        5.2615e-03, 7.9171e-03, 1.0451e-02, 6.4524e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,968][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.3566, 0.1545, 0.0071, 0.0498, 0.0587, 0.0245, 0.0646, 0.0267, 0.0188,
        0.0444, 0.0281, 0.0429, 0.0162, 0.0366, 0.0578, 0.0128],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,969][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([2.5604e-03, 2.5946e-05, 8.5641e-06, 3.9277e-04, 2.0790e-04, 1.6352e-03,
        3.7753e-03, 8.7562e-03, 3.2284e-02, 1.1348e-02, 3.9516e-02, 1.1402e-01,
        1.7907e-01, 1.7461e-01, 1.8609e-01, 2.4570e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,969][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([5.2631e-03, 9.3390e-05, 1.9680e-04, 1.4618e-03, 8.7032e-04, 3.4401e-03,
        1.1980e-02, 6.9047e-03, 2.8947e-02, 1.9681e-02, 1.3242e-01, 8.8758e-02,
        1.1579e-01, 1.7656e-01, 1.2925e-01, 2.7839e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,970][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([6.3883e-04, 1.5558e-06, 2.2807e-06, 3.5763e-05, 9.7546e-05, 8.8980e-04,
        2.6270e-03, 6.0662e-03, 1.3620e-02, 6.9479e-03, 9.7496e-02, 3.6345e-02,
        1.4210e-01, 2.5163e-01, 1.6851e-01, 2.7300e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,970][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.7575, 0.0356, 0.0039, 0.0074, 0.0122, 0.0071, 0.0280, 0.0102, 0.0099,
        0.0075, 0.0202, 0.0387, 0.0092, 0.0157, 0.0309, 0.0060],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,971][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([9.0063e-03, 1.7121e-04, 6.7248e-05, 3.0352e-04, 1.2652e-03, 4.1633e-03,
        1.3603e-02, 2.5952e-02, 4.1933e-02, 3.7733e-02, 8.6922e-02, 5.0721e-02,
        1.0496e-01, 1.7882e-01, 1.9340e-01, 2.5097e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,971][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([6.9417e-03, 4.1317e-05, 3.7056e-05, 4.9677e-04, 8.6860e-04, 2.5410e-03,
        1.0312e-02, 1.1859e-02, 2.4878e-02, 1.2701e-02, 8.7875e-02, 2.6761e-02,
        1.5592e-01, 3.5101e-01, 9.1212e-02, 2.1655e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,971][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([6.2585e-02, 3.2515e-04, 1.3352e-04, 1.7060e-03, 2.7378e-03, 1.0971e-02,
        1.8388e-02, 2.8548e-02, 5.0258e-02, 2.4236e-02, 1.1877e-01, 8.6970e-02,
        9.8265e-02, 2.2358e-01, 2.1129e-01, 6.1238e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,972][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.7902, 0.0108, 0.0023, 0.0040, 0.0078, 0.0125, 0.0120, 0.0141, 0.0149,
        0.0154, 0.0219, 0.0176, 0.0283, 0.0178, 0.0231, 0.0073],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,973][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([9.2884e-01, 3.1393e-03, 6.3089e-04, 9.3599e-04, 2.3244e-03, 2.2865e-03,
        3.2149e-03, 6.2094e-03, 4.8330e-03, 5.6037e-03, 1.3087e-02, 2.9906e-03,
        2.0363e-03, 9.6889e-03, 6.8416e-03, 7.3363e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,974][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.7254, 0.0115, 0.0030, 0.0085, 0.0051, 0.0101, 0.0085, 0.0195, 0.0157,
        0.0134, 0.0383, 0.0411, 0.0242, 0.0208, 0.0171, 0.0378],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:38,976][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.0753e-01, 8.3640e-03, 1.8743e-04, 6.1779e-03, 5.5319e-03, 4.0637e-03,
        7.3755e-03, 3.5896e-03, 5.5779e-03, 6.7333e-03, 5.8689e-03, 4.7513e-03,
        2.9296e-03, 4.8957e-03, 1.3374e-02, 5.2598e-03, 7.7903e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,977][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.4690, 0.0947, 0.0024, 0.0606, 0.0475, 0.0105, 0.0801, 0.0127, 0.0320,
        0.0285, 0.0167, 0.0070, 0.0087, 0.0200, 0.0570, 0.0106, 0.0420],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,978][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([3.3567e-03, 8.0087e-06, 3.0299e-06, 9.4088e-05, 1.4926e-04, 1.1900e-03,
        2.6953e-03, 7.5593e-03, 2.2804e-02, 6.8717e-03, 1.9633e-02, 6.4392e-02,
        1.1456e-01, 7.0255e-02, 9.7541e-02, 4.4819e-01, 1.4070e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,979][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([2.6450e-03, 3.4954e-05, 1.3676e-04, 5.5031e-04, 7.0137e-04, 2.2186e-03,
        7.0477e-03, 5.0343e-03, 1.9393e-02, 1.3720e-02, 1.3393e-01, 5.3700e-02,
        1.1976e-01, 9.5897e-02, 1.0027e-01, 3.2499e-01, 1.1997e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,981][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([4.0580e-04, 5.3324e-07, 6.4720e-07, 1.1696e-05, 4.4948e-05, 3.5581e-04,
        2.0554e-03, 2.3957e-03, 7.3779e-03, 3.9607e-03, 4.7770e-02, 2.4822e-02,
        8.2899e-02, 1.1371e-01, 1.0748e-01, 3.3954e-01, 2.6717e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,983][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.8595, 0.0209, 0.0039, 0.0049, 0.0118, 0.0062, 0.0153, 0.0049, 0.0096,
        0.0062, 0.0096, 0.0087, 0.0037, 0.0045, 0.0170, 0.0069, 0.0065],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,984][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([8.0174e-03, 6.9912e-05, 3.1265e-05, 1.6268e-04, 7.0262e-04, 3.1725e-03,
        8.5791e-03, 1.1035e-02, 4.1014e-02, 1.5275e-02, 8.0831e-02, 2.7906e-02,
        6.1780e-02, 1.0327e-01, 1.2005e-01, 3.0309e-01, 2.1501e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,985][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([5.3538e-03, 2.3413e-05, 1.2815e-05, 1.5448e-04, 9.2418e-04, 1.8938e-03,
        1.2832e-02, 6.0831e-03, 1.2551e-02, 1.8141e-02, 4.7077e-02, 1.5295e-02,
        1.6457e-01, 1.5927e-01, 7.3416e-02, 1.9444e-01, 2.8796e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,986][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([6.2047e-02, 2.4629e-04, 1.7240e-04, 1.4474e-03, 2.5519e-03, 1.2124e-02,
        1.1422e-02, 1.7298e-02, 4.5553e-02, 1.4469e-02, 8.0197e-02, 5.2300e-02,
        5.1435e-02, 9.0096e-02, 1.5536e-01, 2.4838e-01, 1.5490e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,986][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.8068, 0.0136, 0.0012, 0.0024, 0.0075, 0.0088, 0.0107, 0.0106, 0.0112,
        0.0154, 0.0163, 0.0156, 0.0177, 0.0137, 0.0230, 0.0127, 0.0130],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,987][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.2908e-01, 4.1883e-03, 6.9108e-04, 1.6822e-03, 2.2365e-03, 2.7252e-03,
        3.6199e-03, 6.7730e-03, 4.3620e-03, 3.8800e-03, 1.0655e-02, 2.6112e-03,
        1.9162e-03, 7.8489e-03, 5.0416e-03, 4.3794e-03, 8.3136e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,987][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.7797, 0.0073, 0.0016, 0.0041, 0.0060, 0.0114, 0.0066, 0.0104, 0.0103,
        0.0120, 0.0342, 0.0172, 0.0140, 0.0251, 0.0183, 0.0151, 0.0265],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:38,988][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:38,989][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[14277],
        [ 4413],
        [ 4132],
        [ 2298],
        [  719],
        [ 1690],
        [  273],
        [ 1921],
        [  844],
        [  173],
        [  386],
        [ 1167],
        [  185],
        [ 1066],
        [  117],
        [  480],
        [  893]], device='cuda:0')
[2024-07-24 10:17:38,991][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[13218],
        [ 4156],
        [ 5825],
        [ 6693],
        [ 1571],
        [ 3491],
        [ 2140],
        [ 3682],
        [ 1957],
        [  401],
        [  748],
        [ 2074],
        [  445],
        [ 1263],
        [  457],
        [  612],
        [  864]], device='cuda:0')
[2024-07-24 10:17:38,993][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[26101],
        [15502],
        [18664],
        [17557],
        [18616],
        [19128],
        [20217],
        [20871],
        [21185],
        [20904],
        [21318],
        [20851],
        [21337],
        [21327],
        [21316],
        [21863],
        [21694]], device='cuda:0')
[2024-07-24 10:17:38,995][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[25827],
        [26738],
        [25751],
        [38685],
        [28688],
        [31905],
        [30407],
        [38228],
        [31801],
        [30421],
        [33483],
        [34514],
        [35995],
        [34360],
        [32034],
        [33173],
        [35685]], device='cuda:0')
[2024-07-24 10:17:38,996][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[27199],
        [40517],
        [46264],
        [46513],
        [45827],
        [40584],
        [39076],
        [35205],
        [31580],
        [32154],
        [32759],
        [32838],
        [35574],
        [38586],
        [39517],
        [40516],
        [40050]], device='cuda:0')
[2024-07-24 10:17:38,998][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[10437],
        [ 7992],
        [ 7112],
        [ 3691],
        [ 2179],
        [ 5432],
        [ 5944],
        [ 1894],
        [ 2816],
        [ 5902],
        [ 6798],
        [ 6087],
        [ 4559],
        [ 6356],
        [ 6780],
        [ 4332],
        [ 6521]], device='cuda:0')
[2024-07-24 10:17:39,000][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[16036],
        [16574],
        [15844],
        [15935],
        [16247],
        [16216],
        [15879],
        [15772],
        [16082],
        [15929],
        [15931],
        [16289],
        [16550],
        [16421],
        [16082],
        [16150],
        [16027]], device='cuda:0')
[2024-07-24 10:17:39,001][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[41755],
        [15978],
        [18589],
        [13714],
        [15662],
        [16470],
        [15231],
        [15223],
        [15583],
        [16699],
        [16634],
        [16537],
        [16174],
        [16307],
        [16887],
        [16810],
        [16694]], device='cuda:0')
[2024-07-24 10:17:39,003][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[47003],
        [24547],
        [14869],
        [24789],
        [36751],
        [14373],
        [12829],
        [14939],
        [10125],
        [ 9389],
        [ 9627],
        [10715],
        [12906],
        [14235],
        [17311],
        [28049],
        [24249]], device='cuda:0')
[2024-07-24 10:17:39,004][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 2624],
        [44031],
        [17790],
        [48496],
        [45416],
        [46374],
        [41694],
        [41286],
        [40019],
        [42321],
        [34944],
        [42405],
        [27790],
        [26601],
        [36107],
        [34567],
        [33338]], device='cuda:0')
[2024-07-24 10:17:39,005][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[31593],
        [28167],
        [27744],
        [27689],
        [27534],
        [26625],
        [28182],
        [28070],
        [28046],
        [28015],
        [27985],
        [27850],
        [27459],
        [27255],
        [26904],
        [27019],
        [27044]], device='cuda:0')
[2024-07-24 10:17:39,006][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[21218],
        [24763],
        [20698],
        [22388],
        [21715],
        [22211],
        [22311],
        [22426],
        [22627],
        [22332],
        [21916],
        [21840],
        [22490],
        [22363],
        [22083],
        [21995],
        [21947]], device='cuda:0')
[2024-07-24 10:17:39,007][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[ 1226],
        [18081],
        [ 2894],
        [  201],
        [ 1484],
        [ 4938],
        [ 2687],
        [ 1755],
        [  995],
        [40962],
        [30898],
        [37678],
        [11068],
        [32686],
        [11010],
        [33147],
        [34615]], device='cuda:0')
[2024-07-24 10:17:39,009][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[44968],
        [33479],
        [15905],
        [    1],
        [ 2728],
        [  871],
        [  304],
        [ 6646],
        [12378],
        [ 2642],
        [ 3132],
        [ 1129],
        [15885],
        [25587],
        [ 1924],
        [23365],
        [28606]], device='cuda:0')
[2024-07-24 10:17:39,011][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[30247],
        [12845],
        [14491],
        [11317],
        [12179],
        [10368],
        [14995],
        [12647],
        [10544],
        [13361],
        [12171],
        [ 8824],
        [12283],
        [14423],
        [10578],
        [10714],
        [15300]], device='cuda:0')
[2024-07-24 10:17:39,012][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[28418],
        [21270],
        [28059],
        [14719],
        [26494],
        [26660],
        [21450],
        [25229],
        [26691],
        [18553],
        [26872],
        [18490],
        [16105],
        [26833],
        [12696],
        [24893],
        [26232]], device='cuda:0')
[2024-07-24 10:17:39,014][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 8560],
        [ 9942],
        [ 8617],
        [10692],
        [ 8705],
        [ 9906],
        [ 9163],
        [11076],
        [ 8694],
        [ 8941],
        [ 8553],
        [11067],
        [11773],
        [ 9319],
        [ 8440],
        [11663],
        [ 9572]], device='cuda:0')
[2024-07-24 10:17:39,016][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[29729],
        [ 3105],
        [15437],
        [12570],
        [ 4912],
        [ 8065],
        [ 8462],
        [ 9511],
        [10701],
        [ 9896],
        [10234],
        [ 9310],
        [ 7740],
        [ 5404],
        [ 7536],
        [10269],
        [12451]], device='cuda:0')
[2024-07-24 10:17:39,017][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 6223],
        [28002],
        [20815],
        [16063],
        [ 7555],
        [ 8288],
        [20173],
        [23557],
        [22160],
        [23318],
        [20303],
        [27879],
        [31102],
        [31911],
        [31197],
        [23081],
        [20225]], device='cuda:0')
[2024-07-24 10:17:39,019][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[ 4061],
        [  734],
        [24611],
        [17119],
        [13430],
        [22871],
        [21182],
        [20100],
        [18361],
        [20918],
        [28629],
        [27970],
        [30263],
        [26197],
        [22372],
        [24139],
        [22146]], device='cuda:0')
[2024-07-24 10:17:39,020][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[15393],
        [14518],
        [14569],
        [11348],
        [12029],
        [12392],
        [13804],
        [11917],
        [14037],
        [11914],
        [13497],
        [ 9847],
        [10593],
        [13745],
        [13041],
        [13329],
        [13594]], device='cuda:0')
[2024-07-24 10:17:39,022][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[29501],
        [15517],
        [32855],
        [39628],
        [38847],
        [42339],
        [38642],
        [33562],
        [33946],
        [32785],
        [35092],
        [36397],
        [33179],
        [32341],
        [31590],
        [36630],
        [38879]], device='cuda:0')
[2024-07-24 10:17:39,023][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[27687],
        [24993],
        [14684],
        [29708],
        [13734],
        [16603],
        [30519],
        [28508],
        [24627],
        [18613],
        [17238],
        [16803],
        [ 9723],
        [10082],
        [14249],
        [11387],
        [10741]], device='cuda:0')
[2024-07-24 10:17:39,024][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25165],
        [33460],
        [38229],
        [32704],
        [32094],
        [41353],
        [40864],
        [38006],
        [37011],
        [35560],
        [37638],
        [36978],
        [37173],
        [37332],
        [37374],
        [37404],
        [35162]], device='cuda:0')
[2024-07-24 10:17:39,025][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[24510],
        [23367],
        [24127],
        [22257],
        [23351],
        [23486],
        [23050],
        [23377],
        [23786],
        [24337],
        [24190],
        [24023],
        [25183],
        [24846],
        [27207],
        [25748],
        [25322]], device='cuda:0')
[2024-07-24 10:17:39,026][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[48524],
        [49001],
        [48681],
        [49037],
        [48889],
        [48566],
        [48582],
        [48393],
        [48492],
        [48357],
        [48508],
        [48697],
        [48561],
        [48550],
        [48254],
        [48602],
        [48580]], device='cuda:0')
[2024-07-24 10:17:39,028][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12249],
        [11126],
        [11710],
        [20485],
        [16826],
        [14436],
        [14416],
        [11314],
        [11049],
        [11470],
        [11197],
        [17056],
        [20709],
        [11387],
        [27907],
        [19894],
        [13218]], device='cuda:0')
[2024-07-24 10:17:39,030][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[20498],
        [29144],
        [16845],
        [17695],
        [23788],
        [20452],
        [17607],
        [19526],
        [21053],
        [24712],
        [21634],
        [20399],
        [21529],
        [21945],
        [20896],
        [18122],
        [19170]], device='cuda:0')
[2024-07-24 10:17:39,031][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[28951],
        [31608],
        [29742],
        [24575],
        [33469],
        [19816],
        [19433],
        [15092],
        [14292],
        [13488],
        [14150],
        [19064],
        [21718],
        [21468],
        [25008],
        [21286],
        [19512]], device='cuda:0')
[2024-07-24 10:17:39,033][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514],
        [13514]], device='cuda:0')
[2024-07-24 10:17:39,076][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:39,077][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,077][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,078][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,078][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,078][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,079][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,079][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,079][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,080][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,080][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,080][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,081][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,081][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9896, 0.0104], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,081][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.1321, 0.8679], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,083][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.4864, 0.5136], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,084][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.2980, 0.7020], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,086][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0948, 0.9052], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,087][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.3412, 0.6588], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,089][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.4663, 0.5337], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,091][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.5674, 0.4326], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,092][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.6674, 0.3326], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,093][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0148, 0.9852], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,093][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.3840, 0.6160], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,094][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.4651, 0.5349], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,094][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.9903, 0.0066, 0.0030], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,094][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0783, 0.5257, 0.3960], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,095][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.3462, 0.2886, 0.3651], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,095][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.3414, 0.2779, 0.3807], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,095][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0514, 0.1741, 0.7746], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,096][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.3399, 0.3571, 0.3029], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,096][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2427, 0.3385, 0.4189], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,096][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.2777, 0.6581, 0.0642], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,097][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2909, 0.6457, 0.0634], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,098][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0064, 0.4544, 0.5392], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,099][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.2648, 0.3677, 0.3675], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,101][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1482, 0.8151, 0.0367], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,103][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.9554, 0.0195, 0.0112, 0.0139], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,104][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.0419, 0.3558, 0.2855, 0.3167], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,106][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.2563, 0.2094, 0.2632, 0.2710], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,107][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.2195, 0.2542, 0.2397, 0.2866], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,109][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0782, 0.1526, 0.4556, 0.3135], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,110][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0998, 0.2020, 0.4861, 0.2121], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,111][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.1559, 0.1987, 0.2773, 0.3681], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,111][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.2311, 0.2825, 0.2497, 0.2366], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,111][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.1522, 0.1916, 0.5751, 0.0811], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,112][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.0049, 0.3443, 0.3887, 0.2621], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,112][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.1250, 0.2489, 0.3273, 0.2988], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,112][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0732, 0.1990, 0.2894, 0.4383], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,113][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.9732, 0.0059, 0.0109, 0.0086, 0.0013], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,113][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0304, 0.2932, 0.2418, 0.2701, 0.1645], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,113][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.1738, 0.1738, 0.2235, 0.2247, 0.2041], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,114][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.1574, 0.2046, 0.2183, 0.2144, 0.2053], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,114][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0198, 0.0930, 0.4607, 0.2651, 0.1613], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,116][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.1631, 0.1309, 0.2189, 0.2379, 0.2492], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,117][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0826, 0.1453, 0.1991, 0.2809, 0.2921], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,119][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.1983, 0.2259, 0.2886, 0.2364, 0.0508], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,120][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.1699, 0.1814, 0.3720, 0.2255, 0.0512], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,121][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0040, 0.2588, 0.2898, 0.2014, 0.2461], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,123][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.1085, 0.1982, 0.2580, 0.2276, 0.2077], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,125][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0960, 0.0809, 0.0982, 0.4430, 0.2819], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,126][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.9796, 0.0040, 0.0057, 0.0042, 0.0011, 0.0055], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,128][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0311, 0.2327, 0.1873, 0.2234, 0.1427, 0.1827], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,128][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1417, 0.1343, 0.1791, 0.1814, 0.1635, 0.2001], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,128][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1454, 0.1438, 0.1650, 0.1829, 0.1904, 0.1725], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,129][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0122, 0.0649, 0.3088, 0.2092, 0.1277, 0.2770], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,129][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1085, 0.1258, 0.1291, 0.2198, 0.1836, 0.2332], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,130][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0648, 0.0916, 0.1263, 0.1750, 0.1837, 0.3588], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,130][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1537, 0.2853, 0.1134, 0.2988, 0.0903, 0.0585], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,130][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.1132, 0.1300, 0.2175, 0.3356, 0.1664, 0.0372], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,131][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0037, 0.2025, 0.2266, 0.1595, 0.1913, 0.2164], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,131][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0959, 0.1527, 0.2062, 0.1782, 0.1846, 0.1825], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,131][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0511, 0.0794, 0.0360, 0.2554, 0.5565, 0.0214], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,132][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.9172, 0.0056, 0.0158, 0.0108, 0.0024, 0.0410, 0.0072],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,134][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0281, 0.1798, 0.1551, 0.1677, 0.1152, 0.1547, 0.1995],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,135][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.1245, 0.1164, 0.1597, 0.1519, 0.1383, 0.1645, 0.1448],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,137][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.0760, 0.1255, 0.1228, 0.1555, 0.1984, 0.2034, 0.1185],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,138][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0043, 0.0524, 0.3467, 0.1610, 0.1106, 0.1791, 0.1458],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,140][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.1272, 0.0300, 0.1768, 0.0903, 0.1440, 0.2152, 0.2165],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,142][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.0698, 0.0689, 0.1077, 0.1352, 0.1429, 0.2290, 0.2465],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,143][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.1091, 0.0831, 0.0558, 0.1319, 0.0655, 0.0303, 0.5244],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,145][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.1678, 0.1068, 0.0928, 0.0882, 0.1234, 0.1216, 0.2994],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,145][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.0032, 0.1775, 0.1931, 0.1355, 0.1665, 0.1849, 0.1393],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,146][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.0558, 0.1197, 0.2008, 0.1535, 0.1569, 0.1669, 0.1463],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,146][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0899, 0.0313, 0.0539, 0.1308, 0.2889, 0.1071, 0.2980],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,147][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.9593, 0.0039, 0.0053, 0.0066, 0.0015, 0.0120, 0.0078, 0.0036],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,147][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0159, 0.1610, 0.1373, 0.1629, 0.1040, 0.1371, 0.1848, 0.0969],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,147][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1158, 0.0997, 0.1270, 0.1308, 0.1151, 0.1401, 0.1244, 0.1471],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,148][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.1499, 0.1025, 0.1145, 0.1255, 0.1588, 0.1612, 0.0809, 0.1067],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,148][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0085, 0.0632, 0.2473, 0.1547, 0.0968, 0.1514, 0.1100, 0.1681],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,148][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.1129, 0.0472, 0.0409, 0.1639, 0.0946, 0.0723, 0.3680, 0.1003],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,149][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0479, 0.0625, 0.0898, 0.1189, 0.1222, 0.1864, 0.1951, 0.1772],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,150][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0612, 0.0930, 0.0369, 0.1200, 0.0185, 0.0257, 0.6190, 0.0258],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,151][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0392, 0.0131, 0.0212, 0.0771, 0.0356, 0.0321, 0.7571, 0.0246],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,153][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0029, 0.1554, 0.1690, 0.1168, 0.1412, 0.1562, 0.1234, 0.1351],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,154][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0537, 0.1129, 0.1903, 0.1430, 0.1292, 0.1253, 0.1122, 0.1334],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,156][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0231, 0.0211, 0.0016, 0.1292, 0.0827, 0.0123, 0.7227, 0.0073],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,158][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.9185, 0.0058, 0.0123, 0.0092, 0.0026, 0.0210, 0.0089, 0.0113, 0.0104],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,159][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0185, 0.1467, 0.1215, 0.1440, 0.0869, 0.1211, 0.1653, 0.0940, 0.1021],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,161][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0965, 0.0850, 0.1130, 0.1139, 0.0996, 0.1232, 0.1087, 0.1286, 0.1315],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,162][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1069, 0.0890, 0.1079, 0.1195, 0.1538, 0.1269, 0.0732, 0.0971, 0.1257],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,163][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0085, 0.0413, 0.1444, 0.1147, 0.0829, 0.1268, 0.0924, 0.1255, 0.2635],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,163][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0786, 0.0297, 0.0239, 0.0639, 0.0640, 0.0601, 0.3256, 0.1464, 0.2079],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,164][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0486, 0.0491, 0.0763, 0.1075, 0.1133, 0.1681, 0.1738, 0.1478, 0.1153],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,164][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0567, 0.1016, 0.0305, 0.1342, 0.0338, 0.0228, 0.5314, 0.0519, 0.0371],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,165][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0335, 0.0066, 0.0111, 0.0311, 0.0199, 0.0111, 0.6733, 0.1808, 0.0327],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,165][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0030, 0.1364, 0.1472, 0.1038, 0.1232, 0.1379, 0.1077, 0.1193, 0.1215],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,165][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0599, 0.0946, 0.1475, 0.1180, 0.1123, 0.1149, 0.1039, 0.1237, 0.1253],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,166][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0136, 0.0100, 0.0013, 0.0453, 0.0546, 0.0026, 0.8099, 0.0485, 0.0142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,166][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.7115, 0.0097, 0.0142, 0.0121, 0.0038, 0.0808, 0.0100, 0.0375, 0.1164,
        0.0041], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,166][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0160, 0.1294, 0.1154, 0.1211, 0.0811, 0.1114, 0.1475, 0.0887, 0.0945,
        0.0948], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,167][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0978, 0.0768, 0.1013, 0.1007, 0.0899, 0.1073, 0.0973, 0.1104, 0.1155,
        0.1031], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,169][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0721, 0.0864, 0.0912, 0.0984, 0.1399, 0.1353, 0.0763, 0.0974, 0.1322,
        0.0709], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,171][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0064, 0.0493, 0.1359, 0.1084, 0.0589, 0.0988, 0.0703, 0.0807, 0.1960,
        0.1953], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,172][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0969, 0.0252, 0.0482, 0.0634, 0.1019, 0.0726, 0.1501, 0.1421, 0.1780,
        0.1216], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,174][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0433, 0.0372, 0.0636, 0.0970, 0.0970, 0.1413, 0.1710, 0.1305, 0.0982,
        0.1210], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,175][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0597, 0.1037, 0.0283, 0.0946, 0.0742, 0.0410, 0.3349, 0.1102, 0.0554,
        0.0980], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,177][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.1106, 0.0274, 0.0160, 0.0516, 0.0299, 0.0377, 0.3610, 0.0649, 0.1900,
        0.1109], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,179][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0019, 0.1233, 0.1367, 0.0934, 0.1138, 0.1252, 0.0940, 0.1050, 0.1082,
        0.0985], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,180][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0515, 0.0880, 0.1326, 0.1125, 0.0989, 0.0959, 0.0850, 0.1019, 0.1084,
        0.1253], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,181][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0628, 0.0088, 0.0101, 0.0426, 0.0726, 0.0316, 0.2279, 0.3236, 0.0840,
        0.1359], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,181][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.9555, 0.0028, 0.0017, 0.0056, 0.0011, 0.0093, 0.0042, 0.0039, 0.0109,
        0.0018, 0.0033], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,181][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0218, 0.1211, 0.0926, 0.1161, 0.0780, 0.0981, 0.1274, 0.0778, 0.0870,
        0.0905, 0.0895], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,182][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0864, 0.0698, 0.0894, 0.0915, 0.0811, 0.0978, 0.0873, 0.1008, 0.1064,
        0.0928, 0.0968], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,182][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1389, 0.0700, 0.0790, 0.0882, 0.1161, 0.1070, 0.0570, 0.0778, 0.1067,
        0.0618, 0.0975], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,183][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0051, 0.0301, 0.0872, 0.0732, 0.0772, 0.0914, 0.0745, 0.0915, 0.1850,
        0.1387, 0.1461], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,183][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0367, 0.0229, 0.0206, 0.0508, 0.0568, 0.0678, 0.1740, 0.0849, 0.1592,
        0.2538, 0.0725], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,183][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0441, 0.0409, 0.0640, 0.0826, 0.0883, 0.1339, 0.1385, 0.1166, 0.0927,
        0.0988, 0.0994], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,184][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0373, 0.1049, 0.0179, 0.1195, 0.0322, 0.0392, 0.3853, 0.0481, 0.0701,
        0.1274, 0.0181], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,185][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0100, 0.0042, 0.0020, 0.0160, 0.0077, 0.0131, 0.3183, 0.0291, 0.0749,
        0.5238, 0.0011], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,187][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0020, 0.1086, 0.1193, 0.0864, 0.1011, 0.1138, 0.0883, 0.0968, 0.0989,
        0.0904, 0.0943], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,188][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0503, 0.0810, 0.1013, 0.0965, 0.0881, 0.0801, 0.0749, 0.0961, 0.0960,
        0.1122, 0.1237], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,189][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [,] are: tensor([6.2667e-03, 6.6974e-03, 6.0916e-04, 4.5678e-02, 3.0841e-02, 8.2047e-03,
        2.0448e-01, 1.1466e-02, 4.0177e-02, 6.4260e-01, 2.9758e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,191][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.8759, 0.0080, 0.0111, 0.0079, 0.0016, 0.0261, 0.0056, 0.0107, 0.0309,
        0.0030, 0.0118, 0.0074], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,193][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0128, 0.1134, 0.0929, 0.1021, 0.0626, 0.0916, 0.1159, 0.0696, 0.0772,
        0.0799, 0.0840, 0.0980], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,194][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0871, 0.0637, 0.0817, 0.0845, 0.0731, 0.0902, 0.0821, 0.0885, 0.0921,
        0.0840, 0.0890, 0.0842], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,196][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.0590, 0.0579, 0.0772, 0.0781, 0.0892, 0.1083, 0.0647, 0.0848, 0.1165,
        0.0706, 0.1114, 0.0823], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,198][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0081, 0.0543, 0.0783, 0.0815, 0.0741, 0.0817, 0.0569, 0.0758, 0.1599,
        0.1278, 0.1092, 0.0924], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,198][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0262, 0.0156, 0.0341, 0.0336, 0.0531, 0.0387, 0.1216, 0.0785, 0.1178,
        0.2025, 0.1605, 0.1177], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,198][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0444, 0.0355, 0.0586, 0.0812, 0.0780, 0.1201, 0.1265, 0.1044, 0.0842,
        0.0958, 0.0927, 0.0787], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,199][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0778, 0.0310, 0.0352, 0.0673, 0.0199, 0.0217, 0.3353, 0.0468, 0.0488,
        0.1598, 0.0615, 0.0948], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,199][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0378, 0.0019, 0.0081, 0.0201, 0.0094, 0.0159, 0.1959, 0.0507, 0.0434,
        0.5457, 0.0361, 0.0350], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,200][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0017, 0.0993, 0.1160, 0.0784, 0.0951, 0.1012, 0.0783, 0.0848, 0.0889,
        0.0830, 0.0851, 0.0881], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,200][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0525, 0.0743, 0.0906, 0.0869, 0.0767, 0.0751, 0.0653, 0.0773, 0.0861,
        0.1046, 0.1160, 0.0946], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,200][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0272, 0.0055, 0.0231, 0.0406, 0.0202, 0.0173, 0.1450, 0.2496, 0.0687,
        0.1936, 0.0935, 0.1157], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,201][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.8782, 0.0050, 0.0073, 0.0054, 0.0014, 0.0243, 0.0030, 0.0198, 0.0309,
        0.0017, 0.0066, 0.0062, 0.0102], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,201][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0098, 0.1023, 0.0893, 0.0943, 0.0584, 0.0787, 0.1062, 0.0690, 0.0719,
        0.0787, 0.0814, 0.0933, 0.0666], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,203][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0670, 0.0583, 0.0780, 0.0788, 0.0687, 0.0859, 0.0746, 0.0879, 0.0914,
        0.0797, 0.0852, 0.0812, 0.0632], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,205][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0721, 0.0563, 0.0705, 0.0615, 0.0800, 0.1056, 0.0485, 0.0757, 0.1083,
        0.0513, 0.0914, 0.0735, 0.1053], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,206][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0112, 0.0512, 0.0683, 0.0686, 0.0687, 0.0674, 0.0571, 0.0744, 0.1362,
        0.1048, 0.0973, 0.0827, 0.1120], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,208][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0180, 0.0133, 0.0523, 0.0329, 0.0365, 0.0644, 0.0559, 0.0905, 0.1645,
        0.0910, 0.2369, 0.1087, 0.0351], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,208][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0293, 0.0336, 0.0518, 0.0662, 0.0688, 0.1073, 0.1312, 0.1074, 0.0872,
        0.0933, 0.0868, 0.0643, 0.0727], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,210][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0259, 0.0437, 0.0413, 0.0649, 0.0201, 0.0353, 0.1851, 0.1020, 0.0775,
        0.1026, 0.0539, 0.2324, 0.0153], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,212][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0197, 0.0179, 0.0269, 0.0286, 0.0249, 0.0161, 0.1612, 0.0662, 0.0414,
        0.2575, 0.0512, 0.2730, 0.0154], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,214][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0013, 0.0936, 0.1044, 0.0708, 0.0860, 0.0953, 0.0741, 0.0802, 0.0832,
        0.0745, 0.0790, 0.0836, 0.0741], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,215][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0379, 0.0700, 0.0789, 0.0784, 0.0696, 0.0653, 0.0636, 0.0758, 0.0792,
        0.0973, 0.1063, 0.0877, 0.0900], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,216][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0127, 0.0064, 0.0097, 0.0675, 0.0207, 0.0231, 0.0817, 0.1099, 0.0813,
        0.3017, 0.0566, 0.1764, 0.0523], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,216][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.8552, 0.0070, 0.0081, 0.0122, 0.0017, 0.0306, 0.0066, 0.0056, 0.0187,
        0.0039, 0.0126, 0.0074, 0.0196, 0.0108], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,216][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0117, 0.0954, 0.0732, 0.0874, 0.0617, 0.0783, 0.0996, 0.0593, 0.0680,
        0.0733, 0.0746, 0.0884, 0.0648, 0.0643], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,217][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0654, 0.0546, 0.0731, 0.0716, 0.0637, 0.0777, 0.0694, 0.0815, 0.0828,
        0.0759, 0.0778, 0.0732, 0.0607, 0.0726], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,217][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0642, 0.0548, 0.0617, 0.0633, 0.0806, 0.0834, 0.0438, 0.0670, 0.0815,
        0.0474, 0.0793, 0.0732, 0.1114, 0.0883], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,218][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0080, 0.0303, 0.0482, 0.0535, 0.0529, 0.0618, 0.0478, 0.0647, 0.1386,
        0.0829, 0.0939, 0.0829, 0.1048, 0.1296], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,218][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0352, 0.0093, 0.0084, 0.0374, 0.0257, 0.0388, 0.1123, 0.0487, 0.0874,
        0.1649, 0.1027, 0.0929, 0.1376, 0.0987], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,219][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0301, 0.0353, 0.0486, 0.0621, 0.0636, 0.1015, 0.1071, 0.0978, 0.0809,
        0.0819, 0.0816, 0.0661, 0.0685, 0.0751], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,219][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0375, 0.0390, 0.0140, 0.0822, 0.0142, 0.0179, 0.3088, 0.0336, 0.0469,
        0.1101, 0.0241, 0.2005, 0.0343, 0.0369], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,221][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([4.6255e-03, 9.5515e-04, 1.9770e-04, 5.1047e-03, 1.8992e-03, 2.0931e-03,
        2.9048e-01, 4.6062e-03, 1.5226e-02, 5.9882e-01, 1.1031e-03, 3.3488e-02,
        3.8572e-02, 2.8321e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,222][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0015, 0.0865, 0.0966, 0.0659, 0.0788, 0.0881, 0.0675, 0.0748, 0.0785,
        0.0695, 0.0734, 0.0756, 0.0689, 0.0746], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,224][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0363, 0.0597, 0.0753, 0.0725, 0.0662, 0.0619, 0.0574, 0.0725, 0.0730,
        0.0855, 0.0961, 0.0771, 0.0799, 0.0865], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,225][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([3.2585e-03, 1.6081e-03, 1.1023e-04, 1.7068e-02, 9.1704e-03, 1.8116e-03,
        2.2217e-01, 2.8864e-03, 5.5335e-03, 5.4900e-01, 2.0951e-03, 4.5378e-02,
        1.3383e-01, 6.0795e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,226][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.8606, 0.0030, 0.0075, 0.0049, 0.0015, 0.0168, 0.0029, 0.0068, 0.0226,
        0.0019, 0.0111, 0.0036, 0.0209, 0.0304, 0.0056], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,228][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0094, 0.0918, 0.0721, 0.0823, 0.0547, 0.0705, 0.0935, 0.0571, 0.0614,
        0.0689, 0.0691, 0.0855, 0.0609, 0.0592, 0.0636], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,230][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0531, 0.0515, 0.0712, 0.0672, 0.0600, 0.0717, 0.0643, 0.0775, 0.0776,
        0.0720, 0.0729, 0.0701, 0.0581, 0.0673, 0.0654], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,232][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0581, 0.0436, 0.0605, 0.0553, 0.0656, 0.0823, 0.0412, 0.0586, 0.0853,
        0.0422, 0.0837, 0.0620, 0.0915, 0.0854, 0.0848], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,232][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0089, 0.0337, 0.0443, 0.0495, 0.0615, 0.0594, 0.0439, 0.0693, 0.1345,
        0.0776, 0.0830, 0.0622, 0.0903, 0.1205, 0.0614], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,233][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0251, 0.0068, 0.0216, 0.0117, 0.0182, 0.0399, 0.0488, 0.0683, 0.1248,
        0.1308, 0.1423, 0.0538, 0.0578, 0.1872, 0.0627], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,233][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0228, 0.0286, 0.0440, 0.0616, 0.0607, 0.0963, 0.1116, 0.0885, 0.0737,
        0.0820, 0.0743, 0.0572, 0.0628, 0.0655, 0.0702], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,234][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0525, 0.0366, 0.0236, 0.0911, 0.0164, 0.0175, 0.2214, 0.0324, 0.0445,
        0.0993, 0.0334, 0.1666, 0.0476, 0.0627, 0.0543], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,234][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0107, 0.0044, 0.0056, 0.0097, 0.0030, 0.0079, 0.2105, 0.0352, 0.0580,
        0.4186, 0.0167, 0.0825, 0.0589, 0.0730, 0.0052], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,235][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0011, 0.0813, 0.0901, 0.0617, 0.0748, 0.0838, 0.0642, 0.0697, 0.0728,
        0.0658, 0.0680, 0.0741, 0.0663, 0.0714, 0.0549], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,235][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0357, 0.0630, 0.0697, 0.0683, 0.0615, 0.0578, 0.0494, 0.0631, 0.0657,
        0.0786, 0.0898, 0.0716, 0.0726, 0.0798, 0.0734], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,235][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0207, 0.0061, 0.0024, 0.0428, 0.0230, 0.0140, 0.0971, 0.0430, 0.0700,
        0.1015, 0.0262, 0.1690, 0.1073, 0.2289, 0.0480], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,236][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.6622, 0.0098, 0.0225, 0.0092, 0.0051, 0.0644, 0.0131, 0.0203, 0.0421,
        0.0040, 0.0162, 0.0079, 0.0302, 0.0615, 0.0209, 0.0106],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,237][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0121, 0.0780, 0.0673, 0.0766, 0.0498, 0.0666, 0.0924, 0.0534, 0.0563,
        0.0654, 0.0676, 0.0736, 0.0566, 0.0604, 0.0597, 0.0640],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,239][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0490, 0.0476, 0.0668, 0.0644, 0.0561, 0.0663, 0.0601, 0.0708, 0.0709,
        0.0668, 0.0672, 0.0667, 0.0540, 0.0626, 0.0611, 0.0694],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,240][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0603, 0.0425, 0.0586, 0.0536, 0.0712, 0.0760, 0.0324, 0.0488, 0.0724,
        0.0386, 0.0687, 0.0584, 0.0958, 0.0831, 0.0931, 0.0465],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,242][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0079, 0.0230, 0.0358, 0.0339, 0.0491, 0.0526, 0.0370, 0.0653, 0.1230,
        0.0654, 0.0775, 0.0578, 0.0815, 0.1164, 0.0578, 0.1161],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,243][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0316, 0.0068, 0.0063, 0.0172, 0.0214, 0.0176, 0.0939, 0.0457, 0.0465,
        0.1164, 0.0787, 0.0586, 0.1010, 0.1427, 0.1193, 0.0964],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,245][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0231, 0.0265, 0.0407, 0.0571, 0.0532, 0.0953, 0.1010, 0.0831, 0.0685,
        0.0732, 0.0684, 0.0539, 0.0570, 0.0617, 0.0649, 0.0724],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,247][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0332, 0.0298, 0.0081, 0.0368, 0.0143, 0.0118, 0.2980, 0.0281, 0.0207,
        0.0742, 0.0345, 0.1640, 0.0431, 0.0634, 0.1005, 0.0396],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,248][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0080, 0.0028, 0.0045, 0.0057, 0.0046, 0.0032, 0.1574, 0.0775, 0.0231,
        0.2697, 0.0143, 0.0712, 0.0513, 0.1953, 0.0877, 0.0238],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,250][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0011, 0.0752, 0.0837, 0.0589, 0.0702, 0.0797, 0.0599, 0.0658, 0.0676,
        0.0613, 0.0642, 0.0728, 0.0623, 0.0664, 0.0520, 0.0590],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,250][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0373, 0.0518, 0.0668, 0.0627, 0.0570, 0.0539, 0.0446, 0.0600, 0.0629,
        0.0737, 0.0825, 0.0661, 0.0683, 0.0741, 0.0686, 0.0698],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,250][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0038, 0.0019, 0.0005, 0.0121, 0.0096, 0.0015, 0.1544, 0.0389, 0.0085,
        0.2985, 0.0081, 0.0526, 0.0966, 0.1436, 0.1481, 0.0212],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,251][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.7854, 0.0085, 0.0093, 0.0146, 0.0021, 0.0414, 0.0073, 0.0063, 0.0240,
        0.0043, 0.0148, 0.0090, 0.0211, 0.0116, 0.0153, 0.0148, 0.0100],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,251][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0093, 0.0790, 0.0604, 0.0713, 0.0514, 0.0644, 0.0819, 0.0486, 0.0560,
        0.0605, 0.0618, 0.0730, 0.0535, 0.0530, 0.0588, 0.0632, 0.0540],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,252][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0546, 0.0444, 0.0592, 0.0583, 0.0518, 0.0627, 0.0564, 0.0659, 0.0667,
        0.0610, 0.0633, 0.0593, 0.0492, 0.0592, 0.0586, 0.0667, 0.0625],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,252][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0524, 0.0436, 0.0503, 0.0484, 0.0643, 0.0685, 0.0337, 0.0536, 0.0668,
        0.0376, 0.0633, 0.0568, 0.0879, 0.0728, 0.0865, 0.0470, 0.0666],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,253][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0097, 0.0208, 0.0337, 0.0320, 0.0328, 0.0415, 0.0300, 0.0506, 0.1108,
        0.0587, 0.0636, 0.0525, 0.0645, 0.0892, 0.0443, 0.1040, 0.1613],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,253][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0235, 0.0059, 0.0050, 0.0244, 0.0175, 0.0253, 0.0821, 0.0314, 0.0586,
        0.1120, 0.0688, 0.0618, 0.0933, 0.0696, 0.0762, 0.1672, 0.0772],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,255][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0208, 0.0283, 0.0398, 0.0501, 0.0472, 0.0795, 0.0879, 0.0802, 0.0652,
        0.0675, 0.0648, 0.0514, 0.0548, 0.0621, 0.0606, 0.0652, 0.0747],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,257][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0284, 0.0272, 0.0101, 0.0627, 0.0109, 0.0134, 0.2213, 0.0241, 0.0337,
        0.0849, 0.0178, 0.1433, 0.0239, 0.0255, 0.1393, 0.1043, 0.0293],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,258][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([4.6005e-03, 8.0221e-04, 1.6589e-04, 4.4920e-03, 1.8903e-03, 1.7643e-03,
        2.6015e-01, 3.8443e-03, 1.2910e-02, 5.0645e-01, 1.0375e-03, 2.9813e-02,
        3.3720e-02, 2.7780e-03, 1.9037e-02, 1.1294e-01, 3.5998e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,259][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0012, 0.0712, 0.0811, 0.0553, 0.0659, 0.0734, 0.0567, 0.0620, 0.0651,
        0.0589, 0.0609, 0.0635, 0.0579, 0.0615, 0.0488, 0.0543, 0.0623],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,261][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0316, 0.0506, 0.0601, 0.0580, 0.0546, 0.0512, 0.0442, 0.0575, 0.0592,
        0.0687, 0.0753, 0.0603, 0.0626, 0.0701, 0.0649, 0.0628, 0.0682],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,263][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([2.7292e-03, 1.2146e-03, 7.5259e-05, 1.3970e-02, 7.6017e-03, 1.3825e-03,
        1.8050e-01, 2.5346e-03, 4.3372e-03, 4.5916e-01, 1.6506e-03, 3.6815e-02,
        9.9807e-02, 5.1192e-03, 9.9343e-02, 7.6872e-02, 6.8959e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,308][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:39,308][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,309][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,309][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,309][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,310][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,310][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,310][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,310][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,311][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,311][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,311][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,312][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,313][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.5982, 0.4018], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,314][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.3499, 0.6501], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,316][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.6768, 0.3232], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,317][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.8078, 0.1922], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,319][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.9770, 0.0230], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,320][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.3801, 0.6199], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,322][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.9650, 0.0350], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,324][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.2851, 0.7149], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,325][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.7732, 0.2268], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,326][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.2767, 0.7233], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,326][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.7524, 0.2476], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,326][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.3226, 0.6774], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,327][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.1815, 0.2765, 0.5420], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,327][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0252, 0.0941, 0.8807], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,327][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.7340, 0.1794, 0.0866], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,328][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5648, 0.1903, 0.2449], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,328][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.9889, 0.0078, 0.0033], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,328][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0902, 0.0677, 0.8421], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,329][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.9733, 0.0165, 0.0102], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,329][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0699, 0.1240, 0.8062], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,330][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.4094, 0.1212, 0.4694], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,332][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.3339, 0.2108, 0.4554], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,333][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9439, 0.0488, 0.0073], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,335][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0972, 0.1612, 0.7416], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,336][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.1249, 0.0448, 0.2051, 0.6252], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,338][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.0088, 0.0346, 0.4662, 0.4903], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,340][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.3214, 0.2503, 0.2326, 0.1957], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,341][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.2914, 0.1661, 0.3007, 0.2418], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,343][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.8905, 0.0391, 0.0383, 0.0321], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,343][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.0625, 0.0514, 0.4757, 0.4104], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,343][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.8905, 0.0404, 0.0171, 0.0520], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,344][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0686, 0.0820, 0.4851, 0.3643], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,344][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.2722, 0.0340, 0.2525, 0.4413], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,344][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.0626, 0.3211, 0.5848, 0.0315], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,345][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.6177, 0.2193, 0.0614, 0.1017], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,345][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0327, 0.0247, 0.2505, 0.6921], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,345][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0406, 0.0344, 0.1011, 0.6174, 0.2064], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,346][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0186, 0.0173, 0.2881, 0.2239, 0.4521], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,346][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.3912, 0.2112, 0.1163, 0.1955, 0.0858], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,347][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.2362, 0.2445, 0.1917, 0.2380, 0.0896], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,349][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.9264, 0.0306, 0.0117, 0.0183, 0.0129], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,350][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0552, 0.0411, 0.2666, 0.3170, 0.3201], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,352][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.9497, 0.0175, 0.0045, 0.0134, 0.0150], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,353][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0680, 0.0665, 0.3879, 0.3027, 0.1749], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,355][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.1352, 0.0207, 0.1325, 0.4292, 0.2824], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,356][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.3135, 0.1829, 0.2084, 0.0735, 0.2216], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,358][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.7250, 0.1073, 0.0449, 0.0349, 0.0880], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,359][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0298, 0.0208, 0.1196, 0.5500, 0.2799], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,360][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0948, 0.0170, 0.0198, 0.2667, 0.0992, 0.5026], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,361][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0397, 0.0068, 0.0388, 0.0700, 0.0830, 0.7616], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,361][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4862, 0.1225, 0.0463, 0.0954, 0.0428, 0.2068], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,361][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1856, 0.1110, 0.1417, 0.3064, 0.1692, 0.0862], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,362][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.9678, 0.0075, 0.0024, 0.0040, 0.0045, 0.0138], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,362][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0799, 0.0108, 0.0384, 0.0766, 0.0705, 0.7239], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,362][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.9792, 0.0060, 0.0010, 0.0044, 0.0033, 0.0061], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,363][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0881, 0.0455, 0.0931, 0.1546, 0.0942, 0.5245], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,363][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2185, 0.0057, 0.0108, 0.0954, 0.0652, 0.6044], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,363][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2618, 0.1770, 0.1396, 0.0731, 0.2117, 0.1368], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,364][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.7590, 0.0617, 0.0151, 0.0251, 0.0646, 0.0745], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,365][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0429, 0.0047, 0.0212, 0.1094, 0.1322, 0.6895], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,366][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.2030, 0.0055, 0.0046, 0.0559, 0.0325, 0.1897, 0.5088],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,368][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0440, 0.0028, 0.0086, 0.0210, 0.0427, 0.2268, 0.6541],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,370][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.3896, 0.1118, 0.0804, 0.0768, 0.0603, 0.2011, 0.0800],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,371][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0611, 0.0962, 0.0596, 0.1724, 0.2406, 0.1180, 0.2520],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,373][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.9610, 0.0060, 0.0050, 0.0042, 0.0057, 0.0146, 0.0034],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,374][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.0776, 0.0036, 0.0101, 0.0226, 0.0305, 0.2868, 0.5688],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,376][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.9740, 0.0075, 0.0021, 0.0047, 0.0056, 0.0045, 0.0015],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,377][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0810, 0.0188, 0.0229, 0.0682, 0.0462, 0.1722, 0.5906],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,378][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.4086, 0.0018, 0.0016, 0.0179, 0.0142, 0.1155, 0.4405],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,378][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.3407, 0.2195, 0.0635, 0.0888, 0.1443, 0.1240, 0.0192],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,379][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.5773, 0.0984, 0.0335, 0.0368, 0.1018, 0.1107, 0.0417],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,379][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0578, 0.0017, 0.0034, 0.0298, 0.0424, 0.1434, 0.7215],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,379][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0541, 0.0037, 0.0017, 0.0397, 0.0157, 0.0848, 0.5692, 0.2311],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,380][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0226, 0.0015, 0.0040, 0.0162, 0.0246, 0.1657, 0.4022, 0.3633],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,380][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.4503, 0.1131, 0.0453, 0.0731, 0.0421, 0.1447, 0.0762, 0.0551],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,380][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1429, 0.0818, 0.0621, 0.1898, 0.1170, 0.1233, 0.2348, 0.0483],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,381][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.9736, 0.0061, 0.0013, 0.0033, 0.0031, 0.0062, 0.0014, 0.0051],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,381][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0552, 0.0028, 0.0057, 0.0191, 0.0180, 0.1700, 0.4266, 0.3026],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,382][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([9.8771e-01, 3.3736e-03, 5.9632e-04, 2.0228e-03, 2.0725e-03, 2.9295e-03,
        5.2203e-04, 7.6899e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,384][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0464, 0.0160, 0.0204, 0.0470, 0.0240, 0.1229, 0.4714, 0.2519],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,386][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.1218, 0.0006, 0.0005, 0.0098, 0.0054, 0.0565, 0.4475, 0.3579],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,387][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.2124, 0.1172, 0.0803, 0.0649, 0.1099, 0.1043, 0.1044, 0.2065],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,388][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.7480, 0.0493, 0.0160, 0.0181, 0.0507, 0.0698, 0.0195, 0.0285],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,390][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0177, 0.0008, 0.0008, 0.0140, 0.0128, 0.0627, 0.6198, 0.2715],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,392][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0723, 0.0021, 0.0007, 0.0202, 0.0071, 0.0414, 0.2671, 0.1571, 0.4321],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,393][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0344, 0.0008, 0.0013, 0.0069, 0.0087, 0.0879, 0.2051, 0.2598, 0.3951],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,395][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.5592, 0.0769, 0.0272, 0.0492, 0.0240, 0.0974, 0.0451, 0.0341, 0.0869],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,395][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1289, 0.0466, 0.0398, 0.1307, 0.0993, 0.0571, 0.2010, 0.0934, 0.2033],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,396][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.8388e-01, 2.8790e-03, 6.4047e-04, 1.2332e-03, 1.6423e-03, 3.0117e-03,
        5.9144e-04, 3.0142e-03, 3.1050e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,396][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0465, 0.0012, 0.0018, 0.0066, 0.0068, 0.0659, 0.1738, 0.1343, 0.5632],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,397][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([9.8533e-01, 3.9611e-03, 5.2033e-04, 1.9866e-03, 2.2651e-03, 3.0443e-03,
        5.3235e-04, 5.3108e-04, 1.8298e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,397][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0630, 0.0103, 0.0083, 0.0261, 0.0142, 0.0638, 0.2551, 0.1706, 0.3885],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,397][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([1.4477e-01, 2.6774e-04, 1.5116e-04, 2.7816e-03, 1.9678e-03, 1.9679e-02,
        1.5568e-01, 2.3507e-01, 4.3963e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,398][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.3435, 0.1537, 0.0318, 0.0575, 0.0775, 0.0544, 0.0564, 0.1580, 0.0673],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,398][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.7573, 0.0485, 0.0083, 0.0147, 0.0481, 0.0474, 0.0203, 0.0210, 0.0343],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,398][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([1.7749e-02, 2.4627e-04, 2.4781e-04, 4.4895e-03, 4.4954e-03, 2.1968e-02,
        2.3889e-01, 1.9096e-01, 5.2095e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,399][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0564, 0.0018, 0.0008, 0.0153, 0.0067, 0.0508, 0.1217, 0.1229, 0.4192,
        0.2043], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,401][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0105, 0.0004, 0.0010, 0.0034, 0.0073, 0.0359, 0.1520, 0.1454, 0.2836,
        0.3604], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,403][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.2726, 0.0790, 0.0508, 0.0625, 0.0358, 0.1376, 0.0788, 0.0587, 0.1341,
        0.0901], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,404][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0577, 0.0479, 0.0418, 0.0603, 0.0897, 0.0638, 0.2255, 0.0960, 0.1875,
        0.1298], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,406][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.9533, 0.0043, 0.0015, 0.0024, 0.0036, 0.0101, 0.0011, 0.0083, 0.0101,
        0.0052], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,407][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0314, 0.0010, 0.0019, 0.0064, 0.0071, 0.0604, 0.1372, 0.1039, 0.4068,
        0.2439], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,409][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.9672, 0.0068, 0.0020, 0.0043, 0.0048, 0.0072, 0.0014, 0.0014, 0.0034,
        0.0015], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,411][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0202, 0.0074, 0.0080, 0.0196, 0.0187, 0.0678, 0.1594, 0.1396, 0.3083,
        0.2510], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,412][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([7.5597e-02, 2.0539e-04, 1.2500e-04, 2.4535e-03, 1.6130e-03, 1.3513e-02,
        1.3773e-01, 8.7816e-02, 4.7360e-01, 2.0734e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,412][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.2254, 0.1347, 0.0622, 0.0895, 0.1244, 0.0615, 0.0303, 0.1414, 0.0919,
        0.0387], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,413][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.5071, 0.0624, 0.0192, 0.0240, 0.0581, 0.0784, 0.0242, 0.0372, 0.0732,
        0.1161], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,413][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([1.5167e-02, 2.0285e-04, 2.5441e-04, 3.5844e-03, 3.8487e-03, 1.8826e-02,
        1.2734e-01, 1.0919e-01, 3.3147e-01, 3.9012e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,414][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([4.4418e-02, 7.1204e-04, 2.1658e-04, 6.5232e-03, 2.8027e-03, 1.8408e-02,
        1.0300e-01, 5.4638e-02, 1.9036e-01, 1.9205e-01, 3.8687e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,414][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([1.1502e-02, 2.4031e-04, 3.2735e-04, 2.2230e-03, 3.0531e-03, 3.9203e-02,
        7.5585e-02, 7.1230e-02, 1.6139e-01, 2.6390e-01, 3.7135e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,414][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.4876, 0.0646, 0.0240, 0.0402, 0.0203, 0.0751, 0.0397, 0.0304, 0.0623,
        0.0457, 0.1102], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,415][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0378, 0.0233, 0.0219, 0.0816, 0.0871, 0.0701, 0.1270, 0.0653, 0.2741,
        0.1721, 0.0397], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,415][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([9.7254e-01, 2.8875e-03, 6.5167e-04, 1.4827e-03, 1.7324e-03, 3.3318e-03,
        5.7782e-04, 2.9881e-03, 2.8314e-03, 2.3195e-03, 8.6616e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,416][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.6948e-02, 3.8331e-04, 5.5063e-04, 1.9127e-03, 1.9453e-03, 2.6589e-02,
        5.1887e-02, 4.1096e-02, 1.8327e-01, 1.2833e-01, 5.4709e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,416][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([9.5988e-01, 6.7660e-03, 1.3050e-03, 3.5143e-03, 4.7413e-03, 5.8836e-03,
        9.5710e-04, 1.3887e-03, 3.1474e-03, 1.0895e-03, 1.1325e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,418][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0155, 0.0032, 0.0021, 0.0082, 0.0047, 0.0290, 0.1111, 0.0755, 0.2003,
        0.1720, 0.3784], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,419][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([8.5486e-02, 1.1983e-04, 5.1762e-05, 1.1984e-03, 7.9158e-04, 8.9932e-03,
        8.3538e-02, 8.5671e-02, 2.3285e-01, 2.6235e-01, 2.3895e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,420][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.1789, 0.1597, 0.0246, 0.1085, 0.0562, 0.0743, 0.0387, 0.1029, 0.0750,
        0.1093, 0.0719], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,422][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.7390, 0.0354, 0.0088, 0.0149, 0.0331, 0.0421, 0.0134, 0.0182, 0.0292,
        0.0409, 0.0250], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,423][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([7.1774e-03, 5.5098e-05, 3.6669e-05, 7.2159e-04, 9.0780e-04, 6.6852e-03,
        4.4809e-02, 3.4173e-02, 1.3865e-01, 1.6576e-01, 6.0102e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,424][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([4.8616e-02, 6.8305e-04, 3.5425e-04, 6.0729e-03, 2.2646e-03, 8.8078e-03,
        3.9327e-02, 2.1984e-02, 1.0971e-01, 8.8661e-02, 2.3922e-01, 4.3430e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,426][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0105, 0.0007, 0.0018, 0.0091, 0.0075, 0.0472, 0.0406, 0.0387, 0.1034,
        0.1443, 0.3640, 0.2322], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,427][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.2592, 0.0568, 0.0248, 0.0518, 0.0247, 0.0912, 0.0644, 0.0397, 0.0773,
        0.0751, 0.1248, 0.1102], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,429][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.0540, 0.0081, 0.0367, 0.0379, 0.0369, 0.0441, 0.0852, 0.0856, 0.1559,
        0.2856, 0.1402, 0.0299], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,430][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.8981, 0.0067, 0.0022, 0.0051, 0.0049, 0.0117, 0.0040, 0.0086, 0.0116,
        0.0085, 0.0225, 0.0163], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,430][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0174, 0.0008, 0.0013, 0.0048, 0.0041, 0.0226, 0.0613, 0.0396, 0.1331,
        0.1192, 0.4272, 0.1686], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,430][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.9332, 0.0109, 0.0020, 0.0093, 0.0046, 0.0083, 0.0014, 0.0014, 0.0022,
        0.0016, 0.0056, 0.0195], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,431][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0214, 0.0034, 0.0030, 0.0116, 0.0080, 0.0241, 0.1152, 0.0574, 0.1234,
        0.1741, 0.2832, 0.1752], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,431][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([1.5642e-01, 1.1259e-04, 7.0581e-05, 1.7698e-03, 1.2998e-03, 7.3561e-03,
        7.0633e-02, 3.6329e-02, 8.8757e-02, 1.7604e-01, 1.6996e-01, 2.9126e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,432][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0102, 0.0728, 0.4150, 0.0539, 0.1319, 0.0438, 0.0202, 0.0527, 0.0542,
        0.0721, 0.0623, 0.0108], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,432][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.4461, 0.0701, 0.0176, 0.0305, 0.0631, 0.0652, 0.0251, 0.0227, 0.0548,
        0.0980, 0.0568, 0.0501], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,432][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([1.1694e-02, 1.0940e-04, 1.4219e-04, 1.7600e-03, 1.7216e-03, 7.4663e-03,
        3.2185e-02, 4.2640e-02, 8.0792e-02, 7.0333e-02, 4.9638e-01, 2.5477e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,433][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([1.1796e-02, 3.9653e-04, 1.7008e-04, 3.5955e-03, 1.2141e-03, 8.2326e-03,
        3.4063e-02, 1.9938e-02, 6.3134e-02, 5.5664e-02, 1.7972e-01, 2.6754e-01,
        3.5453e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,433][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([5.1303e-03, 1.8307e-04, 3.4853e-04, 1.1532e-03, 2.3752e-03, 1.4671e-02,
        3.8973e-02, 3.1826e-02, 8.1423e-02, 1.3303e-01, 2.3204e-01, 9.4870e-02,
        3.6398e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,435][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.3161, 0.0561, 0.0246, 0.0448, 0.0206, 0.0847, 0.0421, 0.0319, 0.0675,
        0.0377, 0.0988, 0.1251, 0.0500], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,437][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0311, 0.0210, 0.0369, 0.0358, 0.0331, 0.0738, 0.1660, 0.0753, 0.2762,
        0.0643, 0.0746, 0.0721, 0.0399], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,438][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.8953, 0.0065, 0.0023, 0.0031, 0.0067, 0.0124, 0.0013, 0.0099, 0.0090,
        0.0076, 0.0234, 0.0147, 0.0080], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,440][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0131, 0.0005, 0.0006, 0.0025, 0.0019, 0.0187, 0.0346, 0.0320, 0.1233,
        0.0674, 0.4214, 0.1358, 0.1480], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,442][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.9384, 0.0064, 0.0016, 0.0039, 0.0042, 0.0070, 0.0012, 0.0012, 0.0030,
        0.0015, 0.0072, 0.0115, 0.0130], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,444][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0098, 0.0029, 0.0026, 0.0080, 0.0042, 0.0231, 0.0730, 0.0573, 0.1210,
        0.1088, 0.2681, 0.2018, 0.1193], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,445][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([4.6026e-02, 1.4633e-04, 5.1396e-05, 1.0429e-03, 5.2451e-04, 3.9450e-03,
        3.3600e-02, 2.0588e-02, 5.6708e-02, 8.1947e-02, 1.2368e-01, 4.1504e-01,
        2.1670e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,446][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0419, 0.0678, 0.1425, 0.0385, 0.1043, 0.0626, 0.0860, 0.1366, 0.0800,
        0.0879, 0.0965, 0.0369, 0.0184], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,447][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.4043, 0.0622, 0.0147, 0.0240, 0.0464, 0.0498, 0.0275, 0.0316, 0.0463,
        0.1264, 0.0598, 0.0573, 0.0496], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,447][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([9.3381e-03, 6.9621e-05, 5.2704e-05, 1.0818e-03, 6.3023e-04, 4.8328e-03,
        2.3764e-02, 1.5414e-02, 5.9611e-02, 5.9970e-02, 2.9543e-01, 2.4509e-01,
        2.8471e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,447][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.7355e-02, 1.8596e-04, 2.8697e-05, 1.4534e-03, 6.3664e-04, 2.7060e-03,
        1.9026e-02, 7.8946e-03, 3.3486e-02, 3.0503e-02, 6.8304e-02, 2.3680e-01,
        3.8232e-01, 1.9930e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,448][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([7.0234e-03, 7.9444e-05, 9.2053e-05, 6.6679e-04, 1.1552e-03, 7.6012e-03,
        1.6980e-02, 1.5154e-02, 3.9211e-02, 7.7234e-02, 1.4580e-01, 1.1091e-01,
        3.3109e-01, 2.4700e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,448][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.4250, 0.0478, 0.0160, 0.0341, 0.0153, 0.0600, 0.0280, 0.0219, 0.0474,
        0.0356, 0.0849, 0.0823, 0.0492, 0.0525], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,449][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0676, 0.0140, 0.0138, 0.0465, 0.0404, 0.0355, 0.1335, 0.0797, 0.1850,
        0.1080, 0.0570, 0.0473, 0.1140, 0.0576], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,449][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.6560e-01, 2.7717e-03, 4.4752e-04, 1.0323e-03, 1.5893e-03, 3.0051e-03,
        5.4545e-04, 2.4522e-03, 2.6742e-03, 1.9937e-03, 6.1067e-03, 5.3302e-03,
        2.4107e-03, 4.0390e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,450][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.0079e-02, 2.3020e-04, 1.9153e-04, 1.2288e-03, 1.0788e-03, 1.0444e-02,
        2.3299e-02, 1.8588e-02, 7.8678e-02, 5.0876e-02, 2.8736e-01, 9.6782e-02,
        1.6077e-01, 2.5039e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,450][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([9.7961e-01, 2.9233e-03, 4.3814e-04, 1.6597e-03, 1.7017e-03, 2.1424e-03,
        3.4095e-04, 3.8567e-04, 8.9454e-04, 3.1009e-04, 2.2140e-03, 3.7948e-03,
        2.6986e-03, 8.8818e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,451][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0158, 0.0017, 0.0011, 0.0052, 0.0026, 0.0120, 0.0521, 0.0314, 0.0833,
        0.0783, 0.1932, 0.1422, 0.1300, 0.2512], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,452][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([4.1376e-02, 2.4589e-05, 5.5981e-06, 2.2341e-04, 1.2846e-04, 1.1107e-03,
        1.5788e-02, 1.0983e-02, 3.2172e-02, 4.0297e-02, 3.8979e-02, 2.0425e-01,
        3.0962e-01, 3.0504e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,454][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1721, 0.0782, 0.0487, 0.0332, 0.0785, 0.0452, 0.0286, 0.0848, 0.1048,
        0.0613, 0.1230, 0.0415, 0.0484, 0.0519], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,456][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7403, 0.0297, 0.0032, 0.0114, 0.0271, 0.0218, 0.0102, 0.0102, 0.0159,
        0.0396, 0.0188, 0.0267, 0.0228, 0.0222], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,457][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([6.3490e-03, 2.3532e-05, 6.8503e-06, 3.2247e-04, 2.9149e-04, 1.5676e-03,
        1.3916e-02, 8.0389e-03, 3.0849e-02, 3.8844e-02, 1.6772e-01, 1.7117e-01,
        3.2338e-01, 2.3752e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,458][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([4.2577e-03, 1.6128e-04, 6.1668e-05, 1.0472e-03, 5.1084e-04, 2.8516e-03,
        1.1949e-02, 5.0232e-03, 2.9219e-02, 2.2610e-02, 6.8439e-02, 1.0338e-01,
        3.0987e-01, 2.4065e-01, 1.9997e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,459][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([2.5636e-03, 1.1238e-04, 1.4898e-04, 6.0182e-04, 1.0722e-03, 8.7255e-03,
        1.5064e-02, 1.5123e-02, 4.5449e-02, 7.0090e-02, 1.4865e-01, 6.3152e-02,
        2.2700e-01, 2.2635e-01, 1.7590e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,461][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.1857, 0.0558, 0.0201, 0.0388, 0.0211, 0.0803, 0.0354, 0.0321, 0.0664,
        0.0483, 0.1044, 0.1225, 0.0705, 0.0629, 0.0556], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,462][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0680, 0.0263, 0.0327, 0.0429, 0.0445, 0.0362, 0.1772, 0.0578, 0.1414,
        0.1006, 0.0503, 0.0788, 0.0771, 0.0462, 0.0201], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,464][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.8992, 0.0060, 0.0017, 0.0025, 0.0045, 0.0099, 0.0014, 0.0076, 0.0087,
        0.0049, 0.0162, 0.0105, 0.0073, 0.0144, 0.0051], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,464][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([6.2578e-03, 2.0798e-04, 2.9543e-04, 1.0688e-03, 1.0676e-03, 1.1524e-02,
        2.0601e-02, 1.8226e-02, 7.6747e-02, 5.2830e-02, 2.6277e-01, 6.2782e-02,
        1.1908e-01, 2.2482e-01, 1.4173e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,464][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([9.5218e-01, 4.9875e-03, 7.0116e-04, 2.9409e-03, 3.2869e-03, 4.0293e-03,
        8.8395e-04, 6.6902e-04, 1.8549e-03, 7.6101e-04, 3.7396e-03, 7.1492e-03,
        7.0970e-03, 1.5277e-03, 8.1884e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,465][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0073, 0.0017, 0.0015, 0.0053, 0.0026, 0.0120, 0.0407, 0.0248, 0.0588,
        0.0715, 0.1385, 0.1147, 0.1238, 0.2457, 0.1512], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,465][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([1.9611e-02, 3.9355e-05, 1.9324e-05, 3.4117e-04, 1.8878e-04, 1.9031e-03,
        1.4707e-02, 1.1029e-02, 3.9450e-02, 3.8777e-02, 5.9058e-02, 1.2397e-01,
        2.2233e-01, 3.0415e-01, 1.6443e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,466][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0357, 0.0483, 0.0394, 0.0383, 0.0388, 0.0464, 0.0302, 0.0718, 0.0814,
        0.1121, 0.1147, 0.0853, 0.0831, 0.1411, 0.0334], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,466][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.4076, 0.0584, 0.0097, 0.0243, 0.0443, 0.0480, 0.0157, 0.0196, 0.0354,
        0.0752, 0.0492, 0.0525, 0.0466, 0.0544, 0.0591], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,467][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([3.2553e-03, 3.0112e-05, 1.8684e-05, 4.4975e-04, 3.2172e-04, 2.3267e-03,
        1.2337e-02, 7.3289e-03, 3.0906e-02, 2.5623e-02, 1.4741e-01, 1.0745e-01,
        1.8446e-01, 3.0039e-01, 1.7771e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,467][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([7.9993e-03, 1.4954e-04, 3.2198e-05, 7.4866e-04, 5.3786e-04, 1.3001e-03,
        1.0133e-02, 5.3388e-03, 1.8281e-02, 1.5385e-02, 4.1538e-02, 1.1663e-01,
        2.3333e-01, 1.2922e-01, 2.8271e-01, 1.3666e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,468][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([3.4672e-03, 7.2514e-05, 8.7096e-05, 5.9750e-04, 7.3035e-04, 8.3769e-03,
        1.0057e-02, 1.4655e-02, 2.3795e-02, 5.9852e-02, 8.1309e-02, 5.4867e-02,
        2.0759e-01, 1.8900e-01, 1.7092e-01, 1.7463e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,470][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.2610, 0.0413, 0.0201, 0.0324, 0.0184, 0.0677, 0.0275, 0.0296, 0.0511,
        0.0390, 0.0872, 0.0916, 0.0538, 0.0551, 0.0457, 0.0786],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,471][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0321, 0.0136, 0.0184, 0.0340, 0.0409, 0.0511, 0.0670, 0.0463, 0.1441,
        0.0921, 0.0784, 0.0414, 0.0727, 0.1288, 0.0551, 0.0841],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,472][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([9.5111e-01, 2.4150e-03, 5.6496e-04, 1.1157e-03, 1.1585e-03, 2.2176e-03,
        4.6626e-04, 2.2607e-03, 2.4795e-03, 1.9218e-03, 6.3623e-03, 4.0302e-03,
        2.2655e-03, 3.6440e-03, 2.0555e-03, 1.5933e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,473][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([1.4161e-02, 2.2890e-04, 2.5301e-04, 1.1701e-03, 1.0032e-03, 8.1009e-03,
        1.8514e-02, 1.2303e-02, 4.4311e-02, 3.6834e-02, 1.3923e-01, 5.3751e-02,
        9.3986e-02, 1.6634e-01, 1.3729e-01, 2.7252e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,474][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([9.6174e-01, 2.8889e-03, 3.9642e-04, 1.8485e-03, 1.4854e-03, 2.7119e-03,
        4.8359e-04, 5.0466e-04, 1.1293e-03, 4.2033e-04, 3.1205e-03, 4.9404e-03,
        3.0059e-03, 9.1277e-04, 3.2379e-03, 1.1173e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,476][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0079, 0.0011, 0.0008, 0.0026, 0.0019, 0.0078, 0.0278, 0.0163, 0.0425,
        0.0474, 0.1168, 0.0739, 0.1008, 0.1890, 0.1415, 0.2219],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,477][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([1.3006e-02, 1.9747e-05, 6.1435e-06, 1.4862e-04, 1.1331e-04, 6.8221e-04,
        7.1820e-03, 5.9106e-03, 1.6381e-02, 1.4427e-02, 2.3951e-02, 6.7634e-02,
        1.3994e-01, 2.0254e-01, 2.4531e-01, 2.6275e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,479][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0285, 0.0596, 0.0439, 0.0262, 0.0429, 0.0515, 0.0227, 0.0803, 0.0508,
        0.0436, 0.1103, 0.1398, 0.0401, 0.1118, 0.0725, 0.0754],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,481][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.4835, 0.0278, 0.0061, 0.0164, 0.0352, 0.0388, 0.0102, 0.0159, 0.0310,
        0.0646, 0.0345, 0.0386, 0.0456, 0.0380, 0.0465, 0.0674],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,481][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([3.6542e-03, 1.5041e-05, 8.3575e-06, 2.0480e-04, 2.3434e-04, 1.3446e-03,
        8.5510e-03, 8.9391e-03, 2.2056e-02, 2.2991e-02, 1.1388e-01, 6.5278e-02,
        1.1387e-01, 1.7128e-01, 1.5984e-01, 3.0786e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,482][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.1797e-02, 8.5824e-05, 1.2304e-05, 6.0670e-04, 3.0971e-04, 1.1812e-03,
        8.8399e-03, 3.6074e-03, 1.4608e-02, 1.3814e-02, 2.7612e-02, 1.0051e-01,
        1.7944e-01, 8.9559e-02, 2.0160e-01, 2.0878e-01, 1.3764e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,482][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([4.7957e-03, 4.8777e-05, 4.1354e-05, 4.0957e-04, 5.6467e-04, 3.7922e-03,
        8.3002e-03, 7.0981e-03, 1.7826e-02, 3.8425e-02, 6.7145e-02, 6.0791e-02,
        1.5490e-01, 1.2333e-01, 1.4206e-01, 1.8910e-01, 1.8138e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,483][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.3638, 0.0400, 0.0130, 0.0280, 0.0130, 0.0525, 0.0225, 0.0195, 0.0399,
        0.0310, 0.0715, 0.0723, 0.0419, 0.0452, 0.0350, 0.0699, 0.0410],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,483][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0549, 0.0112, 0.0103, 0.0370, 0.0334, 0.0274, 0.1084, 0.0600, 0.1420,
        0.0882, 0.0442, 0.0389, 0.0891, 0.0452, 0.0559, 0.1046, 0.0491],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,483][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.4790e-01, 2.4753e-03, 4.6470e-04, 9.1735e-04, 1.4089e-03, 2.4849e-03,
        4.7281e-04, 2.1366e-03, 2.3281e-03, 1.7543e-03, 5.5249e-03, 4.4770e-03,
        2.1327e-03, 3.7459e-03, 2.2328e-03, 1.5471e-02, 4.0724e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,484][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.1992e-02, 1.1738e-04, 8.8962e-05, 5.7548e-04, 5.5910e-04, 4.8769e-03,
        1.1952e-02, 8.2707e-03, 3.6563e-02, 2.4620e-02, 1.2837e-01, 4.5157e-02,
        7.9062e-02, 1.2114e-01, 1.1070e-01, 2.4785e-01, 1.6810e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,484][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([9.7443e-01, 2.4935e-03, 3.5113e-04, 1.3786e-03, 1.5486e-03, 1.7821e-03,
        3.0633e-04, 3.2275e-04, 7.6786e-04, 2.8846e-04, 1.8338e-03, 3.1755e-03,
        2.3373e-03, 7.6287e-04, 2.4194e-03, 4.7234e-03, 1.0808e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,485][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0105, 0.0009, 0.0006, 0.0028, 0.0015, 0.0063, 0.0264, 0.0154, 0.0409,
        0.0417, 0.0972, 0.0696, 0.0654, 0.1194, 0.1280, 0.2188, 0.1546],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,487][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.9964e-02, 8.3893e-06, 1.7426e-06, 7.0147e-05, 4.5833e-05, 3.2592e-04,
        4.8625e-03, 3.0767e-03, 9.1174e-03, 1.1239e-02, 1.1309e-02, 6.0959e-02,
        9.3680e-02, 9.4715e-02, 1.3877e-01, 3.4700e-01, 2.0486e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,488][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0926, 0.0565, 0.0390, 0.0238, 0.0589, 0.0355, 0.0230, 0.0691, 0.0816,
        0.0515, 0.1026, 0.0320, 0.0360, 0.0473, 0.0927, 0.0721, 0.0858],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,490][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.6679, 0.0284, 0.0029, 0.0113, 0.0261, 0.0202, 0.0098, 0.0094, 0.0142,
        0.0419, 0.0167, 0.0246, 0.0223, 0.0210, 0.0292, 0.0309, 0.0232],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,491][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([3.0947e-03, 9.0979e-06, 2.2445e-06, 1.1997e-04, 1.1385e-04, 5.5939e-04,
        4.9506e-03, 2.9690e-03, 1.0517e-02, 1.4179e-02, 5.6300e-02, 6.1687e-02,
        1.1431e-01, 8.6749e-02, 1.5023e-01, 3.3717e-01, 1.5703e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,492][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:39,495][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[13101],
        [ 3492],
        [ 6642],
        [ 7793],
        [ 1047],
        [ 1479],
        [ 1063],
        [ 2767],
        [ 1632],
        [  472],
        [  971],
        [ 1922],
        [  288],
        [ 1575],
        [  175],
        [  420],
        [  797]], device='cuda:0')
[2024-07-24 10:17:39,496][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[13631],
        [ 4820],
        [ 5237],
        [10846],
        [  458],
        [ 1751],
        [  407],
        [ 1371],
        [  798],
        [  208],
        [  285],
        [ 2081],
        [   62],
        [  715],
        [  102],
        [  452],
        [  357]], device='cuda:0')
[2024-07-24 10:17:39,498][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[22956],
        [23489],
        [23502],
        [24902],
        [24178],
        [24113],
        [28368],
        [25228],
        [27574],
        [34375],
        [25180],
        [29291],
        [29674],
        [31143],
        [31343],
        [36876],
        [34177]], device='cuda:0')
[2024-07-24 10:17:39,499][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[20766],
        [21030],
        [26574],
        [21269],
        [24159],
        [26308],
        [27197],
        [27836],
        [28280],
        [30160],
        [30416],
        [29428],
        [28498],
        [28262],
        [27729],
        [27752],
        [27541]], device='cuda:0')
[2024-07-24 10:17:39,500][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[17698],
        [27988],
        [30766],
        [32512],
        [33605],
        [34595],
        [34688],
        [34299],
        [33835],
        [34274],
        [34687],
        [34353],
        [34222],
        [34132],
        [34164],
        [34009],
        [33980]], device='cuda:0')
[2024-07-24 10:17:39,501][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[4118],
        [5696],
        [9019],
        [7488],
        [6668],
        [6306],
        [5811],
        [6123],
        [6534],
        [6795],
        [6865],
        [6831],
        [6306],
        [5834],
        [5541],
        [5229],
        [5111]], device='cuda:0')
[2024-07-24 10:17:39,502][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[38674],
        [22705],
        [13437],
        [13199],
        [11215],
        [11358],
        [11787],
        [12337],
        [11724],
        [10957],
        [10694],
        [10605],
        [10627],
        [ 9517],
        [ 9010],
        [ 8128],
        [ 7051]], device='cuda:0')
[2024-07-24 10:17:39,504][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[19106],
        [21488],
        [13802],
        [ 9617],
        [12544],
        [12684],
        [ 9781],
        [ 8586],
        [10394],
        [12119],
        [10950],
        [12307],
        [13762],
        [12947],
        [14202],
        [15977],
        [15705]], device='cuda:0')
[2024-07-24 10:17:39,506][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[12563],
        [11981],
        [12685],
        [ 9962],
        [ 8667],
        [ 7072],
        [ 7105],
        [ 7282],
        [ 7059],
        [ 7369],
        [ 7395],
        [ 7358],
        [ 7354],
        [ 7519],
        [ 7366],
        [ 7245],
        [ 7724]], device='cuda:0')
[2024-07-24 10:17:39,507][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 6148],
        [21938],
        [27545],
        [40173],
        [40296],
        [42531],
        [48663],
        [48833],
        [48622],
        [46015],
        [46799],
        [45058],
        [39700],
        [44063],
        [41485],
        [42036],
        [38367]], device='cuda:0')
[2024-07-24 10:17:39,509][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 8846],
        [ 8500],
        [12506],
        [32869],
        [30501],
        [27715],
        [ 8883],
        [ 2761],
        [ 3776],
        [ 5991],
        [ 8590],
        [11903],
        [11998],
        [ 8720],
        [10405],
        [18007],
        [ 9726]], device='cuda:0')
[2024-07-24 10:17:39,511][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[22453],
        [24449],
        [20511],
        [29248],
        [26348],
        [25476],
        [25956],
        [26157],
        [26847],
        [26666],
        [27455],
        [27783],
        [27112],
        [27030],
        [26679],
        [26563],
        [26445]], device='cuda:0')
[2024-07-24 10:17:39,512][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[20749],
        [28418],
        [27991],
        [30174],
        [29042],
        [27803],
        [27273],
        [28002],
        [27912],
        [27650],
        [28572],
        [29460],
        [29299],
        [29173],
        [28907],
        [28766],
        [28448]], device='cuda:0')
[2024-07-24 10:17:39,514][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[28576],
        [18413],
        [16689],
        [19240],
        [17213],
        [17101],
        [21099],
        [26553],
        [27638],
        [27786],
        [40826],
        [29691],
        [33812],
        [42230],
        [28607],
        [32311],
        [37060]], device='cuda:0')
[2024-07-24 10:17:39,516][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 9911],
        [ 7378],
        [19456],
        [10802],
        [29228],
        [23247],
        [16713],
        [31792],
        [29811],
        [25675],
        [30611],
        [11097],
        [25335],
        [34245],
        [27150],
        [30880],
        [40668]], device='cuda:0')
[2024-07-24 10:17:39,517][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[27158],
        [20073],
        [ 9918],
        [ 7192],
        [ 5727],
        [ 6231],
        [14566],
        [15822],
        [11128],
        [ 7017],
        [ 4879],
        [ 4619],
        [ 4329],
        [ 4099],
        [ 3987],
        [ 3187],
        [ 2836]], device='cuda:0')
[2024-07-24 10:17:39,518][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[20263],
        [ 5593],
        [ 5542],
        [ 4435],
        [ 4066],
        [11306],
        [ 8204],
        [ 6721],
        [ 4061],
        [ 2871],
        [ 3184],
        [ 3544],
        [ 5058],
        [ 8031],
        [ 6467],
        [ 6069],
        [ 7296]], device='cuda:0')
[2024-07-24 10:17:39,519][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[8979],
        [3484],
        [3843],
        [3209],
        [4293],
        [4103],
        [5116],
        [4877],
        [4569],
        [4133],
        [4737],
        [4382],
        [3981],
        [3580],
        [3598],
        [3153],
        [3017]], device='cuda:0')
[2024-07-24 10:17:39,520][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[10897],
        [ 3062],
        [ 5385],
        [ 6687],
        [ 5229],
        [ 8456],
        [18122],
        [18870],
        [16614],
        [20039],
        [16307],
        [16729],
        [13809],
        [16171],
        [16634],
        [12184],
        [14020]], device='cuda:0')
[2024-07-24 10:17:39,522][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[23506],
        [22496],
        [23093],
        [18953],
        [19818],
        [22476],
        [22182],
        [22409],
        [22849],
        [21750],
        [22154],
        [19084],
        [18485],
        [21575],
        [17827],
        [21423],
        [21077]], device='cuda:0')
[2024-07-24 10:17:39,523][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 9352],
        [ 8609],
        [25425],
        [25887],
        [18343],
        [17966],
        [24958],
        [30382],
        [35602],
        [34367],
        [39089],
        [39301],
        [38358],
        [33728],
        [32929],
        [30055],
        [28420]], device='cuda:0')
[2024-07-24 10:17:39,525][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[43808],
        [40924],
        [42240],
        [33077],
        [40054],
        [42768],
        [42183],
        [43214],
        [43007],
        [41181],
        [40097],
        [37431],
        [37446],
        [42522],
        [39736],
        [40709],
        [42049]], device='cuda:0')
[2024-07-24 10:17:39,526][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[34681],
        [ 4645],
        [11236],
        [ 5873],
        [ 7255],
        [12041],
        [15335],
        [14872],
        [17260],
        [16185],
        [15165],
        [13536],
        [13426],
        [12720],
        [13977],
        [15230],
        [14350]], device='cuda:0')
[2024-07-24 10:17:39,528][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[41704],
        [47425],
        [45443],
        [38631],
        [35391],
        [34630],
        [46933],
        [43632],
        [40917],
        [41431],
        [43816],
        [46773],
        [45802],
        [41555],
        [40556],
        [37839],
        [36341]], device='cuda:0')
[2024-07-24 10:17:39,530][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[19119],
        [ 3655],
        [ 2232],
        [ 2158],
        [ 2276],
        [ 1950],
        [ 2369],
        [  903],
        [ 1132],
        [ 1032],
        [ 1104],
        [ 1103],
        [  644],
        [  666],
        [  474],
        [  401],
        [  359]], device='cuda:0')
[2024-07-24 10:17:39,532][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[20518],
        [12169],
        [18817],
        [11347],
        [17468],
        [11903],
        [10257],
        [10970],
        [ 9752],
        [ 6694],
        [ 8038],
        [ 6132],
        [ 6473],
        [ 8500],
        [ 7336],
        [ 7632],
        [ 7863]], device='cuda:0')
[2024-07-24 10:17:39,533][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[15062],
        [25564],
        [30046],
        [31228],
        [29664],
        [30004],
        [25851],
        [24313],
        [27703],
        [27222],
        [26053],
        [25394],
        [10029],
        [ 6858],
        [12459],
        [14969],
        [13347]], device='cuda:0')
[2024-07-24 10:17:39,534][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[24314],
        [41496],
        [35673],
        [43253],
        [43510],
        [39441],
        [31816],
        [33117],
        [34072],
        [37281],
        [35279],
        [34605],
        [40346],
        [42053],
        [42298],
        [42493],
        [42926]], device='cuda:0')
[2024-07-24 10:17:39,535][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[29257],
        [47695],
        [44529],
        [42604],
        [42010],
        [30043],
        [37303],
        [27342],
        [25658],
        [28647],
        [30452],
        [39045],
        [39022],
        [36427],
        [37439],
        [38089],
        [39167]], device='cuda:0')
[2024-07-24 10:17:39,536][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314],
        [4314]], device='cuda:0')
[2024-07-24 10:17:39,583][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:39,584][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,585][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,585][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,585][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,586][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,586][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,586][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,587][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,587][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,587][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,587][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,588][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,588][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9704, 0.0296], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,588][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0700, 0.9300], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,589][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([5.3942e-04, 9.9946e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,590][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.3672, 0.6328], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,591][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.2598, 0.7402], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,591][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.7448, 0.2552], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,591][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.4862, 0.5138], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,591][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.3935, 0.6065], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,592][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.1674, 0.8326], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,592][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.2825, 0.7175], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,592][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0959, 0.9041], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,593][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.1207, 0.8793], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,593][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.1182e-01, 8.7276e-02, 9.0594e-04], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,593][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0732, 0.4125, 0.5143], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,594][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ and] are: tensor([4.0126e-05, 4.5710e-02, 9.5425e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,594][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2465, 0.1828, 0.5707], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,594][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1874, 0.6893, 0.1233], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,595][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.7869, 0.1223, 0.0908], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,595][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2503, 0.2252, 0.5245], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,595][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3974, 0.4899, 0.1126], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,596][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0127, 0.3880, 0.5993], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,596][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.1180, 0.2614, 0.6206], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,596][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0199, 0.4764, 0.5037], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,597][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2364, 0.4094, 0.3542], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,597][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([9.0631e-01, 8.3262e-02, 9.8290e-03, 5.9645e-04], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,597][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.0519, 0.2785, 0.3313, 0.3383], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,598][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([1.2615e-05, 8.3144e-02, 9.0951e-01, 7.3308e-03], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,598][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.1197, 0.1047, 0.4267, 0.3489], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,598][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.1295, 0.2660, 0.2902, 0.3143], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,599][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.2581, 0.1119, 0.2603, 0.3697], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,599][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.1463, 0.2806, 0.4009, 0.1721], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,599][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0841, 0.3913, 0.2001, 0.3245], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,600][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.0314, 0.0764, 0.2188, 0.6734], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,601][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.1641, 0.0423, 0.2534, 0.5402], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,603][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.0202, 0.3362, 0.3516, 0.2920], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,604][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0633, 0.2268, 0.3097, 0.4001], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,605][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ got] are: tensor([8.8960e-01, 5.4968e-02, 5.3638e-02, 1.1668e-03, 6.2464e-04],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,607][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0230, 0.1888, 0.2465, 0.2881, 0.2536], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,607][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ got] are: tensor([2.8218e-05, 4.4361e-02, 8.8579e-01, 4.7707e-03, 6.5049e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,609][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.1135, 0.0650, 0.2250, 0.2137, 0.3829], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,611][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.2057, 0.1070, 0.3805, 0.1289, 0.1780], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,612][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.5136, 0.0612, 0.0639, 0.2362, 0.1252], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,612][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1263, 0.1148, 0.4911, 0.1003, 0.1675], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,613][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.2836, 0.2053, 0.0376, 0.1637, 0.3098], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,613][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0134, 0.0668, 0.1516, 0.4121, 0.3562], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,613][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0202, 0.0071, 0.0317, 0.1450, 0.7960], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,614][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0085, 0.2616, 0.2934, 0.2319, 0.2047], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,614][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.1954, 0.1627, 0.1980, 0.2316, 0.2123], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,614][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([9.8075e-01, 1.2479e-02, 1.7743e-03, 7.4014e-04, 2.9422e-04, 3.9621e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,615][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0128, 0.1547, 0.2055, 0.2154, 0.2121, 0.1995], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,615][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([5.2731e-05, 3.9628e-02, 8.1258e-01, 8.1727e-03, 1.0264e-01, 3.6931e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,616][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0781, 0.0583, 0.1616, 0.1594, 0.3001, 0.2425], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,616][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1371, 0.0684, 0.3143, 0.1029, 0.2489, 0.1285], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,618][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.6751, 0.0268, 0.0132, 0.1425, 0.0804, 0.0620], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,620][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1081, 0.0890, 0.3331, 0.1759, 0.1820, 0.1120], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,621][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1482, 0.1567, 0.0296, 0.1263, 0.2685, 0.2707], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,623][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0139, 0.0196, 0.0132, 0.0784, 0.1106, 0.7643], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,624][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0432, 0.0016, 0.0077, 0.0338, 0.2038, 0.7099], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,626][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0062, 0.2129, 0.2298, 0.1950, 0.1675, 0.1886], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,627][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1013, 0.1432, 0.1696, 0.2061, 0.1858, 0.1940], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,628][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([9.4398e-01, 1.4555e-02, 1.0427e-02, 4.2648e-04, 2.5438e-04, 2.9212e-02,
        1.1453e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,630][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0065, 0.1080, 0.1994, 0.1788, 0.1891, 0.2138, 0.1043],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,630][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([6.5683e-05, 7.0584e-02, 8.4348e-01, 1.0103e-02, 4.2572e-02, 2.1263e-02,
        1.1936e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,630][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.0320, 0.0478, 0.1699, 0.1693, 0.2987, 0.1781, 0.1042],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,631][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0571, 0.0257, 0.0611, 0.0277, 0.1181, 0.0932, 0.6171],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,631][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.7782, 0.0195, 0.0064, 0.0675, 0.0326, 0.0360, 0.0598],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,631][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.2713, 0.0674, 0.2811, 0.1141, 0.1207, 0.0974, 0.0480],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,632][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.0898, 0.1395, 0.0363, 0.1202, 0.2266, 0.2845, 0.1031],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,632][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([8.0943e-03, 4.9002e-04, 3.0748e-04, 1.9091e-03, 4.6475e-03, 2.6759e-02,
        9.5779e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,633][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([5.8767e-02, 1.9732e-04, 1.1875e-03, 3.2126e-03, 1.1014e-02, 3.3425e-02,
        8.9220e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,633][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.0157, 0.1700, 0.1740, 0.1577, 0.1369, 0.1593, 0.1865],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,633][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0529, 0.1164, 0.1348, 0.1802, 0.1858, 0.1625, 0.1674],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,634][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ at] are: tensor([9.4106e-01, 2.4034e-02, 4.3776e-03, 3.2848e-03, 1.4423e-03, 1.6779e-02,
        8.1875e-03, 8.3566e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,636][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0067, 0.1021, 0.1738, 0.1948, 0.1611, 0.1789, 0.0856, 0.0971],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,637][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ at] are: tensor([6.0620e-05, 3.9434e-02, 8.1936e-01, 1.3568e-02, 6.3180e-02, 2.4618e-02,
        1.2609e-02, 2.7172e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,638][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0968, 0.0372, 0.1186, 0.1130, 0.1938, 0.1562, 0.0921, 0.1922],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,640][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0511, 0.0160, 0.0206, 0.0258, 0.0999, 0.0863, 0.4959, 0.2044],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,641][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.8828, 0.0120, 0.0026, 0.0354, 0.0137, 0.0156, 0.0186, 0.0193],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,643][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.2477, 0.0691, 0.1329, 0.2862, 0.0410, 0.0435, 0.1308, 0.0488],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,645][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.3378, 0.0902, 0.0163, 0.1012, 0.1194, 0.0952, 0.1120, 0.1278],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,645][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ at] are: tensor([1.1598e-03, 4.0193e-04, 1.3776e-04, 1.3357e-03, 2.7239e-03, 1.8745e-02,
        6.0692e-01, 3.6858e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,646][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ at] are: tensor([4.5390e-02, 3.0486e-04, 6.6562e-04, 3.5384e-03, 9.3460e-03, 2.9682e-02,
        8.1734e-01, 9.3731e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,647][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0069, 0.1455, 0.1468, 0.1341, 0.1158, 0.1272, 0.1692, 0.1545],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,647][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0570, 0.0987, 0.1126, 0.1591, 0.1276, 0.1274, 0.1428, 0.1749],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,648][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.8018e-01, 4.7495e-03, 3.4315e-04, 3.1356e-04, 1.6213e-04, 1.2296e-03,
        1.2015e-03, 7.4405e-03, 4.3820e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,648][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0054, 0.0895, 0.1642, 0.1846, 0.1579, 0.1601, 0.0739, 0.0840, 0.0804],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,649][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.9952e-05, 4.2932e-02, 7.9050e-01, 1.1201e-02, 6.5334e-02, 2.5868e-02,
        9.0056e-03, 2.7402e-02, 2.7733e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,649][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0780, 0.0358, 0.0970, 0.0917, 0.1641, 0.1398, 0.0775, 0.1567, 0.1593],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,649][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0341, 0.0038, 0.0129, 0.0084, 0.0381, 0.0159, 0.2998, 0.5090, 0.0780],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,650][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.6453e-01, 3.7295e-03, 5.7115e-04, 1.1408e-02, 3.1522e-03, 2.6645e-03,
        3.9958e-03, 6.0341e-03, 3.9141e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,650][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2205, 0.0740, 0.1391, 0.1676, 0.0701, 0.0872, 0.1034, 0.0515, 0.0866],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,650][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2205, 0.1085, 0.0191, 0.0920, 0.1517, 0.1149, 0.0706, 0.1237, 0.0990],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,651][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.9895e-03, 1.8146e-04, 5.6788e-05, 7.3792e-04, 8.0202e-04, 5.0435e-03,
        3.3189e-01, 2.0119e-01, 4.5811e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,652][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([4.6613e-02, 9.1429e-05, 2.6118e-04, 1.2789e-03, 3.5443e-03, 1.4940e-02,
        4.7710e-01, 8.0543e-02, 3.7563e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,654][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0061, 0.1276, 0.1251, 0.1158, 0.1002, 0.1119, 0.1428, 0.1335, 0.1370],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,655][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0290, 0.0849, 0.0978, 0.1263, 0.1132, 0.1091, 0.1174, 0.1512, 0.1710],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,656][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ station] are: tensor([9.5747e-01, 4.9228e-03, 2.0961e-03, 8.7522e-05, 9.1986e-05, 7.8921e-03,
        2.6020e-04, 6.2023e-03, 1.9319e-02, 1.6556e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,658][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0085, 0.0765, 0.1335, 0.1536, 0.1336, 0.1610, 0.0657, 0.0777, 0.0889,
        0.1009], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,659][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ station] are: tensor([2.1181e-05, 3.5692e-02, 7.8976e-01, 1.1173e-02, 7.3768e-02, 2.7003e-02,
        5.8131e-03, 2.1200e-02, 1.7739e-02, 1.7828e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,661][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0455, 0.0345, 0.1060, 0.0982, 0.1627, 0.1074, 0.0682, 0.1296, 0.1220,
        0.1259], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,662][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0508, 0.0044, 0.0055, 0.0048, 0.0153, 0.0197, 0.1488, 0.1893, 0.1056,
        0.4557], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,664][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.7635, 0.0127, 0.0048, 0.0352, 0.0216, 0.0226, 0.0151, 0.0470, 0.0210,
        0.0566], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,664][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0738, 0.0304, 0.2147, 0.0476, 0.0809, 0.1068, 0.0522, 0.1540, 0.2080,
        0.0317], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,665][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0394, 0.0879, 0.0176, 0.0840, 0.1469, 0.1656, 0.0725, 0.1764, 0.1784,
        0.0314], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,665][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ station] are: tensor([1.0938e-03, 2.6487e-05, 1.3174e-05, 1.1871e-04, 2.7424e-04, 1.3916e-03,
        1.0228e-01, 5.5325e-02, 1.2847e-01, 7.1101e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,665][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ station] are: tensor([9.0048e-02, 8.0547e-05, 3.0244e-04, 6.6245e-04, 1.7865e-03, 5.9230e-03,
        1.7529e-01, 2.6849e-02, 1.8549e-01, 5.1357e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,666][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0077, 0.1078, 0.1098, 0.1030, 0.0907, 0.0970, 0.1215, 0.1144, 0.1212,
        0.1269], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,666][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0459, 0.0691, 0.0880, 0.1105, 0.1139, 0.0968, 0.0978, 0.1435, 0.1211,
        0.1136], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,667][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [,] are: tensor([9.3068e-01, 5.0273e-03, 4.0473e-05, 4.6639e-04, 1.6702e-04, 4.3239e-03,
        1.6961e-03, 1.2897e-03, 4.5961e-02, 8.9800e-03, 1.3684e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,667][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0058, 0.0693, 0.1304, 0.1486, 0.1213, 0.1404, 0.0598, 0.0710, 0.0767,
        0.0896, 0.0870], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,667][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [,] are: tensor([1.8486e-05, 3.2184e-02, 7.5941e-01, 1.3013e-02, 6.9338e-02, 3.4647e-02,
        7.0226e-03, 2.4051e-02, 2.3871e-02, 1.5215e-02, 2.1230e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,668][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0631, 0.0215, 0.0698, 0.0641, 0.1161, 0.1111, 0.0521, 0.1313, 0.1336,
        0.1373, 0.1000], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,669][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0065, 0.0035, 0.0032, 0.0030, 0.0324, 0.0160, 0.0852, 0.1611, 0.1643,
        0.5074, 0.0174], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,671][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.7783, 0.0062, 0.0016, 0.0213, 0.0109, 0.0091, 0.0172, 0.0300, 0.0232,
        0.0388, 0.0634], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,672][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.1057, 0.0440, 0.0915, 0.1075, 0.0650, 0.0966, 0.0464, 0.0460, 0.1097,
        0.0474, 0.2403], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,674][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1082, 0.1119, 0.0125, 0.0734, 0.1263, 0.1286, 0.1003, 0.1281, 0.1323,
        0.0205, 0.0578], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,675][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [,] are: tensor([2.9533e-04, 2.3147e-05, 6.7794e-06, 8.0290e-05, 1.6122e-04, 1.4484e-03,
        5.5274e-02, 3.6734e-02, 1.0424e-01, 4.6594e-01, 3.3580e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,676][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [,] are: tensor([5.3754e-02, 5.9180e-05, 8.5333e-05, 6.5123e-04, 7.5830e-04, 3.7047e-03,
        8.7737e-02, 1.4431e-02, 8.3878e-02, 4.6521e-01, 2.8973e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,678][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0041, 0.0956, 0.0983, 0.0903, 0.0747, 0.0855, 0.1124, 0.1028, 0.1088,
        0.1152, 0.1124], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,679][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0279, 0.0703, 0.0743, 0.1095, 0.0967, 0.0921, 0.0806, 0.1173, 0.1241,
        0.1128, 0.0944], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,680][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([9.5104e-01, 7.5238e-04, 3.7893e-04, 2.7345e-05, 2.4596e-05, 1.9833e-03,
        5.3451e-05, 1.2120e-03, 9.9440e-03, 4.1227e-04, 2.1773e-03, 3.1993e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,682][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0049, 0.0703, 0.1063, 0.1356, 0.1209, 0.1129, 0.0528, 0.0615, 0.0672,
        0.0810, 0.0745, 0.1119], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,682][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([1.2805e-05, 3.5550e-02, 8.5675e-01, 7.5931e-03, 5.1830e-02, 1.5755e-02,
        2.2790e-03, 7.3305e-03, 8.2699e-03, 8.0271e-03, 6.2409e-03, 3.6594e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,682][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.0728, 0.0197, 0.0825, 0.0598, 0.1255, 0.0956, 0.0454, 0.1041, 0.1012,
        0.1186, 0.0844, 0.0906], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,683][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0207, 0.0114, 0.0064, 0.0299, 0.0156, 0.0277, 0.0605, 0.1992, 0.1192,
        0.2862, 0.0503, 0.1728], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,683][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.5894, 0.0073, 0.0048, 0.0266, 0.0246, 0.0185, 0.0276, 0.0589, 0.0420,
        0.0467, 0.0819, 0.0717], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,684][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0811, 0.0647, 0.1179, 0.0537, 0.0348, 0.0347, 0.0838, 0.0550, 0.0687,
        0.1390, 0.2009, 0.0656], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,684][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0524, 0.0656, 0.0237, 0.0516, 0.1268, 0.1338, 0.0698, 0.1205, 0.1388,
        0.0238, 0.0783, 0.1150], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,685][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([5.4183e-03, 9.0644e-05, 2.5634e-05, 4.3262e-04, 3.3140e-04, 1.5326e-03,
        7.5736e-02, 1.8223e-02, 6.1744e-02, 1.8826e-01, 1.7826e-01, 4.6995e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,685][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([2.7163e-01, 7.1533e-06, 3.4429e-05, 7.6169e-05, 4.9733e-04, 9.5625e-04,
        3.0108e-02, 5.2944e-03, 3.0291e-02, 2.6228e-01, 1.6272e-01, 2.3611e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,685][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0066, 0.0812, 0.0867, 0.0800, 0.0685, 0.0794, 0.1006, 0.0895, 0.0964,
        0.1050, 0.1000, 0.1061], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,687][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0144, 0.0531, 0.0749, 0.0827, 0.0985, 0.0807, 0.0803, 0.1064, 0.1038,
        0.1080, 0.0823, 0.1148], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,688][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([8.4088e-01, 2.1179e-03, 5.3665e-04, 3.8829e-05, 3.1397e-05, 3.3554e-03,
        5.2165e-05, 4.0526e-03, 1.3004e-02, 4.9159e-04, 1.1093e-02, 1.2402e-01,
        3.2640e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,690][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0050, 0.0658, 0.0999, 0.1251, 0.0929, 0.1223, 0.0453, 0.0494, 0.0652,
        0.0732, 0.0642, 0.1024, 0.0892], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,691][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([9.3797e-06, 1.1037e-02, 8.9544e-01, 3.5789e-03, 4.4259e-02, 1.2043e-02,
        2.1479e-03, 1.0169e-02, 6.8213e-03, 6.7257e-03, 5.3188e-03, 2.2518e-04,
        2.2270e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,692][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0673, 0.0204, 0.0705, 0.0594, 0.1052, 0.0862, 0.0444, 0.0925, 0.0909,
        0.1009, 0.0720, 0.0853, 0.1049], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,694][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0158, 0.0044, 0.0072, 0.0040, 0.0115, 0.0084, 0.0959, 0.3376, 0.1006,
        0.1298, 0.1235, 0.0909, 0.0704], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,696][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.4780, 0.0088, 0.0039, 0.0375, 0.0150, 0.0175, 0.0223, 0.0462, 0.0399,
        0.0429, 0.0935, 0.0977, 0.0969], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,697][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0365, 0.0233, 0.0564, 0.0257, 0.0802, 0.0813, 0.0881, 0.0756, 0.1366,
        0.1126, 0.1605, 0.0445, 0.0788], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,699][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0130, 0.0392, 0.0108, 0.0358, 0.1071, 0.1049, 0.0737, 0.1680, 0.1225,
        0.0194, 0.0696, 0.1252, 0.1109], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,699][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([7.4925e-04, 2.1896e-05, 7.1230e-06, 8.4135e-05, 8.0554e-05, 6.5393e-04,
        2.5984e-02, 1.6699e-02, 2.7049e-02, 1.3626e-01, 1.5043e-01, 1.8445e-01,
        4.5753e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,700][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([6.2289e-02, 5.6983e-06, 1.8571e-05, 7.0332e-05, 1.2091e-04, 5.4255e-04,
        2.4283e-02, 4.5033e-03, 2.4420e-02, 1.2330e-01, 1.2929e-01, 3.0869e-01,
        3.2247e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,700][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0051, 0.0771, 0.0791, 0.0716, 0.0642, 0.0696, 0.0920, 0.0837, 0.0880,
        0.0941, 0.0901, 0.1030, 0.0823], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,701][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0205, 0.0402, 0.0694, 0.0628, 0.0801, 0.0658, 0.0659, 0.0878, 0.0852,
        0.0865, 0.0752, 0.1074, 0.1534], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,701][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([9.0043e-01, 5.2890e-04, 3.2325e-05, 8.3939e-05, 2.5400e-05, 6.2717e-04,
        3.7957e-04, 5.9494e-05, 5.9578e-03, 1.9789e-03, 6.8580e-04, 7.5573e-02,
        1.3075e-02, 5.6443e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,701][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0029, 0.0597, 0.1071, 0.1252, 0.0914, 0.1003, 0.0475, 0.0514, 0.0552,
        0.0729, 0.0654, 0.1036, 0.0746, 0.0428], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,702][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([1.1003e-05, 3.7040e-02, 7.9227e-01, 9.8598e-03, 8.3061e-02, 1.8351e-02,
        4.2036e-03, 1.4732e-02, 1.3618e-02, 9.9672e-03, 1.0629e-02, 6.0802e-04,
        2.7355e-03, 2.9149e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,702][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0424, 0.0195, 0.0572, 0.0533, 0.1004, 0.0770, 0.0422, 0.0937, 0.0921,
        0.1027, 0.0667, 0.0869, 0.1029, 0.0630], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,703][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0114, 0.0035, 0.0014, 0.0060, 0.0086, 0.0110, 0.1901, 0.0537, 0.0756,
        0.3765, 0.0491, 0.1237, 0.0473, 0.0422], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,705][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.7520, 0.0043, 0.0010, 0.0142, 0.0068, 0.0072, 0.0090, 0.0156, 0.0126,
        0.0213, 0.0309, 0.0395, 0.0388, 0.0467], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,707][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0911, 0.0277, 0.0621, 0.0663, 0.0441, 0.0149, 0.0454, 0.0185, 0.0277,
        0.0237, 0.3114, 0.0587, 0.1309, 0.0773], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,708][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0463, 0.0515, 0.0085, 0.0428, 0.0847, 0.0850, 0.0547, 0.1244, 0.1123,
        0.0121, 0.0580, 0.1116, 0.1110, 0.0972], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,709][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([2.1292e-04, 8.3912e-06, 7.7894e-07, 1.8622e-05, 2.0834e-05, 1.6225e-04,
        8.0893e-03, 4.7285e-03, 1.5052e-02, 4.8255e-02, 7.3898e-02, 1.2288e-01,
        3.9431e-01, 3.3236e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,710][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([3.9952e-02, 9.1161e-06, 1.5936e-05, 6.9554e-05, 1.7451e-04, 5.1437e-04,
        1.3320e-02, 2.0731e-03, 1.2992e-02, 6.9193e-02, 5.5895e-02, 2.7042e-01,
        3.4808e-01, 1.8730e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,712][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0039, 0.0707, 0.0734, 0.0657, 0.0576, 0.0632, 0.0828, 0.0747, 0.0794,
        0.0848, 0.0821, 0.0946, 0.0767, 0.0903], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,714][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0217, 0.0489, 0.0597, 0.0690, 0.0693, 0.0594, 0.0626, 0.0725, 0.0826,
        0.0793, 0.0679, 0.0955, 0.1282, 0.0837], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,715][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ give] are: tensor([8.9833e-01, 1.8127e-03, 1.1427e-03, 2.9489e-05, 2.3198e-05, 2.1175e-03,
        1.6717e-04, 1.2876e-03, 1.0446e-02, 1.1612e-03, 4.6869e-03, 4.9046e-02,
        2.3903e-03, 2.4414e-02, 2.9456e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,716][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0034, 0.0535, 0.0953, 0.0983, 0.0934, 0.0983, 0.0427, 0.0543, 0.0568,
        0.0661, 0.0648, 0.0842, 0.0801, 0.0470, 0.0618], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,717][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ give] are: tensor([2.1745e-05, 6.7633e-02, 7.9138e-01, 1.0861e-02, 5.2335e-02, 1.8775e-02,
        4.9945e-03, 1.2356e-02, 1.1172e-02, 1.2635e-02, 9.4341e-03, 8.7499e-04,
        3.0569e-03, 2.9902e-03, 1.4819e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,717][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0362, 0.0126, 0.0518, 0.0479, 0.0883, 0.0668, 0.0366, 0.0840, 0.0787,
        0.0947, 0.0650, 0.0738, 0.0975, 0.0576, 0.1084], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,717][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0064, 0.0025, 0.0053, 0.0066, 0.0077, 0.0092, 0.1028, 0.1224, 0.1035,
        0.1297, 0.0787, 0.0675, 0.0930, 0.1399, 0.1247], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,718][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.4164, 0.0083, 0.0031, 0.0270, 0.0127, 0.0118, 0.0146, 0.0320, 0.0207,
        0.0417, 0.0566, 0.0745, 0.0782, 0.1387, 0.0637], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,718][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0491, 0.0249, 0.0690, 0.0278, 0.0451, 0.0341, 0.0384, 0.0389, 0.0772,
        0.0461, 0.2110, 0.0556, 0.0862, 0.1207, 0.0758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,719][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.1097, 0.0463, 0.0082, 0.0344, 0.0627, 0.0815, 0.0448, 0.0888, 0.0963,
        0.0117, 0.0484, 0.0966, 0.0916, 0.0937, 0.0853], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,719][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ give] are: tensor([2.7999e-04, 9.5740e-06, 1.3974e-06, 2.4892e-05, 2.1013e-05, 1.5953e-04,
        8.2541e-03, 3.5099e-03, 8.0524e-03, 4.5152e-02, 4.6370e-02, 6.1603e-02,
        2.5044e-01, 3.3327e-01, 2.4285e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,720][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ give] are: tensor([1.6402e-02, 9.7676e-07, 2.2723e-06, 8.8384e-06, 1.4550e-05, 1.0839e-04,
        3.0646e-03, 5.3198e-04, 4.4217e-03, 1.7531e-02, 2.5119e-02, 5.6957e-02,
        1.1353e-01, 9.9009e-02, 6.6330e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,721][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0043, 0.0658, 0.0664, 0.0611, 0.0519, 0.0575, 0.0740, 0.0678, 0.0710,
        0.0770, 0.0739, 0.0871, 0.0710, 0.0858, 0.0853], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,722][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0304, 0.0387, 0.0510, 0.0597, 0.0692, 0.0567, 0.0518, 0.0728, 0.0709,
        0.0649, 0.0593, 0.0897, 0.1227, 0.0660, 0.0962], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,724][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ it] are: tensor([6.0777e-01, 5.7093e-03, 6.0773e-04, 4.9295e-05, 8.3235e-05, 2.4036e-03,
        2.7680e-04, 4.6812e-03, 3.4588e-03, 3.8732e-03, 6.7716e-03, 2.4180e-01,
        4.3284e-03, 4.4743e-02, 7.1339e-02, 2.0990e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,725][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0026, 0.0457, 0.0944, 0.1004, 0.0792, 0.0905, 0.0386, 0.0449, 0.0499,
        0.0625, 0.0621, 0.0776, 0.0707, 0.0447, 0.0585, 0.0777],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,726][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ it] are: tensor([1.8042e-05, 3.8514e-02, 7.9662e-01, 6.8665e-03, 6.2787e-02, 2.3894e-02,
        3.9646e-03, 1.6973e-02, 1.4268e-02, 1.2985e-02, 1.1024e-02, 7.9338e-04,
        3.3659e-03, 3.3681e-03, 1.3751e-03, 3.1857e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,728][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0322, 0.0136, 0.0500, 0.0433, 0.0851, 0.0669, 0.0321, 0.0807, 0.0768,
        0.0855, 0.0626, 0.0742, 0.0866, 0.0556, 0.1010, 0.0538],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,729][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ it] are: tensor([2.5699e-03, 4.2470e-04, 1.5569e-03, 7.0490e-04, 5.6253e-03, 2.9723e-03,
        2.2803e-02, 1.1815e-01, 4.0619e-02, 4.2359e-02, 3.3598e-02, 1.4116e-02,
        5.0957e-02, 4.7236e-01, 9.7008e-02, 9.4184e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,731][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.5723, 0.0037, 0.0010, 0.0109, 0.0069, 0.0049, 0.0067, 0.0174, 0.0133,
        0.0227, 0.0435, 0.0347, 0.0359, 0.0646, 0.0356, 0.1260],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,732][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.1171, 0.0188, 0.0563, 0.0357, 0.0534, 0.1227, 0.0249, 0.0296, 0.0947,
        0.0136, 0.1038, 0.0235, 0.0740, 0.0610, 0.1359, 0.0350],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,733][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.1236, 0.0389, 0.0092, 0.0353, 0.0606, 0.0719, 0.0349, 0.0782, 0.0748,
        0.0110, 0.0534, 0.0632, 0.0763, 0.1196, 0.0762, 0.0731],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,734][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ it] are: tensor([2.1195e-04, 2.8998e-06, 3.4088e-07, 7.4719e-06, 1.9752e-05, 1.2005e-04,
        6.5507e-03, 2.6926e-03, 9.4213e-03, 4.4941e-02, 4.0575e-02, 3.0430e-02,
        1.5788e-01, 1.6017e-01, 2.6522e-01, 2.8176e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,734][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ it] are: tensor([2.1330e-02, 5.7557e-07, 1.5359e-06, 6.7144e-06, 1.2004e-05, 4.8408e-05,
        1.5579e-03, 4.2105e-04, 2.0771e-03, 1.1209e-02, 1.5176e-02, 4.8918e-02,
        7.0900e-02, 5.0108e-02, 3.2359e-01, 4.5464e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,735][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0044, 0.0604, 0.0606, 0.0584, 0.0509, 0.0554, 0.0666, 0.0624, 0.0658,
        0.0707, 0.0670, 0.0790, 0.0665, 0.0741, 0.0763, 0.0813],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,735][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0171, 0.0300, 0.0423, 0.0495, 0.0586, 0.0505, 0.0473, 0.0688, 0.0651,
        0.0623, 0.0537, 0.0797, 0.1101, 0.0654, 0.0845, 0.1151],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,736][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([8.5287e-01, 4.3609e-04, 2.6874e-05, 7.0033e-05, 1.8504e-05, 4.6488e-04,
        2.7828e-04, 4.0670e-05, 3.9405e-03, 1.4618e-03, 4.7223e-04, 4.7409e-02,
        1.0088e-02, 3.2079e-04, 8.4529e-03, 7.2981e-02, 6.6452e-04],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,736][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0020, 0.0497, 0.0946, 0.1093, 0.0772, 0.0847, 0.0390, 0.0422, 0.0452,
        0.0615, 0.0555, 0.0851, 0.0630, 0.0356, 0.0495, 0.0718, 0.0341],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,736][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([1.8601e-05, 4.4656e-02, 7.7296e-01, 1.0182e-02, 7.9061e-02, 1.9287e-02,
        4.9778e-03, 1.6479e-02, 1.4910e-02, 1.1422e-02, 1.2338e-02, 6.9664e-04,
        3.1761e-03, 3.6500e-03, 1.9173e-03, 2.1835e-03, 2.0835e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,737][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0373, 0.0157, 0.0455, 0.0400, 0.0794, 0.0638, 0.0330, 0.0788, 0.0790,
        0.0861, 0.0559, 0.0669, 0.0797, 0.0496, 0.0755, 0.0466, 0.0670],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,739][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0074, 0.0021, 0.0007, 0.0040, 0.0056, 0.0068, 0.1230, 0.0325, 0.0470,
        0.2312, 0.0287, 0.0799, 0.0309, 0.0261, 0.1871, 0.1542, 0.0329],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,741][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.5902, 0.0038, 0.0009, 0.0095, 0.0058, 0.0058, 0.0062, 0.0133, 0.0098,
        0.0172, 0.0234, 0.0297, 0.0283, 0.0368, 0.0301, 0.1272, 0.0620],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,742][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0730, 0.0195, 0.0449, 0.0447, 0.0331, 0.0113, 0.0334, 0.0136, 0.0226,
        0.0198, 0.2332, 0.0409, 0.0875, 0.0575, 0.1158, 0.0772, 0.0720],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,744][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0318, 0.0369, 0.0065, 0.0325, 0.0644, 0.0627, 0.0383, 0.0911, 0.0785,
        0.0091, 0.0424, 0.0760, 0.0778, 0.0725, 0.0756, 0.0992, 0.1047],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,745][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([1.5246e-04, 3.1775e-06, 2.6368e-07, 6.2261e-06, 8.7514e-06, 5.0947e-05,
        3.1931e-03, 1.4354e-03, 4.2796e-03, 1.8323e-02, 2.2237e-02, 3.5204e-02,
        1.2946e-01, 1.0907e-01, 2.3665e-01, 1.7658e-01, 2.6335e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,746][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([1.9612e-02, 1.6773e-06, 3.0543e-06, 1.2644e-05, 2.9999e-05, 8.1133e-05,
        2.1306e-03, 3.5445e-04, 2.0953e-03, 1.2985e-02, 9.2924e-03, 4.3060e-02,
        6.4876e-02, 3.2438e-02, 2.4358e-01, 4.6889e-01, 1.0057e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,748][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0030, 0.0551, 0.0572, 0.0512, 0.0451, 0.0493, 0.0647, 0.0583, 0.0621,
        0.0662, 0.0637, 0.0725, 0.0602, 0.0707, 0.0732, 0.0785, 0.0689],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,749][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0102, 0.0335, 0.0450, 0.0491, 0.0561, 0.0445, 0.0480, 0.0583, 0.0613,
        0.0660, 0.0517, 0.0730, 0.1019, 0.0604, 0.0810, 0.0991, 0.0611],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,798][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:39,798][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,799][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,799][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,799][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,800][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,800][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,800][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,801][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,801][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,801][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,801][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,802][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:39,803][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.1896, 0.8104], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,804][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.4162, 0.5838], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,804][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,804][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.6706, 0.3294], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,805][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.3404, 0.6596], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,805][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.8405, 0.1595], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,805][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.9140, 0.0860], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,806][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.2310, 0.7690], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,806][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.1674, 0.8326], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,806][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.1730, 0.8270], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,807][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0216, 0.9784], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,807][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.8330, 0.1670], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:39,807][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0379, 0.4561, 0.5060], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,807][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.2701, 0.5493, 0.1807], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,808][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([4.2034e-04, 7.9188e-02, 9.2039e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,810][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2737, 0.2354, 0.4909], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,811][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.1664, 0.2393, 0.5943], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,813][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9110, 0.0651, 0.0239], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,814][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.8307, 0.0452, 0.1242], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,815][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.2638, 0.4308, 0.3054], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,815][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0127, 0.3880, 0.5993], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,816][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0173, 0.2233, 0.7594], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,818][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0013, 0.1530, 0.8457], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,819][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.9669, 0.0260, 0.0071], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:39,821][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.0233, 0.0326, 0.1912, 0.7529], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,821][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.7316, 0.1555, 0.0569, 0.0560], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,821][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([5.3413e-05, 7.1636e-02, 8.8742e-01, 4.0892e-02], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,822][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.1499, 0.0986, 0.2248, 0.5266], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,822][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0565, 0.0740, 0.2263, 0.6431], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,822][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.4115, 0.1187, 0.1850, 0.2848], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,823][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.4008, 0.1286, 0.3116, 0.1590], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,823][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0185, 0.2312, 0.5723, 0.1780], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,823][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.0314, 0.0764, 0.2188, 0.6734], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,824][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.0166, 0.0965, 0.3939, 0.4929], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,824][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([5.4895e-04, 2.4278e-01, 6.9699e-01, 5.9687e-02], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,824][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.3252, 0.2305, 0.2919, 0.1524], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:39,825][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0138, 0.0172, 0.0255, 0.5100, 0.4335], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,827][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.3764, 0.1954, 0.0693, 0.1451, 0.2138], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,828][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([9.4953e-05, 3.7876e-02, 8.2474e-01, 2.4399e-02, 1.1289e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,830][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.1336, 0.0539, 0.0723, 0.4168, 0.3234], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,831][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0357, 0.0355, 0.1101, 0.2810, 0.5378], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,833][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.6599, 0.0639, 0.0415, 0.1559, 0.0788], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,834][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.5961, 0.0803, 0.1326, 0.0960, 0.0950], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,836][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0972, 0.2094, 0.1855, 0.2046, 0.3033], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,837][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0134, 0.0668, 0.1516, 0.4121, 0.3562], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,838][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0073, 0.0432, 0.1377, 0.3480, 0.4637], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,838][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([5.5426e-04, 8.7413e-02, 7.6373e-01, 6.1867e-02, 8.6437e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,839][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.7157, 0.0699, 0.0588, 0.0373, 0.1184], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:39,839][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0205, 0.0037, 0.0016, 0.0947, 0.0928, 0.7867], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,839][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1726, 0.2893, 0.0671, 0.1263, 0.2193, 0.1254], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,840][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([3.5095e-04, 6.4101e-02, 5.7636e-01, 5.6461e-02, 1.7090e-01, 1.3183e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,840][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1454, 0.0199, 0.0121, 0.0915, 0.0619, 0.6693], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,840][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0907, 0.0095, 0.0107, 0.0771, 0.1026, 0.7095], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,841][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.8451, 0.0195, 0.0052, 0.0630, 0.0305, 0.0367], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,841][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.7004, 0.0553, 0.0726, 0.0560, 0.0563, 0.0593], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,842][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1251, 0.1958, 0.1038, 0.1757, 0.2210, 0.1786], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,842][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0139, 0.0196, 0.0132, 0.0784, 0.1106, 0.7643], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,844][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0095, 0.0076, 0.0097, 0.0504, 0.0659, 0.8569], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,846][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0009, 0.1142, 0.4910, 0.0931, 0.1442, 0.1567], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,847][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.8054, 0.0577, 0.0157, 0.0308, 0.0622, 0.0282], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:39,848][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([3.6207e-03, 3.5084e-05, 9.1469e-06, 6.0359e-04, 1.1181e-03, 1.3185e-02,
        9.8143e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,849][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.5293, 0.1231, 0.0481, 0.0439, 0.1211, 0.0864, 0.0480],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,851][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.0006, 0.1622, 0.5690, 0.0804, 0.0741, 0.1010, 0.0127],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,852][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.1052, 0.0039, 0.0015, 0.0205, 0.0113, 0.0823, 0.7753],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,853][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([1.9040e-02, 7.2044e-04, 6.8730e-04, 4.6337e-03, 8.6482e-03, 5.8901e-02,
        9.0737e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,855][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.8799, 0.0131, 0.0025, 0.0332, 0.0144, 0.0289, 0.0279],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,856][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.6964, 0.0427, 0.0738, 0.0648, 0.0548, 0.0419, 0.0256],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,856][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.1115, 0.1365, 0.0935, 0.1600, 0.1692, 0.1696, 0.1597],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,856][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([8.0943e-03, 4.9002e-04, 3.0748e-04, 1.9091e-03, 4.6475e-03, 2.6759e-02,
        9.5779e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,857][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.0052, 0.0010, 0.0009, 0.0072, 0.0061, 0.0650, 0.9145],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,857][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.0051, 0.2274, 0.2506, 0.1368, 0.1248, 0.2412, 0.0141],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,857][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.3723, 0.1665, 0.0764, 0.0625, 0.1710, 0.0838, 0.0674],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:39,858][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([2.5546e-03, 1.8166e-05, 2.6804e-06, 4.0594e-04, 4.9203e-04, 6.3288e-03,
        8.2720e-01, 1.6300e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,858][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.2051, 0.1662, 0.0585, 0.1111, 0.1747, 0.1337, 0.0819, 0.0687],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,859][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([2.3944e-04, 6.6979e-02, 6.2539e-01, 6.8585e-02, 7.2298e-02, 1.0757e-01,
        1.8521e-02, 4.0420e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,859][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0948, 0.0025, 0.0009, 0.0147, 0.0069, 0.0710, 0.5400, 0.2693],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,859][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([1.8220e-02, 5.5872e-04, 3.5750e-04, 3.6145e-03, 5.8494e-03, 3.9753e-02,
        4.3987e-01, 4.9177e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,860][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.9266, 0.0110, 0.0015, 0.0215, 0.0072, 0.0121, 0.0082, 0.0120],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,862][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.8161, 0.0273, 0.0325, 0.0332, 0.0189, 0.0298, 0.0249, 0.0172],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,863][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1948, 0.0954, 0.0542, 0.1639, 0.1526, 0.1028, 0.1831, 0.0531],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,864][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([1.1598e-03, 4.0193e-04, 1.3776e-04, 1.3357e-03, 2.7239e-03, 1.8745e-02,
        6.0692e-01, 3.6858e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,865][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([2.6638e-03, 7.0613e-04, 2.6773e-04, 4.0344e-03, 2.8370e-03, 3.8315e-02,
        6.6426e-01, 2.8692e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,866][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0014, 0.1562, 0.3462, 0.1573, 0.0999, 0.1559, 0.0440, 0.0391],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,868][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.7749, 0.0464, 0.0224, 0.0288, 0.0569, 0.0230, 0.0306, 0.0171],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:39,870][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([2.9362e-03, 8.7661e-06, 1.0362e-06, 1.8522e-04, 1.5066e-04, 1.6960e-03,
        2.9706e-01, 1.5073e-01, 5.4723e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,871][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2654, 0.1462, 0.0503, 0.0889, 0.1539, 0.1060, 0.0662, 0.0629, 0.0602],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,873][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0009, 0.1367, 0.4182, 0.0988, 0.1155, 0.1173, 0.0185, 0.0541, 0.0399],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,873][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([7.5134e-02, 1.2257e-03, 2.9800e-04, 5.3934e-03, 2.6395e-03, 2.6536e-02,
        1.7751e-01, 1.3623e-01, 5.7503e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,873][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([1.2643e-02, 1.6758e-04, 6.9388e-05, 9.4796e-04, 1.5318e-03, 8.3581e-03,
        2.1250e-01, 1.7780e-01, 5.8599e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,874][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.7646e-01, 3.4341e-03, 3.1815e-04, 6.7520e-03, 1.8707e-03, 2.4676e-03,
        2.0380e-03, 4.0987e-03, 2.5627e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,874][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.8249, 0.0242, 0.0266, 0.0269, 0.0184, 0.0231, 0.0175, 0.0136, 0.0248],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,875][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2241, 0.1102, 0.0677, 0.1391, 0.1875, 0.1021, 0.0668, 0.0383, 0.0641],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,875][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([1.9895e-03, 1.8146e-04, 5.6788e-05, 7.3792e-04, 8.0202e-04, 5.0435e-03,
        3.3189e-01, 2.0119e-01, 4.5811e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,875][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.1114e-03, 1.6095e-04, 5.3586e-05, 9.5454e-04, 6.8566e-04, 1.1555e-02,
        2.3795e-01, 1.5550e-01, 5.9204e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,876][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0024, 0.2029, 0.2410, 0.1491, 0.1013, 0.1823, 0.0276, 0.0513, 0.0422],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,876][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.7661, 0.0575, 0.0109, 0.0228, 0.0550, 0.0196, 0.0267, 0.0182, 0.0230],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:39,876][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([1.2230e-03, 3.3089e-06, 2.0374e-07, 5.9877e-05, 4.8404e-05, 9.9216e-04,
        9.5655e-02, 3.1860e-02, 2.3888e-01, 6.3128e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,877][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.6729, 0.0542, 0.0226, 0.0291, 0.0539, 0.0520, 0.0222, 0.0303, 0.0336,
        0.0292], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,878][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([1.3844e-04, 7.4162e-02, 6.3979e-01, 4.0062e-02, 7.2007e-02, 7.4282e-02,
        6.3264e-03, 3.2648e-02, 2.8668e-02, 3.1914e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,879][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([6.4235e-02, 7.0922e-04, 1.6489e-04, 4.0294e-03, 1.5439e-03, 1.5049e-02,
        1.1326e-01, 6.4391e-02, 3.5810e-01, 3.7851e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,880][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([9.7088e-03, 9.8337e-05, 4.9270e-05, 5.2255e-04, 8.0507e-04, 7.8203e-03,
        8.9420e-02, 7.5999e-02, 3.8548e-01, 4.3010e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,882][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.8816, 0.0089, 0.0017, 0.0162, 0.0087, 0.0139, 0.0052, 0.0204, 0.0097,
        0.0338], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,883][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.5898, 0.0412, 0.0635, 0.0524, 0.0365, 0.0509, 0.0382, 0.0316, 0.0584,
        0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,885][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0473, 0.0985, 0.0736, 0.1505, 0.1489, 0.1296, 0.0601, 0.0474, 0.1071,
        0.1369], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,886][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([1.0938e-03, 2.6487e-05, 1.3174e-05, 1.1871e-04, 2.7424e-04, 1.3916e-03,
        1.0228e-01, 5.5325e-02, 1.2847e-01, 7.1101e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,887][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.6106e-03, 1.0566e-04, 3.7682e-05, 5.0780e-04, 3.4141e-04, 5.1067e-03,
        1.0077e-01, 4.3194e-02, 2.7376e-01, 5.7457e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,889][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0005, 0.1007, 0.2280, 0.1046, 0.1515, 0.1874, 0.0275, 0.0669, 0.0732,
        0.0596], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,890][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.6277, 0.0953, 0.0326, 0.0371, 0.0693, 0.0296, 0.0272, 0.0329, 0.0291,
        0.0191], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:39,891][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([7.7505e-04, 1.4495e-06, 8.8947e-08, 2.0143e-05, 2.7346e-05, 4.5433e-04,
        6.5661e-02, 2.3889e-02, 1.1698e-01, 5.9297e-01, 1.9922e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,891][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.1596, 0.1078, 0.0425, 0.0920, 0.1299, 0.1119, 0.0698, 0.0641, 0.0827,
        0.0896, 0.0502], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,892][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0005, 0.1531, 0.3730, 0.0962, 0.0929, 0.1487, 0.0113, 0.0304, 0.0382,
        0.0175, 0.0382], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,892][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([1.8819e-02, 2.7605e-04, 4.5959e-05, 1.1150e-03, 4.4860e-04, 6.3191e-03,
        4.0758e-02, 3.5036e-02, 1.6320e-01, 1.4777e-01, 5.8621e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,892][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([2.6615e-03, 5.6177e-05, 1.9102e-05, 2.5503e-04, 4.2139e-04, 3.2375e-03,
        4.7938e-02, 5.2132e-02, 2.0967e-01, 2.6031e-01, 4.2330e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,893][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([9.1470e-01, 3.8726e-03, 4.6755e-04, 7.6617e-03, 3.9087e-03, 4.6455e-03,
        5.1216e-03, 1.0364e-02, 7.3494e-03, 1.8388e-02, 2.3525e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,893][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.5984, 0.0318, 0.0357, 0.0316, 0.0250, 0.0350, 0.0303, 0.0223, 0.0393,
        0.0358, 0.1147], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,894][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1149, 0.1076, 0.0472, 0.0925, 0.1383, 0.1052, 0.1123, 0.0367, 0.0830,
        0.0686, 0.0936], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,894][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([2.9533e-04, 2.3147e-05, 6.7794e-06, 8.0290e-05, 1.6122e-04, 1.4484e-03,
        5.5274e-02, 3.6734e-02, 1.0424e-01, 4.6594e-01, 3.3580e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,895][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([4.2628e-04, 5.4453e-05, 8.1284e-06, 3.1439e-04, 1.0190e-04, 2.2480e-03,
        3.4065e-02, 1.8591e-02, 1.0260e-01, 3.9535e-01, 4.4624e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,897][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0005, 0.1646, 0.1682, 0.1519, 0.0637, 0.2157, 0.0177, 0.0484, 0.0622,
        0.0610, 0.0462], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,898][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.7033, 0.0873, 0.0111, 0.0458, 0.0485, 0.0245, 0.0207, 0.0138, 0.0198,
        0.0135, 0.0117], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:39,899][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([5.3895e-03, 4.3827e-06, 3.3262e-07, 7.6152e-05, 6.5946e-05, 5.3441e-04,
        6.0017e-02, 9.7742e-03, 7.3017e-02, 2.8895e-01, 2.5049e-01, 3.1168e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,901][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.2921, 0.1139, 0.0325, 0.0564, 0.1000, 0.0671, 0.0410, 0.0496, 0.0505,
        0.0552, 0.0348, 0.1069], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,902][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([1.3518e-05, 2.9411e-02, 6.8227e-01, 2.1041e-02, 5.2369e-02, 6.8255e-02,
        7.4763e-03, 2.7982e-02, 2.9300e-02, 2.5933e-02, 4.3934e-02, 1.2020e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,903][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([5.4671e-02, 3.4535e-04, 1.2011e-04, 1.6065e-03, 1.2642e-03, 7.3607e-03,
        3.4054e-02, 2.2842e-02, 7.6730e-02, 1.0049e-01, 3.1774e-01, 3.8277e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,904][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([5.7593e-03, 1.7028e-04, 4.8867e-05, 9.4748e-04, 8.7218e-04, 4.2716e-03,
        3.3694e-02, 3.6369e-02, 1.2628e-01, 2.0770e-01, 2.6014e-01, 3.2375e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,905][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.7047, 0.0075, 0.0031, 0.0178, 0.0176, 0.0203, 0.0165, 0.0380, 0.0271,
        0.0412, 0.0589, 0.0472], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,907][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.4644, 0.0378, 0.0451, 0.0348, 0.0332, 0.0353, 0.0508, 0.0314, 0.0406,
        0.0622, 0.1041, 0.0604], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,908][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0417, 0.0576, 0.0667, 0.0533, 0.1056, 0.0902, 0.0858, 0.0458, 0.0835,
        0.1031, 0.1597, 0.1068], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,909][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([5.4183e-03, 9.0644e-05, 2.5634e-05, 4.3262e-04, 3.3140e-04, 1.5326e-03,
        7.5736e-02, 1.8223e-02, 6.1744e-02, 1.8826e-01, 1.7826e-01, 4.6995e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,909][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([2.9861e-03, 6.1882e-05, 2.0383e-05, 2.7726e-04, 4.2583e-04, 3.0019e-03,
        2.9723e-02, 1.9916e-02, 7.9150e-02, 3.2848e-01, 3.6730e-01, 1.6866e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,910][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([1.3085e-04, 3.7398e-02, 4.3118e-01, 6.0356e-02, 4.7039e-02, 1.3551e-01,
        1.7210e-02, 3.5709e-02, 7.0963e-02, 8.5018e-02, 6.6036e-02, 1.3444e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,910][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.2815, 0.1296, 0.0741, 0.0586, 0.1086, 0.0897, 0.0307, 0.0312, 0.0545,
        0.0667, 0.0369, 0.0378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:39,910][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([1.1377e-03, 2.0363e-06, 9.3100e-08, 2.6777e-05, 1.5821e-05, 1.6134e-04,
        1.1535e-02, 4.1042e-03, 2.3174e-02, 5.5464e-02, 1.1526e-01, 2.8513e-01,
        5.0399e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,911][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.3077, 0.0985, 0.0334, 0.0537, 0.0707, 0.0808, 0.0274, 0.0301, 0.0420,
        0.0450, 0.0289, 0.1099, 0.0720], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,911][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([1.9704e-05, 1.5968e-02, 7.2341e-01, 1.0996e-02, 5.0533e-02, 4.2037e-02,
        7.1739e-03, 3.0396e-02, 2.2913e-02, 1.9441e-02, 4.5803e-02, 1.0725e-02,
        2.0590e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,912][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([1.0339e-02, 2.0240e-04, 3.8654e-05, 6.6034e-04, 3.2956e-04, 2.9154e-03,
        1.7626e-02, 1.4759e-02, 5.4294e-02, 5.9885e-02, 2.1975e-01, 2.6126e-01,
        3.5794e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,912][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([2.9949e-03, 4.3771e-05, 1.5190e-05, 2.1045e-04, 2.8222e-04, 1.7567e-03,
        2.7921e-02, 2.7798e-02, 8.4173e-02, 1.0195e-01, 3.0581e-01, 1.2585e-01,
        3.2120e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,913][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.6743, 0.0098, 0.0019, 0.0204, 0.0080, 0.0135, 0.0093, 0.0237, 0.0197,
        0.0294, 0.0530, 0.0584, 0.0786], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,914][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.3088, 0.0430, 0.0489, 0.0350, 0.0335, 0.0506, 0.0578, 0.0343, 0.0603,
        0.0506, 0.1511, 0.0587, 0.0675], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,916][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0196, 0.0448, 0.0438, 0.0521, 0.0907, 0.0758, 0.0927, 0.0568, 0.0816,
        0.1068, 0.1195, 0.1095, 0.1063], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,917][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([7.4925e-04, 2.1896e-05, 7.1230e-06, 8.4135e-05, 8.0554e-05, 6.5393e-04,
        2.5984e-02, 1.6699e-02, 2.7049e-02, 1.3626e-01, 1.5043e-01, 1.8445e-01,
        4.5753e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,918][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([7.3669e-04, 4.2042e-05, 9.9605e-06, 1.9874e-04, 1.1065e-04, 1.6809e-03,
        2.7031e-02, 1.5775e-02, 6.3416e-02, 1.9422e-01, 3.1811e-01, 2.0089e-01,
        1.7779e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,919][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0004, 0.0707, 0.2772, 0.0537, 0.0767, 0.1656, 0.0280, 0.0549, 0.0637,
        0.1012, 0.0530, 0.0444, 0.0105], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,921][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.1742, 0.0880, 0.0802, 0.0358, 0.0935, 0.0588, 0.0506, 0.0568, 0.0670,
        0.0503, 0.0909, 0.0751, 0.0787], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:39,922][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.7821e-03, 3.9759e-07, 6.8363e-09, 5.5555e-06, 3.4408e-06, 2.6258e-05,
        3.8361e-03, 9.6333e-04, 6.4360e-03, 2.2208e-02, 3.2366e-02, 1.7720e-01,
        4.4462e-01, 3.1056e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,924][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0587, 0.1239, 0.0352, 0.0819, 0.1054, 0.0803, 0.0581, 0.0432, 0.0543,
        0.0709, 0.0333, 0.1538, 0.0658, 0.0352], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,925][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([2.2771e-04, 7.0576e-02, 4.1264e-01, 4.8721e-02, 1.1114e-01, 8.3479e-02,
        8.8296e-03, 2.4092e-02, 3.7075e-02, 1.3796e-02, 6.9255e-02, 3.1690e-02,
        3.7460e-02, 5.1024e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,926][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.2979e-02, 7.8342e-05, 9.9314e-06, 3.0694e-04, 1.4806e-04, 1.2011e-03,
        8.4026e-03, 5.2150e-03, 2.4393e-02, 2.9556e-02, 1.1200e-01, 1.7949e-01,
        3.3277e-01, 2.9345e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,926][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([3.5760e-03, 1.8099e-05, 3.5335e-06, 9.3086e-05, 1.3419e-04, 7.4407e-04,
        9.6464e-03, 1.0453e-02, 4.7048e-02, 5.2868e-02, 1.2839e-01, 9.5539e-02,
        2.4129e-01, 4.1020e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,927][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([8.5643e-01, 3.9307e-03, 4.3316e-04, 7.3182e-03, 3.1159e-03, 4.8653e-03,
        3.1780e-03, 6.9227e-03, 5.6461e-03, 1.2794e-02, 1.7086e-02, 1.8358e-02,
        2.8907e-02, 3.1020e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,927][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.5590, 0.0264, 0.0282, 0.0259, 0.0208, 0.0237, 0.0245, 0.0177, 0.0294,
        0.0209, 0.0879, 0.0348, 0.0503, 0.0506], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,928][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0624, 0.0530, 0.0207, 0.0552, 0.0825, 0.0629, 0.0720, 0.0332, 0.0625,
        0.0530, 0.0892, 0.0977, 0.1562, 0.0995], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,928][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([2.1292e-04, 8.3912e-06, 7.7894e-07, 1.8622e-05, 2.0834e-05, 1.6225e-04,
        8.0893e-03, 4.7285e-03, 1.5052e-02, 4.8255e-02, 7.3898e-02, 1.2288e-01,
        3.9431e-01, 3.3236e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,928][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.8572e-04, 1.8481e-05, 2.0628e-06, 8.0930e-05, 5.2114e-05, 7.2995e-04,
        1.0479e-02, 4.9165e-03, 2.8883e-02, 1.0428e-01, 1.4158e-01, 1.9280e-01,
        2.3103e-01, 2.8456e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,929][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0008, 0.0826, 0.2625, 0.0773, 0.0858, 0.1344, 0.0188, 0.0274, 0.0609,
        0.0766, 0.0644, 0.0488, 0.0255, 0.0343], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,929][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.6722, 0.0544, 0.0084, 0.0208, 0.0438, 0.0140, 0.0137, 0.0098, 0.0191,
        0.0146, 0.0183, 0.0270, 0.0507, 0.0331], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:39,930][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([5.8073e-04, 3.1958e-07, 1.5637e-08, 4.2399e-06, 3.1520e-06, 2.8634e-05,
        3.7066e-03, 9.5436e-04, 5.2352e-03, 2.1189e-02, 2.7093e-02, 5.6234e-02,
        2.3064e-01, 3.4119e-01, 3.1314e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,932][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.1251, 0.0822, 0.0378, 0.0523, 0.1188, 0.0814, 0.0369, 0.0530, 0.0511,
        0.0495, 0.0373, 0.0907, 0.0725, 0.0444, 0.0670], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,933][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([5.0344e-05, 5.5689e-02, 3.1932e-01, 3.7491e-02, 3.8480e-02, 9.5024e-02,
        1.2540e-02, 2.9029e-02, 3.8731e-02, 3.1562e-02, 9.6110e-02, 4.9636e-02,
        4.7269e-02, 1.1858e-01, 3.0489e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,934][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([6.8277e-03, 9.9239e-05, 1.8227e-05, 3.9769e-04, 1.4051e-04, 1.7284e-03,
        8.8331e-03, 5.8412e-03, 2.6053e-02, 3.2475e-02, 1.0080e-01, 1.1769e-01,
        2.0892e-01, 2.8236e-01, 2.0782e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,935][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([1.0729e-03, 1.4522e-05, 5.2127e-06, 7.2756e-05, 9.8825e-05, 7.1876e-04,
        9.8031e-03, 8.3067e-03, 4.3197e-02, 4.7270e-02, 1.1742e-01, 5.1751e-02,
        1.5000e-01, 3.0772e-01, 2.6254e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,937][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.4895, 0.0108, 0.0023, 0.0195, 0.0094, 0.0130, 0.0084, 0.0210, 0.0144,
        0.0381, 0.0461, 0.0519, 0.0877, 0.1431, 0.0450], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,938][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.3070, 0.0332, 0.0362, 0.0283, 0.0253, 0.0331, 0.0336, 0.0274, 0.0476,
        0.0356, 0.1258, 0.0468, 0.0795, 0.0765, 0.0641], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,940][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0353, 0.0390, 0.0288, 0.0403, 0.0624, 0.0737, 0.0551, 0.0316, 0.0782,
        0.0671, 0.1078, 0.0724, 0.1010, 0.1094, 0.0979], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,941][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([2.7999e-04, 9.5740e-06, 1.3974e-06, 2.4892e-05, 2.1013e-05, 1.5953e-04,
        8.2541e-03, 3.5099e-03, 8.0524e-03, 4.5152e-02, 4.6370e-02, 6.1603e-02,
        2.5044e-01, 3.3327e-01, 2.4285e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,942][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([2.6722e-04, 1.6179e-05, 2.8044e-06, 6.6560e-05, 3.5709e-05, 8.3908e-04,
        8.1169e-03, 4.1584e-03, 2.4887e-02, 6.2170e-02, 1.2432e-01, 7.7089e-02,
        1.3016e-01, 2.7732e-01, 2.9054e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,943][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0004, 0.0764, 0.1768, 0.0697, 0.0397, 0.1180, 0.0170, 0.0458, 0.0612,
        0.1084, 0.0796, 0.0791, 0.0292, 0.0771, 0.0217], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,944][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.2722, 0.1338, 0.0321, 0.0313, 0.0603, 0.0293, 0.0252, 0.0224, 0.0441,
        0.0326, 0.0473, 0.0575, 0.0666, 0.0777, 0.0677], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:39,944][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([1.5204e-03, 3.1349e-07, 4.0653e-09, 2.9674e-06, 2.6061e-06, 1.6912e-05,
        1.6993e-03, 5.1977e-04, 3.4456e-03, 8.8666e-03, 1.4133e-02, 4.1247e-02,
        1.4974e-01, 9.4473e-02, 4.1003e-01, 2.7431e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,945][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.1406, 0.0789, 0.0367, 0.0553, 0.0931, 0.0777, 0.0346, 0.0386, 0.0440,
        0.0467, 0.0345, 0.0888, 0.0569, 0.0399, 0.0621, 0.0716],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,945][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([4.3595e-05, 3.5867e-02, 3.7739e-01, 1.7454e-02, 6.0163e-02, 9.7731e-02,
        5.1552e-03, 3.5147e-02, 3.0835e-02, 2.6380e-02, 9.0387e-02, 4.4972e-02,
        3.2445e-02, 8.5021e-02, 2.1361e-02, 3.9647e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,945][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([1.4167e-02, 6.7891e-05, 1.3132e-05, 2.7738e-04, 1.2541e-04, 1.2543e-03,
        5.7355e-03, 5.0215e-03, 2.0685e-02, 2.0851e-02, 8.2618e-02, 1.0545e-01,
        1.2844e-01, 2.0704e-01, 1.3777e-01, 2.7049e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,946][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([1.4655e-03, 1.2742e-05, 3.4405e-06, 5.7541e-05, 9.2736e-05, 4.6405e-04,
        4.9303e-03, 5.8797e-03, 2.3100e-02, 2.8350e-02, 5.2468e-02, 5.0196e-02,
        1.3085e-01, 2.2778e-01, 1.8982e-01, 2.8453e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,946][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([7.2182e-01, 3.6892e-03, 5.6444e-04, 6.9527e-03, 3.7991e-03, 4.3772e-03,
        3.3396e-03, 9.6545e-03, 7.5891e-03, 1.6606e-02, 2.9220e-02, 2.0959e-02,
        3.1813e-02, 5.0156e-02, 2.2600e-02, 6.6858e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,947][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.5604, 0.0221, 0.0187, 0.0203, 0.0154, 0.0230, 0.0221, 0.0165, 0.0281,
        0.0176, 0.0688, 0.0279, 0.0400, 0.0389, 0.0357, 0.0445],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,948][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0552, 0.0333, 0.0214, 0.0426, 0.0556, 0.0606, 0.0466, 0.0242, 0.0536,
        0.0816, 0.0970, 0.0593, 0.0976, 0.1120, 0.1004, 0.0589],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,949][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([2.1195e-04, 2.8998e-06, 3.4088e-07, 7.4719e-06, 1.9752e-05, 1.2005e-04,
        6.5507e-03, 2.6926e-03, 9.4213e-03, 4.4941e-02, 4.0575e-02, 3.0430e-02,
        1.5788e-01, 1.6017e-01, 2.6522e-01, 2.8176e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,950][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([5.4768e-04, 1.7012e-05, 2.7829e-06, 7.8345e-05, 4.5739e-05, 6.1892e-04,
        5.8229e-03, 5.0947e-03, 1.7948e-02, 4.9569e-02, 1.0628e-01, 8.8278e-02,
        1.1595e-01, 1.9062e-01, 2.0183e-01, 2.1730e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,952][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0006, 0.0659, 0.1664, 0.0653, 0.0742, 0.1712, 0.0112, 0.0330, 0.0432,
        0.0594, 0.0431, 0.0938, 0.0321, 0.0625, 0.0448, 0.0333],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,953][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.5180, 0.0664, 0.0131, 0.0200, 0.0449, 0.0214, 0.0141, 0.0169, 0.0212,
        0.0169, 0.0201, 0.0294, 0.0496, 0.0452, 0.0402, 0.0626],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:39,954][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([7.7347e-04, 1.0326e-07, 1.2111e-09, 1.1662e-06, 7.9841e-07, 5.2670e-06,
        7.6172e-04, 2.0234e-04, 1.1600e-03, 3.9084e-03, 5.6215e-03, 3.1711e-02,
        8.3643e-02, 6.0661e-02, 2.2854e-01, 3.3447e-01, 2.4854e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,956][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0431, 0.1042, 0.0301, 0.0723, 0.0843, 0.0657, 0.0471, 0.0347, 0.0450,
        0.0584, 0.0278, 0.1354, 0.0546, 0.0286, 0.0539, 0.0858, 0.0290],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,957][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.4302e-04, 5.9765e-02, 3.5552e-01, 4.1043e-02, 9.2314e-02, 7.1570e-02,
        8.3419e-03, 2.2972e-02, 3.3468e-02, 1.3380e-02, 6.9118e-02, 2.6950e-02,
        3.0220e-02, 5.0984e-02, 3.6280e-02, 1.9205e-02, 6.8724e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,958][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([7.8997e-03, 3.6153e-05, 4.4326e-06, 1.3531e-04, 7.2134e-05, 5.7165e-04,
        3.7610e-03, 2.3063e-03, 1.1002e-02, 1.4040e-02, 4.7539e-02, 6.6863e-02,
        1.4554e-01, 1.3308e-01, 1.3141e-01, 1.9141e-01, 2.4432e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,959][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([2.3614e-03, 7.0273e-06, 1.1766e-06, 3.6659e-05, 5.3662e-05, 2.6873e-04,
        3.5580e-03, 3.6471e-03, 1.6362e-02, 1.8936e-02, 4.2624e-02, 3.7041e-02,
        9.2254e-02, 1.3919e-01, 1.9448e-01, 2.0525e-01, 2.4393e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,960][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([7.7772e-01, 3.4920e-03, 3.8334e-04, 5.6529e-03, 2.8154e-03, 4.2380e-03,
        2.4666e-03, 5.8364e-03, 4.5039e-03, 1.1134e-02, 1.3195e-02, 1.4931e-02,
        2.2763e-02, 2.4257e-02, 1.5997e-02, 5.6178e-02, 3.4436e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,961][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.5141, 0.0213, 0.0220, 0.0204, 0.0180, 0.0196, 0.0204, 0.0149, 0.0247,
        0.0204, 0.0741, 0.0269, 0.0411, 0.0418, 0.0352, 0.0413, 0.0437],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,961][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0420, 0.0410, 0.0176, 0.0452, 0.0654, 0.0498, 0.0577, 0.0248, 0.0431,
        0.0431, 0.0629, 0.0687, 0.1067, 0.0775, 0.1080, 0.0625, 0.0840],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,962][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.5246e-04, 3.1775e-06, 2.6368e-07, 6.2261e-06, 8.7514e-06, 5.0947e-05,
        3.1931e-03, 1.4354e-03, 4.2796e-03, 1.8323e-02, 2.2237e-02, 3.5204e-02,
        1.2946e-01, 1.0907e-01, 2.3665e-01, 1.7658e-01, 2.6335e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,962][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([3.8567e-04, 7.9439e-06, 7.7361e-07, 3.5341e-05, 2.2862e-05, 3.0062e-04,
        3.8436e-03, 1.8349e-03, 1.0337e-02, 4.2332e-02, 4.9251e-02, 7.1992e-02,
        9.7939e-02, 1.0586e-01, 1.5340e-01, 2.3337e-01, 2.2908e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,963][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0007, 0.0703, 0.2119, 0.0622, 0.0791, 0.1149, 0.0185, 0.0276, 0.0567,
        0.0711, 0.0663, 0.0405, 0.0203, 0.0371, 0.0519, 0.0231, 0.0478],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,963][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.5388, 0.0508, 0.0095, 0.0202, 0.0439, 0.0160, 0.0135, 0.0102, 0.0189,
        0.0168, 0.0187, 0.0249, 0.0452, 0.0357, 0.0494, 0.0445, 0.0431],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:39,964][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:39,966][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[13907],
        [ 1113],
        [ 1028],
        [ 1970],
        [    5],
        [   10],
        [   23],
        [   24],
        [    6],
        [    9],
        [   20],
        [   53],
        [    4],
        [   13],
        [    5],
        [    3],
        [    8]], device='cuda:0')
[2024-07-24 10:17:39,968][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[13677],
        [ 6247],
        [ 8861],
        [18957],
        [  904],
        [ 1229],
        [ 1002],
        [ 1981],
        [ 1141],
        [  261],
        [  426],
        [ 2131],
        [  217],
        [  988],
        [  194],
        [  208],
        [  518]], device='cuda:0')
[2024-07-24 10:17:39,970][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[36750],
        [33966],
        [27609],
        [27868],
        [30300],
        [35685],
        [35977],
        [34626],
        [36680],
        [37410],
        [38399],
        [34973],
        [27310],
        [32639],
        [34458],
        [20008],
        [33490]], device='cuda:0')
[2024-07-24 10:17:39,971][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[17195],
        [ 1890],
        [ 5135],
        [ 1486],
        [ 1619],
        [ 2254],
        [ 2212],
        [ 2053],
        [ 2266],
        [ 2658],
        [ 2829],
        [ 2580],
        [ 3097],
        [ 3155],
        [ 3512],
        [ 3885],
        [ 3764]], device='cuda:0')
[2024-07-24 10:17:39,973][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[24751],
        [27087],
        [27145],
        [27806],
        [27770],
        [28184],
        [28392],
        [28955],
        [29090],
        [28917],
        [29486],
        [28291],
        [27842],
        [28895],
        [28869],
        [28610],
        [29041]], device='cuda:0')
[2024-07-24 10:17:39,975][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[7751],
        [5717],
        [4004],
        [2372],
        [2338],
        [2154],
        [1785],
        [1930],
        [2098],
        [1897],
        [2008],
        [1978],
        [1988],
        [1995],
        [2081],
        [2174],
        [2204]], device='cuda:0')
[2024-07-24 10:17:39,976][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[23285],
        [16321],
        [17923],
        [28693],
        [28583],
        [30515],
        [31190],
        [32390],
        [32208],
        [22770],
        [21967],
        [23388],
        [28265],
        [22274],
        [27538],
        [27087],
        [19286]], device='cuda:0')
[2024-07-24 10:17:39,978][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[18658],
        [30049],
        [33327],
        [50204],
        [50087],
        [49522],
        [48406],
        [40272],
        [25911],
        [45958],
        [45613],
        [46936],
        [46905],
        [42616],
        [46989],
        [41457],
        [40262]], device='cuda:0')
[2024-07-24 10:17:39,979][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[1050],
        [1460],
        [ 769],
        [ 475],
        [ 909],
        [ 922],
        [1082],
        [ 743],
        [ 874],
        [1087],
        [1064],
        [1214],
        [2250],
        [1739],
        [1835],
        [2722],
        [2414]], device='cuda:0')
[2024-07-24 10:17:39,980][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[46902],
        [37816],
        [36060],
        [31395],
        [21220],
        [16236],
        [14548],
        [13776],
        [11372],
        [ 8015],
        [ 9128],
        [ 8432],
        [ 7447],
        [ 7938],
        [ 8266],
        [ 7897],
        [ 6863]], device='cuda:0')
[2024-07-24 10:17:39,981][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 3929],
        [26094],
        [19766],
        [36109],
        [27203],
        [ 6881],
        [16774],
        [14531],
        [11258],
        [17269],
        [12385],
        [15205],
        [ 2683],
        [ 2612],
        [ 3064],
        [ 3988],
        [ 3996]], device='cuda:0')
[2024-07-24 10:17:39,982][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[14495],
        [15481],
        [ 3061],
        [   28],
        [ 5666],
        [ 6967],
        [ 1781],
        [ 2144],
        [ 2682],
        [ 2198],
        [ 2248],
        [ 7172],
        [17225],
        [19089],
        [13137],
        [10915],
        [10888]], device='cuda:0')
[2024-07-24 10:17:39,984][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[17316],
        [12518],
        [11903],
        [34048],
        [27754],
        [22836],
        [15972],
        [13376],
        [11671],
        [11512],
        [10046],
        [10994],
        [10781],
        [10216],
        [10881],
        [11232],
        [10523]], device='cuda:0')
[2024-07-24 10:17:39,986][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[6708],
        [5881],
        [1225],
        [1103],
        [ 976],
        [ 812],
        [ 882],
        [ 861],
        [ 871],
        [ 870],
        [ 857],
        [ 867],
        [ 757],
        [ 715],
        [ 713],
        [ 693],
        [ 691]], device='cuda:0')
[2024-07-24 10:17:39,987][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[1650],
        [1756],
        [3456],
        [1182],
        [ 520],
        [ 536],
        [ 358],
        [ 322],
        [ 460],
        [ 771],
        [ 623],
        [ 248],
        [ 231],
        [ 336],
        [ 258],
        [ 479],
        [ 238]], device='cuda:0')
[2024-07-24 10:17:39,989][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[17441],
        [16488],
        [18640],
        [15389],
        [ 9111],
        [ 4625],
        [ 5023],
        [ 4334],
        [ 3442],
        [ 6587],
        [ 6458],
        [ 4410],
        [ 1002],
        [ 1385],
        [ 2255],
        [ 2911],
        [ 2797]], device='cuda:0')
[2024-07-24 10:17:39,991][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[6723],
        [   8],
        [   9],
        [ 620],
        [  29],
        [  10],
        [  71],
        [  13],
        [  15],
        [ 304],
        [  11],
        [  10],
        [  11],
        [   3],
        [  10],
        [  10],
        [   4]], device='cuda:0')
[2024-07-24 10:17:39,992][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[25748],
        [41133],
        [48822],
        [48673],
        [48124],
        [44791],
        [44586],
        [45838],
        [38500],
        [46210],
        [37396],
        [46977],
        [47710],
        [40703],
        [38288],
        [41042],
        [39700]], device='cuda:0')
[2024-07-24 10:17:39,994][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[  543],
        [15285],
        [15822],
        [11842],
        [ 9168],
        [22571],
        [13514],
        [14974],
        [24606],
        [13460],
        [13462],
        [16289],
        [14867],
        [15165],
        [13316],
        [12559],
        [12623]], device='cuda:0')
[2024-07-24 10:17:39,995][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[23237],
        [18018],
        [ 5672],
        [ 9841],
        [ 9539],
        [ 4148],
        [  415],
        [  938],
        [ 1443],
        [ 1967],
        [ 1858],
        [ 1988],
        [ 1821],
        [ 2108],
        [ 1160],
        [ 1262],
        [ 1049]], device='cuda:0')
[2024-07-24 10:17:39,997][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[39403],
        [38550],
        [38740],
        [ 6538],
        [16008],
        [35391],
        [37713],
        [39767],
        [39816],
        [42616],
        [42232],
        [41581],
        [42775],
        [44264],
        [38590],
        [43708],
        [44507]], device='cuda:0')
[2024-07-24 10:17:39,998][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[40544],
        [42799],
        [42125],
        [32595],
        [38076],
        [34632],
        [36246],
        [38277],
        [36656],
        [20709],
        [19267],
        [14375],
        [13192],
        [18090],
        [12686],
        [16663],
        [15679]], device='cuda:0')
[2024-07-24 10:17:39,999][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[7070],
        [7373],
        [2223],
        [1886],
        [1275],
        [1179],
        [ 605],
        [ 503],
        [ 620],
        [ 451],
        [ 336],
        [ 354],
        [ 334],
        [ 425],
        [ 528],
        [ 633],
        [ 694]], device='cuda:0')
[2024-07-24 10:17:40,000][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[40688],
        [11188],
        [23087],
        [21563],
        [23385],
        [11222],
        [13440],
        [ 9101],
        [12074],
        [27916],
        [26890],
        [20782],
        [20072],
        [19498],
        [17259],
        [15663],
        [16647]], device='cuda:0')
[2024-07-24 10:17:40,001][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[34950],
        [25397],
        [35113],
        [19580],
        [31740],
        [31496],
        [27508],
        [26533],
        [31793],
        [26026],
        [31141],
        [33092],
        [28561],
        [27810],
        [32950],
        [31917],
        [32111]], device='cuda:0')
[2024-07-24 10:17:40,003][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[36679],
        [44998],
        [50182],
        [50162],
        [50193],
        [50092],
        [49672],
        [50025],
        [49832],
        [49881],
        [49566],
        [50080],
        [49953],
        [49971],
        [49894],
        [49717],
        [49968]], device='cuda:0')
[2024-07-24 10:17:40,005][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[19652],
        [10835],
        [17801],
        [21792],
        [28259],
        [24831],
        [30071],
        [27893],
        [26944],
        [28837],
        [25072],
        [31259],
        [32912],
        [31678],
        [29714],
        [31661],
        [32326]], device='cuda:0')
[2024-07-24 10:17:40,006][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[31018],
        [37796],
        [25469],
        [35723],
        [34758],
        [34768],
        [41881],
        [41413],
        [39177],
        [33593],
        [39699],
        [36384],
        [39362],
        [38818],
        [39971],
        [38129],
        [37206]], device='cuda:0')
[2024-07-24 10:17:40,008][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[47570],
        [48965],
        [45271],
        [48227],
        [49217],
        [49827],
        [49860],
        [49935],
        [49878],
        [49447],
        [49813],
        [49982],
        [50021],
        [50032],
        [50116],
        [49974],
        [50089]], device='cuda:0')
[2024-07-24 10:17:40,010][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357],
        [5357]], device='cuda:0')
[2024-07-24 10:17:40,063][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:40,064][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,065][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,067][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,068][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,069][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,070][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,072][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,073][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,074][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,075][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,076][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,076][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,076][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9973, 0.0027], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,077][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0262, 0.9738], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,077][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.9624, 0.0376], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,077][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.7659, 0.2341], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,078][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.1074, 0.8926], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,078][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.5941, 0.4059], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,078][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.8384, 0.1616], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,078][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0710, 0.9290], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,079][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.6926, 0.3074], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,079][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.8534, 0.1466], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,080][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.5257, 0.4743], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,081][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.2695, 0.7305], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,083][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.9753, 0.0035, 0.0212], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,084][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0017, 0.4714, 0.5268], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,086][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.9253, 0.0631, 0.0116], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,086][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.5463, 0.2332, 0.2205], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,088][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0069, 0.8181, 0.1750], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,090][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1236, 0.7231, 0.1532], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,091][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2467, 0.4227, 0.3306], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,093][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0158, 0.8448, 0.1394], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,093][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6121, 0.2498, 0.1381], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,094][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.9262, 0.0433, 0.0305], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,094][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.4750, 0.2539, 0.2711], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,094][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2786, 0.3408, 0.3806], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,095][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.9302, 0.0128, 0.0417, 0.0152], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,095][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([4.4159e-04, 3.2969e-01, 5.9942e-01, 7.0446e-02], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,095][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.7912, 0.0715, 0.0360, 0.1014], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,096][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.2796, 0.2183, 0.2519, 0.2502], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,096][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0097, 0.1646, 0.1023, 0.7234], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,096][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0984, 0.1201, 0.0481, 0.7334], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,097][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.0468, 0.2322, 0.4898, 0.2312], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,097][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0043, 0.0992, 0.0523, 0.8442], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,099][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.1651, 0.3453, 0.1248, 0.3648], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,101][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.4504, 0.1528, 0.1897, 0.2072], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,102][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.0974, 0.3109, 0.2131, 0.3786], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,103][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0306, 0.2004, 0.4455, 0.3235], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,104][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.9565, 0.0036, 0.0321, 0.0045, 0.0034], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,106][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0027, 0.2521, 0.5228, 0.0938, 0.1285], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,108][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.7197, 0.1148, 0.0271, 0.0829, 0.0555], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,109][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.1367, 0.2652, 0.2047, 0.1920, 0.2014], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,110][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0148, 0.0674, 0.0363, 0.2107, 0.6708], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,111][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0549, 0.0943, 0.0165, 0.5245, 0.3097], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,111][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.3134, 0.1842, 0.1408, 0.1527, 0.2089], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,111][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0102, 0.0378, 0.0120, 0.6070, 0.3331], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,112][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.5158, 0.1056, 0.0420, 0.2417, 0.0949], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,112][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.6164, 0.0921, 0.0575, 0.1616, 0.0724], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,112][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.1650, 0.1542, 0.0983, 0.2365, 0.3461], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,113][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0522, 0.1447, 0.1577, 0.2175, 0.4278], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,113][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([2.6382e-01, 2.1952e-04, 1.6390e-03, 3.1522e-04, 1.9706e-04, 7.3381e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,113][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0040, 0.2259, 0.3149, 0.0773, 0.1655, 0.2124], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,114][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.8477, 0.0531, 0.0179, 0.0243, 0.0262, 0.0307], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,114][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.2664, 0.1452, 0.0642, 0.0948, 0.0938, 0.3356], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,116][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0032, 0.0051, 0.0013, 0.0158, 0.0391, 0.9354], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,118][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1661, 0.0461, 0.0031, 0.2336, 0.0928, 0.4582], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,119][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3743, 0.1104, 0.0632, 0.1283, 0.1789, 0.1451], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,120][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([5.3195e-03, 5.1768e-03, 5.9584e-04, 5.1949e-02, 2.6356e-02, 9.1060e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,121][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.8052, 0.0301, 0.0068, 0.0645, 0.0187, 0.0746], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,123][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.8377, 0.0277, 0.0122, 0.0429, 0.0196, 0.0600], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,125][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.2197, 0.0379, 0.0235, 0.0639, 0.0921, 0.5629], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,126][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0408, 0.0250, 0.0154, 0.0267, 0.0419, 0.8501], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,127][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([1.9494e-01, 3.9339e-04, 2.6793e-03, 9.9763e-04, 4.4970e-04, 8.0027e-01,
        2.6651e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,128][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0270, 0.2956, 0.1370, 0.0855, 0.0820, 0.2091, 0.1639],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,128][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.8405, 0.0222, 0.0133, 0.0257, 0.0289, 0.0375, 0.0319],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,129][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.1475, 0.0479, 0.0406, 0.0692, 0.0446, 0.1655, 0.4847],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,129][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([6.1996e-04, 4.1669e-05, 9.3138e-06, 8.6778e-05, 4.6243e-04, 8.3813e-03,
        9.9040e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,129][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([5.5953e-02, 4.3909e-04, 3.4743e-05, 2.4099e-03, 1.3450e-03, 8.3397e-03,
        9.3148e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,130][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.3654, 0.0453, 0.0454, 0.0690, 0.1585, 0.1459, 0.1707],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,130][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([2.0710e-03, 2.0433e-05, 1.7286e-06, 1.8007e-04, 1.8021e-04, 4.0743e-03,
        9.9347e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,130][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.7506, 0.0100, 0.0034, 0.0261, 0.0098, 0.0331, 0.1670],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,131][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.6962, 0.0171, 0.0089, 0.0222, 0.0165, 0.0322, 0.2069],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,131][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.1600, 0.0161, 0.0120, 0.0263, 0.0622, 0.2101, 0.5134],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,132][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0505, 0.0078, 0.0066, 0.0133, 0.0294, 0.4892, 0.4031],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,132][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ at] are: tensor([1.7766e-01, 1.6001e-04, 1.6472e-03, 2.5562e-04, 1.0650e-04, 8.1787e-01,
        4.1014e-05, 2.2577e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,134][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0041, 0.1377, 0.1876, 0.0843, 0.0831, 0.2005, 0.2334, 0.0692],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,136][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.8156, 0.0416, 0.0162, 0.0277, 0.0265, 0.0276, 0.0259, 0.0189],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,137][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.3641, 0.0523, 0.0353, 0.0574, 0.0345, 0.1828, 0.0778, 0.1958],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,138][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ at] are: tensor([6.5696e-04, 5.7122e-05, 5.2893e-06, 1.7020e-04, 4.2988e-04, 5.2233e-03,
        9.0032e-01, 9.3133e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,139][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ at] are: tensor([1.7913e-02, 4.8887e-04, 1.6848e-05, 2.3334e-03, 6.6140e-04, 3.6101e-03,
        8.9967e-01, 7.5304e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,141][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.2582, 0.0590, 0.0323, 0.1290, 0.1064, 0.1262, 0.2058, 0.0831],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,142][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ at] are: tensor([5.3829e-04, 1.3569e-05, 5.7612e-07, 1.1598e-04, 4.9068e-05, 1.5900e-03,
        7.4786e-01, 2.4983e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,143][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.8503, 0.0050, 0.0011, 0.0148, 0.0037, 0.0147, 0.0764, 0.0340],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,145][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.9137, 0.0076, 0.0023, 0.0119, 0.0032, 0.0105, 0.0406, 0.0103],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,146][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.3000, 0.0089, 0.0059, 0.0244, 0.0183, 0.1320, 0.1986, 0.3119],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,146][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0478, 0.0058, 0.0027, 0.0070, 0.0083, 0.2088, 0.1930, 0.5265],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,146][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.0639e-02, 4.8212e-05, 2.6806e-04, 7.0570e-05, 3.6972e-05, 2.5253e-01,
        8.6705e-06, 6.5264e-04, 6.6574e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,147][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0101, 0.2294, 0.1161, 0.0927, 0.0683, 0.1264, 0.1762, 0.0774, 0.1034],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,147][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.8952, 0.0174, 0.0060, 0.0129, 0.0121, 0.0129, 0.0112, 0.0105, 0.0218],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,147][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2396, 0.0406, 0.0171, 0.0396, 0.0183, 0.0976, 0.1001, 0.1287, 0.3184],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,148][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([3.3901e-04, 1.5433e-05, 1.6283e-06, 5.8639e-05, 8.1204e-05, 1.9606e-03,
        3.8850e-01, 8.0958e-02, 5.2809e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,148][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([3.8585e-02, 2.9461e-04, 7.8307e-06, 1.2441e-03, 3.3201e-04, 2.0538e-03,
        6.1852e-01, 8.3622e-02, 2.5534e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,149][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5513, 0.0453, 0.0179, 0.0993, 0.0536, 0.0730, 0.0784, 0.0384, 0.0427],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,149][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([7.8164e-04, 5.0608e-06, 2.5752e-07, 5.3592e-05, 2.1826e-05, 5.6170e-04,
        2.3612e-01, 1.5930e-01, 6.0316e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,149][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([9.4946e-01, 1.3388e-03, 2.0988e-04, 4.5571e-03, 9.1770e-04, 3.6530e-03,
        1.7482e-02, 9.1581e-03, 1.3221e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,151][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.8985, 0.0056, 0.0016, 0.0076, 0.0027, 0.0086, 0.0292, 0.0103, 0.0358],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,153][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1914, 0.0063, 0.0027, 0.0121, 0.0118, 0.0710, 0.1193, 0.1439, 0.4414],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,154][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0467, 0.0031, 0.0008, 0.0031, 0.0031, 0.0704, 0.0726, 0.1737, 0.6264],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,155][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ station] are: tensor([1.0095e-01, 5.9517e-05, 3.2293e-04, 9.5741e-05, 4.2758e-05, 2.5534e-01,
        1.1668e-05, 6.3843e-04, 6.4171e-01, 8.3499e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,156][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0064, 0.2651, 0.1356, 0.0455, 0.0667, 0.1173, 0.0696, 0.0413, 0.1179,
        0.1346], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,158][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.8428, 0.0121, 0.0057, 0.0113, 0.0185, 0.0237, 0.0189, 0.0153, 0.0296,
        0.0222], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,160][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.1656, 0.0270, 0.0171, 0.0329, 0.0193, 0.1134, 0.0786, 0.0988, 0.2598,
        0.1875], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,161][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ station] are: tensor([7.4033e-04, 1.4038e-06, 1.8796e-07, 5.9044e-06, 1.0399e-05, 3.3900e-04,
        2.6166e-02, 8.9147e-03, 6.2744e-02, 9.0108e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,162][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ station] are: tensor([9.4234e-02, 8.5438e-05, 2.1888e-06, 3.4612e-04, 1.1537e-04, 7.9258e-04,
        1.4786e-01, 1.7839e-02, 7.4223e-02, 6.6450e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,163][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.3502, 0.0426, 0.0415, 0.0722, 0.1094, 0.0983, 0.1054, 0.0497, 0.0488,
        0.0819], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,163][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ station] are: tensor([1.0643e-03, 3.0481e-07, 1.2860e-08, 3.3722e-06, 1.2627e-06, 4.6781e-05,
        1.0258e-02, 9.4119e-03, 3.8578e-02, 9.4064e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,164][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ station] are: tensor([8.3891e-01, 3.1775e-03, 7.3016e-04, 6.5753e-03, 2.0952e-03, 1.0098e-02,
        3.7597e-02, 2.4451e-02, 4.4944e-02, 3.1419e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,164][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.8404, 0.0086, 0.0025, 0.0096, 0.0032, 0.0111, 0.0488, 0.0126, 0.0342,
        0.0291], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,165][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.1736, 0.0036, 0.0016, 0.0073, 0.0084, 0.0493, 0.0656, 0.0912, 0.2607,
        0.3386], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,165][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0130, 0.0013, 0.0007, 0.0016, 0.0021, 0.0571, 0.0318, 0.1643, 0.5143,
        0.2138], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,165][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [,] are: tensor([1.5503e-01, 9.2024e-05, 6.5086e-04, 1.5709e-04, 4.1834e-05, 2.6528e-01,
        1.5911e-05, 5.7759e-04, 5.7252e-01, 5.3723e-04, 5.0959e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,166][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0031, 0.2356, 0.0679, 0.0948, 0.0451, 0.1526, 0.0847, 0.0320, 0.0790,
        0.0869, 0.1182], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,166][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.8569, 0.0138, 0.0074, 0.0139, 0.0156, 0.0168, 0.0132, 0.0152, 0.0246,
        0.0092, 0.0134], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,166][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0749, 0.0285, 0.0101, 0.0211, 0.0153, 0.0884, 0.0740, 0.0875, 0.2564,
        0.1769, 0.1669], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,167][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [,] are: tensor([1.6785e-04, 2.1892e-06, 2.4686e-07, 5.8577e-06, 9.4447e-06, 2.7078e-04,
        4.1683e-02, 8.7832e-03, 7.6622e-02, 6.8409e-01, 1.8837e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,168][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [,] are: tensor([1.3224e-02, 7.2035e-05, 1.6133e-06, 2.0953e-04, 6.2537e-05, 5.2253e-04,
        8.9530e-02, 1.4338e-02, 5.0711e-02, 5.7741e-01, 2.5392e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,170][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.1309, 0.0615, 0.0184, 0.0993, 0.0888, 0.1344, 0.1142, 0.0737, 0.0819,
        0.0618, 0.1352], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,171][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [,] are: tensor([1.0575e-04, 1.9006e-07, 6.0588e-09, 1.8517e-06, 4.8091e-07, 3.1275e-05,
        1.3012e-02, 4.2735e-03, 3.0098e-02, 8.4167e-01, 1.1081e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,172][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [,] are: tensor([8.3589e-01, 1.9361e-03, 3.1484e-04, 5.1077e-03, 1.5807e-03, 5.9263e-03,
        3.2380e-02, 1.5442e-02, 2.6662e-02, 2.0700e-02, 5.4060e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,173][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.7816, 0.0071, 0.0020, 0.0086, 0.0035, 0.0106, 0.0366, 0.0175, 0.0567,
        0.0339, 0.0420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,175][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0607, 0.0034, 0.0011, 0.0069, 0.0056, 0.0303, 0.0475, 0.0529, 0.1810,
        0.1719, 0.4387], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,176][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [,] are: tensor([7.7315e-03, 1.2200e-03, 3.1192e-04, 8.8975e-04, 8.6719e-04, 3.2695e-02,
        1.8287e-02, 6.5745e-02, 3.2501e-01, 1.2703e-01, 4.2022e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,177][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([2.4391e-01, 2.5938e-04, 9.8451e-04, 4.7289e-04, 1.2561e-04, 2.5259e-01,
        5.2126e-05, 8.8132e-04, 4.8186e-01, 1.0974e-03, 6.2510e-03, 1.1521e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,179][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0003, 0.0358, 0.0522, 0.0128, 0.0188, 0.1143, 0.0909, 0.0387, 0.1292,
        0.2029, 0.2189, 0.0851], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,180][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.8376, 0.0122, 0.0070, 0.0232, 0.0096, 0.0136, 0.0165, 0.0073, 0.0209,
        0.0161, 0.0105, 0.0256], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,181][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.1820, 0.0106, 0.0054, 0.0115, 0.0146, 0.0590, 0.0769, 0.0650, 0.1885,
        0.1614, 0.1165, 0.1086], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,181][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([2.4810e-03, 9.1256e-06, 7.3731e-07, 2.5204e-05, 2.9943e-05, 2.6820e-04,
        1.9930e-02, 6.1701e-03, 4.7649e-02, 4.7449e-01, 1.1568e-01, 3.3327e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,181][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([2.8338e-02, 3.7765e-05, 1.2876e-06, 1.6619e-04, 3.7965e-05, 1.4907e-04,
        1.3094e-02, 4.4811e-03, 1.0353e-02, 1.3669e-01, 9.3788e-02, 7.1286e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,182][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.1196, 0.0239, 0.0187, 0.0276, 0.0568, 0.0841, 0.0964, 0.0650, 0.0989,
        0.1059, 0.1631, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,182][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([2.5141e-03, 6.3387e-07, 1.2437e-08, 5.1552e-06, 1.0997e-06, 1.4674e-05,
        3.9088e-03, 1.3280e-03, 6.5091e-03, 2.0698e-01, 4.2354e-02, 7.3639e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,183][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.6727, 0.0060, 0.0008, 0.0094, 0.0032, 0.0108, 0.0511, 0.0160, 0.0368,
        0.0231, 0.0608, 0.1094], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,183][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.6657, 0.0111, 0.0036, 0.0107, 0.0047, 0.0129, 0.0522, 0.0195, 0.0553,
        0.0505, 0.0503, 0.0636], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,183][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.1076, 0.0045, 0.0013, 0.0064, 0.0060, 0.0234, 0.0518, 0.0387, 0.1137,
        0.1492, 0.2439, 0.2534], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,184][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0085, 0.0005, 0.0006, 0.0006, 0.0012, 0.0268, 0.0229, 0.0754, 0.2915,
        0.1646, 0.3733, 0.0341], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,184][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([1.6618e-01, 9.0691e-05, 8.7245e-04, 1.9706e-04, 1.0524e-04, 2.5792e-01,
        4.9975e-05, 1.2895e-03, 5.3952e-01, 1.3189e-03, 7.7517e-03, 6.2577e-03,
        1.8451e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,186][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0005, 0.0930, 0.0944, 0.0203, 0.0391, 0.0788, 0.0882, 0.0313, 0.0984,
        0.1292, 0.1931, 0.0888, 0.0449], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,188][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.4729, 0.0366, 0.0196, 0.0266, 0.0389, 0.0518, 0.0340, 0.0356, 0.0867,
        0.0403, 0.0428, 0.0476, 0.0665], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,189][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0839, 0.0305, 0.0118, 0.0183, 0.0100, 0.0523, 0.0665, 0.0684, 0.1720,
        0.1109, 0.1336, 0.1267, 0.1150], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,190][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([5.4566e-04, 1.4374e-06, 1.7171e-07, 4.2518e-06, 7.2120e-06, 1.7659e-04,
        7.9285e-03, 5.0178e-03, 2.9795e-02, 2.8664e-01, 5.5319e-02, 7.7944e-02,
        5.3662e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,191][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([9.3821e-03, 1.7545e-05, 5.2434e-07, 5.4242e-05, 1.5705e-05, 1.4198e-04,
        1.3875e-02, 3.0573e-03, 8.5101e-03, 1.0857e-01, 6.1914e-02, 4.9191e-01,
        3.0256e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,193][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0439, 0.0381, 0.0118, 0.0439, 0.0343, 0.0533, 0.1115, 0.0511, 0.0715,
        0.0937, 0.1439, 0.1226, 0.1804], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,194][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([3.9286e-04, 1.9853e-07, 5.3981e-09, 1.2057e-06, 5.9980e-07, 2.2165e-05,
        2.5468e-03, 1.9316e-03, 8.2704e-03, 1.2071e-01, 4.3421e-02, 4.0071e-01,
        4.2199e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,195][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([6.9478e-01, 2.4862e-03, 5.5408e-04, 6.5169e-03, 2.0567e-03, 9.3706e-03,
        3.2459e-02, 2.1839e-02, 3.3647e-02, 3.2134e-02, 5.4554e-02, 3.4802e-02,
        7.4805e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,197][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.4918, 0.0143, 0.0052, 0.0097, 0.0084, 0.0261, 0.0649, 0.0367, 0.0905,
        0.0764, 0.0686, 0.0546, 0.0527], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,198][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0291, 0.0023, 0.0007, 0.0034, 0.0035, 0.0185, 0.0338, 0.0326, 0.1095,
        0.1243, 0.2273, 0.1628, 0.2521], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,198][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0075, 0.0006, 0.0004, 0.0005, 0.0011, 0.0279, 0.0239, 0.0765, 0.2375,
        0.1372, 0.3420, 0.0501, 0.0948], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,199][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([1.2186e-01, 5.0531e-05, 3.2649e-04, 9.0247e-05, 3.2392e-05, 2.2272e-01,
        8.8910e-06, 6.8526e-04, 6.0539e-01, 4.7022e-04, 4.6238e-03, 4.0066e-03,
        8.5990e-03, 3.1125e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,199][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0028, 0.0871, 0.0614, 0.0356, 0.0452, 0.0901, 0.0748, 0.0233, 0.0830,
        0.0829, 0.1635, 0.0815, 0.0975, 0.0712], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,200][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.8024, 0.0178, 0.0071, 0.0122, 0.0120, 0.0140, 0.0122, 0.0102, 0.0267,
        0.0074, 0.0135, 0.0202, 0.0222, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,200][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1351, 0.0177, 0.0069, 0.0160, 0.0089, 0.0551, 0.0350, 0.0529, 0.1407,
        0.0859, 0.1009, 0.1180, 0.0892, 0.1376], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,200][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([2.3312e-04, 9.4116e-07, 3.3575e-08, 1.7437e-06, 2.0210e-06, 6.2009e-05,
        5.9974e-03, 2.4534e-03, 1.4858e-02, 1.2080e-01, 2.6587e-02, 8.6314e-02,
        3.8121e-01, 3.6148e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,201][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([5.3393e-03, 8.9489e-06, 9.9402e-08, 2.7177e-05, 5.5693e-06, 3.0666e-05,
        7.5424e-03, 1.0345e-03, 3.5558e-03, 4.6193e-02, 2.8694e-02, 4.7729e-01,
        2.5275e-01, 1.7753e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,201][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0744, 0.0282, 0.0111, 0.0577, 0.0458, 0.0812, 0.0515, 0.0429, 0.0699,
        0.0264, 0.0999, 0.1314, 0.1149, 0.1646], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,202][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([3.8881e-04, 4.0975e-08, 5.8471e-10, 3.5223e-07, 6.2091e-08, 1.8164e-06,
        6.5704e-04, 4.0847e-04, 1.2978e-03, 3.4582e-02, 1.2808e-02, 2.8801e-01,
        2.2370e-01, 4.3815e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,203][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([8.1404e-01, 1.1460e-03, 1.5082e-04, 2.7401e-03, 9.1814e-04, 3.0452e-03,
        1.3019e-02, 8.1306e-03, 1.2931e-02, 8.9648e-03, 2.4254e-02, 2.1416e-02,
        3.8820e-02, 5.0426e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,205][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.7372, 0.0076, 0.0020, 0.0074, 0.0030, 0.0104, 0.0313, 0.0122, 0.0370,
        0.0267, 0.0286, 0.0320, 0.0206, 0.0439], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,206][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0398, 0.0010, 0.0004, 0.0021, 0.0021, 0.0118, 0.0162, 0.0243, 0.0686,
        0.0575, 0.1736, 0.1195, 0.1370, 0.3459], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,207][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.0406e-02, 5.2664e-04, 1.8633e-04, 4.6772e-04, 7.2922e-04, 1.7715e-02,
        1.1400e-02, 3.3619e-02, 1.4996e-01, 6.1480e-02, 2.3801e-01, 5.3110e-02,
        6.6845e-02, 3.5555e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,208][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ give] are: tensor([1.0268e-01, 7.6200e-05, 5.2761e-04, 1.8508e-04, 8.3100e-05, 2.4706e-01,
        2.8134e-05, 1.2495e-03, 5.6031e-01, 8.7914e-04, 7.4216e-03, 6.2333e-03,
        1.8757e-02, 5.2409e-02, 2.1055e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,210][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0006, 0.0730, 0.0471, 0.0125, 0.0231, 0.0722, 0.0508, 0.0304, 0.0965,
        0.1414, 0.1856, 0.0888, 0.0679, 0.0727, 0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,212][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.6405, 0.0177, 0.0095, 0.0193, 0.0191, 0.0225, 0.0235, 0.0182, 0.0365,
        0.0131, 0.0183, 0.0297, 0.0389, 0.0305, 0.0625], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,214][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0259, 0.0126, 0.0049, 0.0078, 0.0065, 0.0352, 0.0790, 0.0347, 0.1297,
        0.0738, 0.1112, 0.0797, 0.0786, 0.1599, 0.1604], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,215][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ give] are: tensor([1.3769e-04, 2.5254e-07, 2.1048e-08, 7.2553e-07, 9.5441e-07, 3.2250e-05,
        2.3503e-03, 9.1516e-04, 8.9611e-03, 8.4979e-02, 1.8437e-02, 2.2320e-02,
        1.4738e-01, 2.2643e-01, 4.8806e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,215][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ give] are: tensor([5.0362e-03, 1.1497e-05, 1.0117e-07, 2.0398e-05, 4.5189e-06, 4.4747e-05,
        4.1576e-03, 5.6611e-04, 3.2495e-03, 3.7317e-02, 1.8467e-02, 2.9484e-01,
        1.9174e-01, 1.8368e-01, 2.6087e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,216][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0507, 0.0272, 0.0118, 0.0345, 0.0262, 0.0817, 0.0393, 0.0327, 0.0678,
        0.0412, 0.0831, 0.1228, 0.0792, 0.1209, 0.1809], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,216][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ give] are: tensor([2.1530e-04, 2.2984e-08, 3.2499e-10, 1.3344e-07, 4.2530e-08, 1.8560e-06,
        2.5609e-04, 2.0683e-04, 7.5195e-04, 1.6199e-02, 5.7778e-03, 7.8134e-02,
        8.8828e-02, 1.9645e-01, 6.1318e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,217][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.3393, 0.0022, 0.0005, 0.0052, 0.0022, 0.0104, 0.0234, 0.0188, 0.0439,
        0.0364, 0.0764, 0.0412, 0.0818, 0.1780, 0.1403], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,217][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.4263, 0.0096, 0.0025, 0.0085, 0.0050, 0.0167, 0.0531, 0.0203, 0.0718,
        0.0609, 0.0503, 0.0476, 0.0323, 0.1071, 0.0881], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,217][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0158, 0.0011, 0.0003, 0.0016, 0.0016, 0.0099, 0.0153, 0.0183, 0.0610,
        0.0615, 0.1310, 0.0778, 0.1071, 0.2589, 0.2387], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,218][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ give] are: tensor([1.3458e-03, 2.1537e-04, 9.2176e-05, 2.0284e-04, 4.7640e-04, 1.0095e-02,
        1.0543e-02, 3.1268e-02, 1.3797e-01, 8.2874e-02, 2.3309e-01, 2.6205e-02,
        6.8047e-02, 3.2609e-01, 7.1491e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,218][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ it] are: tensor([1.5918e-01, 7.0710e-05, 4.9369e-04, 1.2902e-04, 5.8350e-05, 1.8650e-01,
        1.9777e-05, 7.3176e-04, 5.2358e-01, 8.4879e-04, 6.8110e-03, 5.1377e-03,
        1.5865e-02, 3.6171e-02, 1.0542e-03, 6.3342e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,219][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0021, 0.0928, 0.0471, 0.0221, 0.0231, 0.0958, 0.0524, 0.0269, 0.0710,
        0.1094, 0.1525, 0.0773, 0.0505, 0.0575, 0.0713, 0.0483],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,220][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.6547, 0.0084, 0.0068, 0.0099, 0.0088, 0.0181, 0.0153, 0.0131, 0.0410,
        0.0089, 0.0245, 0.0169, 0.0290, 0.0338, 0.0513, 0.0593],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,222][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0746, 0.0090, 0.0036, 0.0069, 0.0050, 0.0287, 0.0277, 0.0337, 0.0812,
        0.0619, 0.0738, 0.0899, 0.0686, 0.0898, 0.1048, 0.2407],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,223][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ it] are: tensor([2.2287e-04, 4.0723e-07, 2.9169e-08, 9.2023e-07, 1.1837e-06, 3.2035e-05,
        1.9084e-03, 8.2176e-04, 5.7174e-03, 4.5929e-02, 9.5610e-03, 2.1650e-02,
        1.3663e-01, 1.1480e-01, 3.9518e-01, 2.6754e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,224][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ it] are: tensor([3.2743e-03, 4.6797e-06, 7.2471e-08, 1.5012e-05, 3.4866e-06, 1.9464e-05,
        4.2535e-03, 6.0361e-04, 2.3630e-03, 2.4008e-02, 1.7299e-02, 1.9980e-01,
        1.6659e-01, 1.0860e-01, 2.1627e-01, 2.5690e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,225][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0481, 0.0197, 0.0092, 0.0394, 0.0257, 0.0775, 0.0251, 0.0334, 0.0558,
        0.0407, 0.1029, 0.0946, 0.0682, 0.1408, 0.1092, 0.1097],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,226][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ it] are: tensor([1.9605e-04, 1.6263e-08, 1.6883e-10, 9.8682e-08, 2.7259e-08, 7.6417e-07,
        1.4880e-04, 8.4086e-05, 3.2718e-04, 7.4680e-03, 2.5191e-03, 4.3197e-02,
        5.4225e-02, 6.7839e-02, 3.0461e-01, 5.1939e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,227][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ it] are: tensor([6.0030e-01, 7.1065e-04, 1.7029e-04, 2.2314e-03, 6.5308e-04, 3.8178e-03,
        1.3126e-02, 1.0320e-02, 1.8136e-02, 1.2260e-02, 4.5241e-02, 1.8915e-02,
        4.6478e-02, 8.0182e-02, 7.9578e-02, 6.7884e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,229][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.6365, 0.0052, 0.0011, 0.0043, 0.0033, 0.0098, 0.0274, 0.0142, 0.0423,
        0.0215, 0.0324, 0.0241, 0.0184, 0.0450, 0.0473, 0.0673],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,230][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ it] are: tensor([1.9664e-02, 4.4391e-04, 1.8134e-04, 8.0077e-04, 8.8756e-04, 6.3386e-03,
        7.2468e-03, 1.1477e-02, 3.6108e-02, 2.6169e-02, 8.7622e-02, 4.3614e-02,
        7.3515e-02, 1.6190e-01, 1.2392e-01, 4.0010e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,231][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ it] are: tensor([8.5115e-03, 1.3504e-04, 8.5687e-05, 1.5431e-04, 3.2964e-04, 1.0693e-02,
        5.9867e-03, 3.2560e-02, 1.2152e-01, 4.7943e-02, 1.6702e-01, 2.6359e-02,
        4.3826e-02, 3.2204e-01, 6.3483e-02, 1.4935e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,232][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([1.0615e-01, 5.2915e-05, 3.3966e-04, 8.3051e-05, 3.3346e-05, 1.9781e-01,
        9.5811e-06, 6.6591e-04, 5.6358e-01, 4.9973e-04, 4.2201e-03, 3.4416e-03,
        8.6619e-03, 2.8782e-02, 7.4887e-04, 5.4017e-02, 3.0899e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,233][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0020, 0.0829, 0.0560, 0.0320, 0.0362, 0.0806, 0.0701, 0.0214, 0.0669,
        0.0729, 0.1290, 0.0612, 0.0536, 0.0595, 0.0624, 0.0385, 0.0746],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,233][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.7915, 0.0126, 0.0051, 0.0098, 0.0085, 0.0101, 0.0087, 0.0073, 0.0183,
        0.0054, 0.0095, 0.0159, 0.0158, 0.0157, 0.0243, 0.0233, 0.0183],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,234][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0896, 0.0124, 0.0048, 0.0109, 0.0061, 0.0351, 0.0211, 0.0393, 0.0868,
        0.0562, 0.0684, 0.0745, 0.0551, 0.0825, 0.0568, 0.1902, 0.1099],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,234][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.6467e-04, 2.2947e-07, 6.7902e-09, 4.5278e-07, 4.7230e-07, 1.3344e-05,
        1.1002e-03, 4.6672e-04, 2.5449e-03, 2.4801e-02, 3.9855e-03, 1.7259e-02,
        8.4574e-02, 6.6450e-02, 3.3830e-01, 2.1594e-01, 2.4440e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,235][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([4.9696e-03, 3.8882e-06, 3.6018e-08, 1.1501e-05, 2.5844e-06, 1.2223e-05,
        2.6582e-03, 3.7397e-04, 1.3244e-03, 1.7407e-02, 1.0011e-02, 1.6885e-01,
        1.1150e-01, 6.3550e-02, 1.7631e-01, 2.2672e-01, 2.1629e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,235][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0428, 0.0277, 0.0072, 0.0503, 0.0304, 0.0530, 0.0395, 0.0266, 0.0423,
        0.0232, 0.0607, 0.1052, 0.0704, 0.1087, 0.1128, 0.0876, 0.1116],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,236][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.7618e-04, 6.8660e-09, 5.7343e-11, 4.6626e-08, 8.7154e-09, 2.2747e-07,
        7.4376e-05, 4.3128e-05, 1.3319e-04, 3.8711e-03, 1.2107e-03, 3.1187e-02,
        2.8080e-02, 4.2790e-02, 2.0563e-01, 4.3061e-01, 2.5619e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,236][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([7.1875e-01, 1.0275e-03, 1.4749e-04, 2.4349e-03, 8.8065e-04, 2.8605e-03,
        1.0444e-02, 7.1933e-03, 1.1211e-02, 8.5748e-03, 2.1462e-02, 1.5739e-02,
        3.1917e-02, 4.4379e-02, 4.2537e-02, 3.4106e-02, 4.6337e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,237][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.6887, 0.0060, 0.0015, 0.0054, 0.0024, 0.0075, 0.0248, 0.0093, 0.0260,
        0.0193, 0.0196, 0.0220, 0.0142, 0.0304, 0.0325, 0.0558, 0.0346],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,238][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([2.3726e-02, 4.7178e-04, 1.8397e-04, 9.7114e-04, 9.8561e-04, 5.0629e-03,
        6.8300e-03, 1.0206e-02, 2.8243e-02, 2.4203e-02, 7.2766e-02, 4.8151e-02,
        6.1652e-02, 1.4080e-01, 1.1364e-01, 2.6752e-01, 1.9459e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,240][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([7.5518e-03, 3.1489e-04, 1.2065e-04, 2.8476e-04, 4.9060e-04, 1.1809e-02,
        7.0435e-03, 2.3303e-02, 9.9528e-02, 3.8847e-02, 1.4935e-01, 2.9604e-02,
        3.8125e-02, 2.2349e-01, 5.5704e-02, 1.1910e-01, 1.9533e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,293][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:40,293][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,294][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,294][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,294][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,295][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,295][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,295][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,296][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,296][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,296][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,297][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,297][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,299][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.9973, 0.0027], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,300][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0262, 0.9738], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,302][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.9624, 0.0376], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,303][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.7659, 0.2341], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,305][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.1074, 0.8926], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,307][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.5941, 0.4059], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,308][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.8384, 0.1616], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,309][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0710, 0.9290], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,310][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.6926, 0.3074], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,311][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.8534, 0.1466], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,311][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.5257, 0.4743], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,311][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.2695, 0.7305], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,312][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.9753, 0.0035, 0.0212], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,312][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0017, 0.4714, 0.5268], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,312][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.9253, 0.0631, 0.0116], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,313][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5463, 0.2332, 0.2205], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,313][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0069, 0.8181, 0.1750], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,313][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1236, 0.7231, 0.1532], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,314][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2467, 0.4227, 0.3306], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,314][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0158, 0.8448, 0.1394], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,315][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.6121, 0.2498, 0.1381], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,316][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.9262, 0.0433, 0.0305], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,318][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.4750, 0.2539, 0.2711], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,320][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.2786, 0.3408, 0.3806], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,321][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.9302, 0.0128, 0.0417, 0.0152], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,322][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([4.4159e-04, 3.2969e-01, 5.9942e-01, 7.0446e-02], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,324][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.7912, 0.0715, 0.0360, 0.1014], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,325][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.2796, 0.2183, 0.2519, 0.2502], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,327][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0097, 0.1646, 0.1023, 0.7234], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,329][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.0984, 0.1201, 0.0481, 0.7334], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,330][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.0468, 0.2322, 0.4898, 0.2312], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,332][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0043, 0.0992, 0.0523, 0.8442], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,333][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.1651, 0.3453, 0.1248, 0.3648], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,333][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.4504, 0.1528, 0.1897, 0.2072], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,334][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.0974, 0.3109, 0.2131, 0.3786], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,334][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0306, 0.2004, 0.4455, 0.3235], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,334][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.9565, 0.0036, 0.0321, 0.0045, 0.0034], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,335][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0027, 0.2521, 0.5228, 0.0938, 0.1285], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,335][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.7197, 0.1148, 0.0271, 0.0829, 0.0555], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,335][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.1367, 0.2652, 0.2047, 0.1920, 0.2014], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,336][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0148, 0.0674, 0.0363, 0.2107, 0.6708], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,336][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0549, 0.0943, 0.0165, 0.5245, 0.3097], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,336][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.3134, 0.1842, 0.1408, 0.1527, 0.2089], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,337][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0102, 0.0378, 0.0120, 0.6070, 0.3331], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,339][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.5158, 0.1056, 0.0420, 0.2417, 0.0949], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,340][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.6164, 0.0921, 0.0575, 0.1616, 0.0724], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,342][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.1650, 0.1542, 0.0983, 0.2365, 0.3461], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,343][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0522, 0.1447, 0.1577, 0.2175, 0.4278], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,344][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([2.6382e-01, 2.1952e-04, 1.6390e-03, 3.1522e-04, 1.9706e-04, 7.3381e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,346][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0040, 0.2259, 0.3149, 0.0773, 0.1655, 0.2124], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,348][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.8477, 0.0531, 0.0179, 0.0243, 0.0262, 0.0307], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,349][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.2664, 0.1452, 0.0642, 0.0948, 0.0938, 0.3356], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,350][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0032, 0.0051, 0.0013, 0.0158, 0.0391, 0.9354], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,351][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1661, 0.0461, 0.0031, 0.2336, 0.0928, 0.4582], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,351][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.3743, 0.1104, 0.0632, 0.1283, 0.1789, 0.1451], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,351][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([5.3195e-03, 5.1768e-03, 5.9584e-04, 5.1949e-02, 2.6356e-02, 9.1060e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,352][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.8052, 0.0301, 0.0068, 0.0645, 0.0187, 0.0746], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,352][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.8377, 0.0277, 0.0122, 0.0429, 0.0196, 0.0600], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,352][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.2197, 0.0379, 0.0235, 0.0639, 0.0921, 0.5629], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,353][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0408, 0.0250, 0.0154, 0.0267, 0.0419, 0.8501], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,353][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([1.9494e-01, 3.9339e-04, 2.6793e-03, 9.9763e-04, 4.4970e-04, 8.0027e-01,
        2.6651e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,353][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0270, 0.2956, 0.1370, 0.0855, 0.0820, 0.2091, 0.1639],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,354][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.8405, 0.0222, 0.0133, 0.0257, 0.0289, 0.0375, 0.0319],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,355][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.1475, 0.0479, 0.0406, 0.0692, 0.0446, 0.1655, 0.4847],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,356][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([6.1996e-04, 4.1669e-05, 9.3138e-06, 8.6778e-05, 4.6243e-04, 8.3813e-03,
        9.9040e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,357][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([5.5953e-02, 4.3909e-04, 3.4743e-05, 2.4099e-03, 1.3450e-03, 8.3397e-03,
        9.3148e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,359][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.3654, 0.0453, 0.0454, 0.0690, 0.1585, 0.1459, 0.1707],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,359][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([2.0710e-03, 2.0433e-05, 1.7286e-06, 1.8007e-04, 1.8021e-04, 4.0743e-03,
        9.9347e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,361][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.7506, 0.0100, 0.0034, 0.0261, 0.0098, 0.0331, 0.1670],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,363][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.6962, 0.0171, 0.0089, 0.0222, 0.0165, 0.0322, 0.2069],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,364][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([0.1600, 0.0161, 0.0120, 0.0263, 0.0622, 0.2101, 0.5134],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,366][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0505, 0.0078, 0.0066, 0.0133, 0.0294, 0.4892, 0.4031],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,367][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([1.7766e-01, 1.6001e-04, 1.6472e-03, 2.5562e-04, 1.0650e-04, 8.1787e-01,
        4.1014e-05, 2.2577e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,368][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0041, 0.1377, 0.1876, 0.0843, 0.0831, 0.2005, 0.2334, 0.0692],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,368][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.8156, 0.0416, 0.0162, 0.0277, 0.0265, 0.0276, 0.0259, 0.0189],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,368][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.3641, 0.0523, 0.0353, 0.0574, 0.0345, 0.1828, 0.0778, 0.1958],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,369][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([6.5696e-04, 5.7122e-05, 5.2893e-06, 1.7020e-04, 4.2988e-04, 5.2233e-03,
        9.0032e-01, 9.3133e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,369][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([1.7913e-02, 4.8887e-04, 1.6848e-05, 2.3334e-03, 6.6140e-04, 3.6101e-03,
        8.9967e-01, 7.5304e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,369][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.2582, 0.0590, 0.0323, 0.1290, 0.1064, 0.1262, 0.2058, 0.0831],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,370][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([5.3829e-04, 1.3569e-05, 5.7612e-07, 1.1598e-04, 4.9068e-05, 1.5900e-03,
        7.4786e-01, 2.4983e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,370][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.8503, 0.0050, 0.0011, 0.0148, 0.0037, 0.0147, 0.0764, 0.0340],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,371][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.9137, 0.0076, 0.0023, 0.0119, 0.0032, 0.0105, 0.0406, 0.0103],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,371][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.3000, 0.0089, 0.0059, 0.0244, 0.0183, 0.1320, 0.1986, 0.3119],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,371][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0478, 0.0058, 0.0027, 0.0070, 0.0083, 0.2088, 0.1930, 0.5265],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,372][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([8.0639e-02, 4.8212e-05, 2.6806e-04, 7.0570e-05, 3.6972e-05, 2.5253e-01,
        8.6705e-06, 6.5264e-04, 6.6574e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,374][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0101, 0.2294, 0.1161, 0.0927, 0.0683, 0.1264, 0.1762, 0.0774, 0.1034],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,376][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.8952, 0.0174, 0.0060, 0.0129, 0.0121, 0.0129, 0.0112, 0.0105, 0.0218],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,377][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2396, 0.0406, 0.0171, 0.0396, 0.0183, 0.0976, 0.1001, 0.1287, 0.3184],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,378][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([3.3901e-04, 1.5433e-05, 1.6283e-06, 5.8639e-05, 8.1204e-05, 1.9606e-03,
        3.8850e-01, 8.0958e-02, 5.2809e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,379][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([3.8585e-02, 2.9461e-04, 7.8307e-06, 1.2441e-03, 3.3201e-04, 2.0538e-03,
        6.1852e-01, 8.3622e-02, 2.5534e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,381][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.5513, 0.0453, 0.0179, 0.0993, 0.0536, 0.0730, 0.0784, 0.0384, 0.0427],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,382][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([7.8164e-04, 5.0608e-06, 2.5752e-07, 5.3592e-05, 2.1826e-05, 5.6170e-04,
        2.3612e-01, 1.5930e-01, 6.0316e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,383][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([9.4946e-01, 1.3388e-03, 2.0988e-04, 4.5571e-03, 9.1770e-04, 3.6530e-03,
        1.7482e-02, 9.1581e-03, 1.3221e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,384][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.8985, 0.0056, 0.0016, 0.0076, 0.0027, 0.0086, 0.0292, 0.0103, 0.0358],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,385][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1914, 0.0063, 0.0027, 0.0121, 0.0118, 0.0710, 0.1193, 0.1439, 0.4414],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,385][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0467, 0.0031, 0.0008, 0.0031, 0.0031, 0.0704, 0.0726, 0.1737, 0.6264],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,386][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([1.0095e-01, 5.9517e-05, 3.2293e-04, 9.5741e-05, 4.2758e-05, 2.5534e-01,
        1.1668e-05, 6.3843e-04, 6.4171e-01, 8.3499e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,386][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0064, 0.2651, 0.1356, 0.0455, 0.0667, 0.1173, 0.0696, 0.0413, 0.1179,
        0.1346], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,386][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.8428, 0.0121, 0.0057, 0.0113, 0.0185, 0.0237, 0.0189, 0.0153, 0.0296,
        0.0222], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,387][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.1656, 0.0270, 0.0171, 0.0329, 0.0193, 0.1134, 0.0786, 0.0988, 0.2598,
        0.1875], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,387][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([7.4033e-04, 1.4038e-06, 1.8796e-07, 5.9044e-06, 1.0399e-05, 3.3900e-04,
        2.6166e-02, 8.9147e-03, 6.2744e-02, 9.0108e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,388][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([9.4234e-02, 8.5438e-05, 2.1888e-06, 3.4612e-04, 1.1537e-04, 7.9258e-04,
        1.4786e-01, 1.7839e-02, 7.4223e-02, 6.6450e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,388][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.3502, 0.0426, 0.0415, 0.0722, 0.1094, 0.0983, 0.1054, 0.0497, 0.0488,
        0.0819], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,388][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([1.0643e-03, 3.0481e-07, 1.2860e-08, 3.3722e-06, 1.2627e-06, 4.6781e-05,
        1.0258e-02, 9.4119e-03, 3.8578e-02, 9.4064e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,389][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([8.3891e-01, 3.1775e-03, 7.3016e-04, 6.5753e-03, 2.0952e-03, 1.0098e-02,
        3.7597e-02, 2.4451e-02, 4.4944e-02, 3.1419e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,391][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.8404, 0.0086, 0.0025, 0.0096, 0.0032, 0.0111, 0.0488, 0.0126, 0.0342,
        0.0291], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,393][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.1736, 0.0036, 0.0016, 0.0073, 0.0084, 0.0493, 0.0656, 0.0912, 0.2607,
        0.3386], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,394][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0130, 0.0013, 0.0007, 0.0016, 0.0021, 0.0571, 0.0318, 0.1643, 0.5143,
        0.2138], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,395][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([1.5503e-01, 9.2024e-05, 6.5086e-04, 1.5709e-04, 4.1834e-05, 2.6528e-01,
        1.5911e-05, 5.7759e-04, 5.7252e-01, 5.3723e-04, 5.0959e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,397][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0031, 0.2356, 0.0679, 0.0948, 0.0451, 0.1526, 0.0847, 0.0320, 0.0790,
        0.0869, 0.1182], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,398][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.8569, 0.0138, 0.0074, 0.0139, 0.0156, 0.0168, 0.0132, 0.0152, 0.0246,
        0.0092, 0.0134], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,400][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0749, 0.0285, 0.0101, 0.0211, 0.0153, 0.0884, 0.0740, 0.0875, 0.2564,
        0.1769, 0.1669], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,401][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([1.6785e-04, 2.1892e-06, 2.4686e-07, 5.8577e-06, 9.4447e-06, 2.7078e-04,
        4.1683e-02, 8.7832e-03, 7.6622e-02, 6.8409e-01, 1.8837e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,402][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.3224e-02, 7.2035e-05, 1.6133e-06, 2.0953e-04, 6.2537e-05, 5.2253e-04,
        8.9530e-02, 1.4338e-02, 5.0711e-02, 5.7741e-01, 2.5392e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,402][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.1309, 0.0615, 0.0184, 0.0993, 0.0888, 0.1344, 0.1142, 0.0737, 0.0819,
        0.0618, 0.1352], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,403][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([1.0575e-04, 1.9006e-07, 6.0588e-09, 1.8517e-06, 4.8091e-07, 3.1275e-05,
        1.3012e-02, 4.2735e-03, 3.0098e-02, 8.4167e-01, 1.1081e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,403][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([8.3589e-01, 1.9361e-03, 3.1484e-04, 5.1077e-03, 1.5807e-03, 5.9263e-03,
        3.2380e-02, 1.5442e-02, 2.6662e-02, 2.0700e-02, 5.4060e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,404][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.7816, 0.0071, 0.0020, 0.0086, 0.0035, 0.0106, 0.0366, 0.0175, 0.0567,
        0.0339, 0.0420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,404][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0607, 0.0034, 0.0011, 0.0069, 0.0056, 0.0303, 0.0475, 0.0529, 0.1810,
        0.1719, 0.4387], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,404][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([7.7315e-03, 1.2200e-03, 3.1192e-04, 8.8975e-04, 8.6719e-04, 3.2695e-02,
        1.8287e-02, 6.5745e-02, 3.2501e-01, 1.2703e-01, 4.2022e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,405][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([2.4391e-01, 2.5938e-04, 9.8451e-04, 4.7289e-04, 1.2561e-04, 2.5259e-01,
        5.2126e-05, 8.8132e-04, 4.8186e-01, 1.0974e-03, 6.2510e-03, 1.1521e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,405][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0003, 0.0358, 0.0522, 0.0128, 0.0188, 0.1143, 0.0909, 0.0387, 0.1292,
        0.2029, 0.2189, 0.0851], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,406][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.8376, 0.0122, 0.0070, 0.0232, 0.0096, 0.0136, 0.0165, 0.0073, 0.0209,
        0.0161, 0.0105, 0.0256], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,407][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.1820, 0.0106, 0.0054, 0.0115, 0.0146, 0.0590, 0.0769, 0.0650, 0.1885,
        0.1614, 0.1165, 0.1086], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,408][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([2.4810e-03, 9.1256e-06, 7.3731e-07, 2.5204e-05, 2.9943e-05, 2.6820e-04,
        1.9930e-02, 6.1701e-03, 4.7649e-02, 4.7449e-01, 1.1568e-01, 3.3327e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,409][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([2.8338e-02, 3.7765e-05, 1.2876e-06, 1.6619e-04, 3.7965e-05, 1.4907e-04,
        1.3094e-02, 4.4811e-03, 1.0353e-02, 1.3669e-01, 9.3788e-02, 7.1286e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,411][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.1196, 0.0239, 0.0187, 0.0276, 0.0568, 0.0841, 0.0964, 0.0650, 0.0989,
        0.1059, 0.1631, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,412][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([2.5141e-03, 6.3387e-07, 1.2437e-08, 5.1552e-06, 1.0997e-06, 1.4674e-05,
        3.9088e-03, 1.3280e-03, 6.5091e-03, 2.0698e-01, 4.2354e-02, 7.3639e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,413][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.6727, 0.0060, 0.0008, 0.0094, 0.0032, 0.0108, 0.0511, 0.0160, 0.0368,
        0.0231, 0.0608, 0.1094], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,415][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.6657, 0.0111, 0.0036, 0.0107, 0.0047, 0.0129, 0.0522, 0.0195, 0.0553,
        0.0505, 0.0503, 0.0636], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,417][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.1076, 0.0045, 0.0013, 0.0064, 0.0060, 0.0234, 0.0518, 0.0387, 0.1137,
        0.1492, 0.2439, 0.2534], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,418][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0085, 0.0005, 0.0006, 0.0006, 0.0012, 0.0268, 0.0229, 0.0754, 0.2915,
        0.1646, 0.3733, 0.0341], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,419][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([1.6618e-01, 9.0691e-05, 8.7245e-04, 1.9706e-04, 1.0524e-04, 2.5792e-01,
        4.9975e-05, 1.2895e-03, 5.3952e-01, 1.3189e-03, 7.7517e-03, 6.2577e-03,
        1.8451e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,420][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0005, 0.0930, 0.0944, 0.0203, 0.0391, 0.0788, 0.0882, 0.0313, 0.0984,
        0.1292, 0.1931, 0.0888, 0.0449], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,420][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.4729, 0.0366, 0.0196, 0.0266, 0.0389, 0.0518, 0.0340, 0.0356, 0.0867,
        0.0403, 0.0428, 0.0476, 0.0665], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,420][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0839, 0.0305, 0.0118, 0.0183, 0.0100, 0.0523, 0.0665, 0.0684, 0.1720,
        0.1109, 0.1336, 0.1267, 0.1150], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,421][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([5.4566e-04, 1.4374e-06, 1.7171e-07, 4.2518e-06, 7.2120e-06, 1.7659e-04,
        7.9285e-03, 5.0178e-03, 2.9795e-02, 2.8664e-01, 5.5319e-02, 7.7944e-02,
        5.3662e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,421][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([9.3821e-03, 1.7545e-05, 5.2434e-07, 5.4242e-05, 1.5705e-05, 1.4198e-04,
        1.3875e-02, 3.0573e-03, 8.5101e-03, 1.0857e-01, 6.1914e-02, 4.9191e-01,
        3.0256e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,422][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0439, 0.0381, 0.0118, 0.0439, 0.0343, 0.0533, 0.1115, 0.0511, 0.0715,
        0.0937, 0.1439, 0.1226, 0.1804], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,422][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([3.9286e-04, 1.9853e-07, 5.3981e-09, 1.2057e-06, 5.9980e-07, 2.2165e-05,
        2.5468e-03, 1.9316e-03, 8.2704e-03, 1.2071e-01, 4.3421e-02, 4.0071e-01,
        4.2199e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,422][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([6.9478e-01, 2.4862e-03, 5.5408e-04, 6.5169e-03, 2.0567e-03, 9.3706e-03,
        3.2459e-02, 2.1839e-02, 3.3647e-02, 3.2134e-02, 5.4554e-02, 3.4802e-02,
        7.4805e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,423][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.4918, 0.0143, 0.0052, 0.0097, 0.0084, 0.0261, 0.0649, 0.0367, 0.0905,
        0.0764, 0.0686, 0.0546, 0.0527], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,424][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0291, 0.0023, 0.0007, 0.0034, 0.0035, 0.0185, 0.0338, 0.0326, 0.1095,
        0.1243, 0.2273, 0.1628, 0.2521], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,425][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0075, 0.0006, 0.0004, 0.0005, 0.0011, 0.0279, 0.0239, 0.0765, 0.2375,
        0.1372, 0.3420, 0.0501, 0.0948], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,427][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.2186e-01, 5.0531e-05, 3.2649e-04, 9.0247e-05, 3.2392e-05, 2.2272e-01,
        8.8910e-06, 6.8526e-04, 6.0539e-01, 4.7022e-04, 4.6238e-03, 4.0066e-03,
        8.5990e-03, 3.1125e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,428][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0028, 0.0871, 0.0614, 0.0356, 0.0452, 0.0901, 0.0748, 0.0233, 0.0830,
        0.0829, 0.1635, 0.0815, 0.0975, 0.0712], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,430][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.8024, 0.0178, 0.0071, 0.0122, 0.0120, 0.0140, 0.0122, 0.0102, 0.0267,
        0.0074, 0.0135, 0.0202, 0.0222, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,432][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1351, 0.0177, 0.0069, 0.0160, 0.0089, 0.0551, 0.0350, 0.0529, 0.1407,
        0.0859, 0.1009, 0.1180, 0.0892, 0.1376], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,433][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([2.3312e-04, 9.4116e-07, 3.3575e-08, 1.7437e-06, 2.0210e-06, 6.2009e-05,
        5.9974e-03, 2.4534e-03, 1.4858e-02, 1.2080e-01, 2.6587e-02, 8.6314e-02,
        3.8121e-01, 3.6148e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,434][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([5.3393e-03, 8.9489e-06, 9.9402e-08, 2.7177e-05, 5.5693e-06, 3.0666e-05,
        7.5424e-03, 1.0345e-03, 3.5558e-03, 4.6193e-02, 2.8694e-02, 4.7729e-01,
        2.5275e-01, 1.7753e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,436][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0744, 0.0282, 0.0111, 0.0577, 0.0458, 0.0812, 0.0515, 0.0429, 0.0699,
        0.0264, 0.0999, 0.1314, 0.1149, 0.1646], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,437][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([3.8881e-04, 4.0975e-08, 5.8471e-10, 3.5223e-07, 6.2091e-08, 1.8164e-06,
        6.5704e-04, 4.0847e-04, 1.2978e-03, 3.4582e-02, 1.2808e-02, 2.8801e-01,
        2.2370e-01, 4.3815e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,437][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([8.1404e-01, 1.1460e-03, 1.5082e-04, 2.7401e-03, 9.1814e-04, 3.0452e-03,
        1.3019e-02, 8.1306e-03, 1.2931e-02, 8.9648e-03, 2.4254e-02, 2.1416e-02,
        3.8820e-02, 5.0426e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,438][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.7372, 0.0076, 0.0020, 0.0074, 0.0030, 0.0104, 0.0313, 0.0122, 0.0370,
        0.0267, 0.0286, 0.0320, 0.0206, 0.0439], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,438][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0398, 0.0010, 0.0004, 0.0021, 0.0021, 0.0118, 0.0162, 0.0243, 0.0686,
        0.0575, 0.1736, 0.1195, 0.1370, 0.3459], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,438][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.0406e-02, 5.2664e-04, 1.8633e-04, 4.6772e-04, 7.2922e-04, 1.7715e-02,
        1.1400e-02, 3.3619e-02, 1.4996e-01, 6.1480e-02, 2.3801e-01, 5.3110e-02,
        6.6845e-02, 3.5555e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,439][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([1.0268e-01, 7.6200e-05, 5.2761e-04, 1.8508e-04, 8.3100e-05, 2.4706e-01,
        2.8134e-05, 1.2495e-03, 5.6031e-01, 8.7914e-04, 7.4216e-03, 6.2333e-03,
        1.8757e-02, 5.2409e-02, 2.1055e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,439][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0006, 0.0730, 0.0471, 0.0125, 0.0231, 0.0722, 0.0508, 0.0304, 0.0965,
        0.1414, 0.1856, 0.0888, 0.0679, 0.0727, 0.0375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,440][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.6405, 0.0177, 0.0095, 0.0193, 0.0191, 0.0225, 0.0235, 0.0182, 0.0365,
        0.0131, 0.0183, 0.0297, 0.0389, 0.0305, 0.0625], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,440][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0259, 0.0126, 0.0049, 0.0078, 0.0065, 0.0352, 0.0790, 0.0347, 0.1297,
        0.0738, 0.1112, 0.0797, 0.0786, 0.1599, 0.1604], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,441][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([1.3769e-04, 2.5254e-07, 2.1048e-08, 7.2553e-07, 9.5441e-07, 3.2250e-05,
        2.3503e-03, 9.1516e-04, 8.9611e-03, 8.4979e-02, 1.8437e-02, 2.2320e-02,
        1.4738e-01, 2.2643e-01, 4.8806e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,442][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([5.0362e-03, 1.1497e-05, 1.0117e-07, 2.0398e-05, 4.5189e-06, 4.4747e-05,
        4.1576e-03, 5.6611e-04, 3.2495e-03, 3.7317e-02, 1.8467e-02, 2.9484e-01,
        1.9174e-01, 1.8368e-01, 2.6087e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,444][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0507, 0.0272, 0.0118, 0.0345, 0.0262, 0.0817, 0.0393, 0.0327, 0.0678,
        0.0412, 0.0831, 0.1228, 0.0792, 0.1209, 0.1809], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,445][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([2.1530e-04, 2.2984e-08, 3.2499e-10, 1.3344e-07, 4.2530e-08, 1.8560e-06,
        2.5609e-04, 2.0683e-04, 7.5195e-04, 1.6199e-02, 5.7778e-03, 7.8134e-02,
        8.8828e-02, 1.9645e-01, 6.1318e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,446][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.3393, 0.0022, 0.0005, 0.0052, 0.0022, 0.0104, 0.0234, 0.0188, 0.0439,
        0.0364, 0.0764, 0.0412, 0.0818, 0.1780, 0.1403], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,448][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.4263, 0.0096, 0.0025, 0.0085, 0.0050, 0.0167, 0.0531, 0.0203, 0.0718,
        0.0609, 0.0503, 0.0476, 0.0323, 0.1071, 0.0881], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,450][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0158, 0.0011, 0.0003, 0.0016, 0.0016, 0.0099, 0.0153, 0.0183, 0.0610,
        0.0615, 0.1310, 0.0778, 0.1071, 0.2589, 0.2387], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,451][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([1.3458e-03, 2.1537e-04, 9.2176e-05, 2.0284e-04, 4.7640e-04, 1.0095e-02,
        1.0543e-02, 3.1268e-02, 1.3797e-01, 8.2874e-02, 2.3309e-01, 2.6205e-02,
        6.8047e-02, 3.2609e-01, 7.1491e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,452][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([1.5918e-01, 7.0710e-05, 4.9369e-04, 1.2902e-04, 5.8350e-05, 1.8650e-01,
        1.9777e-05, 7.3176e-04, 5.2358e-01, 8.4879e-04, 6.8110e-03, 5.1377e-03,
        1.5865e-02, 3.6171e-02, 1.0542e-03, 6.3342e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,453][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0021, 0.0928, 0.0471, 0.0221, 0.0231, 0.0958, 0.0524, 0.0269, 0.0710,
        0.1094, 0.1525, 0.0773, 0.0505, 0.0575, 0.0713, 0.0483],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,454][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.6547, 0.0084, 0.0068, 0.0099, 0.0088, 0.0181, 0.0153, 0.0131, 0.0410,
        0.0089, 0.0245, 0.0169, 0.0290, 0.0338, 0.0513, 0.0593],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,454][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0746, 0.0090, 0.0036, 0.0069, 0.0050, 0.0287, 0.0277, 0.0337, 0.0812,
        0.0619, 0.0738, 0.0899, 0.0686, 0.0898, 0.1048, 0.2407],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,455][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([2.2287e-04, 4.0723e-07, 2.9169e-08, 9.2023e-07, 1.1837e-06, 3.2035e-05,
        1.9084e-03, 8.2176e-04, 5.7174e-03, 4.5929e-02, 9.5610e-03, 2.1650e-02,
        1.3663e-01, 1.1480e-01, 3.9518e-01, 2.6754e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,455][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([3.2743e-03, 4.6797e-06, 7.2471e-08, 1.5012e-05, 3.4866e-06, 1.9464e-05,
        4.2535e-03, 6.0361e-04, 2.3630e-03, 2.4008e-02, 1.7299e-02, 1.9980e-01,
        1.6659e-01, 1.0860e-01, 2.1627e-01, 2.5690e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,456][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0481, 0.0197, 0.0092, 0.0394, 0.0257, 0.0775, 0.0251, 0.0334, 0.0558,
        0.0407, 0.1029, 0.0946, 0.0682, 0.1408, 0.1092, 0.1097],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,456][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([1.9605e-04, 1.6263e-08, 1.6883e-10, 9.8682e-08, 2.7259e-08, 7.6417e-07,
        1.4880e-04, 8.4086e-05, 3.2718e-04, 7.4680e-03, 2.5191e-03, 4.3197e-02,
        5.4225e-02, 6.7839e-02, 3.0461e-01, 5.1939e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,457][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([6.0030e-01, 7.1065e-04, 1.7029e-04, 2.2314e-03, 6.5308e-04, 3.8178e-03,
        1.3126e-02, 1.0320e-02, 1.8136e-02, 1.2260e-02, 4.5241e-02, 1.8915e-02,
        4.6478e-02, 8.0182e-02, 7.9578e-02, 6.7884e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,457][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.6365, 0.0052, 0.0011, 0.0043, 0.0033, 0.0098, 0.0274, 0.0142, 0.0423,
        0.0215, 0.0324, 0.0241, 0.0184, 0.0450, 0.0473, 0.0673],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,458][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([1.9664e-02, 4.4391e-04, 1.8134e-04, 8.0077e-04, 8.8756e-04, 6.3386e-03,
        7.2468e-03, 1.1477e-02, 3.6108e-02, 2.6169e-02, 8.7622e-02, 4.3614e-02,
        7.3515e-02, 1.6190e-01, 1.2392e-01, 4.0010e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,458][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([8.5115e-03, 1.3504e-04, 8.5687e-05, 1.5431e-04, 3.2964e-04, 1.0693e-02,
        5.9867e-03, 3.2560e-02, 1.2152e-01, 4.7943e-02, 1.6702e-01, 2.6359e-02,
        4.3826e-02, 3.2204e-01, 6.3483e-02, 1.4935e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,459][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.0615e-01, 5.2915e-05, 3.3966e-04, 8.3051e-05, 3.3346e-05, 1.9781e-01,
        9.5811e-06, 6.6591e-04, 5.6358e-01, 4.9973e-04, 4.2201e-03, 3.4416e-03,
        8.6619e-03, 2.8782e-02, 7.4887e-04, 5.4017e-02, 3.0899e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,461][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0020, 0.0829, 0.0560, 0.0320, 0.0362, 0.0806, 0.0701, 0.0214, 0.0669,
        0.0729, 0.1290, 0.0612, 0.0536, 0.0595, 0.0624, 0.0385, 0.0746],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,463][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.7915, 0.0126, 0.0051, 0.0098, 0.0085, 0.0101, 0.0087, 0.0073, 0.0183,
        0.0054, 0.0095, 0.0159, 0.0158, 0.0157, 0.0243, 0.0233, 0.0183],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,464][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0896, 0.0124, 0.0048, 0.0109, 0.0061, 0.0351, 0.0211, 0.0393, 0.0868,
        0.0562, 0.0684, 0.0745, 0.0551, 0.0825, 0.0568, 0.1902, 0.1099],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,465][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([1.6467e-04, 2.2947e-07, 6.7902e-09, 4.5278e-07, 4.7230e-07, 1.3344e-05,
        1.1002e-03, 4.6672e-04, 2.5449e-03, 2.4801e-02, 3.9855e-03, 1.7259e-02,
        8.4574e-02, 6.6450e-02, 3.3830e-01, 2.1594e-01, 2.4440e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,466][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([4.9696e-03, 3.8882e-06, 3.6018e-08, 1.1501e-05, 2.5844e-06, 1.2223e-05,
        2.6582e-03, 3.7397e-04, 1.3244e-03, 1.7407e-02, 1.0011e-02, 1.6885e-01,
        1.1150e-01, 6.3550e-02, 1.7631e-01, 2.2672e-01, 2.1629e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,468][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0428, 0.0277, 0.0072, 0.0503, 0.0304, 0.0530, 0.0395, 0.0266, 0.0423,
        0.0232, 0.0607, 0.1052, 0.0704, 0.1087, 0.1128, 0.0876, 0.1116],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,469][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([1.7618e-04, 6.8660e-09, 5.7343e-11, 4.6626e-08, 8.7154e-09, 2.2747e-07,
        7.4376e-05, 4.3128e-05, 1.3319e-04, 3.8711e-03, 1.2107e-03, 3.1187e-02,
        2.8080e-02, 4.2790e-02, 2.0563e-01, 4.3061e-01, 2.5619e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,470][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([7.1875e-01, 1.0275e-03, 1.4749e-04, 2.4349e-03, 8.8065e-04, 2.8605e-03,
        1.0444e-02, 7.1933e-03, 1.1211e-02, 8.5748e-03, 2.1462e-02, 1.5739e-02,
        3.1917e-02, 4.4379e-02, 4.2537e-02, 3.4106e-02, 4.6337e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,471][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.6887, 0.0060, 0.0015, 0.0054, 0.0024, 0.0075, 0.0248, 0.0093, 0.0260,
        0.0193, 0.0196, 0.0220, 0.0142, 0.0304, 0.0325, 0.0558, 0.0346],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,472][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([2.3726e-02, 4.7178e-04, 1.8397e-04, 9.7114e-04, 9.8561e-04, 5.0629e-03,
        6.8300e-03, 1.0206e-02, 2.8243e-02, 2.4203e-02, 7.2766e-02, 4.8151e-02,
        6.1652e-02, 1.4080e-01, 1.1364e-01, 2.6752e-01, 1.9459e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,472][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([7.5518e-03, 3.1489e-04, 1.2065e-04, 2.8476e-04, 4.9060e-04, 1.1809e-02,
        7.0435e-03, 2.3303e-02, 9.9528e-02, 3.8847e-02, 1.4935e-01, 2.9604e-02,
        3.8125e-02, 2.2349e-01, 5.5704e-02, 1.1910e-01, 1.9533e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,473][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:40,474][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[2740],
        [  94],
        [   1],
        [   9],
        [   1],
        [   1],
        [   4],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1]], device='cuda:0')
[2024-07-24 10:17:40,476][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[2263],
        [ 215],
        [   2],
        [  13],
        [   1],
        [   1],
        [   6],
        [   1],
        [   1],
        [   4],
        [   1],
        [   3],
        [   2],
        [   1],
        [   1],
        [   1],
        [   1]], device='cuda:0')
[2024-07-24 10:17:40,477][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[12659],
        [12983],
        [14062],
        [17063],
        [14879],
        [34160],
        [34724],
        [34836],
        [36324],
        [36218],
        [35927],
        [35743],
        [35829],
        [36084],
        [36098],
        [36569],
        [36628]], device='cuda:0')
[2024-07-24 10:17:40,479][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[13027],
        [44178],
        [24653],
        [23566],
        [24540],
        [23498],
        [34064],
        [25467],
        [30446],
        [28684],
        [31139],
        [17621],
        [20631],
        [21611],
        [18542],
        [18569],
        [18212]], device='cuda:0')
[2024-07-24 10:17:40,481][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[1564],
        [1800],
        [1963],
        [9604],
        [7060],
        [1427],
        [1043],
        [1068],
        [ 573],
        [ 345],
        [ 393],
        [ 843],
        [ 521],
        [ 471],
        [ 938],
        [ 523],
        [ 582]], device='cuda:0')
[2024-07-24 10:17:40,482][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[35216],
        [44097],
        [44959],
        [44839],
        [46032],
        [45908],
        [48154],
        [45466],
        [40442],
        [36972],
        [33661],
        [32244],
        [31183],
        [28119],
        [29706],
        [23349],
        [22491]], device='cuda:0')
[2024-07-24 10:17:40,484][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[43473],
        [10994],
        [ 9533],
        [25561],
        [30633],
        [12284],
        [25468],
        [23924],
        [12787],
        [28226],
        [23167],
        [19687],
        [16258],
        [14046],
        [18444],
        [14906],
        [13266]], device='cuda:0')
[2024-07-24 10:17:40,486][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 1389],
        [18504],
        [29080],
        [30319],
        [36312],
        [37606],
        [33222],
        [33521],
        [32821],
        [32801],
        [34347],
        [41753],
        [40832],
        [40975],
        [40628],
        [42193],
        [41911]], device='cuda:0')
[2024-07-24 10:17:40,487][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[14370],
        [39233],
        [41490],
        [43973],
        [42969],
        [35965],
        [18155],
        [29961],
        [33595],
        [24660],
        [30424],
        [34284],
        [37295],
        [39926],
        [37273],
        [37769],
        [39674]], device='cuda:0')
[2024-07-24 10:17:40,489][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[25157],
        [41362],
        [41967],
        [47542],
        [47540],
        [43200],
        [46415],
        [44296],
        [41131],
        [43464],
        [43770],
        [45433],
        [45858],
        [44228],
        [37781],
        [38038],
        [38752]], device='cuda:0')
[2024-07-24 10:17:40,490][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[13819],
        [24809],
        [25221],
        [47969],
        [48877],
        [40938],
        [45348],
        [37304],
        [21343],
        [32433],
        [30459],
        [39587],
        [34872],
        [25981],
        [36004],
        [32620],
        [29663]], device='cuda:0')
[2024-07-24 10:17:40,491][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[23623],
        [13695],
        [20908],
        [15273],
        [14126],
        [16826],
        [24322],
        [23044],
        [20181],
        [18912],
        [15959],
        [14730],
        [13941],
        [14292],
        [12151],
        [13629],
        [14004]], device='cuda:0')
[2024-07-24 10:17:40,492][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[47630],
        [21738],
        [20361],
        [ 1523],
        [ 4154],
        [15195],
        [ 3821],
        [12817],
        [14959],
        [18712],
        [17667],
        [17819],
        [21340],
        [23560],
        [24623],
        [21085],
        [23894]], device='cuda:0')
[2024-07-24 10:17:40,493][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[ 6445],
        [ 9192],
        [21409],
        [50245],
        [50095],
        [34633],
        [41068],
        [42858],
        [42001],
        [46062],
        [46559],
        [46865],
        [46990],
        [45378],
        [46370],
        [46248],
        [45539]], device='cuda:0')
[2024-07-24 10:17:40,495][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[18415],
        [ 6103],
        [ 2797],
        [ 3119],
        [ 4900],
        [ 5281],
        [ 2771],
        [ 3928],
        [ 4116],
        [ 2182],
        [ 2094],
        [ 1752],
        [ 3972],
        [ 2908],
        [ 2800],
        [ 2887],
        [ 3370]], device='cuda:0')
[2024-07-24 10:17:40,497][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[19193],
        [18654],
        [17075],
        [11373],
        [15098],
        [18514],
        [18394],
        [18377],
        [12611],
        [12707],
        [13107],
        [13537],
        [12370],
        [11044],
        [ 9911],
        [ 9987],
        [ 9602]], device='cuda:0')
[2024-07-24 10:17:40,498][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[12511],
        [ 2362],
        [18494],
        [24477],
        [25114],
        [25012],
        [15615],
        [24167],
        [18484],
        [16036],
        [14086],
        [12924],
        [13093],
        [12154],
        [10357],
        [10038],
        [10905]], device='cuda:0')
[2024-07-24 10:17:40,500][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[38467],
        [40336],
        [41376],
        [41457],
        [41941],
        [46109],
        [47244],
        [47165],
        [45744],
        [47059],
        [47230],
        [46877],
        [42908],
        [47439],
        [45164],
        [46441],
        [47474]], device='cuda:0')
[2024-07-24 10:17:40,501][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[26855],
        [ 9195],
        [ 6086],
        [11698],
        [10249],
        [ 7868],
        [ 3115],
        [ 4277],
        [ 5720],
        [ 8748],
        [11057],
        [13590],
        [15662],
        [18070],
        [15907],
        [18215],
        [19254]], device='cuda:0')
[2024-07-24 10:17:40,503][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[ 3156],
        [20691],
        [21075],
        [ 3760],
        [10812],
        [19738],
        [32690],
        [30867],
        [31610],
        [30759],
        [29362],
        [26507],
        [36586],
        [39500],
        [29279],
        [37887],
        [41275]], device='cuda:0')
[2024-07-24 10:17:40,505][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[33633],
        [18615],
        [17774],
        [16020],
        [19773],
        [28701],
        [25405],
        [26269],
        [30859],
        [37878],
        [35740],
        [16858],
        [22316],
        [21305],
        [24217],
        [24935],
        [24131]], device='cuda:0')
[2024-07-24 10:17:40,506][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[23379],
        [ 6581],
        [ 4650],
        [ 6238],
        [ 5316],
        [ 8577],
        [19496],
        [14536],
        [11661],
        [18991],
        [15377],
        [16122],
        [16954],
        [13522],
        [13346],
        [15200],
        [13499]], device='cuda:0')
[2024-07-24 10:17:40,508][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 275],
        [8066],
        [7392],
        [4593],
        [7378],
        [5549],
        [1252],
        [3220],
        [2938],
        [3528],
        [2887],
        [3983],
        [2904],
        [3499],
        [2267],
        [5299],
        [5492]], device='cuda:0')
[2024-07-24 10:17:40,509][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[7542],
        [6915],
        [5631],
        [1119],
        [ 956],
        [3089],
        [3766],
        [4295],
        [5830],
        [5204],
        [4961],
        [1752],
        [5342],
        [7532],
        [4020],
        [5500],
        [6806]], device='cuda:0')
[2024-07-24 10:17:40,510][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[36410],
        [11937],
        [25242],
        [ 2043],
        [ 7476],
        [19278],
        [29606],
        [32709],
        [30157],
        [26317],
        [23115],
        [16197],
        [13148],
        [18636],
        [14075],
        [17923],
        [18902]], device='cuda:0')
[2024-07-24 10:17:40,511][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 4236],
        [20898],
        [16013],
        [15967],
        [14865],
        [ 9288],
        [ 8997],
        [ 6122],
        [10897],
        [ 9350],
        [10943],
        [18617],
        [12823],
        [ 9900],
        [ 9354],
        [11230],
        [ 8832]], device='cuda:0')
[2024-07-24 10:17:40,512][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[13447],
        [10375],
        [22798],
        [41943],
        [47332],
        [45227],
        [49632],
        [48768],
        [47514],
        [46485],
        [47147],
        [46999],
        [46279],
        [45392],
        [45508],
        [45537],
        [44756]], device='cuda:0')
[2024-07-24 10:17:40,514][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[39777],
        [43860],
        [40814],
        [47117],
        [43782],
        [34156],
        [24852],
        [22971],
        [24299],
        [21488],
        [23026],
        [31303],
        [31411],
        [26293],
        [34955],
        [26533],
        [24713]], device='cuda:0')
[2024-07-24 10:17:40,516][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[31322],
        [38723],
        [27094],
        [37665],
        [31411],
        [27689],
        [38699],
        [32302],
        [26784],
        [28276],
        [28434],
        [31344],
        [26624],
        [23644],
        [25483],
        [25664],
        [26587]], device='cuda:0')
[2024-07-24 10:17:40,517][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676],
        [9676]], device='cuda:0')
[2024-07-24 10:17:40,564][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:40,565][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,565][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,566][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,567][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,568][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,569][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,569][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,571][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,572][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,573][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,575][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,576][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,578][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.9590, 0.0410], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,580][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0046, 0.9954], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,582][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.9056, 0.0944], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,582][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.9213, 0.0787], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,583][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.8387, 0.1613], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,584][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.9576, 0.0424], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,584][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0027, 0.9973], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,585][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.3238, 0.6762], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,586][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.8915, 0.1085], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,588][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,590][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.3138, 0.6862], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,591][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0211, 0.9789], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,593][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.7095, 0.1927, 0.0978], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,595][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0034, 0.9585, 0.0381], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,597][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.8431, 0.0584, 0.0985], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,598][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0171, 0.9642, 0.0187], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,599][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.4476, 0.3167, 0.2357], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,600][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.9190, 0.0507, 0.0303], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,600][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ and] are: tensor([5.9771e-04, 6.5014e-01, 3.4927e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,601][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0729, 0.5845, 0.3426], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,602][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.8674, 0.0700, 0.0626], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,602][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ and] are: tensor([1.4825e-04, 2.2137e-01, 7.7848e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,604][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0026, 0.9623, 0.0351], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,606][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0129, 0.4616, 0.5255], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,608][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.1456, 0.3420, 0.1939, 0.3184], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,609][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([2.5108e-04, 8.5447e-01, 4.0382e-02, 1.0489e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,611][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.1650, 0.2495, 0.2540, 0.3314], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,613][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.2233, 0.2560, 0.0164, 0.5043], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,614][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0659, 0.5794, 0.2110, 0.1436], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,616][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.4773, 0.2019, 0.0702, 0.2506], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,617][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([1.7524e-04, 5.2341e-01, 3.0942e-01, 1.6700e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,617][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0322, 0.3986, 0.2201, 0.3491], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,618][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.1437, 0.2078, 0.4111, 0.2373], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,619][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([5.8098e-05, 1.3610e-01, 5.5009e-01, 3.1376e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,619][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.0225, 0.3160, 0.0356, 0.6259], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,621][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([0.0021, 0.3849, 0.5019, 0.1112], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,622][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.6675, 0.0741, 0.0711, 0.1093, 0.0780], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,624][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0017, 0.8228, 0.0243, 0.1300, 0.0212], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,626][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.6979, 0.0376, 0.0758, 0.0877, 0.1010], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,627][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.2640, 0.1740, 0.0040, 0.4180, 0.1400], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,629][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.1415, 0.4313, 0.0917, 0.2164, 0.1191], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,631][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.7166, 0.0395, 0.0174, 0.0589, 0.1676], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,632][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ got] are: tensor([1.1588e-04, 6.8912e-01, 6.8524e-02, 1.3748e-01, 1.0476e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,633][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0393, 0.1874, 0.1044, 0.1919, 0.4770], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,634][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.7483, 0.0667, 0.0522, 0.0888, 0.0440], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,635][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ got] are: tensor([4.8280e-05, 1.5192e-01, 3.4886e-01, 3.6565e-01, 1.3352e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,636][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0058, 0.1407, 0.0062, 0.2602, 0.5871], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,636][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0032, 0.3818, 0.2487, 0.1086, 0.2575], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,637][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.9277, 0.0268, 0.0052, 0.0267, 0.0064, 0.0072], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,639][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0086, 0.5557, 0.0600, 0.2430, 0.0807, 0.0520], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,641][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.9419, 0.0062, 0.0086, 0.0184, 0.0069, 0.0181], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,642][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([5.9716e-01, 4.5690e-02, 5.5818e-04, 1.3104e-01, 4.0693e-02, 1.8487e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,644][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.3111, 0.2096, 0.1102, 0.1742, 0.0776, 0.1173], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,646][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.9264, 0.0057, 0.0027, 0.0108, 0.0223, 0.0321], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,647][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([1.3767e-04, 2.3767e-01, 5.8133e-02, 7.8688e-02, 1.1828e-01, 5.0709e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,649][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1408, 0.0734, 0.0438, 0.0772, 0.1622, 0.5027], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,651][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.8502, 0.0426, 0.0184, 0.0504, 0.0128, 0.0256], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,651][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.2152e-04, 1.0897e-01, 1.7887e-01, 1.5809e-01, 1.8679e-01, 3.6717e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,652][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([3.6328e-03, 2.5437e-02, 7.0835e-04, 3.8738e-02, 6.0000e-02, 8.7148e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,653][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0079, 0.2873, 0.1868, 0.1348, 0.2634, 0.1199], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,654][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.8852, 0.0123, 0.0090, 0.0342, 0.0068, 0.0090, 0.0435],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,654][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0039, 0.5302, 0.0631, 0.2325, 0.0418, 0.0425, 0.0861],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,656][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.8354, 0.0081, 0.0139, 0.0281, 0.0186, 0.0297, 0.0661],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,657][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([6.1565e-01, 6.3366e-05, 4.0156e-07, 1.4925e-04, 3.4169e-05, 2.9372e-04,
        3.8381e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,659][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0207, 0.0743, 0.0416, 0.0653, 0.0844, 0.1559, 0.5579],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,661][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.7633, 0.0093, 0.0042, 0.0134, 0.0419, 0.0413, 0.1267],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,662][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([8.6989e-05, 2.3567e-01, 4.7240e-02, 1.1764e-01, 9.6443e-02, 4.4313e-01,
        5.9795e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,663][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.0462, 0.0075, 0.0029, 0.0089, 0.0178, 0.0578, 0.8588],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,664][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.8690, 0.0162, 0.0194, 0.0238, 0.0114, 0.0353, 0.0249],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,665][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([7.5550e-05, 1.9365e-01, 1.9082e-01, 1.3200e-01, 1.5859e-01, 2.3210e-01,
        9.2761e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,666][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([5.4466e-04, 4.9871e-05, 3.4245e-07, 4.9174e-05, 6.5832e-05, 1.0883e-03,
        9.9820e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,667][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([0.0029, 0.1465, 0.1295, 0.0925, 0.2889, 0.2343, 0.1053],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:40,669][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.9057, 0.0196, 0.0026, 0.0234, 0.0045, 0.0052, 0.0157, 0.0233],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,671][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0042, 0.5324, 0.0548, 0.1975, 0.0358, 0.0459, 0.0849, 0.0446],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,671][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.9382, 0.0024, 0.0041, 0.0118, 0.0029, 0.0096, 0.0161, 0.0148],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,672][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ at] are: tensor([9.2101e-02, 8.3831e-05, 4.9742e-07, 1.6422e-04, 5.7040e-05, 3.4180e-04,
        8.0962e-01, 9.7635e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,673][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1784, 0.0549, 0.0811, 0.1075, 0.0769, 0.1165, 0.2504, 0.1343],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,673][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.9200, 0.0038, 0.0016, 0.0097, 0.0117, 0.0116, 0.0228, 0.0187],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,674][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ at] are: tensor([8.7980e-05, 2.4205e-01, 5.0008e-02, 1.1368e-01, 1.0137e-01, 3.2105e-01,
        8.7010e-02, 8.4735e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,676][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0598, 0.0100, 0.0029, 0.0104, 0.0147, 0.0508, 0.7738, 0.0776],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,678][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.7694, 0.0642, 0.0104, 0.0568, 0.0102, 0.0214, 0.0374, 0.0302],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,679][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ at] are: tensor([4.6690e-05, 4.8661e-02, 1.1690e-01, 1.2525e-01, 6.3071e-02, 2.9548e-01,
        1.1351e-01, 2.3709e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,680][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ at] are: tensor([4.8491e-04, 4.7852e-05, 7.9789e-07, 6.7217e-05, 8.7641e-05, 1.1320e-03,
        7.4034e-01, 2.5784e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,682][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0048, 0.1988, 0.1567, 0.1025, 0.1776, 0.1175, 0.1476, 0.0946],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:40,683][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.9151e-01, 1.7962e-03, 2.3602e-04, 3.9544e-03, 2.0774e-04, 2.1827e-04,
        7.6740e-04, 9.2049e-04, 3.9162e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,685][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0091, 0.5351, 0.0355, 0.2053, 0.0343, 0.0370, 0.0990, 0.0263, 0.0185],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,686][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([9.8998e-01, 5.2641e-04, 4.9466e-04, 3.9560e-03, 3.0217e-04, 1.0687e-03,
        1.4360e-03, 1.1502e-03, 1.0905e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,687][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([3.7461e-01, 3.9005e-05, 2.3291e-07, 9.0106e-05, 1.8166e-05, 1.1216e-04,
        4.7465e-01, 6.7284e-02, 8.3198e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,688][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2725, 0.0435, 0.0302, 0.0903, 0.0393, 0.0734, 0.1783, 0.1027, 0.1699],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,689][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.6847e-01, 1.1433e-03, 5.8403e-04, 2.9584e-03, 2.6589e-03, 2.9398e-03,
        9.0071e-03, 3.9349e-03, 8.3013e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,690][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([1.3344e-04, 3.5074e-01, 3.2524e-02, 1.1213e-01, 5.9057e-02, 2.2205e-01,
        7.5975e-02, 5.4255e-02, 9.3129e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,690][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1335, 0.0064, 0.0024, 0.0063, 0.0102, 0.0336, 0.5110, 0.0687, 0.2279],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,691][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.9077, 0.0190, 0.0043, 0.0185, 0.0036, 0.0065, 0.0096, 0.0161, 0.0149],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,692][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.3549e-04, 7.3069e-02, 6.4763e-02, 1.8802e-01, 5.1849e-02, 1.9495e-01,
        8.3200e-02, 1.5693e-01, 1.8709e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,694][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([7.0288e-04, 2.0194e-05, 2.2272e-07, 3.3402e-05, 4.4473e-05, 5.6015e-04,
        4.1204e-01, 2.1373e-01, 3.7287e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,696][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0048, 0.2684, 0.0876, 0.0861, 0.1612, 0.0943, 0.1151, 0.0823, 0.1002],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:40,697][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.8187, 0.0064, 0.0018, 0.0122, 0.0040, 0.0047, 0.0160, 0.0178, 0.0090,
        0.1093], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,699][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0013, 0.6306, 0.0207, 0.1346, 0.0159, 0.0341, 0.0375, 0.0156, 0.0179,
        0.0918], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,701][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.9171, 0.0010, 0.0037, 0.0073, 0.0031, 0.0092, 0.0111, 0.0136, 0.0096,
        0.0244], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,702][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ station] are: tensor([1.9680e-01, 1.9117e-07, 4.4533e-10, 3.5440e-07, 3.9029e-08, 1.0148e-06,
        6.2779e-04, 3.6044e-04, 9.9038e-04, 8.0122e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,704][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0107, 0.0425, 0.0154, 0.0573, 0.0315, 0.0891, 0.1797, 0.1052, 0.2698,
        0.1988], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,705][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.7754, 0.0039, 0.0013, 0.0054, 0.0117, 0.0192, 0.0397, 0.0180, 0.0370,
        0.0884], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,706][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ station] are: tensor([4.0264e-05, 2.6069e-01, 2.8180e-02, 6.5807e-02, 3.2888e-02, 2.7303e-01,
        4.4643e-02, 1.0083e-01, 1.5034e-01, 4.3542e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,707][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ station] are: tensor([8.0612e-02, 1.5572e-03, 5.3789e-04, 1.1647e-03, 2.5516e-03, 1.1701e-02,
        1.0461e-01, 2.6000e-02, 7.9297e-02, 6.9197e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,707][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.7500, 0.0327, 0.0146, 0.0336, 0.0097, 0.0129, 0.0173, 0.0375, 0.0207,
        0.0711], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,708][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ station] are: tensor([1.7070e-05, 5.2906e-02, 7.4212e-02, 1.2613e-01, 3.8682e-02, 1.8145e-01,
        3.7781e-02, 1.6131e-01, 1.6135e-01, 1.6616e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,709][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ station] are: tensor([9.4485e-04, 8.6939e-07, 2.2941e-09, 4.4847e-07, 5.6288e-07, 1.5396e-05,
        5.9171e-03, 4.8525e-03, 9.7954e-03, 9.7847e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,711][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0040, 0.2117, 0.0751, 0.0711, 0.1231, 0.1120, 0.0637, 0.0826, 0.1398,
        0.1169], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:40,713][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.7673, 0.0157, 0.0022, 0.0279, 0.0050, 0.0048, 0.0162, 0.0187, 0.0140,
        0.0671, 0.0609], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,715][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0034, 0.4283, 0.0259, 0.1579, 0.0345, 0.0549, 0.0537, 0.0274, 0.0225,
        0.1563, 0.0352], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,716][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.8625, 0.0027, 0.0033, 0.0121, 0.0035, 0.0080, 0.0126, 0.0124, 0.0128,
        0.0323, 0.0378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,718][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [,] are: tensor([7.7031e-03, 4.9692e-07, 2.2301e-09, 1.1839e-06, 1.1179e-07, 7.9134e-07,
        2.3524e-03, 3.9314e-04, 6.3536e-04, 9.6522e-01, 2.3691e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,719][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0078, 0.0292, 0.0077, 0.0164, 0.0146, 0.0318, 0.0999, 0.0636, 0.2111,
        0.1452, 0.3727], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,721][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.8151, 0.0030, 0.0015, 0.0067, 0.0087, 0.0113, 0.0219, 0.0163, 0.0311,
        0.0624, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,722][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [,] are: tensor([2.7412e-05, 3.6790e-01, 2.0619e-02, 1.2346e-01, 2.9559e-02, 1.7839e-01,
        5.8736e-02, 4.1710e-02, 1.2516e-01, 3.5531e-02, 1.8906e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,723][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [,] are: tensor([1.9308e-02, 1.6258e-03, 4.8758e-04, 2.0384e-03, 1.8286e-03, 5.3431e-03,
        1.1557e-01, 1.7492e-02, 5.3903e-02, 6.5136e-01, 1.3104e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,724][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.7132, 0.0412, 0.0049, 0.0214, 0.0055, 0.0086, 0.0144, 0.0183, 0.0177,
        0.0358, 0.1191], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,725][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [,] are: tensor([9.6477e-06, 6.4862e-02, 5.0570e-02, 1.4823e-01, 2.8582e-02, 1.4010e-01,
        4.1511e-02, 1.0811e-01, 2.0300e-01, 8.4173e-02, 1.3086e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,725][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [,] are: tensor([1.1458e-05, 4.7173e-07, 4.2413e-09, 4.7363e-07, 9.2716e-07, 1.4670e-05,
        7.3777e-03, 3.5185e-03, 1.1100e-02, 9.2910e-01, 4.8875e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,726][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0047, 0.3794, 0.0695, 0.1353, 0.1046, 0.0775, 0.0581, 0.0448, 0.0599,
        0.0434, 0.0228], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:40,728][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.6018, 0.0228, 0.0042, 0.0282, 0.0082, 0.0100, 0.0221, 0.0293, 0.0184,
        0.1193, 0.0547, 0.0810], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,730][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0023, 0.3341, 0.0187, 0.0746, 0.0102, 0.0262, 0.0378, 0.0179, 0.0191,
        0.1303, 0.0328, 0.2961], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,732][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.6761, 0.0070, 0.0057, 0.0218, 0.0069, 0.0156, 0.0226, 0.0186, 0.0226,
        0.0969, 0.0482, 0.0580], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,733][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([1.3263e-01, 2.5656e-07, 4.9552e-10, 3.8616e-07, 3.6308e-08, 1.8851e-07,
        1.5382e-04, 5.3879e-05, 8.3745e-05, 2.0318e-01, 1.9167e-03, 6.6198e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,735][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0292, 0.0271, 0.0149, 0.0207, 0.0201, 0.0679, 0.1231, 0.0794, 0.2051,
        0.1318, 0.1777, 0.1029], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,737][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.6766, 0.0057, 0.0022, 0.0096, 0.0157, 0.0235, 0.0330, 0.0243, 0.0503,
        0.0653, 0.0180, 0.0758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,738][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([4.7754e-05, 5.2418e-02, 1.8297e-02, 3.6233e-02, 1.4906e-02, 2.5516e-01,
        6.2142e-02, 6.5253e-02, 2.6211e-01, 8.8869e-02, 3.5742e-02, 1.0883e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,739][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0587, 0.0030, 0.0006, 0.0019, 0.0020, 0.0079, 0.1027, 0.0194, 0.0618,
        0.4825, 0.1027, 0.1568], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,740][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.3182, 0.0379, 0.0123, 0.0338, 0.0100, 0.0165, 0.0211, 0.0281, 0.0256,
        0.1014, 0.1884, 0.2067], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,741][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([7.7434e-06, 1.9748e-02, 3.1034e-02, 3.5094e-02, 2.1965e-02, 1.2634e-01,
        4.4211e-02, 1.2676e-01, 1.8271e-01, 1.4320e-01, 1.1815e-01, 1.5078e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,742][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([6.2294e-04, 1.0940e-07, 1.0624e-09, 1.9235e-07, 1.7625e-07, 1.9645e-06,
        5.2031e-04, 3.4607e-04, 6.4361e-04, 2.0058e-01, 7.0638e-03, 7.9022e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,743][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0029, 0.1978, 0.0685, 0.0341, 0.0995, 0.1186, 0.0638, 0.0626, 0.0844,
        0.1134, 0.0506, 0.1037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:40,744][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.2124, 0.0131, 0.0038, 0.0162, 0.0092, 0.0118, 0.0281, 0.0386, 0.0249,
        0.1951, 0.1140, 0.0446, 0.2883], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,746][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0014, 0.4275, 0.0188, 0.1226, 0.0140, 0.0226, 0.0725, 0.0160, 0.0142,
        0.0949, 0.0251, 0.1527, 0.0178], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,747][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.5433, 0.0066, 0.0074, 0.0131, 0.0112, 0.0290, 0.0284, 0.0432, 0.0422,
        0.0897, 0.0969, 0.0344, 0.0548], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,749][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([3.5220e-02, 9.4150e-08, 3.6090e-10, 1.4649e-07, 2.4344e-08, 3.2827e-07,
        1.4265e-04, 9.0844e-05, 1.5911e-04, 1.1920e-01, 3.3812e-03, 5.0650e-01,
        3.3531e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,750][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0056, 0.0406, 0.0078, 0.0134, 0.0114, 0.0240, 0.1134, 0.0598, 0.1983,
        0.1476, 0.1900, 0.0549, 0.1333], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,753][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.2818, 0.0086, 0.0030, 0.0095, 0.0222, 0.0273, 0.0517, 0.0380, 0.0716,
        0.0973, 0.0321, 0.0627, 0.2943], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,754][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([5.5951e-05, 2.9159e-01, 3.0156e-02, 5.6795e-02, 3.7355e-02, 1.3864e-01,
        5.5719e-02, 5.8361e-02, 9.6096e-02, 3.7909e-02, 1.7963e-02, 1.2838e-01,
        5.0984e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,756][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0164, 0.0015, 0.0005, 0.0008, 0.0012, 0.0052, 0.0546, 0.0125, 0.0289,
        0.3442, 0.0579, 0.1031, 0.3733], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,757][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.4090, 0.0254, 0.0082, 0.0272, 0.0154, 0.0218, 0.0131, 0.0396, 0.0390,
        0.0690, 0.1668, 0.0640, 0.1014], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,757][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([1.0527e-05, 6.3413e-02, 3.3440e-02, 7.2885e-02, 2.4118e-02, 7.9720e-02,
        4.9544e-02, 9.7162e-02, 1.5998e-01, 1.2152e-01, 8.5853e-02, 1.7113e-01,
        4.1228e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,758][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([1.3716e-04, 1.3033e-07, 1.0736e-09, 1.3495e-07, 1.6449e-07, 2.4224e-06,
        5.0651e-04, 4.3097e-04, 6.3217e-04, 7.2968e-02, 5.1957e-03, 5.4725e-01,
        3.7287e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,759][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0005, 0.1991, 0.0509, 0.0280, 0.0564, 0.0651, 0.0726, 0.0760, 0.0928,
        0.0781, 0.0665, 0.1502, 0.0638], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:40,760][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.5903, 0.0124, 0.0019, 0.0131, 0.0041, 0.0036, 0.0100, 0.0143, 0.0082,
        0.0590, 0.0394, 0.0263, 0.1629, 0.0546], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,762][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0020, 0.2869, 0.0272, 0.1077, 0.0317, 0.0434, 0.0462, 0.0284, 0.0189,
        0.0987, 0.0392, 0.1826, 0.0409, 0.0463], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,764][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.8562, 0.0020, 0.0030, 0.0082, 0.0024, 0.0067, 0.0105, 0.0104, 0.0098,
        0.0257, 0.0216, 0.0087, 0.0128, 0.0221], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,765][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([1.7627e-02, 5.0913e-08, 1.2335e-10, 1.0252e-07, 8.4618e-09, 7.8024e-08,
        1.2146e-04, 3.0586e-05, 5.7727e-05, 8.2289e-02, 2.8269e-03, 4.6799e-01,
        3.2840e-01, 1.0065e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,767][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0190, 0.0134, 0.0092, 0.0175, 0.0111, 0.0257, 0.0571, 0.0460, 0.1417,
        0.0913, 0.2497, 0.0902, 0.0963, 0.1318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,769][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.7604, 0.0021, 0.0008, 0.0045, 0.0055, 0.0089, 0.0081, 0.0115, 0.0217,
        0.0250, 0.0086, 0.0287, 0.0725, 0.0416], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,770][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([5.7831e-05, 1.5747e-01, 2.4254e-02, 6.8324e-02, 5.2823e-02, 1.9277e-01,
        3.9877e-02, 3.8169e-02, 9.6554e-02, 3.4805e-02, 1.9607e-02, 1.0190e-01,
        8.4807e-02, 8.8582e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,771][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([2.7450e-02, 1.1926e-03, 3.3196e-04, 6.3522e-04, 1.3400e-03, 3.4678e-03,
        5.0252e-02, 8.3526e-03, 2.0767e-02, 1.9857e-01, 5.5428e-02, 9.3283e-02,
        3.8397e-01, 1.5496e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,773][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.4845, 0.0270, 0.0049, 0.0289, 0.0063, 0.0091, 0.0155, 0.0139, 0.0160,
        0.0393, 0.1218, 0.1161, 0.0617, 0.0550], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,774][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([2.2028e-05, 3.2568e-02, 5.6164e-02, 7.7870e-02, 4.3058e-02, 1.2528e-01,
        2.4223e-02, 8.7023e-02, 1.2238e-01, 6.0705e-02, 9.2063e-02, 1.2949e-01,
        4.4183e-02, 1.0497e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,775][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([7.4098e-05, 4.9339e-08, 2.4323e-10, 5.1874e-08, 4.0505e-08, 5.3072e-07,
        3.1818e-04, 1.8495e-04, 3.5283e-04, 4.4172e-02, 3.0387e-03, 3.9834e-01,
        3.1333e-01, 2.4018e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,776][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0021, 0.1590, 0.0344, 0.0375, 0.0774, 0.0464, 0.0900, 0.0437, 0.0640,
        0.0555, 0.0504, 0.1067, 0.1046, 0.1282], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:40,776][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.2531, 0.0147, 0.0018, 0.0092, 0.0044, 0.0045, 0.0125, 0.0216, 0.0133,
        0.1083, 0.0737, 0.0392, 0.2258, 0.1085, 0.1093], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,777][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0012, 0.3061, 0.0210, 0.1082, 0.0141, 0.0245, 0.0369, 0.0278, 0.0185,
        0.0744, 0.0460, 0.2070, 0.0174, 0.0460, 0.0510], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,779][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.5524, 0.0058, 0.0040, 0.0140, 0.0049, 0.0192, 0.0203, 0.0209, 0.0291,
        0.0685, 0.0584, 0.0260, 0.0403, 0.0737, 0.0626], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,781][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ give] are: tensor([1.9237e-02, 2.2138e-08, 1.7522e-11, 1.6264e-08, 1.3147e-09, 2.5593e-08,
        1.5928e-05, 9.8084e-06, 1.7798e-05, 2.2207e-02, 5.8483e-04, 1.5889e-01,
        1.1759e-01, 4.5066e-02, 6.3637e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,783][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0020, 0.0129, 0.0041, 0.0097, 0.0074, 0.0266, 0.0472, 0.0287, 0.1416,
        0.0962, 0.1437, 0.0575, 0.0717, 0.1219, 0.2288], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,784][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.3075, 0.0042, 0.0014, 0.0043, 0.0094, 0.0254, 0.0190, 0.0278, 0.0665,
        0.0683, 0.0287, 0.0495, 0.2166, 0.1095, 0.0619], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,786][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ give] are: tensor([3.1195e-05, 1.4926e-01, 1.1617e-02, 4.8509e-02, 2.0257e-02, 1.3095e-01,
        2.9212e-02, 4.5169e-02, 1.1931e-01, 4.5573e-02, 2.2863e-02, 1.3831e-01,
        4.7554e-02, 7.5509e-02, 1.1588e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,787][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ give] are: tensor([2.0090e-02, 6.4036e-04, 2.1069e-04, 3.0987e-04, 1.0099e-03, 2.7910e-03,
        4.1420e-02, 7.5763e-03, 1.6046e-02, 2.2266e-01, 3.7715e-02, 4.7870e-02,
        2.2179e-01, 1.4983e-01, 2.3004e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,789][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.2319, 0.0184, 0.0032, 0.0133, 0.0064, 0.0154, 0.0159, 0.0293, 0.0373,
        0.0493, 0.1790, 0.1093, 0.0649, 0.1116, 0.1149], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,790][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ give] are: tensor([9.7181e-06, 2.9442e-02, 2.3465e-02, 9.9786e-02, 1.8582e-02, 5.3711e-02,
        3.4464e-02, 7.0597e-02, 1.0937e-01, 8.8361e-02, 1.1517e-01, 1.6747e-01,
        4.2239e-02, 1.2212e-01, 2.5213e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,791][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ give] are: tensor([7.8183e-05, 1.5202e-08, 4.0161e-11, 1.5563e-08, 6.0914e-09, 2.0939e-07,
        7.0334e-05, 3.5062e-05, 1.0058e-04, 2.5387e-02, 6.7645e-04, 1.0482e-01,
        7.0595e-02, 9.0247e-02, 7.0799e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,792][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0007, 0.1626, 0.0316, 0.0343, 0.0390, 0.0559, 0.0317, 0.0306, 0.0619,
        0.0376, 0.0342, 0.1362, 0.0419, 0.1276, 0.1742], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:40,793][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.4055, 0.0126, 0.0006, 0.0062, 0.0018, 0.0028, 0.0072, 0.0145, 0.0138,
        0.0686, 0.0385, 0.0291, 0.2104, 0.0685, 0.0675, 0.0523],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,794][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0042, 0.2729, 0.0317, 0.0938, 0.0222, 0.0290, 0.0421, 0.0230, 0.0192,
        0.0728, 0.0427, 0.1497, 0.0325, 0.0438, 0.0625, 0.0577],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,795][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.7346, 0.0012, 0.0014, 0.0037, 0.0019, 0.0075, 0.0084, 0.0116, 0.0155,
        0.0330, 0.0349, 0.0092, 0.0278, 0.0408, 0.0285, 0.0400],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,797][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ it] are: tensor([9.1313e-03, 1.3588e-08, 1.6370e-11, 1.0982e-08, 2.4475e-09, 2.7629e-08,
        1.9532e-05, 5.7454e-06, 1.3762e-05, 1.9287e-02, 5.6959e-04, 6.4870e-02,
        7.4641e-02, 2.4768e-02, 6.7404e-01, 1.3266e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,799][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0038, 0.0054, 0.0035, 0.0058, 0.0065, 0.0194, 0.0272, 0.0285, 0.1033,
        0.0396, 0.1508, 0.0565, 0.0638, 0.1001, 0.2140, 0.1718],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,800][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.5353, 0.0024, 0.0006, 0.0033, 0.0044, 0.0080, 0.0103, 0.0099, 0.0201,
        0.0267, 0.0108, 0.0347, 0.0809, 0.0363, 0.0262, 0.1902],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,801][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ it] are: tensor([6.4558e-05, 6.5603e-02, 1.2624e-02, 4.0491e-02, 2.4154e-02, 1.5905e-01,
        2.4040e-02, 4.8809e-02, 1.0735e-01, 2.9695e-02, 2.0456e-02, 1.1076e-01,
        4.6307e-02, 1.1058e-01, 1.0620e-01, 9.3813e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,803][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ it] are: tensor([3.8536e-02, 7.0414e-04, 1.4135e-04, 3.0064e-04, 6.3483e-04, 2.0239e-03,
        2.0137e-02, 5.1556e-03, 1.3101e-02, 1.1919e-01, 4.1114e-02, 7.5566e-02,
        2.0426e-01, 1.1245e-01, 1.5513e-01, 2.1156e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,805][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.6456, 0.0079, 0.0025, 0.0080, 0.0026, 0.0050, 0.0050, 0.0125, 0.0101,
        0.0157, 0.0673, 0.0492, 0.0303, 0.0523, 0.0510, 0.0351],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,806][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ it] are: tensor([2.3284e-05, 1.3486e-02, 2.1367e-02, 2.6279e-02, 1.4848e-02, 9.3910e-02,
        1.6185e-02, 9.4923e-02, 1.4111e-01, 6.9495e-02, 8.6286e-02, 1.2451e-01,
        2.8218e-02, 1.7528e-01, 2.5133e-02, 6.8955e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,807][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ it] are: tensor([2.7883e-05, 1.3076e-08, 3.2370e-11, 6.6813e-09, 8.9726e-09, 1.7571e-07,
        4.3534e-05, 3.3372e-05, 1.0582e-04, 1.3839e-02, 5.5472e-04, 5.4428e-02,
        5.8062e-02, 5.7601e-02, 5.5742e-01, 2.5788e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,808][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0037, 0.0895, 0.0427, 0.0391, 0.0576, 0.0470, 0.0525, 0.0403, 0.0448,
        0.0428, 0.0333, 0.0644, 0.0497, 0.1084, 0.1950, 0.0892],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:40,809][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.5138, 0.0105, 0.0015, 0.0108, 0.0038, 0.0037, 0.0094, 0.0144, 0.0095,
        0.0569, 0.0366, 0.0213, 0.1253, 0.0497, 0.0511, 0.0348, 0.0468],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,810][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0019, 0.2311, 0.0237, 0.1030, 0.0241, 0.0353, 0.0463, 0.0248, 0.0163,
        0.0882, 0.0366, 0.1545, 0.0317, 0.0377, 0.0573, 0.0618, 0.0258],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,811][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.7789, 0.0020, 0.0031, 0.0082, 0.0027, 0.0084, 0.0091, 0.0126, 0.0122,
        0.0295, 0.0228, 0.0086, 0.0124, 0.0240, 0.0185, 0.0246, 0.0223],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,812][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([1.8777e-02, 1.4164e-08, 2.0718e-11, 2.1157e-08, 1.7053e-09, 1.3847e-08,
        1.9135e-05, 4.9338e-06, 8.1782e-06, 1.5956e-02, 4.4910e-04, 8.6371e-02,
        6.3699e-02, 1.7117e-02, 5.3715e-01, 1.3714e-01, 1.2330e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,814][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0106, 0.0073, 0.0048, 0.0097, 0.0078, 0.0167, 0.0358, 0.0258, 0.0800,
        0.0668, 0.1539, 0.0523, 0.0679, 0.0840, 0.1811, 0.1296, 0.0659],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,815][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([6.4299e-01, 1.5423e-03, 6.0412e-04, 3.4760e-03, 4.3404e-03, 6.1526e-03,
        6.1583e-03, 8.6043e-03, 1.4046e-02, 2.0993e-02, 6.3143e-03, 2.0016e-02,
        6.1782e-02, 2.9703e-02, 1.9924e-02, 1.1090e-01, 4.2446e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,816][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([3.3183e-05, 1.1179e-01, 1.7560e-02, 5.2436e-02, 4.1083e-02, 1.5075e-01,
        3.1713e-02, 3.1856e-02, 7.5110e-02, 2.7341e-02, 1.3240e-02, 7.0641e-02,
        5.3010e-02, 6.7183e-02, 1.1452e-01, 7.8902e-02, 6.2840e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,818][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([2.5808e-02, 8.2071e-04, 1.8967e-04, 4.1745e-04, 8.5531e-04, 2.2950e-03,
        2.6500e-02, 4.9084e-03, 1.1394e-02, 1.2410e-01, 2.8329e-02, 4.7684e-02,
        2.1243e-01, 8.6911e-02, 1.4924e-01, 1.6402e-01, 1.1409e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,820][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.3780, 0.0195, 0.0041, 0.0217, 0.0058, 0.0092, 0.0131, 0.0155, 0.0161,
        0.0355, 0.1103, 0.0977, 0.0505, 0.0577, 0.0646, 0.0435, 0.0573],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,821][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([1.4461e-05, 2.9525e-02, 4.4764e-02, 7.3249e-02, 3.9989e-02, 1.0758e-01,
        2.3464e-02, 7.3987e-02, 1.0745e-01, 5.4913e-02, 7.5751e-02, 1.0405e-01,
        3.1305e-02, 8.8184e-02, 2.7764e-02, 3.0832e-02, 8.7175e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,823][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([8.2421e-05, 9.9306e-09, 3.5032e-11, 8.3632e-09, 7.3666e-09, 8.3434e-08,
        3.7184e-05, 2.7890e-05, 4.7519e-05, 6.7467e-03, 3.7078e-04, 5.5028e-02,
        5.5980e-02, 3.6332e-02, 3.9668e-01, 2.4596e-01, 2.0271e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,824][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0016, 0.0948, 0.0221, 0.0234, 0.0493, 0.0298, 0.0624, 0.0277, 0.0388,
        0.0365, 0.0270, 0.0586, 0.0661, 0.0766, 0.2163, 0.0705, 0.0985],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:40,922][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:40,923][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,924][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,924][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,925][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,927][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,928][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,930][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,931][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,932][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,933][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,935][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,936][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:40,938][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.9590, 0.0410], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,939][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0046, 0.9954], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,940][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.9056, 0.0944], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,941][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.9213, 0.0787], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,942][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.8387, 0.1613], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,942][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.9576, 0.0424], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,943][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0027, 0.9973], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,944][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.3238, 0.6762], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,946][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.8915, 0.1085], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,948][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,949][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.3138, 0.6862], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,951][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0211, 0.9789], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:40,953][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7095, 0.1927, 0.0978], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,954][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0034, 0.9585, 0.0381], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,956][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.8431, 0.0584, 0.0985], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,957][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0171, 0.9642, 0.0187], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,957][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.4476, 0.3167, 0.2357], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,958][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9190, 0.0507, 0.0303], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,959][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([5.9771e-04, 6.5014e-01, 3.4927e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,960][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0729, 0.5845, 0.3426], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,960][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.8674, 0.0700, 0.0626], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,961][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([1.4825e-04, 2.2137e-01, 7.7848e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,963][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0026, 0.9623, 0.0351], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,965][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0129, 0.4616, 0.5255], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:40,966][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.1456, 0.3420, 0.1939, 0.3184], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,967][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([2.5108e-04, 8.5447e-01, 4.0382e-02, 1.0489e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,969][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.1650, 0.2495, 0.2540, 0.3314], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,971][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.2233, 0.2560, 0.0164, 0.5043], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,973][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0659, 0.5794, 0.2110, 0.1436], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,974][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.4773, 0.2019, 0.0702, 0.2506], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,974][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([1.7524e-04, 5.2341e-01, 3.0942e-01, 1.6700e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,975][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0322, 0.3986, 0.2201, 0.3491], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,976][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.1437, 0.2078, 0.4111, 0.2373], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,977][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([5.8098e-05, 1.3610e-01, 5.5009e-01, 3.1376e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,977][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([0.0225, 0.3160, 0.0356, 0.6259], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,979][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0021, 0.3849, 0.5019, 0.1112], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:40,981][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.6675, 0.0741, 0.0711, 0.1093, 0.0780], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,983][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0017, 0.8228, 0.0243, 0.1300, 0.0212], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,984][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.6979, 0.0376, 0.0758, 0.0877, 0.1010], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,986][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.2640, 0.1740, 0.0040, 0.4180, 0.1400], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,988][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.1415, 0.4313, 0.0917, 0.2164, 0.1191], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,990][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.7166, 0.0395, 0.0174, 0.0589, 0.1676], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,991][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([1.1588e-04, 6.8912e-01, 6.8524e-02, 1.3748e-01, 1.0476e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,992][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0393, 0.1874, 0.1044, 0.1919, 0.4770], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,992][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.7483, 0.0667, 0.0522, 0.0888, 0.0440], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,993][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([4.8280e-05, 1.5192e-01, 3.4886e-01, 3.6565e-01, 1.3352e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,994][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0058, 0.1407, 0.0062, 0.2602, 0.5871], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,994][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0032, 0.3818, 0.2487, 0.1086, 0.2575], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:40,996][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.9277, 0.0268, 0.0052, 0.0267, 0.0064, 0.0072], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:40,998][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0086, 0.5557, 0.0600, 0.2430, 0.0807, 0.0520], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,000][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.9419, 0.0062, 0.0086, 0.0184, 0.0069, 0.0181], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,001][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([5.9716e-01, 4.5690e-02, 5.5818e-04, 1.3104e-01, 4.0693e-02, 1.8487e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,003][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.3111, 0.2096, 0.1102, 0.1742, 0.0776, 0.1173], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,005][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.9264, 0.0057, 0.0027, 0.0108, 0.0223, 0.0321], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,006][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([1.3767e-04, 2.3767e-01, 5.8133e-02, 7.8688e-02, 1.1828e-01, 5.0709e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,008][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1408, 0.0734, 0.0438, 0.0772, 0.1622, 0.5027], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,008][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.8502, 0.0426, 0.0184, 0.0504, 0.0128, 0.0256], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,009][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([1.2152e-04, 1.0897e-01, 1.7887e-01, 1.5809e-01, 1.8679e-01, 3.6717e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,010][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([3.6328e-03, 2.5437e-02, 7.0835e-04, 3.8738e-02, 6.0000e-02, 8.7148e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,011][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0079, 0.2873, 0.1868, 0.1348, 0.2634, 0.1199], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,011][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.8852, 0.0123, 0.0090, 0.0342, 0.0068, 0.0090, 0.0435],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,012][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0039, 0.5302, 0.0631, 0.2325, 0.0418, 0.0425, 0.0861],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,014][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.8354, 0.0081, 0.0139, 0.0281, 0.0186, 0.0297, 0.0661],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,016][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([6.1565e-01, 6.3366e-05, 4.0156e-07, 1.4925e-04, 3.4169e-05, 2.9372e-04,
        3.8381e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,017][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0207, 0.0743, 0.0416, 0.0653, 0.0844, 0.1559, 0.5579],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,019][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.7633, 0.0093, 0.0042, 0.0134, 0.0419, 0.0413, 0.1267],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,020][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([8.6989e-05, 2.3567e-01, 4.7240e-02, 1.1764e-01, 9.6443e-02, 4.4313e-01,
        5.9795e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,022][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0462, 0.0075, 0.0029, 0.0089, 0.0178, 0.0578, 0.8588],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,024][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.8690, 0.0162, 0.0194, 0.0238, 0.0114, 0.0353, 0.0249],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,025][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([7.5550e-05, 1.9365e-01, 1.9082e-01, 1.3200e-01, 1.5859e-01, 2.3210e-01,
        9.2761e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,026][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([5.4466e-04, 4.9871e-05, 3.4245e-07, 4.9174e-05, 6.5832e-05, 1.0883e-03,
        9.9820e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,027][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0029, 0.1465, 0.1295, 0.0925, 0.2889, 0.2343, 0.1053],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,028][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.9057, 0.0196, 0.0026, 0.0234, 0.0045, 0.0052, 0.0157, 0.0233],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,028][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0042, 0.5324, 0.0548, 0.1975, 0.0358, 0.0459, 0.0849, 0.0446],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,029][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.9382, 0.0024, 0.0041, 0.0118, 0.0029, 0.0096, 0.0161, 0.0148],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,030][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([9.2101e-02, 8.3831e-05, 4.9742e-07, 1.6422e-04, 5.7040e-05, 3.4180e-04,
        8.0962e-01, 9.7635e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,032][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1784, 0.0549, 0.0811, 0.1075, 0.0769, 0.1165, 0.2504, 0.1343],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,034][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.9200, 0.0038, 0.0016, 0.0097, 0.0117, 0.0116, 0.0228, 0.0187],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,035][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([8.7980e-05, 2.4205e-01, 5.0008e-02, 1.1368e-01, 1.0137e-01, 3.2105e-01,
        8.7010e-02, 8.4735e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,036][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0598, 0.0100, 0.0029, 0.0104, 0.0147, 0.0508, 0.7738, 0.0776],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,038][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.7694, 0.0642, 0.0104, 0.0568, 0.0102, 0.0214, 0.0374, 0.0302],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,039][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([4.6690e-05, 4.8661e-02, 1.1690e-01, 1.2525e-01, 6.3071e-02, 2.9548e-01,
        1.1351e-01, 2.3709e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,041][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([4.8491e-04, 4.7852e-05, 7.9789e-07, 6.7217e-05, 8.7641e-05, 1.1320e-03,
        7.4034e-01, 2.5784e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,042][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0048, 0.1988, 0.1567, 0.1025, 0.1776, 0.1175, 0.1476, 0.0946],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,043][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([9.9151e-01, 1.7962e-03, 2.3602e-04, 3.9544e-03, 2.0774e-04, 2.1827e-04,
        7.6740e-04, 9.2049e-04, 3.9162e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,044][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0091, 0.5351, 0.0355, 0.2053, 0.0343, 0.0370, 0.0990, 0.0263, 0.0185],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,045][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([9.8998e-01, 5.2641e-04, 4.9466e-04, 3.9560e-03, 3.0217e-04, 1.0687e-03,
        1.4360e-03, 1.1502e-03, 1.0905e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,045][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([3.7461e-01, 3.9005e-05, 2.3291e-07, 9.0106e-05, 1.8166e-05, 1.1216e-04,
        4.7465e-01, 6.7284e-02, 8.3198e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,046][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2725, 0.0435, 0.0302, 0.0903, 0.0393, 0.0734, 0.1783, 0.1027, 0.1699],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,047][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.6847e-01, 1.1433e-03, 5.8403e-04, 2.9584e-03, 2.6589e-03, 2.9398e-03,
        9.0071e-03, 3.9349e-03, 8.3013e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,049][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([1.3344e-04, 3.5074e-01, 3.2524e-02, 1.1213e-01, 5.9057e-02, 2.2205e-01,
        7.5975e-02, 5.4255e-02, 9.3129e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,051][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1335, 0.0064, 0.0024, 0.0063, 0.0102, 0.0336, 0.5110, 0.0687, 0.2279],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,052][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.9077, 0.0190, 0.0043, 0.0185, 0.0036, 0.0065, 0.0096, 0.0161, 0.0149],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,053][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.3549e-04, 7.3069e-02, 6.4763e-02, 1.8802e-01, 5.1849e-02, 1.9495e-01,
        8.3200e-02, 1.5693e-01, 1.8709e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,055][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([7.0288e-04, 2.0194e-05, 2.2272e-07, 3.3402e-05, 4.4473e-05, 5.6015e-04,
        4.1204e-01, 2.1373e-01, 3.7287e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,057][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0048, 0.2684, 0.0876, 0.0861, 0.1612, 0.0943, 0.1151, 0.0823, 0.1002],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,059][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.8187, 0.0064, 0.0018, 0.0122, 0.0040, 0.0047, 0.0160, 0.0178, 0.0090,
        0.1093], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,060][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0013, 0.6306, 0.0207, 0.1346, 0.0159, 0.0341, 0.0375, 0.0156, 0.0179,
        0.0918], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,061][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.9171, 0.0010, 0.0037, 0.0073, 0.0031, 0.0092, 0.0111, 0.0136, 0.0096,
        0.0244], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,062][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([1.9680e-01, 1.9117e-07, 4.4533e-10, 3.5440e-07, 3.9029e-08, 1.0148e-06,
        6.2779e-04, 3.6044e-04, 9.9038e-04, 8.0122e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,062][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0107, 0.0425, 0.0154, 0.0573, 0.0315, 0.0891, 0.1797, 0.1052, 0.2698,
        0.1988], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,063][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.7754, 0.0039, 0.0013, 0.0054, 0.0117, 0.0192, 0.0397, 0.0180, 0.0370,
        0.0884], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,064][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([4.0264e-05, 2.6069e-01, 2.8180e-02, 6.5807e-02, 3.2888e-02, 2.7303e-01,
        4.4643e-02, 1.0083e-01, 1.5034e-01, 4.3542e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,066][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([8.0612e-02, 1.5572e-03, 5.3789e-04, 1.1647e-03, 2.5516e-03, 1.1701e-02,
        1.0461e-01, 2.6000e-02, 7.9297e-02, 6.9197e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,068][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.7500, 0.0327, 0.0146, 0.0336, 0.0097, 0.0129, 0.0173, 0.0375, 0.0207,
        0.0711], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,069][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.7070e-05, 5.2906e-02, 7.4212e-02, 1.2613e-01, 3.8682e-02, 1.8145e-01,
        3.7781e-02, 1.6131e-01, 1.6135e-01, 1.6616e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,070][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([9.4485e-04, 8.6939e-07, 2.2941e-09, 4.4847e-07, 5.6288e-07, 1.5396e-05,
        5.9171e-03, 4.8525e-03, 9.7954e-03, 9.7847e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,072][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0040, 0.2117, 0.0751, 0.0711, 0.1231, 0.1120, 0.0637, 0.0826, 0.1398,
        0.1169], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,074][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.7673, 0.0157, 0.0022, 0.0279, 0.0050, 0.0048, 0.0162, 0.0187, 0.0140,
        0.0671, 0.0609], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,076][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0034, 0.4283, 0.0259, 0.1579, 0.0345, 0.0549, 0.0537, 0.0274, 0.0225,
        0.1563, 0.0352], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,078][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.8625, 0.0027, 0.0033, 0.0121, 0.0035, 0.0080, 0.0126, 0.0124, 0.0128,
        0.0323, 0.0378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,078][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([7.7031e-03, 4.9692e-07, 2.2301e-09, 1.1839e-06, 1.1179e-07, 7.9134e-07,
        2.3524e-03, 3.9314e-04, 6.3536e-04, 9.6522e-01, 2.3691e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,079][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0078, 0.0292, 0.0077, 0.0164, 0.0146, 0.0318, 0.0999, 0.0636, 0.2111,
        0.1452, 0.3727], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,080][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.8151, 0.0030, 0.0015, 0.0067, 0.0087, 0.0113, 0.0219, 0.0163, 0.0311,
        0.0624, 0.0220], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,081][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([2.7412e-05, 3.6790e-01, 2.0619e-02, 1.2346e-01, 2.9559e-02, 1.7839e-01,
        5.8736e-02, 4.1710e-02, 1.2516e-01, 3.5531e-02, 1.8906e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,082][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([1.9308e-02, 1.6258e-03, 4.8758e-04, 2.0384e-03, 1.8286e-03, 5.3431e-03,
        1.1557e-01, 1.7492e-02, 5.3903e-02, 6.5136e-01, 1.3104e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,084][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.7132, 0.0412, 0.0049, 0.0214, 0.0055, 0.0086, 0.0144, 0.0183, 0.0177,
        0.0358, 0.1191], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,085][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([9.6477e-06, 6.4862e-02, 5.0570e-02, 1.4823e-01, 2.8582e-02, 1.4010e-01,
        4.1511e-02, 1.0811e-01, 2.0300e-01, 8.4173e-02, 1.3086e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,086][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([1.1458e-05, 4.7173e-07, 4.2413e-09, 4.7363e-07, 9.2716e-07, 1.4670e-05,
        7.3777e-03, 3.5185e-03, 1.1100e-02, 9.2910e-01, 4.8875e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,088][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0047, 0.3794, 0.0695, 0.1353, 0.1046, 0.0775, 0.0581, 0.0448, 0.0599,
        0.0434, 0.0228], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,090][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.6018, 0.0228, 0.0042, 0.0282, 0.0082, 0.0100, 0.0221, 0.0293, 0.0184,
        0.1193, 0.0547, 0.0810], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,092][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0023, 0.3341, 0.0187, 0.0746, 0.0102, 0.0262, 0.0378, 0.0179, 0.0191,
        0.1303, 0.0328, 0.2961], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,094][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.6761, 0.0070, 0.0057, 0.0218, 0.0069, 0.0156, 0.0226, 0.0186, 0.0226,
        0.0969, 0.0482, 0.0580], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,095][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([1.3263e-01, 2.5656e-07, 4.9552e-10, 3.8616e-07, 3.6308e-08, 1.8851e-07,
        1.5382e-04, 5.3879e-05, 8.3745e-05, 2.0318e-01, 1.9167e-03, 6.6198e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,096][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0292, 0.0271, 0.0149, 0.0207, 0.0201, 0.0679, 0.1231, 0.0794, 0.2051,
        0.1318, 0.1777, 0.1029], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,096][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.6766, 0.0057, 0.0022, 0.0096, 0.0157, 0.0235, 0.0330, 0.0243, 0.0503,
        0.0653, 0.0180, 0.0758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,097][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([4.7754e-05, 5.2418e-02, 1.8297e-02, 3.6233e-02, 1.4906e-02, 2.5516e-01,
        6.2142e-02, 6.5253e-02, 2.6211e-01, 8.8869e-02, 3.5742e-02, 1.0883e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,098][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0587, 0.0030, 0.0006, 0.0019, 0.0020, 0.0079, 0.1027, 0.0194, 0.0618,
        0.4825, 0.1027, 0.1568], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,100][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.3182, 0.0379, 0.0123, 0.0338, 0.0100, 0.0165, 0.0211, 0.0281, 0.0256,
        0.1014, 0.1884, 0.2067], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,102][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([7.7434e-06, 1.9748e-02, 3.1034e-02, 3.5094e-02, 2.1965e-02, 1.2634e-01,
        4.4211e-02, 1.2676e-01, 1.8271e-01, 1.4320e-01, 1.1815e-01, 1.5078e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,103][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([6.2294e-04, 1.0940e-07, 1.0624e-09, 1.9235e-07, 1.7625e-07, 1.9645e-06,
        5.2031e-04, 3.4607e-04, 6.4361e-04, 2.0058e-01, 7.0638e-03, 7.9022e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,105][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0029, 0.1978, 0.0685, 0.0341, 0.0995, 0.1186, 0.0638, 0.0626, 0.0844,
        0.1134, 0.0506, 0.1037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,106][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.2124, 0.0131, 0.0038, 0.0162, 0.0092, 0.0118, 0.0281, 0.0386, 0.0249,
        0.1951, 0.1140, 0.0446, 0.2883], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,108][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0014, 0.4275, 0.0188, 0.1226, 0.0140, 0.0226, 0.0725, 0.0160, 0.0142,
        0.0949, 0.0251, 0.1527, 0.0178], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,110][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.5433, 0.0066, 0.0074, 0.0131, 0.0112, 0.0290, 0.0284, 0.0432, 0.0422,
        0.0897, 0.0969, 0.0344, 0.0548], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,111][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([3.5220e-02, 9.4150e-08, 3.6090e-10, 1.4649e-07, 2.4344e-08, 3.2827e-07,
        1.4265e-04, 9.0844e-05, 1.5911e-04, 1.1920e-01, 3.3812e-03, 5.0650e-01,
        3.3531e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,112][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0056, 0.0406, 0.0078, 0.0134, 0.0114, 0.0240, 0.1134, 0.0598, 0.1983,
        0.1476, 0.1900, 0.0549, 0.1333], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,113][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.2818, 0.0086, 0.0030, 0.0095, 0.0222, 0.0273, 0.0517, 0.0380, 0.0716,
        0.0973, 0.0321, 0.0627, 0.2943], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,114][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([5.5951e-05, 2.9159e-01, 3.0156e-02, 5.6795e-02, 3.7355e-02, 1.3864e-01,
        5.5719e-02, 5.8361e-02, 9.6096e-02, 3.7909e-02, 1.7963e-02, 1.2838e-01,
        5.0984e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,114][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0164, 0.0015, 0.0005, 0.0008, 0.0012, 0.0052, 0.0546, 0.0125, 0.0289,
        0.3442, 0.0579, 0.1031, 0.3733], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,115][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.4090, 0.0254, 0.0082, 0.0272, 0.0154, 0.0218, 0.0131, 0.0396, 0.0390,
        0.0690, 0.1668, 0.0640, 0.1014], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,116][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([1.0527e-05, 6.3413e-02, 3.3440e-02, 7.2885e-02, 2.4118e-02, 7.9720e-02,
        4.9544e-02, 9.7162e-02, 1.5998e-01, 1.2152e-01, 8.5853e-02, 1.7113e-01,
        4.1228e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,118][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([1.3716e-04, 1.3033e-07, 1.0736e-09, 1.3495e-07, 1.6449e-07, 2.4224e-06,
        5.0651e-04, 4.3097e-04, 6.3217e-04, 7.2968e-02, 5.1957e-03, 5.4725e-01,
        3.7287e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,120][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0005, 0.1991, 0.0509, 0.0280, 0.0564, 0.0651, 0.0726, 0.0760, 0.0928,
        0.0781, 0.0665, 0.1502, 0.0638], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,121][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.5903, 0.0124, 0.0019, 0.0131, 0.0041, 0.0036, 0.0100, 0.0143, 0.0082,
        0.0590, 0.0394, 0.0263, 0.1629, 0.0546], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,123][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0020, 0.2869, 0.0272, 0.1077, 0.0317, 0.0434, 0.0462, 0.0284, 0.0189,
        0.0987, 0.0392, 0.1826, 0.0409, 0.0463], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,125][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.8562, 0.0020, 0.0030, 0.0082, 0.0024, 0.0067, 0.0105, 0.0104, 0.0098,
        0.0257, 0.0216, 0.0087, 0.0128, 0.0221], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,127][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.7627e-02, 5.0913e-08, 1.2335e-10, 1.0252e-07, 8.4618e-09, 7.8024e-08,
        1.2146e-04, 3.0586e-05, 5.7727e-05, 8.2289e-02, 2.8269e-03, 4.6799e-01,
        3.2840e-01, 1.0065e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,128][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0190, 0.0134, 0.0092, 0.0175, 0.0111, 0.0257, 0.0571, 0.0460, 0.1417,
        0.0913, 0.2497, 0.0902, 0.0963, 0.1318], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,129][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.7604, 0.0021, 0.0008, 0.0045, 0.0055, 0.0089, 0.0081, 0.0115, 0.0217,
        0.0250, 0.0086, 0.0287, 0.0725, 0.0416], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,130][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([5.7831e-05, 1.5747e-01, 2.4254e-02, 6.8324e-02, 5.2823e-02, 1.9277e-01,
        3.9877e-02, 3.8169e-02, 9.6554e-02, 3.4805e-02, 1.9607e-02, 1.0190e-01,
        8.4807e-02, 8.8582e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,131][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.7450e-02, 1.1926e-03, 3.3196e-04, 6.3522e-04, 1.3400e-03, 3.4678e-03,
        5.0252e-02, 8.3526e-03, 2.0767e-02, 1.9857e-01, 5.5428e-02, 9.3283e-02,
        3.8397e-01, 1.5496e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,132][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.4845, 0.0270, 0.0049, 0.0289, 0.0063, 0.0091, 0.0155, 0.0139, 0.0160,
        0.0393, 0.1218, 0.1161, 0.0617, 0.0550], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,132][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([2.2028e-05, 3.2568e-02, 5.6164e-02, 7.7870e-02, 4.3058e-02, 1.2528e-01,
        2.4223e-02, 8.7023e-02, 1.2238e-01, 6.0705e-02, 9.2063e-02, 1.2949e-01,
        4.4183e-02, 1.0497e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,134][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([7.4098e-05, 4.9339e-08, 2.4323e-10, 5.1874e-08, 4.0505e-08, 5.3072e-07,
        3.1818e-04, 1.8495e-04, 3.5283e-04, 4.4172e-02, 3.0387e-03, 3.9834e-01,
        3.1333e-01, 2.4018e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,136][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0021, 0.1590, 0.0344, 0.0375, 0.0774, 0.0464, 0.0900, 0.0437, 0.0640,
        0.0555, 0.0504, 0.1067, 0.1046, 0.1282], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,138][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.2531, 0.0147, 0.0018, 0.0092, 0.0044, 0.0045, 0.0125, 0.0216, 0.0133,
        0.1083, 0.0737, 0.0392, 0.2258, 0.1085, 0.1093], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,139][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0012, 0.3061, 0.0210, 0.1082, 0.0141, 0.0245, 0.0369, 0.0278, 0.0185,
        0.0744, 0.0460, 0.2070, 0.0174, 0.0460, 0.0510], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,141][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.5524, 0.0058, 0.0040, 0.0140, 0.0049, 0.0192, 0.0203, 0.0209, 0.0291,
        0.0685, 0.0584, 0.0260, 0.0403, 0.0737, 0.0626], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,143][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([1.9237e-02, 2.2138e-08, 1.7522e-11, 1.6264e-08, 1.3147e-09, 2.5593e-08,
        1.5928e-05, 9.8084e-06, 1.7798e-05, 2.2207e-02, 5.8483e-04, 1.5889e-01,
        1.1759e-01, 4.5066e-02, 6.3637e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,145][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0020, 0.0129, 0.0041, 0.0097, 0.0074, 0.0266, 0.0472, 0.0287, 0.1416,
        0.0962, 0.1437, 0.0575, 0.0717, 0.1219, 0.2288], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,146][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.3075, 0.0042, 0.0014, 0.0043, 0.0094, 0.0254, 0.0190, 0.0278, 0.0665,
        0.0683, 0.0287, 0.0495, 0.2166, 0.1095, 0.0619], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,147][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([3.1195e-05, 1.4926e-01, 1.1617e-02, 4.8509e-02, 2.0257e-02, 1.3095e-01,
        2.9212e-02, 4.5169e-02, 1.1931e-01, 4.5573e-02, 2.2863e-02, 1.3831e-01,
        4.7554e-02, 7.5509e-02, 1.1588e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,148][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([2.0090e-02, 6.4036e-04, 2.1069e-04, 3.0987e-04, 1.0099e-03, 2.7910e-03,
        4.1420e-02, 7.5763e-03, 1.6046e-02, 2.2266e-01, 3.7715e-02, 4.7870e-02,
        2.2179e-01, 1.4983e-01, 2.3004e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,148][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.2319, 0.0184, 0.0032, 0.0133, 0.0064, 0.0154, 0.0159, 0.0293, 0.0373,
        0.0493, 0.1790, 0.1093, 0.0649, 0.1116, 0.1149], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,149][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([9.7181e-06, 2.9442e-02, 2.3465e-02, 9.9786e-02, 1.8582e-02, 5.3711e-02,
        3.4464e-02, 7.0597e-02, 1.0937e-01, 8.8361e-02, 1.1517e-01, 1.6747e-01,
        4.2239e-02, 1.2212e-01, 2.5213e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,150][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([7.8183e-05, 1.5202e-08, 4.0161e-11, 1.5563e-08, 6.0914e-09, 2.0939e-07,
        7.0334e-05, 3.5062e-05, 1.0058e-04, 2.5387e-02, 6.7645e-04, 1.0482e-01,
        7.0595e-02, 9.0247e-02, 7.0799e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,152][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0007, 0.1626, 0.0316, 0.0343, 0.0390, 0.0559, 0.0317, 0.0306, 0.0619,
        0.0376, 0.0342, 0.1362, 0.0419, 0.1276, 0.1742], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,154][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.4055, 0.0126, 0.0006, 0.0062, 0.0018, 0.0028, 0.0072, 0.0145, 0.0138,
        0.0686, 0.0385, 0.0291, 0.2104, 0.0685, 0.0675, 0.0523],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,156][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0042, 0.2729, 0.0317, 0.0938, 0.0222, 0.0290, 0.0421, 0.0230, 0.0192,
        0.0728, 0.0427, 0.1497, 0.0325, 0.0438, 0.0625, 0.0577],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,158][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.7346, 0.0012, 0.0014, 0.0037, 0.0019, 0.0075, 0.0084, 0.0116, 0.0155,
        0.0330, 0.0349, 0.0092, 0.0278, 0.0408, 0.0285, 0.0400],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,159][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([9.1313e-03, 1.3588e-08, 1.6370e-11, 1.0982e-08, 2.4475e-09, 2.7629e-08,
        1.9532e-05, 5.7454e-06, 1.3762e-05, 1.9287e-02, 5.6959e-04, 6.4870e-02,
        7.4641e-02, 2.4768e-02, 6.7404e-01, 1.3266e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,161][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0038, 0.0054, 0.0035, 0.0058, 0.0065, 0.0194, 0.0272, 0.0285, 0.1033,
        0.0396, 0.1508, 0.0565, 0.0638, 0.1001, 0.2140, 0.1718],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,163][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.5353, 0.0024, 0.0006, 0.0033, 0.0044, 0.0080, 0.0103, 0.0099, 0.0201,
        0.0267, 0.0108, 0.0347, 0.0809, 0.0363, 0.0262, 0.1902],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,164][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([6.4558e-05, 6.5603e-02, 1.2624e-02, 4.0491e-02, 2.4154e-02, 1.5905e-01,
        2.4040e-02, 4.8809e-02, 1.0735e-01, 2.9695e-02, 2.0456e-02, 1.1076e-01,
        4.6307e-02, 1.1058e-01, 1.0620e-01, 9.3813e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,165][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([3.8536e-02, 7.0414e-04, 1.4135e-04, 3.0064e-04, 6.3483e-04, 2.0239e-03,
        2.0137e-02, 5.1556e-03, 1.3101e-02, 1.1919e-01, 4.1114e-02, 7.5566e-02,
        2.0426e-01, 1.1245e-01, 1.5513e-01, 2.1156e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,166][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.6456, 0.0079, 0.0025, 0.0080, 0.0026, 0.0050, 0.0050, 0.0125, 0.0101,
        0.0157, 0.0673, 0.0492, 0.0303, 0.0523, 0.0510, 0.0351],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,166][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([2.3284e-05, 1.3486e-02, 2.1367e-02, 2.6279e-02, 1.4848e-02, 9.3910e-02,
        1.6185e-02, 9.4923e-02, 1.4111e-01, 6.9495e-02, 8.6286e-02, 1.2451e-01,
        2.8218e-02, 1.7528e-01, 2.5133e-02, 6.8955e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,167][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([2.7883e-05, 1.3076e-08, 3.2370e-11, 6.6813e-09, 8.9726e-09, 1.7571e-07,
        4.3534e-05, 3.3372e-05, 1.0582e-04, 1.3839e-02, 5.5472e-04, 5.4428e-02,
        5.8062e-02, 5.7601e-02, 5.5742e-01, 2.5788e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,169][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0037, 0.0895, 0.0427, 0.0391, 0.0576, 0.0470, 0.0525, 0.0403, 0.0448,
        0.0428, 0.0333, 0.0644, 0.0497, 0.1084, 0.1950, 0.0892],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,171][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.5138, 0.0105, 0.0015, 0.0108, 0.0038, 0.0037, 0.0094, 0.0144, 0.0095,
        0.0569, 0.0366, 0.0213, 0.1253, 0.0497, 0.0511, 0.0348, 0.0468],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,173][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0019, 0.2311, 0.0237, 0.1030, 0.0241, 0.0353, 0.0463, 0.0248, 0.0163,
        0.0882, 0.0366, 0.1545, 0.0317, 0.0377, 0.0573, 0.0618, 0.0258],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,175][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.7789, 0.0020, 0.0031, 0.0082, 0.0027, 0.0084, 0.0091, 0.0126, 0.0122,
        0.0295, 0.0228, 0.0086, 0.0124, 0.0240, 0.0185, 0.0246, 0.0223],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,176][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.8777e-02, 1.4164e-08, 2.0718e-11, 2.1157e-08, 1.7053e-09, 1.3847e-08,
        1.9135e-05, 4.9338e-06, 8.1782e-06, 1.5956e-02, 4.4910e-04, 8.6371e-02,
        6.3699e-02, 1.7117e-02, 5.3715e-01, 1.3714e-01, 1.2330e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,178][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0106, 0.0073, 0.0048, 0.0097, 0.0078, 0.0167, 0.0358, 0.0258, 0.0800,
        0.0668, 0.1539, 0.0523, 0.0679, 0.0840, 0.1811, 0.1296, 0.0659],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,179][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([6.4299e-01, 1.5423e-03, 6.0412e-04, 3.4760e-03, 4.3404e-03, 6.1526e-03,
        6.1583e-03, 8.6043e-03, 1.4046e-02, 2.0993e-02, 6.3143e-03, 2.0016e-02,
        6.1782e-02, 2.9703e-02, 1.9924e-02, 1.1090e-01, 4.2446e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,181][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([3.3183e-05, 1.1179e-01, 1.7560e-02, 5.2436e-02, 4.1083e-02, 1.5075e-01,
        3.1713e-02, 3.1856e-02, 7.5110e-02, 2.7341e-02, 1.3240e-02, 7.0641e-02,
        5.3010e-02, 6.7183e-02, 1.1452e-01, 7.8902e-02, 6.2840e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,181][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.5808e-02, 8.2071e-04, 1.8967e-04, 4.1745e-04, 8.5531e-04, 2.2950e-03,
        2.6500e-02, 4.9084e-03, 1.1394e-02, 1.2410e-01, 2.8329e-02, 4.7684e-02,
        2.1243e-01, 8.6911e-02, 1.4924e-01, 1.6402e-01, 1.1409e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,182][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.3780, 0.0195, 0.0041, 0.0217, 0.0058, 0.0092, 0.0131, 0.0155, 0.0161,
        0.0355, 0.1103, 0.0977, 0.0505, 0.0577, 0.0646, 0.0435, 0.0573],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,183][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.4461e-05, 2.9525e-02, 4.4764e-02, 7.3249e-02, 3.9989e-02, 1.0758e-01,
        2.3464e-02, 7.3987e-02, 1.0745e-01, 5.4913e-02, 7.5751e-02, 1.0405e-01,
        3.1305e-02, 8.8184e-02, 2.7764e-02, 3.0832e-02, 8.7175e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,184][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([8.2421e-05, 9.9306e-09, 3.5032e-11, 8.3632e-09, 7.3666e-09, 8.3434e-08,
        3.7184e-05, 2.7890e-05, 4.7519e-05, 6.7467e-03, 3.7078e-04, 5.5028e-02,
        5.5980e-02, 3.6332e-02, 3.9668e-01, 2.4596e-01, 2.0271e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,186][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0016, 0.0948, 0.0221, 0.0234, 0.0493, 0.0298, 0.0624, 0.0277, 0.0388,
        0.0365, 0.0270, 0.0586, 0.0661, 0.0766, 0.2163, 0.0705, 0.0985],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,189][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:41,191][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[3490],
        [ 221],
        [   4],
        [  50],
        [   2],
        [   4],
        [   4],
        [   1],
        [   1],
        [   3],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1]], device='cuda:0')
[2024-07-24 10:17:41,193][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[2967],
        [ 217],
        [   6],
        [  29],
        [   3],
        [   4],
        [   5],
        [   1],
        [   1],
        [   6],
        [   2],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1],
        [   1]], device='cuda:0')
[2024-07-24 10:17:41,195][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 9214],
        [ 8195],
        [12727],
        [39480],
        [36334],
        [16850],
        [24064],
        [19074],
        [10161],
        [22749],
        [28205],
        [29901],
        [30226],
        [24060],
        [23774],
        [24783],
        [24575]], device='cuda:0')
[2024-07-24 10:17:41,197][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[24113],
        [31421],
        [31509],
        [32760],
        [33256],
        [35561],
        [35842],
        [34808],
        [35021],
        [33947],
        [34873],
        [41124],
        [38915],
        [40350],
        [40915],
        [40629],
        [40925]], device='cuda:0')
[2024-07-24 10:17:41,199][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 1989],
        [ 1120],
        [ 1653],
        [43423],
        [26814],
        [ 4933],
        [20546],
        [ 4932],
        [ 2417],
        [ 4280],
        [ 4939],
        [ 9826],
        [ 8616],
        [ 3974],
        [ 8187],
        [ 4075],
        [ 4574]], device='cuda:0')
[2024-07-24 10:17:41,201][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[24224],
        [26314],
        [38098],
        [44543],
        [44264],
        [38742],
        [40789],
        [45558],
        [44078],
        [48123],
        [48450],
        [48990],
        [49225],
        [49139],
        [48053],
        [47614],
        [47729]], device='cuda:0')
[2024-07-24 10:17:41,202][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[18257],
        [ 3533],
        [ 4202],
        [ 2023],
        [ 1598],
        [ 1984],
        [ 7576],
        [ 5477],
        [ 3613],
        [ 4458],
        [ 4140],
        [ 3425],
        [ 4069],
        [ 3115],
        [ 5324],
        [ 4877],
        [ 4688]], device='cuda:0')
[2024-07-24 10:17:41,203][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[44014],
        [45782],
        [46301],
        [35266],
        [43585],
        [44571],
        [42007],
        [44389],
        [44415],
        [41111],
        [41947],
        [41035],
        [41771],
        [44254],
        [41595],
        [38475],
        [40690]], device='cuda:0')
[2024-07-24 10:17:41,205][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[46507],
        [30056],
        [32453],
        [44461],
        [42032],
        [38379],
        [39797],
        [37243],
        [36503],
        [29742],
        [37656],
        [24644],
        [31949],
        [34224],
        [36257],
        [34923],
        [37072]], device='cuda:0')
[2024-07-24 10:17:41,207][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[1272],
        [9308],
        [2026],
        [1677],
        [1818],
        [ 442],
        [3463],
        [3019],
        [1411],
        [1120],
        [1192],
        [1243],
        [ 816],
        [ 557],
        [ 407],
        [ 430],
        [ 356]], device='cuda:0')
[2024-07-24 10:17:41,209][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 6792],
        [ 7988],
        [ 4416],
        [22211],
        [23882],
        [14655],
        [ 3715],
        [ 7960],
        [ 5253],
        [ 5930],
        [ 4886],
        [ 8590],
        [ 4773],
        [ 5338],
        [ 5836],
        [ 5802],
        [ 6280]], device='cuda:0')
[2024-07-24 10:17:41,211][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[16780],
        [42432],
        [43961],
        [50257],
        [50257],
        [49293],
        [49060],
        [48734],
        [50064],
        [49564],
        [49730],
        [44682],
        [48194],
        [47978],
        [49072],
        [44732],
        [48038]], device='cuda:0')
[2024-07-24 10:17:41,214][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[14311],
        [26899],
        [27792],
        [29486],
        [17367],
        [16442],
        [ 4301],
        [ 7407],
        [11083],
        [15722],
        [15230],
        [12932],
        [ 8295],
        [ 8371],
        [ 7896],
        [ 8066],
        [ 8937]], device='cuda:0')
[2024-07-24 10:17:41,215][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[10967],
        [48029],
        [45802],
        [47672],
        [47551],
        [47340],
        [42175],
        [44795],
        [44950],
        [44253],
        [47491],
        [44158],
        [43359],
        [41926],
        [44288],
        [42860],
        [42172]], device='cuda:0')
[2024-07-24 10:17:41,217][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[30323],
        [ 6263],
        [ 3588],
        [ 2998],
        [ 4531],
        [10324],
        [16567],
        [ 5384],
        [ 5390],
        [ 5080],
        [ 3621],
        [ 3074],
        [ 7798],
        [ 6672],
        [ 5191],
        [ 9095],
        [ 6058]], device='cuda:0')
[2024-07-24 10:17:41,219][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 8061],
        [ 7765],
        [26485],
        [33878],
        [33404],
        [ 8849],
        [22184],
        [16326],
        [ 7905],
        [29590],
        [34956],
        [34988],
        [34125],
        [34320],
        [32432],
        [32169],
        [32938]], device='cuda:0')
[2024-07-24 10:17:41,220][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[26988],
        [ 2649],
        [ 2602],
        [ 1827],
        [ 1563],
        [  907],
        [ 1463],
        [ 1824],
        [ 1778],
        [ 2042],
        [ 3291],
        [ 2764],
        [ 2336],
        [ 2895],
        [ 2674],
        [ 3591],
        [ 4368]], device='cuda:0')
[2024-07-24 10:17:41,222][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[ 2462],
        [ 9399],
        [11775],
        [21237],
        [17678],
        [ 2979],
        [ 4261],
        [ 2015],
        [ 2248],
        [ 2871],
        [ 4436],
        [10721],
        [12998],
        [ 6239],
        [16325],
        [11970],
        [10742]], device='cuda:0')
[2024-07-24 10:17:41,224][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 2710],
        [ 2918],
        [18960],
        [13860],
        [10375],
        [ 4645],
        [ 1444],
        [ 2419],
        [ 2140],
        [  921],
        [ 1034],
        [ 1921],
        [ 2130],
        [ 2617],
        [ 3497],
        [ 4003],
        [ 4520]], device='cuda:0')
[2024-07-24 10:17:41,226][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[25092],
        [20401],
        [22047],
        [23760],
        [25162],
        [28013],
        [27370],
        [31301],
        [33577],
        [35583],
        [38103],
        [36801],
        [36820],
        [38345],
        [38315],
        [39317],
        [39016]], device='cuda:0')
[2024-07-24 10:17:41,227][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 6115],
        [ 6093],
        [ 6590],
        [16604],
        [13495],
        [ 4588],
        [ 9974],
        [ 4219],
        [ 4775],
        [10909],
        [ 9028],
        [12594],
        [ 9119],
        [ 7353],
        [ 9281],
        [11748],
        [10408]], device='cuda:0')
[2024-07-24 10:17:41,229][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[36153],
        [ 4913],
        [ 6428],
        [ 4794],
        [ 4734],
        [12258],
        [12247],
        [14447],
        [10719],
        [16666],
        [ 9144],
        [26438],
        [11944],
        [17032],
        [14346],
        [20588],
        [17449]], device='cuda:0')
[2024-07-24 10:17:41,232][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 5755],
        [43859],
        [41857],
        [38102],
        [34184],
        [16170],
        [16113],
        [15972],
        [19365],
        [19287],
        [19061],
        [22905],
        [16657],
        [15116],
        [12572],
        [12503],
        [12596]], device='cuda:0')
[2024-07-24 10:17:41,234][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[29816],
        [34167],
        [28258],
        [17234],
        [13560],
        [20633],
        [21232],
        [22863],
        [29466],
        [22513],
        [25527],
        [36729],
        [35487],
        [37509],
        [38927],
        [35234],
        [38913]], device='cuda:0')
[2024-07-24 10:17:41,236][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 2754],
        [46835],
        [33637],
        [41504],
        [41928],
        [48214],
        [46755],
        [47708],
        [48117],
        [48448],
        [48897],
        [48870],
        [48404],
        [48772],
        [48636],
        [48904],
        [48757]], device='cuda:0')
[2024-07-24 10:17:41,237][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 3134],
        [19059],
        [19102],
        [10686],
        [25518],
        [34664],
        [40337],
        [40037],
        [32002],
        [30246],
        [30083],
        [24817],
        [29532],
        [29874],
        [29651],
        [31147],
        [30498]], device='cuda:0')
[2024-07-24 10:17:41,238][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[13081],
        [ 1179],
        [ 3170],
        [ 2536],
        [ 3675],
        [ 4376],
        [ 7318],
        [ 5122],
        [ 3990],
        [ 5184],
        [ 2580],
        [ 4528],
        [ 3661],
        [ 4625],
        [ 3980],
        [ 5419],
        [ 5498]], device='cuda:0')
[2024-07-24 10:17:41,240][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[47812],
        [44361],
        [34564],
        [31181],
        [30525],
        [44228],
        [43142],
        [42723],
        [43273],
        [39584],
        [37841],
        [26904],
        [30758],
        [30252],
        [27838],
        [25567],
        [25285]], device='cuda:0')
[2024-07-24 10:17:41,242][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[21875],
        [44645],
        [45043],
        [43239],
        [44846],
        [39241],
        [16184],
        [40386],
        [41089],
        [37178],
        [43270],
        [46187],
        [33970],
        [35887],
        [37764],
        [36443],
        [37897]], device='cuda:0')
[2024-07-24 10:17:41,244][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559],
        [17559]], device='cuda:0')
[2024-07-24 10:17:41,352][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:41,356][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,358][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,361][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,363][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,365][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,366][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,367][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,368][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,369][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,369][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,370][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,371][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,371][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.4545, 0.5455], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,373][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0104, 0.9896], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,374][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0700, 0.9300], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,376][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.1434, 0.8566], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,378][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.7273, 0.2727], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,379][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0331, 0.9669], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,381][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.1428, 0.8572], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,383][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.2618, 0.7382], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,384][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0072, 0.9928], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,385][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0170, 0.9830], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,386][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0621, 0.9379], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,387][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([0.0266, 0.9734], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,387][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0114, 0.1274, 0.8612], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,388][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ and] are: tensor([5.3837e-04, 6.9545e-01, 3.0401e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,389][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0015, 0.1590, 0.8395], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,390][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0145, 0.4784, 0.5071], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,392][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0209, 0.1095, 0.8695], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,393][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ and] are: tensor([1.7548e-04, 9.3046e-01, 6.9368e-02], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,394][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ and] are: tensor([4.2507e-04, 9.9354e-02, 9.0022e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,396][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0686, 0.5894, 0.3421], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,398][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0013, 0.8317, 0.1670], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,399][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ and] are: tensor([1.0755e-05, 9.2373e-01, 7.6260e-02], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,400][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ and] are: tensor([4.1396e-03, 9.9565e-01, 2.0807e-04], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,401][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ and] are: tensor([9.8353e-05, 6.4720e-01, 3.5271e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,402][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.0045, 0.1834, 0.5925, 0.2196], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,403][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([1.9849e-04, 4.3604e-01, 8.3073e-02, 4.8068e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,404][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.0007, 0.1371, 0.3804, 0.4818], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,404][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.0024, 0.5069, 0.1735, 0.3172], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,405][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0149, 0.1358, 0.6515, 0.1978], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,406][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0028, 0.5699, 0.0263, 0.4010], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,407][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([1.2525e-04, 9.5160e-02, 4.4491e-01, 4.5980e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,409][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.0115, 0.4575, 0.1475, 0.3835], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,410][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([0.0008, 0.6605, 0.0852, 0.2535], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,412][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.0011, 0.7265, 0.0365, 0.2359], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,413][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([2.9569e-03, 8.8139e-01, 7.5488e-04, 1.1489e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,414][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([6.7470e-05, 3.6202e-01, 2.3659e-01, 4.0132e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,416][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0171, 0.1563, 0.3586, 0.2500, 0.2179], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,418][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ got] are: tensor([3.6939e-04, 3.2853e-01, 8.1229e-02, 5.2600e-01, 6.3862e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,419][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0013, 0.1746, 0.2628, 0.3977, 0.1636], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,420][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0073, 0.4502, 0.1569, 0.2257, 0.1598], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,421][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0761, 0.1016, 0.4132, 0.1194, 0.2897], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,421][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0027, 0.4606, 0.0325, 0.2649, 0.2393], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,422][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0008, 0.1013, 0.3280, 0.4962, 0.0737], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,423][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0201, 0.3589, 0.1742, 0.3239, 0.1228], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,424][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0013, 0.4774, 0.1910, 0.1519, 0.1784], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,426][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0007, 0.6169, 0.0365, 0.1409, 0.2051], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,427][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ got] are: tensor([2.4049e-03, 8.8561e-01, 5.9637e-04, 1.0273e-01, 8.6675e-03],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,429][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0006, 0.3528, 0.0981, 0.2247, 0.3239], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,430][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0468, 0.1074, 0.1382, 0.1662, 0.0901, 0.4512], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,432][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0004, 0.2651, 0.0655, 0.4125, 0.0539, 0.2026], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,434][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0012, 0.0946, 0.2101, 0.2695, 0.1083, 0.3162], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,436][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0071, 0.3093, 0.1454, 0.1989, 0.1143, 0.2250], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,437][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0415, 0.0344, 0.1745, 0.0551, 0.1736, 0.5209], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,437][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0013, 0.1240, 0.0068, 0.0593, 0.0762, 0.7324], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,438][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0024, 0.0902, 0.2218, 0.4349, 0.0545, 0.1962], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,439][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0481, 0.1703, 0.1439, 0.3126, 0.1205, 0.2046], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,440][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0018, 0.4197, 0.0949, 0.1537, 0.1058, 0.2240], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,440][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0011, 0.4026, 0.0115, 0.0738, 0.0821, 0.4289], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,441][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([4.1338e-03, 8.6194e-01, 5.9845e-04, 1.0539e-01, 7.4666e-03, 2.0470e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,443][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0032, 0.1636, 0.0405, 0.1388, 0.1839, 0.4700], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,445][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.0028, 0.0685, 0.1172, 0.0699, 0.0782, 0.4084, 0.2550],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,446][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([3.1346e-04, 2.2079e-01, 4.9995e-02, 2.4253e-01, 5.0359e-02, 3.2159e-01,
        1.1443e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,447][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.0006, 0.0686, 0.1652, 0.1649, 0.0839, 0.4410, 0.0758],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,449][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.0043, 0.2377, 0.0968, 0.1285, 0.0876, 0.2576, 0.1876],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,451][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0052, 0.0107, 0.1074, 0.0186, 0.1134, 0.6585, 0.0863],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,452][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([2.1623e-03, 1.4278e-02, 4.6049e-04, 4.3146e-03, 1.3424e-02, 1.3768e-01,
        8.2768e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,454][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.0004, 0.0678, 0.1588, 0.2205, 0.0615, 0.3149, 0.1763],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,455][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.0130, 0.1886, 0.0984, 0.2229, 0.0890, 0.2125, 0.1757],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,455][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([0.0006, 0.3552, 0.0655, 0.1469, 0.0685, 0.2849, 0.0785],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,456][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([9.8048e-04, 2.1044e-02, 3.3388e-04, 2.5990e-03, 1.3146e-02, 3.6457e-02,
        9.2544e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,457][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([7.0935e-03, 8.1610e-01, 3.5978e-04, 9.4466e-02, 1.3921e-02, 6.1239e-02,
        6.8175e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,458][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([2.4645e-05, 1.1163e-01, 4.0717e-02, 6.2171e-02, 1.2833e-01, 6.1374e-01,
        4.3390e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,459][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0058, 0.0406, 0.0466, 0.0487, 0.0278, 0.2028, 0.1633, 0.4644],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,461][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ at] are: tensor([2.0969e-04, 2.0874e-01, 5.5656e-02, 3.5134e-01, 3.4770e-02, 1.8405e-01,
        1.0108e-01, 6.4151e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,462][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0007, 0.0599, 0.1676, 0.1793, 0.0669, 0.2465, 0.0716, 0.2076],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,464][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0064, 0.1769, 0.0755, 0.1777, 0.0934, 0.2021, 0.1636, 0.1044],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,465][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0219, 0.0171, 0.0718, 0.0413, 0.0748, 0.2623, 0.1395, 0.3712],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,466][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ at] are: tensor([2.2950e-03, 1.2165e-02, 3.5318e-04, 4.9400e-03, 8.6186e-03, 6.1086e-02,
        6.5674e-01, 2.5380e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,468][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0010, 0.0600, 0.1310, 0.3091, 0.0358, 0.1842, 0.1662, 0.1127],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,470][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0221, 0.1080, 0.0797, 0.2924, 0.0770, 0.1623, 0.1497, 0.1088],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,471][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0009, 0.3162, 0.0644, 0.1487, 0.0799, 0.2329, 0.0858, 0.0711],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,472][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ at] are: tensor([1.5258e-03, 1.5103e-02, 2.8180e-04, 1.9986e-03, 4.5883e-03, 1.8486e-02,
        6.6684e-01, 2.9118e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,473][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ at] are: tensor([7.6676e-03, 8.1160e-01, 6.0722e-04, 1.1625e-01, 9.7083e-03, 3.1929e-02,
        6.6319e-03, 1.5601e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,473][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0016, 0.1435, 0.0407, 0.1362, 0.1592, 0.3087, 0.0605, 0.1496],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,474][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0224, 0.0341, 0.0279, 0.0453, 0.0156, 0.1230, 0.1267, 0.3632, 0.2418],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,475][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0006, 0.2108, 0.0320, 0.3685, 0.0295, 0.1388, 0.0765, 0.0495, 0.0938],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,477][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0014, 0.0743, 0.1128, 0.1705, 0.0472, 0.1751, 0.0748, 0.1469, 0.1971],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,479][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0090, 0.2502, 0.0595, 0.1874, 0.0702, 0.1264, 0.1089, 0.0677, 0.1208],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,481][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0346, 0.0108, 0.0515, 0.0248, 0.0576, 0.2058, 0.0977, 0.3004, 0.2169],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,482][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.4461e-03, 4.9639e-03, 1.1828e-04, 1.9570e-03, 3.9224e-03, 3.1543e-02,
        4.2810e-01, 1.5878e-01, 3.6817e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,484][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0035, 0.0579, 0.0921, 0.3544, 0.0278, 0.1180, 0.1535, 0.0790, 0.1138],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,485][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0367, 0.0915, 0.0540, 0.3611, 0.0489, 0.1124, 0.0972, 0.0768, 0.1213],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,487][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0011, 0.3398, 0.0375, 0.1097, 0.0485, 0.1480, 0.0772, 0.0456, 0.1925],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,488][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.1515e-03, 7.8729e-03, 1.1377e-04, 1.0811e-03, 2.0947e-03, 1.0520e-02,
        4.7050e-01, 1.6966e-01, 3.3600e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,489][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([1.0930e-02, 8.3026e-01, 3.6775e-04, 1.0204e-01, 4.9948e-03, 1.5586e-02,
        4.2133e-03, 8.2379e-03, 2.3374e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,490][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0191, 0.1143, 0.0300, 0.1735, 0.1193, 0.2018, 0.0667, 0.1253, 0.1501],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,491][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0028, 0.0409, 0.0391, 0.0390, 0.0258, 0.1404, 0.0960, 0.2486, 0.2214,
        0.1460], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,491][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ station] are: tensor([1.0100e-04, 1.9212e-01, 3.3043e-02, 2.5564e-01, 2.5856e-02, 1.8069e-01,
        5.9850e-02, 5.0242e-02, 1.1046e-01, 9.1991e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,492][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ station] are: tensor([1.1429e-04, 4.8097e-02, 9.3657e-02, 1.2979e-01, 3.7610e-02, 2.4405e-01,
        4.0242e-02, 1.5093e-01, 2.1873e-01, 3.6777e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,494][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0032, 0.2308, 0.0600, 0.1438, 0.0635, 0.1496, 0.1218, 0.0589, 0.0978,
        0.0707], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,496][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0050, 0.0067, 0.0654, 0.0144, 0.0557, 0.2915, 0.0574, 0.2418, 0.1839,
        0.0785], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,497][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ station] are: tensor([2.1760e-03, 1.0986e-03, 1.5538e-05, 1.6514e-04, 7.6038e-04, 7.5527e-03,
        4.6770e-02, 3.5990e-02, 9.3824e-02, 8.1165e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,499][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0004, 0.0549, 0.0897, 0.2517, 0.0350, 0.1707, 0.1045, 0.0842, 0.1173,
        0.0916], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,501][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0084, 0.1381, 0.0445, 0.2917, 0.0558, 0.1107, 0.0914, 0.0586, 0.1038,
        0.0970], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,502][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ station] are: tensor([2.2933e-04, 2.6184e-01, 3.6085e-02, 8.1348e-02, 4.7636e-02, 1.9248e-01,
        3.2213e-02, 5.7139e-02, 2.3744e-01, 5.3587e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,503][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ station] are: tensor([1.5764e-03, 1.2858e-03, 5.3002e-06, 5.9158e-05, 3.4049e-04, 1.7145e-03,
        3.2935e-02, 2.4611e-02, 4.2381e-02, 8.9509e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,505][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ station] are: tensor([2.5774e-03, 8.2247e-01, 1.9832e-04, 9.6808e-02, 5.3809e-03, 2.5865e-02,
        2.5954e-03, 9.2227e-03, 3.0020e-02, 4.8650e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,506][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ station] are: tensor([1.1997e-05, 6.7022e-02, 2.1424e-02, 4.7825e-02, 6.8911e-02, 3.6542e-01,
        2.2350e-02, 1.1201e-01, 2.6960e-01, 2.5422e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,506][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0022, 0.0327, 0.0264, 0.0408, 0.0144, 0.1004, 0.0683, 0.1958, 0.1515,
        0.1234, 0.2441], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,507][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [,] are: tensor([7.7493e-05, 1.8278e-01, 2.2271e-02, 3.4628e-01, 2.1741e-02, 1.2162e-01,
        4.5882e-02, 4.2855e-02, 7.6243e-02, 7.0055e-02, 7.0193e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,508][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0002, 0.0596, 0.0748, 0.1560, 0.0390, 0.1857, 0.0350, 0.1212, 0.1781,
        0.0317, 0.1186], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,509][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0036, 0.2231, 0.0465, 0.1709, 0.0531, 0.1271, 0.0932, 0.0536, 0.0944,
        0.0541, 0.0806], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,510][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0086, 0.0063, 0.0479, 0.0124, 0.0465, 0.1897, 0.0513, 0.2163, 0.1535,
        0.0821, 0.1854], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,511][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [,] are: tensor([1.2529e-04, 7.6979e-04, 3.8443e-05, 2.6252e-04, 6.6996e-04, 6.5595e-03,
        5.2759e-02, 2.7185e-02, 7.3985e-02, 6.8392e-01, 1.5373e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,513][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0003, 0.0499, 0.0944, 0.2931, 0.0225, 0.1058, 0.0656, 0.0559, 0.0778,
        0.0733, 0.1613], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,515][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0105, 0.0773, 0.0386, 0.2935, 0.0447, 0.1028, 0.0771, 0.0540, 0.0924,
        0.0785, 0.1305], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,517][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0004, 0.2799, 0.0266, 0.1045, 0.0393, 0.1419, 0.0467, 0.0455, 0.1823,
        0.0686, 0.0642], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,518][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [,] are: tensor([7.0662e-05, 9.3790e-04, 1.1959e-05, 1.1107e-04, 2.6421e-04, 1.2238e-03,
        3.2075e-02, 1.6186e-02, 3.5889e-02, 7.3584e-01, 1.7739e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,519][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [,] are: tensor([2.3618e-03, 8.4338e-01, 1.6478e-04, 1.0267e-01, 4.1096e-03, 1.4406e-02,
        2.0776e-03, 6.4149e-03, 1.9272e-02, 3.4666e-03, 1.6769e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,521][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [,] are: tensor([8.8190e-05, 1.1688e-01, 2.2657e-02, 1.0574e-01, 6.9087e-02, 2.2792e-01,
        3.4257e-02, 9.1995e-02, 2.1969e-01, 4.4416e-02, 6.7275e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,522][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.0010, 0.0156, 0.0196, 0.0157, 0.0075, 0.0944, 0.0389, 0.1651, 0.1414,
        0.0897, 0.1903, 0.2208], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,523][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([7.4516e-05, 8.5664e-02, 1.1317e-02, 1.5012e-01, 1.2488e-02, 1.0764e-01,
        4.2919e-02, 4.0129e-02, 8.3112e-02, 7.5116e-02, 5.9606e-02, 3.3181e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,524][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0003, 0.0319, 0.0848, 0.0861, 0.0237, 0.1859, 0.0330, 0.0968, 0.1902,
        0.0287, 0.0938, 0.1448], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,525][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.0028, 0.1096, 0.0276, 0.0824, 0.0359, 0.1293, 0.0582, 0.0381, 0.0816,
        0.0444, 0.0482, 0.3420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,525][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0092, 0.0047, 0.0330, 0.0144, 0.0355, 0.2060, 0.0544, 0.1390, 0.1439,
        0.0924, 0.1700, 0.0976], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,526][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([8.3139e-04, 1.6845e-04, 1.2792e-06, 2.9016e-05, 1.5061e-04, 1.1890e-03,
        1.1833e-02, 7.8455e-03, 1.6652e-02, 3.7738e-01, 4.5458e-02, 5.3847e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,527][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([1.7254e-04, 2.3886e-02, 5.4585e-02, 1.0561e-01, 1.1998e-02, 1.0703e-01,
        6.0305e-02, 5.6869e-02, 9.0172e-02, 7.5101e-02, 1.6474e-01, 2.4954e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,529][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0067, 0.0828, 0.0337, 0.1830, 0.0284, 0.0762, 0.0562, 0.0391, 0.0736,
        0.0638, 0.0944, 0.2621], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,531][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([0.0004, 0.2684, 0.0251, 0.0871, 0.0339, 0.1430, 0.0345, 0.0337, 0.1484,
        0.0374, 0.0506, 0.1375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,532][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([8.4668e-04, 1.2359e-04, 3.5948e-07, 9.1362e-06, 5.0911e-05, 1.7664e-04,
        7.3671e-03, 4.5612e-03, 7.4357e-03, 3.5214e-01, 4.2550e-02, 5.8474e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,533][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([4.5062e-03, 5.0681e-01, 2.2678e-04, 8.7309e-02, 4.4897e-03, 2.2794e-02,
        3.8481e-03, 1.3865e-02, 3.4001e-02, 8.0671e-03, 3.6403e-03, 3.1044e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,535][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([1.0457e-04, 6.9788e-02, 1.9390e-02, 6.7594e-02, 7.5751e-02, 2.2723e-01,
        2.8846e-02, 1.1901e-01, 2.0090e-01, 2.9789e-02, 6.1852e-02, 9.9739e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,537][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0017, 0.0209, 0.0148, 0.0281, 0.0119, 0.0862, 0.0566, 0.1298, 0.0972,
        0.0962, 0.1477, 0.2561, 0.0528], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,538][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([7.8865e-05, 1.2913e-01, 1.7250e-02, 1.4788e-01, 1.5439e-02, 8.1018e-02,
        2.9652e-02, 3.5112e-02, 5.6039e-02, 5.5753e-02, 5.0461e-02, 2.8875e-01,
        9.3439e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,539][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([1.0277e-04, 4.2459e-02, 4.8555e-02, 8.6651e-02, 2.7999e-02, 1.4245e-01,
        2.4981e-02, 9.1878e-02, 1.5874e-01, 2.4907e-02, 8.3414e-02, 1.5328e-01,
        1.1459e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,540][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0014, 0.1085, 0.0290, 0.0591, 0.0366, 0.0950, 0.0705, 0.0450, 0.0819,
        0.0493, 0.0591, 0.2417, 0.1229], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,541][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0080, 0.0047, 0.0296, 0.0090, 0.0285, 0.1059, 0.0325, 0.1352, 0.1036,
        0.0718, 0.2183, 0.0631, 0.1898], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,542][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([1.2447e-04, 8.1107e-05, 9.1766e-07, 1.3727e-05, 6.3628e-05, 5.7356e-04,
        6.9641e-03, 3.4095e-03, 9.4727e-03, 1.2875e-01, 2.5154e-02, 2.5526e-01,
        5.7014e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,542][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([1.6795e-04, 3.5265e-02, 6.7481e-02, 1.4003e-01, 1.8999e-02, 1.0796e-01,
        4.7864e-02, 4.6164e-02, 6.8025e-02, 6.2933e-02, 1.4199e-01, 2.2321e-01,
        3.9918e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,543][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0035, 0.0852, 0.0272, 0.1819, 0.0367, 0.0798, 0.0458, 0.0368, 0.0835,
        0.0575, 0.0859, 0.1576, 0.1185], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,544][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([1.1843e-04, 1.4977e-01, 2.9591e-02, 4.6461e-02, 2.8538e-02, 1.1284e-01,
        3.2527e-02, 3.6576e-02, 1.5476e-01, 5.0429e-02, 6.6601e-02, 1.2610e-01,
        1.6569e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,546][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([4.1437e-05, 8.5933e-05, 1.7795e-07, 2.9115e-06, 1.8414e-05, 1.0060e-04,
        2.6636e-03, 1.5582e-03, 3.6852e-03, 1.2373e-01, 2.2738e-02, 3.7958e-01,
        4.6579e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,547][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([1.4209e-03, 7.0390e-01, 2.0965e-04, 7.3312e-02, 3.5135e-03, 1.2658e-02,
        1.8001e-03, 4.4904e-03, 1.3771e-02, 2.5366e-03, 1.3050e-03, 1.6956e-01,
        1.1516e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,548][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([3.4725e-05, 7.4863e-02, 2.0431e-02, 4.9529e-02, 4.0968e-02, 2.1033e-01,
        1.6224e-02, 1.1070e-01, 1.7529e-01, 2.6457e-02, 5.6227e-02, 8.1938e-02,
        1.3700e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,550][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0023, 0.0151, 0.0196, 0.0197, 0.0072, 0.0525, 0.0456, 0.1064, 0.0710,
        0.0789, 0.1584, 0.2370, 0.0603, 0.1259], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,552][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([8.9162e-05, 9.0182e-02, 2.2364e-02, 1.8661e-01, 1.0957e-02, 6.7253e-02,
        2.3389e-02, 2.0131e-02, 3.4828e-02, 4.9333e-02, 3.8780e-02, 2.4348e-01,
        7.1790e-02, 1.4081e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,554][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0003, 0.0253, 0.0766, 0.1104, 0.0253, 0.1032, 0.0207, 0.0598, 0.0890,
        0.0211, 0.0626, 0.1177, 0.0990, 0.1890], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,555][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0018, 0.0888, 0.0449, 0.0826, 0.0339, 0.0625, 0.0378, 0.0239, 0.0483,
        0.0297, 0.0527, 0.2397, 0.1285, 0.1249], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,557][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0065, 0.0033, 0.0218, 0.0092, 0.0233, 0.0792, 0.0378, 0.0916, 0.0717,
        0.0658, 0.1463, 0.0594, 0.1679, 0.2161], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,558][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([9.8428e-05, 6.0623e-05, 5.9156e-07, 1.1799e-05, 4.2100e-05, 4.7116e-04,
        5.9292e-03, 2.3410e-03, 6.4397e-03, 9.7989e-02, 1.9745e-02, 2.3648e-01,
        3.7259e-01, 2.5780e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,559][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0003, 0.0218, 0.0794, 0.1526, 0.0112, 0.0682, 0.0474, 0.0284, 0.0464,
        0.0480, 0.1159, 0.2071, 0.0342, 0.1391], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,560][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0075, 0.0431, 0.0298, 0.1752, 0.0268, 0.0581, 0.0390, 0.0341, 0.0525,
        0.0571, 0.0881, 0.1651, 0.0994, 0.1241], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,560][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0003, 0.1663, 0.0335, 0.0767, 0.0306, 0.0931, 0.0388, 0.0252, 0.1160,
        0.0505, 0.0609, 0.0944, 0.1184, 0.0952], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,561][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([8.8195e-05, 4.0901e-05, 9.5899e-08, 2.9683e-06, 7.2443e-06, 4.4543e-05,
        1.8353e-03, 8.3084e-04, 1.6927e-03, 9.1821e-02, 1.5055e-02, 3.5798e-01,
        2.9992e-01, 2.3068e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,563][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([2.6133e-03, 6.1484e-01, 3.8433e-04, 1.1190e-01, 3.4596e-03, 1.2564e-02,
        2.0975e-03, 4.7253e-03, 1.3959e-02, 3.7294e-03, 2.2350e-03, 1.9572e-01,
        1.5174e-02, 1.6597e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,565][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0026, 0.0599, 0.0258, 0.1193, 0.0534, 0.0619, 0.0243, 0.0327, 0.0378,
        0.0341, 0.0372, 0.1167, 0.1779, 0.2162], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,567][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0006, 0.0177, 0.0111, 0.0143, 0.0074, 0.0540, 0.0370, 0.0984, 0.0845,
        0.0642, 0.1390, 0.2144, 0.0482, 0.1434, 0.0655], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,568][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ give] are: tensor([1.7388e-05, 1.2320e-01, 1.0256e-02, 1.0581e-01, 6.6040e-03, 5.3168e-02,
        1.5400e-02, 1.9418e-02, 3.8723e-02, 3.5569e-02, 3.6515e-02, 2.8590e-01,
        5.8714e-02, 1.5664e-01, 5.4076e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,569][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ give] are: tensor([5.4861e-05, 2.6533e-02, 4.0890e-02, 8.6467e-02, 1.8204e-02, 9.4198e-02,
        1.3766e-02, 6.8318e-02, 1.0169e-01, 1.3329e-02, 6.0273e-02, 1.0134e-01,
        6.3839e-02, 1.8601e-01, 1.2510e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,571][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0005, 0.0847, 0.0225, 0.0589, 0.0247, 0.0782, 0.0395, 0.0303, 0.0545,
        0.0341, 0.0472, 0.2363, 0.0702, 0.1140, 0.1044], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,573][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0034, 0.0040, 0.0190, 0.0080, 0.0198, 0.0734, 0.0239, 0.0961, 0.0793,
        0.0457, 0.1366, 0.0616, 0.1459, 0.2474, 0.0359], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,574][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ give] are: tensor([7.3368e-05, 4.0802e-05, 2.1198e-07, 4.8234e-06, 2.5344e-05, 2.9049e-04,
        3.1832e-03, 1.3853e-03, 3.9334e-03, 7.7681e-02, 8.9572e-03, 1.0971e-01,
        2.0710e-01, 1.5675e-01, 4.3087e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,575][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ give] are: tensor([7.7311e-05, 3.0605e-02, 4.2808e-02, 1.1535e-01, 1.1631e-02, 7.5587e-02,
        3.6964e-02, 3.3166e-02, 5.8187e-02, 4.0873e-02, 1.1625e-01, 1.8704e-01,
        2.6343e-02, 1.8875e-01, 3.6379e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,576][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0028, 0.0893, 0.0287, 0.1642, 0.0196, 0.0524, 0.0410, 0.0296, 0.0548,
        0.0424, 0.0575, 0.1674, 0.0794, 0.0980, 0.0728], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,576][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ give] are: tensor([8.9260e-05, 1.6903e-01, 3.0606e-02, 5.0143e-02, 2.5456e-02, 1.0090e-01,
        2.3035e-02, 2.9089e-02, 1.2102e-01, 3.7228e-02, 5.7151e-02, 1.1040e-01,
        9.2261e-02, 8.3135e-02, 7.0455e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,577][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ give] are: tensor([1.8753e-05, 1.9809e-05, 2.8058e-08, 4.7696e-07, 4.1697e-06, 2.9359e-05,
        9.8136e-04, 5.1601e-04, 9.7604e-04, 5.8900e-02, 7.0975e-03, 1.0662e-01,
        1.6415e-01, 1.3093e-01, 5.2977e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,578][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ give] are: tensor([6.1677e-04, 5.5214e-01, 2.6651e-04, 9.4704e-02, 3.4034e-03, 1.5434e-02,
        2.3711e-03, 6.7854e-03, 1.8896e-02, 4.2133e-03, 2.1614e-03, 2.2730e-01,
        1.5733e-02, 1.7755e-02, 3.8221e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,580][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ give] are: tensor([1.0408e-04, 4.3831e-02, 7.3933e-03, 2.6105e-02, 2.2584e-02, 8.2007e-02,
        9.8990e-03, 4.0765e-02, 5.7709e-02, 1.6308e-02, 2.7926e-02, 5.3179e-02,
        8.0511e-02, 2.7871e-01, 2.5297e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,582][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0025, 0.0091, 0.0073, 0.0091, 0.0038, 0.0298, 0.0237, 0.0906, 0.0690,
        0.0586, 0.1459, 0.2136, 0.0497, 0.1403, 0.0566, 0.0904],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,583][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ it] are: tensor([3.1577e-05, 5.5050e-02, 1.4751e-02, 1.0101e-01, 7.3756e-03, 4.9574e-02,
        1.9566e-02, 1.8846e-02, 3.4072e-02, 4.0787e-02, 3.9910e-02, 2.0009e-01,
        4.9716e-02, 1.4631e-01, 4.6246e-02, 1.7667e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,584][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ it] are: tensor([7.2912e-05, 1.1989e-02, 3.8302e-02, 4.8227e-02, 1.3269e-02, 7.5231e-02,
        1.3790e-02, 5.2164e-02, 8.3245e-02, 1.5422e-02, 5.7721e-02, 8.5323e-02,
        6.7733e-02, 1.4991e-01, 1.0939e-01, 1.7821e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,586][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0007, 0.0619, 0.0221, 0.0540, 0.0226, 0.0511, 0.0269, 0.0235, 0.0495,
        0.0255, 0.0501, 0.1671, 0.0882, 0.1189, 0.0878, 0.1502],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,588][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0047, 0.0021, 0.0089, 0.0060, 0.0146, 0.0690, 0.0216, 0.0648, 0.0811,
        0.0516, 0.1500, 0.0524, 0.1284, 0.2179, 0.0334, 0.0934],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,589][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ it] are: tensor([4.8710e-05, 3.5945e-05, 2.3805e-07, 4.1601e-06, 2.2198e-05, 2.7953e-04,
        2.5458e-03, 1.3428e-03, 4.3210e-03, 5.1169e-02, 7.7756e-03, 7.9620e-02,
        1.7996e-01, 1.3379e-01, 3.5457e-01, 1.8452e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,591][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ it] are: tensor([1.5376e-04, 1.5405e-02, 2.9522e-02, 9.0330e-02, 7.7778e-03, 5.9030e-02,
        3.1209e-02, 2.7205e-02, 4.8173e-02, 3.9793e-02, 1.1925e-01, 1.9036e-01,
        2.5334e-02, 1.4988e-01, 2.8101e-02, 1.3847e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,592][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0041, 0.0386, 0.0245, 0.1228, 0.0205, 0.0417, 0.0396, 0.0307, 0.0489,
        0.0480, 0.0776, 0.1488, 0.0863, 0.0930, 0.0594, 0.1155],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,593][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ it] are: tensor([8.9662e-05, 1.0163e-01, 2.6933e-02, 4.4932e-02, 2.5236e-02, 8.2288e-02,
        2.2691e-02, 2.6168e-02, 1.2410e-01, 3.9266e-02, 6.9196e-02, 8.0353e-02,
        1.2188e-01, 8.3253e-02, 6.0285e-02, 9.1708e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,593][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ it] are: tensor([3.9960e-05, 1.4856e-05, 2.5919e-08, 5.0965e-07, 2.7158e-06, 2.0934e-05,
        6.9758e-04, 3.8202e-04, 8.1144e-04, 3.2311e-02, 6.0232e-03, 8.1132e-02,
        1.3259e-01, 1.1797e-01, 4.3816e-01, 1.8984e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,594][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0016, 0.3498, 0.0005, 0.0747, 0.0041, 0.0190, 0.0031, 0.0095, 0.0261,
        0.0068, 0.0046, 0.2503, 0.0259, 0.0286, 0.0509, 0.1444],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,596][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0010, 0.0404, 0.0081, 0.0547, 0.0405, 0.0577, 0.0130, 0.0292, 0.0402,
        0.0196, 0.0241, 0.0825, 0.0941, 0.1807, 0.2000, 0.1142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,598][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0018, 0.0109, 0.0136, 0.0144, 0.0052, 0.0423, 0.0326, 0.0896, 0.0610,
        0.0599, 0.1294, 0.1807, 0.0440, 0.1038, 0.0478, 0.0719, 0.0912],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,600][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([5.1411e-05, 5.6393e-02, 1.5030e-02, 1.3148e-01, 7.8431e-03, 4.8393e-02,
        1.6413e-02, 1.4444e-02, 2.7480e-02, 3.6499e-02, 3.2360e-02, 1.9174e-01,
        5.4663e-02, 1.0403e-01, 4.0847e-02, 1.3784e-01, 8.4484e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,601][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0002, 0.0171, 0.0435, 0.0717, 0.0150, 0.0660, 0.0141, 0.0389, 0.0629,
        0.0138, 0.0395, 0.0822, 0.0648, 0.1209, 0.0943, 0.1333, 0.1217],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,603][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0010, 0.0701, 0.0311, 0.0688, 0.0234, 0.0468, 0.0256, 0.0166, 0.0342,
        0.0199, 0.0346, 0.1967, 0.0860, 0.0829, 0.0844, 0.1107, 0.0674],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,605][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0038, 0.0026, 0.0137, 0.0075, 0.0166, 0.0575, 0.0270, 0.0635, 0.0544,
        0.0462, 0.0941, 0.0409, 0.1137, 0.1571, 0.0298, 0.0799, 0.1916],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,607][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.0771e-04, 2.5346e-05, 1.4722e-07, 3.1753e-06, 1.6522e-05, 1.9959e-04,
        2.0400e-03, 1.0642e-03, 2.7448e-03, 4.1422e-02, 6.9583e-03, 8.8243e-02,
        1.4513e-01, 9.7324e-02, 2.4611e-01, 1.7420e-01, 1.9441e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,608][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0002, 0.0150, 0.0496, 0.1244, 0.0075, 0.0509, 0.0318, 0.0198, 0.0367,
        0.0352, 0.0932, 0.1683, 0.0231, 0.1091, 0.0224, 0.0931, 0.1198],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,609][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0050, 0.0331, 0.0213, 0.1355, 0.0193, 0.0414, 0.0334, 0.0257, 0.0397,
        0.0451, 0.0653, 0.1286, 0.0820, 0.0896, 0.0557, 0.0984, 0.0810],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,610][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0003, 0.1249, 0.0238, 0.0568, 0.0219, 0.0730, 0.0341, 0.0203, 0.0966,
        0.0393, 0.0534, 0.0777, 0.0846, 0.0775, 0.0627, 0.0779, 0.0754],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,611][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([7.2649e-05, 1.3693e-05, 2.0113e-08, 7.4229e-07, 2.4801e-06, 1.5835e-05,
        6.8571e-04, 3.0264e-04, 5.6414e-04, 3.7703e-02, 5.0843e-03, 1.2092e-01,
        1.0829e-01, 7.7732e-02, 3.0747e-01, 1.5746e-01, 1.8369e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,612][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.9242e-03, 4.7713e-01, 4.1092e-04, 1.0421e-01, 3.4661e-03, 1.3978e-02,
        2.2257e-03, 5.3735e-03, 1.5374e-02, 4.1365e-03, 2.6135e-03, 1.8373e-01,
        1.5458e-02, 1.7415e-02, 3.4263e-02, 1.0191e-01, 1.6380e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,614][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0026, 0.0433, 0.0142, 0.0784, 0.0326, 0.0435, 0.0171, 0.0209, 0.0250,
        0.0260, 0.0231, 0.0883, 0.1092, 0.1370, 0.1545, 0.0841, 0.1003],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,731][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:41,732][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,732][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,733][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,734][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,734][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,735][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,737][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,737][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,738][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,739][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,739][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,741][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:41,743][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.4545, 0.5455], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,744][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0104, 0.9896], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,746][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0700, 0.9300], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,746][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.1434, 0.8566], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,748][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.7273, 0.2727], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,750][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0331, 0.9669], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,752][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.1428, 0.8572], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,752][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.2618, 0.7382], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,753][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.0072, 0.9928], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,754][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0170, 0.9830], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,754][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([0.0621, 0.9379], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,755][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0266, 0.9734], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:41,756][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0114, 0.1274, 0.8612], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,757][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([5.3837e-04, 6.9545e-01, 3.0401e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,758][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0015, 0.1590, 0.8395], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,760][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0145, 0.4784, 0.5071], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,762][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0209, 0.1095, 0.8695], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,763][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([1.7548e-04, 9.3046e-01, 6.9368e-02], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,764][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([4.2507e-04, 9.9354e-02, 9.0022e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,766][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0686, 0.5894, 0.3421], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,768][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0013, 0.8317, 0.1670], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,769][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([1.0755e-05, 9.2373e-01, 7.6260e-02], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,770][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([4.1396e-03, 9.9565e-01, 2.0807e-04], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,770][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([9.8353e-05, 6.4720e-01, 3.5271e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:41,771][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.0045, 0.1834, 0.5925, 0.2196], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,772][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([1.9849e-04, 4.3604e-01, 8.3073e-02, 4.8068e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,772][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.0007, 0.1371, 0.3804, 0.4818], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,773][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.0024, 0.5069, 0.1735, 0.3172], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,775][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0149, 0.1358, 0.6515, 0.1978], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,777][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.0028, 0.5699, 0.0263, 0.4010], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,778][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([1.2525e-04, 9.5160e-02, 4.4491e-01, 4.5980e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,779][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0115, 0.4575, 0.1475, 0.3835], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,781][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([0.0008, 0.6605, 0.0852, 0.2535], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,783][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.0011, 0.7265, 0.0365, 0.2359], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,784][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([2.9569e-03, 8.8139e-01, 7.5488e-04, 1.1489e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,786][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([6.7470e-05, 3.6202e-01, 2.3659e-01, 4.0132e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:41,787][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0171, 0.1563, 0.3586, 0.2500, 0.2179], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,788][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([3.6939e-04, 3.2853e-01, 8.1229e-02, 5.2600e-01, 6.3862e-02],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,788][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0013, 0.1746, 0.2628, 0.3977, 0.1636], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,789][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0073, 0.4502, 0.1569, 0.2257, 0.1598], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,790][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0761, 0.1016, 0.4132, 0.1194, 0.2897], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,790][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0027, 0.4606, 0.0325, 0.2649, 0.2393], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,792][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0008, 0.1013, 0.3280, 0.4962, 0.0737], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,794][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0201, 0.3589, 0.1742, 0.3239, 0.1228], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,796][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0013, 0.4774, 0.1910, 0.1519, 0.1784], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,797][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0007, 0.6169, 0.0365, 0.1409, 0.2051], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,799][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([2.4049e-03, 8.8561e-01, 5.9637e-04, 1.0273e-01, 8.6675e-03],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,801][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0006, 0.3528, 0.0981, 0.2247, 0.3239], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:41,802][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0468, 0.1074, 0.1382, 0.1662, 0.0901, 0.4512], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,804][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0004, 0.2651, 0.0655, 0.4125, 0.0539, 0.2026], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,805][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0012, 0.0946, 0.2101, 0.2695, 0.1083, 0.3162], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,806][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0071, 0.3093, 0.1454, 0.1989, 0.1143, 0.2250], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,806][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0415, 0.0344, 0.1745, 0.0551, 0.1736, 0.5209], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,807][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0013, 0.1240, 0.0068, 0.0593, 0.0762, 0.7324], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,808][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0024, 0.0902, 0.2218, 0.4349, 0.0545, 0.1962], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,810][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0481, 0.1703, 0.1439, 0.3126, 0.1205, 0.2046], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,812][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0018, 0.4197, 0.0949, 0.1537, 0.1058, 0.2240], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,813][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0011, 0.4026, 0.0115, 0.0738, 0.0821, 0.4289], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,814][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([4.1338e-03, 8.6194e-01, 5.9845e-04, 1.0539e-01, 7.4666e-03, 2.0470e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,816][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0032, 0.1636, 0.0405, 0.1388, 0.1839, 0.4700], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:41,818][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.0028, 0.0685, 0.1172, 0.0699, 0.0782, 0.4084, 0.2550],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,819][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([3.1346e-04, 2.2079e-01, 4.9995e-02, 2.4253e-01, 5.0359e-02, 3.2159e-01,
        1.1443e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,821][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.0006, 0.0686, 0.1652, 0.1649, 0.0839, 0.4410, 0.0758],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,822][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.0043, 0.2377, 0.0968, 0.1285, 0.0876, 0.2576, 0.1876],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,823][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0052, 0.0107, 0.1074, 0.0186, 0.1134, 0.6585, 0.0863],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,823][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([2.1623e-03, 1.4278e-02, 4.6049e-04, 4.3146e-03, 1.3424e-02, 1.3768e-01,
        8.2768e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,824][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0004, 0.0678, 0.1588, 0.2205, 0.0615, 0.3149, 0.1763],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,825][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0130, 0.1886, 0.0984, 0.2229, 0.0890, 0.2125, 0.1757],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,826][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([0.0006, 0.3552, 0.0655, 0.1469, 0.0685, 0.2849, 0.0785],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,827][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([9.8048e-04, 2.1044e-02, 3.3388e-04, 2.5990e-03, 1.3146e-02, 3.6457e-02,
        9.2544e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,829][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([7.0935e-03, 8.1610e-01, 3.5978e-04, 9.4466e-02, 1.3921e-02, 6.1239e-02,
        6.8175e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,830][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([2.4645e-05, 1.1163e-01, 4.0717e-02, 6.2171e-02, 1.2833e-01, 6.1374e-01,
        4.3390e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:41,832][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0058, 0.0406, 0.0466, 0.0487, 0.0278, 0.2028, 0.1633, 0.4644],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,833][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([2.0969e-04, 2.0874e-01, 5.5656e-02, 3.5134e-01, 3.4770e-02, 1.8405e-01,
        1.0108e-01, 6.4151e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,835][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0007, 0.0599, 0.1676, 0.1793, 0.0669, 0.2465, 0.0716, 0.2076],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,837][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0064, 0.1769, 0.0755, 0.1777, 0.0934, 0.2021, 0.1636, 0.1044],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,838][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0219, 0.0171, 0.0718, 0.0413, 0.0748, 0.2623, 0.1395, 0.3712],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,839][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([2.2950e-03, 1.2165e-02, 3.5318e-04, 4.9400e-03, 8.6186e-03, 6.1086e-02,
        6.5674e-01, 2.5380e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,840][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0010, 0.0600, 0.1310, 0.3091, 0.0358, 0.1842, 0.1662, 0.1127],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,841][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0221, 0.1080, 0.0797, 0.2924, 0.0770, 0.1623, 0.1497, 0.1088],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,841][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0009, 0.3162, 0.0644, 0.1487, 0.0799, 0.2329, 0.0858, 0.0711],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,842][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([1.5258e-03, 1.5103e-02, 2.8180e-04, 1.9986e-03, 4.5883e-03, 1.8486e-02,
        6.6684e-01, 2.9118e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,843][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([7.6676e-03, 8.1160e-01, 6.0722e-04, 1.1625e-01, 9.7083e-03, 3.1929e-02,
        6.6319e-03, 1.5601e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,845][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0016, 0.1435, 0.0407, 0.1362, 0.1592, 0.3087, 0.0605, 0.1496],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:41,847][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0224, 0.0341, 0.0279, 0.0453, 0.0156, 0.1230, 0.1267, 0.3632, 0.2418],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,848][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0006, 0.2108, 0.0320, 0.3685, 0.0295, 0.1388, 0.0765, 0.0495, 0.0938],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,850][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0014, 0.0743, 0.1128, 0.1705, 0.0472, 0.1751, 0.0748, 0.1469, 0.1971],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,852][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0090, 0.2502, 0.0595, 0.1874, 0.0702, 0.1264, 0.1089, 0.0677, 0.1208],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,854][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0346, 0.0108, 0.0515, 0.0248, 0.0576, 0.2058, 0.0977, 0.3004, 0.2169],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,855][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([2.4461e-03, 4.9639e-03, 1.1828e-04, 1.9570e-03, 3.9224e-03, 3.1543e-02,
        4.2810e-01, 1.5878e-01, 3.6817e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,856][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0035, 0.0579, 0.0921, 0.3544, 0.0278, 0.1180, 0.1535, 0.0790, 0.1138],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,857][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0367, 0.0915, 0.0540, 0.3611, 0.0489, 0.1124, 0.0972, 0.0768, 0.1213],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,858][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0011, 0.3398, 0.0375, 0.1097, 0.0485, 0.1480, 0.0772, 0.0456, 0.1925],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,858][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([2.1515e-03, 7.8729e-03, 1.1377e-04, 1.0811e-03, 2.0947e-03, 1.0520e-02,
        4.7050e-01, 1.6966e-01, 3.3600e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,859][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.0930e-02, 8.3026e-01, 3.6775e-04, 1.0204e-01, 4.9948e-03, 1.5586e-02,
        4.2133e-03, 8.2379e-03, 2.3374e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,860][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0191, 0.1143, 0.0300, 0.1735, 0.1193, 0.2018, 0.0667, 0.1253, 0.1501],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:41,862][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0028, 0.0409, 0.0391, 0.0390, 0.0258, 0.1404, 0.0960, 0.2486, 0.2214,
        0.1460], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,863][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([1.0100e-04, 1.9212e-01, 3.3043e-02, 2.5564e-01, 2.5856e-02, 1.8069e-01,
        5.9850e-02, 5.0242e-02, 1.1046e-01, 9.1991e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,865][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([1.1429e-04, 4.8097e-02, 9.3657e-02, 1.2979e-01, 3.7610e-02, 2.4405e-01,
        4.0242e-02, 1.5093e-01, 2.1873e-01, 3.6777e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,866][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0032, 0.2308, 0.0600, 0.1438, 0.0635, 0.1496, 0.1218, 0.0589, 0.0978,
        0.0707], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,869][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0050, 0.0067, 0.0654, 0.0144, 0.0557, 0.2915, 0.0574, 0.2418, 0.1839,
        0.0785], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,870][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([2.1760e-03, 1.0986e-03, 1.5538e-05, 1.6514e-04, 7.6038e-04, 7.5527e-03,
        4.6770e-02, 3.5990e-02, 9.3824e-02, 8.1165e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,872][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0004, 0.0549, 0.0897, 0.2517, 0.0350, 0.1707, 0.1045, 0.0842, 0.1173,
        0.0916], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,874][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0084, 0.1381, 0.0445, 0.2917, 0.0558, 0.1107, 0.0914, 0.0586, 0.1038,
        0.0970], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,875][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([2.2933e-04, 2.6184e-01, 3.6085e-02, 8.1348e-02, 4.7636e-02, 1.9248e-01,
        3.2213e-02, 5.7139e-02, 2.3744e-01, 5.3587e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,876][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.5764e-03, 1.2858e-03, 5.3002e-06, 5.9158e-05, 3.4049e-04, 1.7145e-03,
        3.2935e-02, 2.4611e-02, 4.2381e-02, 8.9509e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,877][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([2.5774e-03, 8.2247e-01, 1.9832e-04, 9.6808e-02, 5.3809e-03, 2.5865e-02,
        2.5954e-03, 9.2227e-03, 3.0020e-02, 4.8650e-03], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,878][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([1.1997e-05, 6.7022e-02, 2.1424e-02, 4.7825e-02, 6.8911e-02, 3.6542e-01,
        2.2350e-02, 1.1201e-01, 2.6960e-01, 2.5422e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:41,879][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0022, 0.0327, 0.0264, 0.0408, 0.0144, 0.1004, 0.0683, 0.1958, 0.1515,
        0.1234, 0.2441], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,880][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([7.7493e-05, 1.8278e-01, 2.2271e-02, 3.4628e-01, 2.1741e-02, 1.2162e-01,
        4.5882e-02, 4.2855e-02, 7.6243e-02, 7.0055e-02, 7.0193e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,881][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0002, 0.0596, 0.0748, 0.1560, 0.0390, 0.1857, 0.0350, 0.1212, 0.1781,
        0.0317, 0.1186], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,882][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0036, 0.2231, 0.0465, 0.1709, 0.0531, 0.1271, 0.0932, 0.0536, 0.0944,
        0.0541, 0.0806], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,884][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0086, 0.0063, 0.0479, 0.0124, 0.0465, 0.1897, 0.0513, 0.2163, 0.1535,
        0.0821, 0.1854], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,885][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.2529e-04, 7.6979e-04, 3.8443e-05, 2.6252e-04, 6.6996e-04, 6.5595e-03,
        5.2759e-02, 2.7185e-02, 7.3985e-02, 6.8392e-01, 1.5373e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,887][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0003, 0.0499, 0.0944, 0.2931, 0.0225, 0.1058, 0.0656, 0.0559, 0.0778,
        0.0733, 0.1613], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,889][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0105, 0.0773, 0.0386, 0.2935, 0.0447, 0.1028, 0.0771, 0.0540, 0.0924,
        0.0785, 0.1305], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,891][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0004, 0.2799, 0.0266, 0.1045, 0.0393, 0.1419, 0.0467, 0.0455, 0.1823,
        0.0686, 0.0642], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,892][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([7.0662e-05, 9.3790e-04, 1.1959e-05, 1.1107e-04, 2.6421e-04, 1.2238e-03,
        3.2075e-02, 1.6186e-02, 3.5889e-02, 7.3584e-01, 1.7739e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,893][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([2.3618e-03, 8.4338e-01, 1.6478e-04, 1.0267e-01, 4.1096e-03, 1.4406e-02,
        2.0776e-03, 6.4149e-03, 1.9272e-02, 3.4666e-03, 1.6769e-03],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,894][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([8.8190e-05, 1.1688e-01, 2.2657e-02, 1.0574e-01, 6.9087e-02, 2.2792e-01,
        3.4257e-02, 9.1995e-02, 2.1969e-01, 4.4416e-02, 6.7275e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:41,895][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0010, 0.0156, 0.0196, 0.0157, 0.0075, 0.0944, 0.0389, 0.1651, 0.1414,
        0.0897, 0.1903, 0.2208], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,896][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([7.4516e-05, 8.5664e-02, 1.1317e-02, 1.5012e-01, 1.2488e-02, 1.0764e-01,
        4.2919e-02, 4.0129e-02, 8.3112e-02, 7.5116e-02, 5.9606e-02, 3.3181e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,897][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0003, 0.0319, 0.0848, 0.0861, 0.0237, 0.1859, 0.0330, 0.0968, 0.1902,
        0.0287, 0.0938, 0.1448], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,898][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.0028, 0.1096, 0.0276, 0.0824, 0.0359, 0.1293, 0.0582, 0.0381, 0.0816,
        0.0444, 0.0482, 0.3420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,899][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0092, 0.0047, 0.0330, 0.0144, 0.0355, 0.2060, 0.0544, 0.1390, 0.1439,
        0.0924, 0.1700, 0.0976], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,900][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([8.3139e-04, 1.6845e-04, 1.2792e-06, 2.9016e-05, 1.5061e-04, 1.1890e-03,
        1.1833e-02, 7.8455e-03, 1.6652e-02, 3.7738e-01, 4.5458e-02, 5.3847e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,901][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([1.7254e-04, 2.3886e-02, 5.4585e-02, 1.0561e-01, 1.1998e-02, 1.0703e-01,
        6.0305e-02, 5.6869e-02, 9.0172e-02, 7.5101e-02, 1.6474e-01, 2.4954e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,903][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0067, 0.0828, 0.0337, 0.1830, 0.0284, 0.0762, 0.0562, 0.0391, 0.0736,
        0.0638, 0.0944, 0.2621], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,905][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([0.0004, 0.2684, 0.0251, 0.0871, 0.0339, 0.1430, 0.0345, 0.0337, 0.1484,
        0.0374, 0.0506, 0.1375], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,906][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([8.4668e-04, 1.2359e-04, 3.5948e-07, 9.1362e-06, 5.0911e-05, 1.7664e-04,
        7.3671e-03, 4.5612e-03, 7.4357e-03, 3.5214e-01, 4.2550e-02, 5.8474e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,907][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([4.5062e-03, 5.0681e-01, 2.2678e-04, 8.7309e-02, 4.4897e-03, 2.2794e-02,
        3.8481e-03, 1.3865e-02, 3.4001e-02, 8.0671e-03, 3.6403e-03, 3.1044e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,909][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([1.0457e-04, 6.9788e-02, 1.9390e-02, 6.7594e-02, 7.5751e-02, 2.2723e-01,
        2.8846e-02, 1.1901e-01, 2.0090e-01, 2.9789e-02, 6.1852e-02, 9.9739e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:41,910][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0017, 0.0209, 0.0148, 0.0281, 0.0119, 0.0862, 0.0566, 0.1298, 0.0972,
        0.0962, 0.1477, 0.2561, 0.0528], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,912][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([7.8865e-05, 1.2913e-01, 1.7250e-02, 1.4788e-01, 1.5439e-02, 8.1018e-02,
        2.9652e-02, 3.5112e-02, 5.6039e-02, 5.5753e-02, 5.0461e-02, 2.8875e-01,
        9.3439e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,913][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([1.0277e-04, 4.2459e-02, 4.8555e-02, 8.6651e-02, 2.7999e-02, 1.4245e-01,
        2.4981e-02, 9.1878e-02, 1.5874e-01, 2.4907e-02, 8.3414e-02, 1.5328e-01,
        1.1459e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,913][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0014, 0.1085, 0.0290, 0.0591, 0.0366, 0.0950, 0.0705, 0.0450, 0.0819,
        0.0493, 0.0591, 0.2417, 0.1229], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,914][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0080, 0.0047, 0.0296, 0.0090, 0.0285, 0.1059, 0.0325, 0.1352, 0.1036,
        0.0718, 0.2183, 0.0631, 0.1898], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,915][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([1.2447e-04, 8.1107e-05, 9.1766e-07, 1.3727e-05, 6.3628e-05, 5.7356e-04,
        6.9641e-03, 3.4095e-03, 9.4727e-03, 1.2875e-01, 2.5154e-02, 2.5526e-01,
        5.7014e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,916][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([1.6795e-04, 3.5265e-02, 6.7481e-02, 1.4003e-01, 1.8999e-02, 1.0796e-01,
        4.7864e-02, 4.6164e-02, 6.8025e-02, 6.2933e-02, 1.4199e-01, 2.2321e-01,
        3.9918e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,918][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0035, 0.0852, 0.0272, 0.1819, 0.0367, 0.0798, 0.0458, 0.0368, 0.0835,
        0.0575, 0.0859, 0.1576, 0.1185], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,919][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([1.1843e-04, 1.4977e-01, 2.9591e-02, 4.6461e-02, 2.8538e-02, 1.1284e-01,
        3.2527e-02, 3.6576e-02, 1.5476e-01, 5.0429e-02, 6.6601e-02, 1.2610e-01,
        1.6569e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,921][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([4.1437e-05, 8.5933e-05, 1.7795e-07, 2.9115e-06, 1.8414e-05, 1.0060e-04,
        2.6636e-03, 1.5582e-03, 3.6852e-03, 1.2373e-01, 2.2738e-02, 3.7958e-01,
        4.6579e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,922][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([1.4209e-03, 7.0390e-01, 2.0965e-04, 7.3312e-02, 3.5135e-03, 1.2658e-02,
        1.8001e-03, 4.4904e-03, 1.3771e-02, 2.5366e-03, 1.3050e-03, 1.6956e-01,
        1.1516e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,923][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([3.4725e-05, 7.4863e-02, 2.0431e-02, 4.9529e-02, 4.0968e-02, 2.1033e-01,
        1.6224e-02, 1.1070e-01, 1.7529e-01, 2.6457e-02, 5.6227e-02, 8.1938e-02,
        1.3700e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:41,925][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0023, 0.0151, 0.0196, 0.0197, 0.0072, 0.0525, 0.0456, 0.1064, 0.0710,
        0.0789, 0.1584, 0.2370, 0.0603, 0.1259], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,926][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([8.9162e-05, 9.0182e-02, 2.2364e-02, 1.8661e-01, 1.0957e-02, 6.7253e-02,
        2.3389e-02, 2.0131e-02, 3.4828e-02, 4.9333e-02, 3.8780e-02, 2.4348e-01,
        7.1790e-02, 1.4081e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,928][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0003, 0.0253, 0.0766, 0.1104, 0.0253, 0.1032, 0.0207, 0.0598, 0.0890,
        0.0211, 0.0626, 0.1177, 0.0990, 0.1890], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,929][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0018, 0.0888, 0.0449, 0.0826, 0.0339, 0.0625, 0.0378, 0.0239, 0.0483,
        0.0297, 0.0527, 0.2397, 0.1285, 0.1249], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,930][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0065, 0.0033, 0.0218, 0.0092, 0.0233, 0.0792, 0.0378, 0.0916, 0.0717,
        0.0658, 0.1463, 0.0594, 0.1679, 0.2161], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,931][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([9.8428e-05, 6.0623e-05, 5.9156e-07, 1.1799e-05, 4.2100e-05, 4.7116e-04,
        5.9292e-03, 2.3410e-03, 6.4397e-03, 9.7989e-02, 1.9745e-02, 2.3648e-01,
        3.7259e-01, 2.5780e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,932][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0003, 0.0218, 0.0794, 0.1526, 0.0112, 0.0682, 0.0474, 0.0284, 0.0464,
        0.0480, 0.1159, 0.2071, 0.0342, 0.1391], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,933][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0075, 0.0431, 0.0298, 0.1752, 0.0268, 0.0581, 0.0390, 0.0341, 0.0525,
        0.0571, 0.0881, 0.1651, 0.0994, 0.1241], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,935][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0003, 0.1663, 0.0335, 0.0767, 0.0306, 0.0931, 0.0388, 0.0252, 0.1160,
        0.0505, 0.0609, 0.0944, 0.1184, 0.0952], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,936][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([8.8195e-05, 4.0901e-05, 9.5899e-08, 2.9683e-06, 7.2443e-06, 4.4543e-05,
        1.8353e-03, 8.3084e-04, 1.6927e-03, 9.1821e-02, 1.5055e-02, 3.5798e-01,
        2.9992e-01, 2.3068e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,938][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([2.6133e-03, 6.1484e-01, 3.8433e-04, 1.1190e-01, 3.4596e-03, 1.2564e-02,
        2.0975e-03, 4.7253e-03, 1.3959e-02, 3.7294e-03, 2.2350e-03, 1.9572e-01,
        1.5174e-02, 1.6597e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,939][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0026, 0.0599, 0.0258, 0.1193, 0.0534, 0.0619, 0.0243, 0.0327, 0.0378,
        0.0341, 0.0372, 0.1167, 0.1779, 0.2162], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:41,941][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0006, 0.0177, 0.0111, 0.0143, 0.0074, 0.0540, 0.0370, 0.0984, 0.0845,
        0.0642, 0.1390, 0.2144, 0.0482, 0.1434, 0.0655], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,943][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([1.7388e-05, 1.2320e-01, 1.0256e-02, 1.0581e-01, 6.6040e-03, 5.3168e-02,
        1.5400e-02, 1.9418e-02, 3.8723e-02, 3.5569e-02, 3.6515e-02, 2.8590e-01,
        5.8714e-02, 1.5664e-01, 5.4076e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,944][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([5.4861e-05, 2.6533e-02, 4.0890e-02, 8.6467e-02, 1.8204e-02, 9.4198e-02,
        1.3766e-02, 6.8318e-02, 1.0169e-01, 1.3329e-02, 6.0273e-02, 1.0134e-01,
        6.3839e-02, 1.8601e-01, 1.2510e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,946][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0005, 0.0847, 0.0225, 0.0589, 0.0247, 0.0782, 0.0395, 0.0303, 0.0545,
        0.0341, 0.0472, 0.2363, 0.0702, 0.1140, 0.1044], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,947][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0034, 0.0040, 0.0190, 0.0080, 0.0198, 0.0734, 0.0239, 0.0961, 0.0793,
        0.0457, 0.1366, 0.0616, 0.1459, 0.2474, 0.0359], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,948][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([7.3368e-05, 4.0802e-05, 2.1198e-07, 4.8234e-06, 2.5344e-05, 2.9049e-04,
        3.1832e-03, 1.3853e-03, 3.9334e-03, 7.7681e-02, 8.9572e-03, 1.0971e-01,
        2.0710e-01, 1.5675e-01, 4.3087e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,948][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([7.7311e-05, 3.0605e-02, 4.2808e-02, 1.1535e-01, 1.1631e-02, 7.5587e-02,
        3.6964e-02, 3.3166e-02, 5.8187e-02, 4.0873e-02, 1.1625e-01, 1.8704e-01,
        2.6343e-02, 1.8875e-01, 3.6379e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,949][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0028, 0.0893, 0.0287, 0.1642, 0.0196, 0.0524, 0.0410, 0.0296, 0.0548,
        0.0424, 0.0575, 0.1674, 0.0794, 0.0980, 0.0728], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,950][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([8.9260e-05, 1.6903e-01, 3.0606e-02, 5.0143e-02, 2.5456e-02, 1.0090e-01,
        2.3035e-02, 2.9089e-02, 1.2102e-01, 3.7228e-02, 5.7151e-02, 1.1040e-01,
        9.2261e-02, 8.3135e-02, 7.0455e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,952][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([1.8753e-05, 1.9809e-05, 2.8058e-08, 4.7696e-07, 4.1697e-06, 2.9359e-05,
        9.8136e-04, 5.1601e-04, 9.7604e-04, 5.8900e-02, 7.0975e-03, 1.0662e-01,
        1.6415e-01, 1.3093e-01, 5.2977e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,954][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([6.1677e-04, 5.5214e-01, 2.6651e-04, 9.4704e-02, 3.4034e-03, 1.5434e-02,
        2.3711e-03, 6.7854e-03, 1.8896e-02, 4.2133e-03, 2.1614e-03, 2.2730e-01,
        1.5733e-02, 1.7755e-02, 3.8221e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,955][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([1.0408e-04, 4.3831e-02, 7.3933e-03, 2.6105e-02, 2.2584e-02, 8.2007e-02,
        9.8990e-03, 4.0765e-02, 5.7709e-02, 1.6308e-02, 2.7926e-02, 5.3179e-02,
        8.0511e-02, 2.7871e-01, 2.5297e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:41,957][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0025, 0.0091, 0.0073, 0.0091, 0.0038, 0.0298, 0.0237, 0.0906, 0.0690,
        0.0586, 0.1459, 0.2136, 0.0497, 0.1403, 0.0566, 0.0904],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,958][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([3.1577e-05, 5.5050e-02, 1.4751e-02, 1.0101e-01, 7.3756e-03, 4.9574e-02,
        1.9566e-02, 1.8846e-02, 3.4072e-02, 4.0787e-02, 3.9910e-02, 2.0009e-01,
        4.9716e-02, 1.4631e-01, 4.6246e-02, 1.7667e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,960][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([7.2912e-05, 1.1989e-02, 3.8302e-02, 4.8227e-02, 1.3269e-02, 7.5231e-02,
        1.3790e-02, 5.2164e-02, 8.3245e-02, 1.5422e-02, 5.7721e-02, 8.5323e-02,
        6.7733e-02, 1.4991e-01, 1.0939e-01, 1.7821e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,962][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0007, 0.0619, 0.0221, 0.0540, 0.0226, 0.0511, 0.0269, 0.0235, 0.0495,
        0.0255, 0.0501, 0.1671, 0.0882, 0.1189, 0.0878, 0.1502],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,963][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0047, 0.0021, 0.0089, 0.0060, 0.0146, 0.0690, 0.0216, 0.0648, 0.0811,
        0.0516, 0.1500, 0.0524, 0.1284, 0.2179, 0.0334, 0.0934],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,964][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([4.8710e-05, 3.5945e-05, 2.3805e-07, 4.1601e-06, 2.2198e-05, 2.7953e-04,
        2.5458e-03, 1.3428e-03, 4.3210e-03, 5.1169e-02, 7.7756e-03, 7.9620e-02,
        1.7996e-01, 1.3379e-01, 3.5457e-01, 1.8452e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,965][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([1.5376e-04, 1.5405e-02, 2.9522e-02, 9.0330e-02, 7.7778e-03, 5.9030e-02,
        3.1209e-02, 2.7205e-02, 4.8173e-02, 3.9793e-02, 1.1925e-01, 1.9036e-01,
        2.5334e-02, 1.4988e-01, 2.8101e-02, 1.3847e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,966][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0041, 0.0386, 0.0245, 0.1228, 0.0205, 0.0417, 0.0396, 0.0307, 0.0489,
        0.0480, 0.0776, 0.1488, 0.0863, 0.0930, 0.0594, 0.1155],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,967][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([8.9662e-05, 1.0163e-01, 2.6933e-02, 4.4932e-02, 2.5236e-02, 8.2288e-02,
        2.2691e-02, 2.6168e-02, 1.2410e-01, 3.9266e-02, 6.9196e-02, 8.0353e-02,
        1.2188e-01, 8.3253e-02, 6.0285e-02, 9.1708e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,968][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([3.9960e-05, 1.4856e-05, 2.5919e-08, 5.0965e-07, 2.7158e-06, 2.0934e-05,
        6.9758e-04, 3.8202e-04, 8.1144e-04, 3.2311e-02, 6.0232e-03, 8.1132e-02,
        1.3259e-01, 1.1797e-01, 4.3816e-01, 1.8984e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,970][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0016, 0.3498, 0.0005, 0.0747, 0.0041, 0.0190, 0.0031, 0.0095, 0.0261,
        0.0068, 0.0046, 0.2503, 0.0259, 0.0286, 0.0509, 0.1444],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,972][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0010, 0.0404, 0.0081, 0.0547, 0.0405, 0.0577, 0.0130, 0.0292, 0.0402,
        0.0196, 0.0241, 0.0825, 0.0941, 0.1807, 0.2000, 0.1142],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:41,973][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0018, 0.0109, 0.0136, 0.0144, 0.0052, 0.0423, 0.0326, 0.0896, 0.0610,
        0.0599, 0.1294, 0.1807, 0.0440, 0.1038, 0.0478, 0.0719, 0.0912],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,975][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([5.1411e-05, 5.6393e-02, 1.5030e-02, 1.3148e-01, 7.8431e-03, 4.8393e-02,
        1.6413e-02, 1.4444e-02, 2.7480e-02, 3.6499e-02, 3.2360e-02, 1.9174e-01,
        5.4663e-02, 1.0403e-01, 4.0847e-02, 1.3784e-01, 8.4484e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,977][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0002, 0.0171, 0.0435, 0.0717, 0.0150, 0.0660, 0.0141, 0.0389, 0.0629,
        0.0138, 0.0395, 0.0822, 0.0648, 0.1209, 0.0943, 0.1333, 0.1217],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,979][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0010, 0.0701, 0.0311, 0.0688, 0.0234, 0.0468, 0.0256, 0.0166, 0.0342,
        0.0199, 0.0346, 0.1967, 0.0860, 0.0829, 0.0844, 0.1107, 0.0674],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,980][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0038, 0.0026, 0.0137, 0.0075, 0.0166, 0.0575, 0.0270, 0.0635, 0.0544,
        0.0462, 0.0941, 0.0409, 0.1137, 0.1571, 0.0298, 0.0799, 0.1916],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,981][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.0771e-04, 2.5346e-05, 1.4722e-07, 3.1753e-06, 1.6522e-05, 1.9959e-04,
        2.0400e-03, 1.0642e-03, 2.7448e-03, 4.1422e-02, 6.9583e-03, 8.8243e-02,
        1.4513e-01, 9.7324e-02, 2.4611e-01, 1.7420e-01, 1.9441e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,982][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0002, 0.0150, 0.0496, 0.1244, 0.0075, 0.0509, 0.0318, 0.0198, 0.0367,
        0.0352, 0.0932, 0.1683, 0.0231, 0.1091, 0.0224, 0.0931, 0.1198],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,983][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0050, 0.0331, 0.0213, 0.1355, 0.0193, 0.0414, 0.0334, 0.0257, 0.0397,
        0.0451, 0.0653, 0.1286, 0.0820, 0.0896, 0.0557, 0.0984, 0.0810],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,984][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0003, 0.1249, 0.0238, 0.0568, 0.0219, 0.0730, 0.0341, 0.0203, 0.0966,
        0.0393, 0.0534, 0.0777, 0.0846, 0.0775, 0.0627, 0.0779, 0.0754],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,985][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([7.2649e-05, 1.3693e-05, 2.0113e-08, 7.4229e-07, 2.4801e-06, 1.5835e-05,
        6.8571e-04, 3.0264e-04, 5.6414e-04, 3.7703e-02, 5.0843e-03, 1.2092e-01,
        1.0829e-01, 7.7732e-02, 3.0747e-01, 1.5746e-01, 1.8369e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,987][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.9242e-03, 4.7713e-01, 4.1092e-04, 1.0421e-01, 3.4661e-03, 1.3978e-02,
        2.2257e-03, 5.3735e-03, 1.5374e-02, 4.1365e-03, 2.6135e-03, 1.8373e-01,
        1.5458e-02, 1.7415e-02, 3.4263e-02, 1.0191e-01, 1.6380e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,989][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0026, 0.0433, 0.0142, 0.0784, 0.0326, 0.0435, 0.0171, 0.0209, 0.0250,
        0.0260, 0.0231, 0.0883, 0.1092, 0.1370, 0.1545, 0.0841, 0.1003],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:41,992][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:41,994][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[7515],
        [2027],
        [ 755],
        [  10],
        [ 333],
        [ 232],
        [ 406],
        [  25],
        [  26],
        [ 659],
        [  52],
        [  35],
        [ 256],
        [  11],
        [  73],
        [ 128],
        [   4]], device='cuda:0')
[2024-07-24 10:17:41,996][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[5610],
        [1700],
        [ 911],
        [  10],
        [ 222],
        [ 211],
        [ 421],
        [  35],
        [  38],
        [ 666],
        [  74],
        [  35],
        [ 263],
        [  17],
        [  98],
        [ 171],
        [   4]], device='cuda:0')
[2024-07-24 10:17:41,998][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 5855],
        [ 9291],
        [34981],
        [47872],
        [49302],
        [48533],
        [45837],
        [42053],
        [40541],
        [42459],
        [41706],
        [37809],
        [38097],
        [35444],
        [36332],
        [35434],
        [36512]], device='cuda:0')
[2024-07-24 10:17:42,000][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[11436],
        [38013],
        [33246],
        [47345],
        [47686],
        [45013],
        [36244],
        [41436],
        [40948],
        [33329],
        [39381],
        [27783],
        [29455],
        [30897],
        [26367],
        [21017],
        [23905]], device='cuda:0')
[2024-07-24 10:17:42,001][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 1347],
        [47277],
        [ 3789],
        [47305],
        [45449],
        [22725],
        [ 8575],
        [16537],
        [17955],
        [11694],
        [16166],
        [17140],
        [20363],
        [24692],
        [25964],
        [19376],
        [23762]], device='cuda:0')
[2024-07-24 10:17:42,002][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 2493],
        [24168],
        [32394],
        [36331],
        [32368],
        [32640],
        [25688],
        [25146],
        [25313],
        [22634],
        [23268],
        [13518],
        [12745],
        [15252],
        [13894],
        [14835],
        [15330]], device='cuda:0')
[2024-07-24 10:17:42,004][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[34849],
        [38608],
        [28070],
        [33102],
        [32751],
        [38662],
        [39692],
        [40405],
        [41006],
        [41142],
        [42225],
        [43694],
        [44323],
        [44723],
        [44837],
        [44957],
        [45023]], device='cuda:0')
[2024-07-24 10:17:42,006][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[38838],
        [47187],
        [47245],
        [46459],
        [46376],
        [44105],
        [45194],
        [45306],
        [44938],
        [46765],
        [46656],
        [48046],
        [47110],
        [47133],
        [47651],
        [47616],
        [47493]], device='cuda:0')
[2024-07-24 10:17:42,008][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[31833],
        [35933],
        [44438],
        [49295],
        [49286],
        [48405],
        [43263],
        [46724],
        [47261],
        [44890],
        [46913],
        [41284],
        [43156],
        [43994],
        [42905],
        [41891],
        [43634]], device='cuda:0')
[2024-07-24 10:17:42,010][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[26689],
        [ 7344],
        [ 7280],
        [    7],
        [   19],
        [   16],
        [  204],
        [   40],
        [    8],
        [   24],
        [   29],
        [  291],
        [  286],
        [  274],
        [  303],
        [  560],
        [  405]], device='cuda:0')
[2024-07-24 10:17:42,012][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[20726],
        [10896],
        [ 9578],
        [10389],
        [ 9166],
        [10387],
        [11478],
        [11979],
        [11354],
        [10714],
        [10745],
        [11896],
        [10117],
        [10891],
        [11445],
        [11341],
        [12108]], device='cuda:0')
[2024-07-24 10:17:42,014][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[31607],
        [30720],
        [33450],
        [41659],
        [42657],
        [47453],
        [43178],
        [44891],
        [46601],
        [48898],
        [48869],
        [46398],
        [47057],
        [46980],
        [48691],
        [49273],
        [49136]], device='cuda:0')
[2024-07-24 10:17:42,016][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[23044],
        [34079],
        [34148],
        [45765],
        [45134],
        [45342],
        [44855],
        [46097],
        [45240],
        [44955],
        [45235],
        [46527],
        [44518],
        [47092],
        [46653],
        [47276],
        [47863]], device='cuda:0')
[2024-07-24 10:17:42,018][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[11358],
        [42361],
        [40974],
        [45115],
        [46249],
        [48107],
        [48306],
        [47709],
        [47639],
        [47520],
        [47187],
        [47181],
        [47069],
        [45600],
        [43946],
        [45098],
        [44969]], device='cuda:0')
[2024-07-24 10:17:42,019][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[28990],
        [15337],
        [ 6045],
        [ 8224],
        [ 8713],
        [ 8952],
        [ 7457],
        [ 6996],
        [ 6640],
        [ 6416],
        [ 5124],
        [ 7700],
        [ 9426],
        [ 5343],
        [ 6945],
        [ 4779],
        [ 5618]], device='cuda:0')
[2024-07-24 10:17:42,021][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[10755],
        [17155],
        [24128],
        [23742],
        [22329],
        [23140],
        [20084],
        [21818],
        [23259],
        [23532],
        [26391],
        [25299],
        [23840],
        [24843],
        [25770],
        [27209],
        [26565]], device='cuda:0')
[2024-07-24 10:17:42,023][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[46012],
        [10005],
        [ 9682],
        [22588],
        [25040],
        [22347],
        [20261],
        [22494],
        [23563],
        [21454],
        [22818],
        [19698],
        [20304],
        [22404],
        [21475],
        [22684],
        [22956]], device='cuda:0')
[2024-07-24 10:17:42,025][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[41309],
        [ 1389],
        [32915],
        [ 8971],
        [ 5724],
        [10932],
        [14063],
        [10472],
        [ 8166],
        [ 9337],
        [ 8528],
        [ 6986],
        [ 6063],
        [ 6179],
        [ 5431],
        [ 5970],
        [ 5639]], device='cuda:0')
[2024-07-24 10:17:42,026][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[16776],
        [16844],
        [ 5327],
        [ 9647],
        [ 9515],
        [ 9236],
        [ 9253],
        [ 9743],
        [11215],
        [10312],
        [10343],
        [15750],
        [12847],
        [11848],
        [13885],
        [12232],
        [12736]], device='cuda:0')
[2024-07-24 10:17:42,028][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[13099],
        [18011],
        [14553],
        [16835],
        [18786],
        [21292],
        [23626],
        [18067],
        [18481],
        [19166],
        [20316],
        [21828],
        [24387],
        [23879],
        [23057],
        [22720],
        [21819]], device='cuda:0')
[2024-07-24 10:17:42,031][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[21629],
        [15366],
        [14441],
        [13861],
        [13303],
        [ 7207],
        [10182],
        [ 7145],
        [ 6639],
        [ 7859],
        [ 7205],
        [ 6717],
        [ 5484],
        [ 5161],
        [ 5465],
        [ 6686],
        [ 6594]], device='cuda:0')
[2024-07-24 10:17:42,033][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[12969],
        [ 8496],
        [ 3519],
        [ 3987],
        [ 4083],
        [ 3980],
        [ 3487],
        [ 3368],
        [ 3368],
        [ 3046],
        [ 2854],
        [ 2979],
        [ 3013],
        [ 2939],
        [ 2891],
        [ 2892],
        [ 3008]], device='cuda:0')
[2024-07-24 10:17:42,035][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[17729],
        [34506],
        [38445],
        [20898],
        [20927],
        [16876],
        [22909],
        [18698],
        [15576],
        [18822],
        [20839],
        [32700],
        [31317],
        [31377],
        [32602],
        [33828],
        [32332]], device='cuda:0')
[2024-07-24 10:17:42,036][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[14495],
        [16797],
        [17047],
        [16248],
        [16655],
        [16743],
        [16874],
        [17023],
        [17821],
        [18571],
        [19228],
        [18849],
        [21344],
        [21128],
        [21779],
        [24112],
        [23356]], device='cuda:0')
[2024-07-24 10:17:42,037][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[21581],
        [18278],
        [15162],
        [15793],
        [11419],
        [ 9303],
        [24041],
        [18728],
        [15642],
        [21438],
        [17985],
        [15847],
        [ 9470],
        [ 7266],
        [ 4751],
        [ 4007],
        [ 4189]], device='cuda:0')
[2024-07-24 10:17:42,039][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[36574],
        [20838],
        [20532],
        [18910],
        [18969],
        [18558],
        [17740],
        [17814],
        [18040],
        [17708],
        [18099],
        [15142],
        [17358],
        [16176],
        [15663],
        [12746],
        [13869]], device='cuda:0')
[2024-07-24 10:17:42,041][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[16686],
        [ 6369],
        [ 8895],
        [ 5004],
        [ 4855],
        [ 6521],
        [ 7880],
        [ 8408],
        [ 8091],
        [10568],
        [10612],
        [12823],
        [14154],
        [14286],
        [15898],
        [16779],
        [16721]], device='cuda:0')
[2024-07-24 10:17:42,043][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[13052],
        [40811],
        [34304],
        [40621],
        [41940],
        [41563],
        [37266],
        [40669],
        [41570],
        [37912],
        [37550],
        [34851],
        [36836],
        [37521],
        [38067],
        [37257],
        [37661]], device='cuda:0')
[2024-07-24 10:17:42,045][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[12498],
        [33135],
        [38219],
        [41508],
        [42978],
        [41467],
        [35801],
        [40008],
        [39688],
        [40141],
        [38775],
        [37234],
        [37332],
        [39827],
        [36196],
        [39500],
        [37964]], device='cuda:0')
[2024-07-24 10:17:42,046][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828],
        [26828]], device='cuda:0')
[2024-07-24 10:17:42,174][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:42,175][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,176][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,176][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,177][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,178][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,179][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,180][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,182][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,184][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,185][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,186][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,187][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,189][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.8948, 0.1052], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,191][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.2367, 0.7633], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,192][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0715, 0.9285], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,193][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.8234, 0.1766], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,194][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0132, 0.9868], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,194][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,195][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.1637, 0.8363], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,196][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.3224, 0.6776], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,196][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([1.0000e+00, 2.3736e-07], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,198][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0651, 0.9349], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,200][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.7848, 0.2152], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,201][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([2.2320e-04, 9.9978e-01], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,203][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.6291, 0.1719, 0.1990], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,205][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.1299, 0.5303, 0.3399], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,206][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0196, 0.4532, 0.5272], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,208][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.4724, 0.4627, 0.0649], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,209][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0045, 0.7532, 0.2423], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,210][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0051, 0.4659, 0.5290], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,211][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0595, 0.6786, 0.2618], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,211][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3245, 0.3963, 0.2791], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,212][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ and] are: tensor([1.0000e+00, 1.0887e-10, 7.2481e-16], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,213][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0088, 0.8511, 0.1401], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,214][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0643, 0.1804, 0.7553], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,215][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ and] are: tensor([4.8849e-05, 2.1172e-01, 7.8823e-01], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,217][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Megan] are: tensor([0.5897, 0.1169, 0.1367, 0.1567], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,219][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Megan] are: tensor([0.1564, 0.2873, 0.1830, 0.3732], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,220][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Megan] are: tensor([0.0153, 0.2570, 0.2601, 0.4675], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,222][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Megan] are: tensor([0.1987, 0.3382, 0.1807, 0.2824], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,224][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Megan] are: tensor([0.0042, 0.3653, 0.1968, 0.4337], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,226][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Megan] are: tensor([0.0029, 0.2848, 0.3812, 0.3311], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,227][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Megan] are: tensor([0.0561, 0.3863, 0.1562, 0.4014], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,227][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Megan] are: tensor([0.2629, 0.2264, 0.1047, 0.4060], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,228][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Megan] are: tensor([9.9998e-01, 1.1665e-06, 3.7095e-10, 1.8085e-05], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,229][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Megan] are: tensor([0.0087, 0.4840, 0.1212, 0.3862], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,229][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Megan] are: tensor([0.1588, 0.0720, 0.1631, 0.6061], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,230][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Megan] are: tensor([1.2783e-04, 1.3356e-01, 4.0786e-01, 4.5845e-01], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,232][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.4156, 0.1057, 0.1229, 0.1216, 0.2342], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,234][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0639, 0.2702, 0.1471, 0.3340, 0.1849], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,235][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0121, 0.1608, 0.1366, 0.4835, 0.2071], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,237][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.3348, 0.2543, 0.0753, 0.1794, 0.1562], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,239][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0016, 0.3281, 0.1399, 0.3903, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,241][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0032, 0.2866, 0.2608, 0.2831, 0.1664], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,242][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0186, 0.3733, 0.1041, 0.3750, 0.1288], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,244][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0668, 0.2535, 0.0927, 0.4468, 0.1403], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,244][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ got] are: tensor([9.9961e-01, 6.4150e-06, 1.4968e-09, 1.6835e-05, 3.6813e-04],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,245][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0096, 0.4315, 0.1037, 0.2978, 0.1574], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,246][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0482, 0.0650, 0.1424, 0.4311, 0.3133], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,246][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ got] are: tensor([2.0988e-04, 1.2445e-01, 3.1631e-01, 4.3883e-01, 1.2020e-01],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,247][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.4036, 0.0725, 0.0897, 0.0899, 0.1603, 0.1840], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,249][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0556, 0.1823, 0.1303, 0.3042, 0.1524, 0.1752], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,251][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0128, 0.0946, 0.1135, 0.4570, 0.1434, 0.1787], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,253][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0518, 0.3025, 0.0543, 0.4189, 0.1677, 0.0048], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,254][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0019, 0.2315, 0.0965, 0.2915, 0.0999, 0.2788], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,256][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0148, 0.2427, 0.2070, 0.2467, 0.1368, 0.1519], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,258][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0670, 0.2590, 0.0830, 0.3600, 0.1081, 0.1229], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,260][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0311, 0.1995, 0.0974, 0.4111, 0.1324, 0.1285], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,261][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([9.9928e-01, 3.4940e-07, 9.4665e-11, 7.3114e-07, 7.0527e-04, 8.7843e-06],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,262][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0388, 0.3231, 0.0949, 0.3028, 0.1582, 0.0822], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,262][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1133, 0.0301, 0.0967, 0.3134, 0.2520, 0.1945], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,263][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([5.5017e-06, 1.0312e-01, 2.0896e-01, 5.5131e-01, 7.1743e-02, 6.4865e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,264][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ necklace] are: tensor([0.4711, 0.0395, 0.0566, 0.0573, 0.1434, 0.1219, 0.1103],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,264][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ necklace] are: tensor([0.0116, 0.1925, 0.0793, 0.2355, 0.1190, 0.2304, 0.1318],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,266][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ necklace] are: tensor([0.0040, 0.0962, 0.0683, 0.2476, 0.1211, 0.1917, 0.2712],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,268][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ necklace] are: tensor([0.3538, 0.1482, 0.0456, 0.1769, 0.1650, 0.0071, 0.1034],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,270][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ necklace] are: tensor([0.0009, 0.2347, 0.0438, 0.1996, 0.0805, 0.2831, 0.1573],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,272][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ necklace] are: tensor([0.0014, 0.2243, 0.1577, 0.1868, 0.0814, 0.1615, 0.1870],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,274][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ necklace] are: tensor([0.0075, 0.2055, 0.0382, 0.3148, 0.1142, 0.2436, 0.0762],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,275][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ necklace] are: tensor([0.0725, 0.1548, 0.0767, 0.3208, 0.1327, 0.1235, 0.1189],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,276][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ necklace] are: tensor([9.9986e-01, 2.1028e-07, 2.8799e-12, 6.5919e-07, 1.3755e-04, 1.6408e-06,
        1.5457e-08], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,278][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ necklace] are: tensor([0.0025, 0.2582, 0.0604, 0.3111, 0.1117, 0.1138, 0.1423],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,279][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ necklace] are: tensor([0.0135, 0.0447, 0.1184, 0.2812, 0.2266, 0.2469, 0.0687],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,279][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ necklace] are: tensor([6.7001e-05, 1.6387e-01, 2.9477e-01, 2.8113e-01, 5.9711e-02, 9.0940e-02,
        1.0952e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,280][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.3775, 0.0591, 0.0664, 0.0699, 0.1263, 0.1222, 0.0947, 0.0839],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,281][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0169, 0.1662, 0.0760, 0.1851, 0.0972, 0.1371, 0.1199, 0.2015],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,282][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0112, 0.0892, 0.0686, 0.2478, 0.1004, 0.1369, 0.1887, 0.1573],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,284][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0217, 0.1853, 0.0641, 0.2831, 0.2827, 0.0064, 0.1455, 0.0112],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,286][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0013, 0.1722, 0.0556, 0.2032, 0.0703, 0.1911, 0.1209, 0.1855],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,287][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0063, 0.2134, 0.1367, 0.1813, 0.0809, 0.0996, 0.1681, 0.1137],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,289][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0468, 0.2278, 0.0403, 0.3171, 0.0857, 0.1077, 0.0649, 0.1097],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,291][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0253, 0.1512, 0.0885, 0.3557, 0.1281, 0.0889, 0.0906, 0.0718],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,292][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ at] are: tensor([1.0000e+00, 4.6791e-10, 3.6267e-14, 8.6146e-09, 1.9624e-06, 1.3395e-08,
        2.7610e-10, 3.1689e-13], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,294][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0144, 0.2649, 0.0681, 0.3033, 0.1093, 0.0513, 0.1270, 0.0616],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,295][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0699, 0.0238, 0.0644, 0.3070, 0.1592, 0.1017, 0.0335, 0.2404],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,296][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ at] are: tensor([8.4332e-07, 6.0720e-02, 1.6867e-01, 3.4599e-01, 3.9531e-02, 3.5785e-02,
        6.4295e-02, 2.8501e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,297][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.3917, 0.0498, 0.0575, 0.0561, 0.0935, 0.1052, 0.0837, 0.0730, 0.0896],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,298][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0213, 0.1111, 0.0621, 0.1842, 0.0857, 0.0891, 0.1642, 0.1665, 0.1159],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,298][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0073, 0.0508, 0.0517, 0.1912, 0.0992, 0.1106, 0.1847, 0.1700, 0.1347],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,300][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0116, 0.2289, 0.0803, 0.2231, 0.2510, 0.0064, 0.1814, 0.0111, 0.0062],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,302][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0017, 0.1349, 0.0536, 0.1722, 0.0546, 0.1289, 0.1081, 0.1315, 0.2145],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,304][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0203, 0.1752, 0.1162, 0.1696, 0.0823, 0.0794, 0.1564, 0.0992, 0.1014],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,305][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1444, 0.1912, 0.0344, 0.2600, 0.0640, 0.0640, 0.0481, 0.0643, 0.1295],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,307][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0215, 0.1314, 0.0669, 0.3712, 0.1013, 0.0854, 0.0948, 0.0636, 0.0639],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,308][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([9.9996e-01, 1.7194e-10, 4.6475e-15, 2.1937e-09, 3.9373e-05, 1.8498e-07,
        3.5492e-12, 4.7558e-14, 2.6651e-14], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,310][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0602, 0.2926, 0.0716, 0.2480, 0.0968, 0.0307, 0.1321, 0.0397, 0.0283],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,312][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3158, 0.0109, 0.0353, 0.1735, 0.0960, 0.0565, 0.0156, 0.1447, 0.1517],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,313][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.2839e-07, 5.1802e-02, 9.1176e-02, 2.6376e-01, 1.8092e-02, 1.8884e-02,
        4.7455e-02, 2.8498e-01, 2.2385e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,313][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.4862, 0.0293, 0.0378, 0.0350, 0.0899, 0.0777, 0.0729, 0.0521, 0.0630,
        0.0561], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,314][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0078, 0.1176, 0.0476, 0.1399, 0.0742, 0.1408, 0.1129, 0.1522, 0.1426,
        0.0644], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,315][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0047, 0.0676, 0.0686, 0.1545, 0.0991, 0.1748, 0.1361, 0.1179, 0.1083,
        0.0684], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,316][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0524, 0.0968, 0.0404, 0.0680, 0.1356, 0.0060, 0.0478, 0.0072, 0.0039,
        0.5420], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,317][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0010, 0.1643, 0.0325, 0.1314, 0.0429, 0.1211, 0.0907, 0.1225, 0.2097,
        0.0838], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,319][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0021, 0.1692, 0.0778, 0.1238, 0.0632, 0.0898, 0.1285, 0.0875, 0.1259,
        0.1322], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,321][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0140, 0.1964, 0.0225, 0.2321, 0.0525, 0.1001, 0.0438, 0.0861, 0.2038,
        0.0487], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,322][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0325, 0.1472, 0.0552, 0.2719, 0.0959, 0.0945, 0.0934, 0.0529, 0.0726,
        0.0839], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,323][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ station] are: tensor([1.0000e+00, 4.5941e-10, 1.4870e-15, 1.3806e-08, 2.3434e-06, 1.1791e-07,
        4.2090e-11, 5.8108e-14, 7.1630e-14, 2.6749e-14], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,326][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0042, 0.2899, 0.0373, 0.2349, 0.0823, 0.0585, 0.1232, 0.0549, 0.0492,
        0.0656], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,327][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0131, 0.0217, 0.0627, 0.1653, 0.1008, 0.0960, 0.0246, 0.1812, 0.2498,
        0.0848], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,329][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ station] are: tensor([7.7647e-07, 4.7366e-02, 8.6366e-02, 1.2866e-01, 1.3155e-02, 2.9470e-02,
        2.9817e-02, 2.5896e-01, 3.0521e-01, 1.0100e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,330][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.3329, 0.0406, 0.0458, 0.0412, 0.0930, 0.0887, 0.0818, 0.0672, 0.0737,
        0.0692, 0.0659], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,331][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0110, 0.1170, 0.0452, 0.1381, 0.0701, 0.0774, 0.1056, 0.1202, 0.1037,
        0.0989, 0.1128], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,332][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0023, 0.0488, 0.0424, 0.1516, 0.0698, 0.0930, 0.1471, 0.1090, 0.0955,
        0.1602, 0.0801], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,332][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0198, 0.1138, 0.0173, 0.0573, 0.0761, 0.0023, 0.0984, 0.0046, 0.0021,
        0.6051, 0.0031], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,333][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0008, 0.1251, 0.0335, 0.1169, 0.0421, 0.1140, 0.0789, 0.1177, 0.1910,
        0.0742, 0.1059], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,334][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0038, 0.1041, 0.0680, 0.1155, 0.0582, 0.0700, 0.1014, 0.0856, 0.1036,
        0.1229, 0.1667], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,336][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0246, 0.1404, 0.0231, 0.2202, 0.0530, 0.0852, 0.0342, 0.0794, 0.1717,
        0.0425, 0.1258], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,338][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0201, 0.1066, 0.0552, 0.2887, 0.0898, 0.0732, 0.0842, 0.0498, 0.0598,
        0.0859, 0.0867], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,339][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [,] are: tensor([1.0000e+00, 9.2066e-12, 1.9149e-16, 2.0144e-10, 8.2786e-08, 3.0027e-10,
        1.7179e-12, 6.3569e-16, 7.1847e-17, 1.2128e-17, 1.5172e-20],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,341][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0090, 0.2258, 0.0388, 0.2331, 0.0853, 0.0405, 0.1044, 0.0480, 0.0380,
        0.0677, 0.1094], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,343][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0389, 0.0138, 0.0432, 0.1675, 0.0975, 0.0719, 0.0152, 0.1603, 0.1960,
        0.0641, 0.1315], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,345][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [,] are: tensor([4.1893e-07, 3.6986e-02, 6.3269e-02, 1.1740e-01, 1.6921e-02, 2.2583e-02,
        4.4308e-02, 1.8635e-01, 2.2177e-01, 1.2344e-01, 1.6697e-01],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,346][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Sean] are: tensor([0.5528, 0.0188, 0.0296, 0.0247, 0.0707, 0.0574, 0.0552, 0.0361, 0.0382,
        0.0392, 0.0368, 0.0406], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,347][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Sean] are: tensor([0.0044, 0.0740, 0.0363, 0.0895, 0.0555, 0.1020, 0.0863, 0.1155, 0.1239,
        0.0910, 0.1042, 0.1173], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,348][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Sean] are: tensor([0.0014, 0.0418, 0.0606, 0.1454, 0.0626, 0.0974, 0.1403, 0.0918, 0.0758,
        0.1228, 0.1033, 0.0569], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,349][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Sean] are: tensor([0.3456, 0.0840, 0.0314, 0.0384, 0.0701, 0.0032, 0.0186, 0.0051, 0.0032,
        0.2755, 0.0025, 0.1223], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,350][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Sean] are: tensor([0.0009, 0.0974, 0.0263, 0.1084, 0.0359, 0.1076, 0.0609, 0.0997, 0.1774,
        0.0631, 0.0770, 0.1455], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,351][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Sean] are: tensor([0.0014, 0.0806, 0.0464, 0.0804, 0.0403, 0.0633, 0.0778, 0.0628, 0.0895,
        0.0879, 0.1387, 0.2307], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,352][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Sean] are: tensor([0.0204, 0.1241, 0.0166, 0.1647, 0.0389, 0.0634, 0.0329, 0.0600, 0.1233,
        0.0352, 0.0877, 0.2328], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,354][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Sean] are: tensor([0.0324, 0.0951, 0.0447, 0.2082, 0.0734, 0.0735, 0.0769, 0.0390, 0.0633,
        0.0768, 0.0723, 0.1445], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,355][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Sean] are: tensor([1.0000e+00, 1.1370e-08, 4.1016e-14, 8.4730e-08, 2.0734e-06, 2.5404e-07,
        6.9804e-10, 9.7098e-13, 3.9846e-12, 2.3806e-12, 1.8444e-17, 1.6025e-11],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,357][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Sean] are: tensor([0.0068, 0.1711, 0.0211, 0.1611, 0.0548, 0.0367, 0.0922, 0.0383, 0.0338,
        0.0515, 0.0752, 0.2575], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,358][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Sean] are: tensor([0.0330, 0.0160, 0.0411, 0.1312, 0.0793, 0.0671, 0.0153, 0.1416, 0.1649,
        0.0573, 0.1120, 0.1412], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,360][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Sean] are: tensor([9.3698e-07, 3.0468e-02, 7.5024e-02, 9.9125e-02, 1.1480e-02, 2.4846e-02,
        3.4710e-02, 1.5843e-01, 2.3249e-01, 1.0473e-01, 1.8602e-01, 4.2679e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,362][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.3768, 0.0221, 0.0351, 0.0309, 0.0680, 0.0562, 0.0579, 0.0440, 0.0478,
        0.0469, 0.0423, 0.0491, 0.1232], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,363][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0069, 0.0585, 0.0334, 0.1094, 0.0488, 0.0955, 0.0873, 0.1172, 0.1106,
        0.0903, 0.0840, 0.1046, 0.0533], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,365][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0015, 0.0377, 0.0465, 0.1045, 0.0713, 0.1343, 0.0806, 0.1168, 0.1071,
        0.0957, 0.0764, 0.0605, 0.0671], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,365][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.5679, 0.0610, 0.0150, 0.0296, 0.0411, 0.0035, 0.0137, 0.0041, 0.0033,
        0.1390, 0.0033, 0.0806, 0.0378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,366][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0003, 0.0977, 0.0267, 0.1180, 0.0343, 0.0880, 0.0551, 0.0878, 0.1628,
        0.0544, 0.0732, 0.1410, 0.0608], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,367][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0006, 0.0727, 0.0454, 0.0742, 0.0383, 0.0576, 0.0617, 0.0491, 0.0771,
        0.0736, 0.1101, 0.1858, 0.1539], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,368][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0035, 0.1015, 0.0176, 0.1678, 0.0318, 0.0587, 0.0257, 0.0549, 0.1203,
        0.0357, 0.0842, 0.2333, 0.0649], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,369][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0405, 0.1034, 0.0467, 0.2436, 0.0708, 0.0612, 0.0731, 0.0337, 0.0525,
        0.0625, 0.0569, 0.1058, 0.0492], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,371][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([9.9997e-01, 2.1871e-07, 9.8709e-12, 6.8759e-07, 2.9119e-05, 5.7182e-07,
        4.7696e-08, 4.5516e-11, 1.9477e-10, 1.3611e-09, 4.9771e-15, 1.1573e-09,
        4.0868e-08], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,372][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0018, 0.1448, 0.0243, 0.1624, 0.0623, 0.0348, 0.0785, 0.0353, 0.0282,
        0.0499, 0.0726, 0.2341, 0.0709], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,374][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0039, 0.0103, 0.0257, 0.0980, 0.0555, 0.0563, 0.0099, 0.1153, 0.1472,
        0.0432, 0.0974, 0.1365, 0.2009], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,376][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([9.9191e-06, 3.1859e-02, 7.2021e-02, 1.2735e-01, 1.4535e-02, 2.2378e-02,
        2.6468e-02, 8.6951e-02, 1.3961e-01, 6.5703e-02, 1.3255e-01, 2.3899e-02,
        2.5666e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,378][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.3879, 0.0254, 0.0334, 0.0293, 0.0621, 0.0638, 0.0496, 0.0455, 0.0491,
        0.0398, 0.0408, 0.0504, 0.0836, 0.0393], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,380][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0020, 0.0644, 0.0227, 0.0645, 0.0408, 0.0560, 0.0605, 0.0958, 0.0823,
        0.0717, 0.0789, 0.1128, 0.0519, 0.1959], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,382][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0005, 0.0300, 0.0192, 0.1122, 0.0477, 0.0806, 0.1005, 0.0790, 0.0848,
        0.1173, 0.0543, 0.0663, 0.0697, 0.1378], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,384][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0098, 0.0938, 0.0267, 0.0825, 0.1407, 0.0027, 0.0487, 0.0046, 0.0030,
        0.3743, 0.0037, 0.1277, 0.0801, 0.0016], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,386][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0004, 0.0748, 0.0253, 0.0895, 0.0335, 0.0816, 0.0605, 0.0695, 0.1188,
        0.0597, 0.0621, 0.1232, 0.0542, 0.1470], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,387][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0027, 0.0875, 0.0481, 0.0711, 0.0314, 0.0428, 0.0700, 0.0388, 0.0463,
        0.0686, 0.0935, 0.1603, 0.1028, 0.1362], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,388][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0115, 0.0657, 0.0143, 0.1325, 0.0298, 0.0434, 0.0207, 0.0445, 0.0992,
        0.0305, 0.0812, 0.1713, 0.0565, 0.1989], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,389][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0123, 0.0896, 0.0407, 0.2131, 0.0609, 0.0571, 0.0670, 0.0369, 0.0438,
        0.0597, 0.0606, 0.1336, 0.0469, 0.0779], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,390][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([1.0000e+00, 8.9268e-13, 3.3279e-18, 4.7939e-11, 4.7065e-08, 1.0798e-10,
        1.6839e-13, 4.6467e-17, 2.5825e-17, 3.5169e-18, 1.5005e-22, 6.6643e-17,
        3.9405e-14, 1.7104e-22], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,390][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0149, 0.2021, 0.0312, 0.1655, 0.0635, 0.0259, 0.0686, 0.0294, 0.0205,
        0.0465, 0.0870, 0.1502, 0.0548, 0.0398], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,392][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0321, 0.0064, 0.0224, 0.1234, 0.0564, 0.0377, 0.0089, 0.0839, 0.0979,
        0.0376, 0.0762, 0.0980, 0.1498, 0.1694], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,394][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([6.8770e-08, 3.0844e-02, 4.3756e-02, 1.0953e-01, 9.5765e-03, 1.0265e-02,
        2.1179e-02, 1.0973e-01, 1.3365e-01, 7.4183e-02, 9.3523e-02, 2.0739e-02,
        2.2097e-01, 1.2205e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,396][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.2465, 0.0186, 0.0300, 0.0253, 0.0671, 0.0655, 0.0460, 0.0472, 0.0574,
        0.0444, 0.0462, 0.0510, 0.1193, 0.0477, 0.0877], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,398][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0019, 0.0664, 0.0250, 0.0771, 0.0476, 0.0741, 0.0582, 0.0867, 0.0999,
        0.0488, 0.0677, 0.0839, 0.0413, 0.1535, 0.0681], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,400][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0006, 0.0302, 0.0165, 0.0929, 0.0425, 0.0856, 0.1036, 0.0679, 0.0850,
        0.0986, 0.0443, 0.0697, 0.0607, 0.0915, 0.1104], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,401][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.2730, 0.0597, 0.0312, 0.0294, 0.0724, 0.0047, 0.0276, 0.0070, 0.0055,
        0.2803, 0.0065, 0.1079, 0.0742, 0.0038, 0.0168], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,403][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ give] are: tensor([1.1872e-04, 7.6408e-02, 1.6826e-02, 8.6300e-02, 2.6303e-02, 7.1253e-02,
        4.2813e-02, 6.4190e-02, 1.3522e-01, 4.3437e-02, 4.7203e-02, 1.0567e-01,
        4.1546e-02, 1.6115e-01, 8.1556e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,404][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0005, 0.0925, 0.0318, 0.0627, 0.0267, 0.0434, 0.0519, 0.0333, 0.0531,
        0.0480, 0.0825, 0.1615, 0.0871, 0.1323, 0.0928], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,405][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0015, 0.0680, 0.0074, 0.1119, 0.0227, 0.0450, 0.0170, 0.0448, 0.1104,
        0.0249, 0.0613, 0.1631, 0.0392, 0.2078, 0.0751], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,406][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0270, 0.1030, 0.0387, 0.1888, 0.0572, 0.0497, 0.0646, 0.0311, 0.0397,
        0.0481, 0.0463, 0.1212, 0.0483, 0.0568, 0.0796], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,407][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ give] are: tensor([1.0000e+00, 1.6501e-09, 1.7092e-14, 3.3647e-08, 1.2764e-06, 1.8654e-08,
        6.7045e-10, 5.1900e-14, 2.1213e-13, 3.0386e-13, 4.5846e-18, 5.6110e-13,
        2.8005e-10, 1.6629e-17, 7.5865e-16], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,408][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0022, 0.2239, 0.0220, 0.1535, 0.0525, 0.0306, 0.0579, 0.0336, 0.0314,
        0.0348, 0.0690, 0.1648, 0.0437, 0.0457, 0.0341], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,410][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0032, 0.0069, 0.0193, 0.0798, 0.0370, 0.0351, 0.0082, 0.0847, 0.1053,
        0.0350, 0.0695, 0.1070, 0.1253, 0.1929, 0.0908], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,411][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ give] are: tensor([8.7040e-07, 2.6559e-02, 3.7694e-02, 5.9841e-02, 6.6742e-03, 1.3924e-02,
        1.6742e-02, 6.8147e-02, 1.2028e-01, 4.9487e-02, 8.7557e-02, 1.6022e-02,
        1.8809e-01, 9.4021e-02, 2.1496e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,413][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.3459, 0.0177, 0.0293, 0.0236, 0.0564, 0.0539, 0.0434, 0.0374, 0.0415,
        0.0339, 0.0350, 0.0364, 0.0880, 0.0328, 0.0611, 0.0636],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,415][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0016, 0.0583, 0.0207, 0.0777, 0.0368, 0.0591, 0.0567, 0.0848, 0.0622,
        0.0491, 0.0637, 0.1027, 0.0366, 0.1513, 0.0649, 0.0735],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,417][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0006, 0.0224, 0.0188, 0.0813, 0.0432, 0.0774, 0.0833, 0.0641, 0.0641,
        0.0910, 0.0398, 0.0435, 0.0504, 0.1004, 0.1104, 0.1094],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,419][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0438, 0.0746, 0.0350, 0.0646, 0.0916, 0.0032, 0.0261, 0.0067, 0.0051,
        0.3674, 0.0050, 0.1276, 0.1065, 0.0032, 0.0304, 0.0091],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,420][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0002, 0.0519, 0.0134, 0.0673, 0.0230, 0.0623, 0.0372, 0.0571, 0.1083,
        0.0434, 0.0446, 0.0941, 0.0398, 0.1468, 0.0743, 0.1363],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,421][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0009, 0.0719, 0.0309, 0.0541, 0.0194, 0.0324, 0.0432, 0.0282, 0.0401,
        0.0462, 0.0736, 0.1464, 0.0809, 0.1168, 0.0822, 0.1330],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,422][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0040, 0.0429, 0.0070, 0.0767, 0.0169, 0.0327, 0.0143, 0.0394, 0.0918,
        0.0242, 0.0616, 0.1412, 0.0413, 0.1876, 0.0656, 0.1529],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,423][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0237, 0.0770, 0.0377, 0.1939, 0.0627, 0.0429, 0.0582, 0.0275, 0.0305,
        0.0496, 0.0509, 0.1121, 0.0429, 0.0504, 0.0803, 0.0595],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,424][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ it] are: tensor([9.9998e-01, 2.6544e-09, 2.5232e-14, 1.2617e-08, 1.9013e-05, 3.2284e-08,
        2.9673e-10, 7.7469e-14, 1.4104e-13, 2.0611e-13, 3.2362e-18, 5.9936e-13,
        4.3977e-11, 2.0526e-17, 4.6771e-15, 5.1774e-13], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,425][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0054, 0.1952, 0.0216, 0.1500, 0.0513, 0.0226, 0.0493, 0.0307, 0.0233,
        0.0350, 0.0709, 0.1545, 0.0506, 0.0414, 0.0355, 0.0628],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,427][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0196, 0.0045, 0.0155, 0.0814, 0.0365, 0.0272, 0.0077, 0.0823, 0.0845,
        0.0309, 0.0610, 0.0831, 0.1105, 0.1502, 0.0703, 0.1347],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,428][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ it] are: tensor([9.8088e-07, 1.6199e-02, 3.2638e-02, 4.8002e-02, 5.9900e-03, 8.0418e-03,
        1.5588e-02, 5.7530e-02, 6.6314e-02, 5.1109e-02, 7.4161e-02, 1.5441e-02,
        1.5325e-01, 6.2130e-02, 2.1914e-01, 1.7446e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,430][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.3799, 0.0195, 0.0259, 0.0214, 0.0496, 0.0524, 0.0390, 0.0353, 0.0376,
        0.0299, 0.0321, 0.0387, 0.0636, 0.0295, 0.0549, 0.0585, 0.0322],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,432][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0014, 0.0427, 0.0166, 0.0413, 0.0284, 0.0433, 0.0423, 0.0710, 0.0641,
        0.0493, 0.0643, 0.0747, 0.0368, 0.1482, 0.0503, 0.0832, 0.1422],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,434][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0005, 0.0222, 0.0150, 0.0798, 0.0331, 0.0632, 0.0637, 0.0571, 0.0625,
        0.0723, 0.0429, 0.0484, 0.0503, 0.0944, 0.0777, 0.1255, 0.0914],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,436][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0050, 0.0981, 0.0336, 0.0697, 0.1387, 0.0026, 0.0585, 0.0053, 0.0034,
        0.3719, 0.0039, 0.1089, 0.0753, 0.0017, 0.0138, 0.0051, 0.0047],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,438][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0003, 0.0492, 0.0185, 0.0673, 0.0241, 0.0624, 0.0439, 0.0542, 0.0866,
        0.0444, 0.0472, 0.0891, 0.0399, 0.1080, 0.0573, 0.1013, 0.1062],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,439][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0027, 0.0665, 0.0335, 0.0569, 0.0247, 0.0360, 0.0506, 0.0305, 0.0368,
        0.0484, 0.0699, 0.1152, 0.0704, 0.0973, 0.0700, 0.1042, 0.0864],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,440][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0087, 0.0508, 0.0113, 0.0872, 0.0205, 0.0314, 0.0165, 0.0330, 0.0722,
        0.0213, 0.0612, 0.1264, 0.0385, 0.1403, 0.0514, 0.1108, 0.1185],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,441][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0122, 0.0726, 0.0337, 0.1654, 0.0503, 0.0436, 0.0579, 0.0307, 0.0330,
        0.0536, 0.0494, 0.1061, 0.0375, 0.0576, 0.0754, 0.0668, 0.0543],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,441][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([1.0000e+00, 1.4728e-12, 5.7813e-18, 6.9684e-11, 6.3983e-08, 1.5455e-10,
        3.3294e-13, 8.2472e-17, 4.3669e-17, 5.8386e-18, 3.0809e-22, 1.1475e-16,
        6.3792e-14, 3.1505e-22, 4.4501e-19, 2.0087e-15, 1.9370e-22],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,443][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0106, 0.1728, 0.0275, 0.1414, 0.0539, 0.0209, 0.0605, 0.0248, 0.0177,
        0.0447, 0.0803, 0.1446, 0.0555, 0.0341, 0.0308, 0.0455, 0.0344],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,445][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0356, 0.0049, 0.0152, 0.0815, 0.0377, 0.0275, 0.0069, 0.0667, 0.0745,
        0.0266, 0.0526, 0.0723, 0.1059, 0.1206, 0.0531, 0.1004, 0.1180],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,446][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([6.0132e-08, 1.4770e-02, 2.5095e-02, 5.1952e-02, 5.0342e-03, 5.7450e-03,
        1.1547e-02, 6.3465e-02, 7.0929e-02, 4.1353e-02, 5.0593e-02, 1.0888e-02,
        1.3548e-01, 6.0831e-02, 2.1296e-01, 1.6758e-01, 7.1777e-02],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,568][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:17:42,569][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,569][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,571][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,572][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,573][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,574][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,575][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,577][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,578][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,580][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,582][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,583][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:17:42,584][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.3478, 0.6522], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,585][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.2543, 0.7457], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,586][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0846, 0.9154], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,587][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.7313, 0.2687], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,587][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0132, 0.9868], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,588][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,589][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.1637, 0.8363], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,591][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.1006, 0.8994], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,592][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([9.9990e-01, 9.6398e-05], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,594][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0368, 0.9632], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,595][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([9.9997e-01, 3.1550e-05], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,596][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0045, 0.9955], device='cuda:0') for source tokens [When Sean]
[2024-07-24 10:17:42,598][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0581, 0.2780, 0.6639], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,599][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0917, 0.5929, 0.3155], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,601][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0207, 0.6129, 0.3664], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,602][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.4106, 0.3379, 0.2516], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,603][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0045, 0.7532, 0.2423], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,604][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0051, 0.4659, 0.5290], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,605][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0595, 0.6786, 0.2618], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,605][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0811, 0.5602, 0.3587], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,606][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([1.0000e+00, 6.6279e-07, 6.5042e-08], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,607][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0048, 0.8466, 0.1486], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,608][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([9.9994e-01, 3.9508e-05, 1.5940e-05], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,610][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0057, 0.8422, 0.1521], device='cuda:0') for source tokens [When Sean and]
[2024-07-24 10:17:42,612][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Megan] are: tensor([0.1142, 0.1737, 0.3875, 0.3246], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,614][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Megan] are: tensor([0.0532, 0.3208, 0.1328, 0.4932], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,616][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Megan] are: tensor([0.0177, 0.3595, 0.1997, 0.4231], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,617][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Megan] are: tensor([0.4901, 0.2099, 0.1438, 0.1562], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,619][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Megan] are: tensor([0.0042, 0.3653, 0.1968, 0.4337], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,620][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Megan] are: tensor([0.0029, 0.2848, 0.3812, 0.3311], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,620][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Megan] are: tensor([0.0561, 0.3863, 0.1562, 0.4014], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,621][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Megan] are: tensor([0.0482, 0.3605, 0.2025, 0.3888], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,622][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Megan] are: tensor([1.0000e+00, 3.9336e-07, 9.8677e-08, 1.1040e-07], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,622][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Megan] are: tensor([0.0046, 0.4869, 0.1244, 0.3840], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,623][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Megan] are: tensor([9.9993e-01, 4.3176e-05, 1.4344e-05, 1.2332e-05], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,625][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Megan] are: tensor([0.0041, 0.7093, 0.1022, 0.1844], device='cuda:0') for source tokens [When Sean and Megan]
[2024-07-24 10:17:42,627][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0861, 0.1240, 0.2738, 0.1986, 0.3175], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,629][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0277, 0.2875, 0.1063, 0.4064, 0.1720], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,630][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0060, 0.3330, 0.1348, 0.3427, 0.1836], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,632][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.3550, 0.1506, 0.1275, 0.1724, 0.1945], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,634][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0016, 0.3281, 0.1399, 0.3903, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,636][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0032, 0.2866, 0.2608, 0.2831, 0.1664], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,637][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0186, 0.3733, 0.1041, 0.3750, 0.1288], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,637][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0223, 0.2935, 0.1540, 0.3299, 0.2003], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,638][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([1.0000e+00, 1.4122e-07, 3.5667e-08, 1.2858e-08, 1.6717e-08],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,639][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0055, 0.4289, 0.1071, 0.2952, 0.1632], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,640][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([9.9962e-01, 8.9559e-05, 3.8958e-05, 3.9247e-05, 2.1399e-04],
       device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,640][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0016, 0.4996, 0.0735, 0.1217, 0.3036], device='cuda:0') for source tokens [When Sean and Megan got]
[2024-07-24 10:17:42,642][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1102, 0.0558, 0.1804, 0.1581, 0.2214, 0.2741], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,644][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0558, 0.1680, 0.0857, 0.3777, 0.1358, 0.1771], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,646][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0128, 0.2009, 0.0963, 0.2947, 0.1531, 0.2423], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,647][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.6665, 0.0564, 0.0595, 0.0860, 0.0846, 0.0471], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,649][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0019, 0.2315, 0.0965, 0.2915, 0.0999, 0.2788], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,651][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0148, 0.2427, 0.2070, 0.2467, 0.1368, 0.1519], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,653][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0670, 0.2590, 0.0830, 0.3600, 0.1081, 0.1229], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,654][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0479, 0.1835, 0.1287, 0.2464, 0.1804, 0.2130], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,655][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([1.0000e+00, 4.6453e-08, 1.0720e-08, 4.2450e-09, 2.8662e-09, 6.4205e-09],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,655][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0201, 0.3255, 0.0952, 0.2947, 0.1629, 0.1015], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,656][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([9.9979e-01, 2.7559e-05, 1.2058e-05, 1.4076e-05, 8.5509e-05, 6.8576e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,657][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0017, 0.2860, 0.0514, 0.0764, 0.2212, 0.3633], device='cuda:0') for source tokens [When Sean and Megan got a]
[2024-07-24 10:17:42,658][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ necklace] are: tensor([0.0369, 0.0838, 0.1342, 0.1148, 0.1828, 0.3222, 0.1252],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,660][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ necklace] are: tensor([0.0038, 0.1991, 0.0847, 0.2987, 0.1184, 0.1958, 0.0995],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,662][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ necklace] are: tensor([0.0023, 0.1815, 0.0745, 0.2103, 0.1224, 0.2557, 0.1533],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,663][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ necklace] are: tensor([0.1041, 0.1393, 0.0724, 0.1300, 0.1540, 0.1778, 0.2224],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,665][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ necklace] are: tensor([0.0009, 0.2347, 0.0438, 0.1996, 0.0805, 0.2831, 0.1573],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,667][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ necklace] are: tensor([0.0014, 0.2243, 0.1577, 0.1868, 0.0814, 0.1615, 0.1870],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,669][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ necklace] are: tensor([0.0075, 0.2055, 0.0382, 0.3148, 0.1142, 0.2436, 0.0762],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,670][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ necklace] are: tensor([0.0052, 0.1857, 0.0894, 0.2479, 0.1212, 0.1905, 0.1601],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,671][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ necklace] are: tensor([9.9999e-01, 1.5570e-06, 2.3050e-07, 2.1952e-07, 1.6410e-07, 2.5045e-07,
        5.7606e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,672][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ necklace] are: tensor([0.0017, 0.2650, 0.0630, 0.2872, 0.1148, 0.1263, 0.1420],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,673][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ necklace] are: tensor([9.9966e-01, 2.4580e-05, 1.2384e-05, 7.0265e-06, 6.0429e-05, 7.1100e-05,
        1.6044e-04], device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,674][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ necklace] are: tensor([0.0007, 0.2901, 0.0408, 0.0720, 0.1683, 0.2824, 0.1458],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace]
[2024-07-24 10:17:42,674][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0699, 0.0381, 0.1315, 0.1132, 0.1573, 0.1921, 0.0741, 0.2238],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,676][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0096, 0.1502, 0.0527, 0.3182, 0.0820, 0.1200, 0.0770, 0.1903],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,678][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0061, 0.1651, 0.0585, 0.2309, 0.1094, 0.1759, 0.0838, 0.1702],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,679][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.4214, 0.0594, 0.0506, 0.0977, 0.0679, 0.0382, 0.1833, 0.0815],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,681][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0013, 0.1722, 0.0556, 0.2032, 0.0703, 0.1911, 0.1209, 0.1855],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,682][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0063, 0.2134, 0.1367, 0.1813, 0.0809, 0.0996, 0.1681, 0.1137],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,684][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0468, 0.2278, 0.0403, 0.3171, 0.0857, 0.1077, 0.0649, 0.1097],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,686][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0205, 0.1561, 0.0867, 0.2201, 0.1247, 0.1360, 0.1345, 0.1213],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,687][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([1.0000e+00, 5.9414e-08, 1.9577e-08, 1.2456e-08, 5.9968e-09, 1.5013e-08,
        5.8775e-07, 3.8407e-09], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,688][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0077, 0.2720, 0.0695, 0.2828, 0.1116, 0.0622, 0.1276, 0.0666],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,689][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([9.9976e-01, 1.7512e-05, 7.3693e-06, 6.7316e-06, 4.4370e-05, 2.5166e-05,
        1.0384e-04, 3.1305e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,690][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0006, 0.2274, 0.0292, 0.0528, 0.1427, 0.2271, 0.0988, 0.2215],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at]
[2024-07-24 10:17:42,691][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0933, 0.0319, 0.1094, 0.0893, 0.1245, 0.1316, 0.0574, 0.1504, 0.2122],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,692][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0340, 0.1125, 0.0426, 0.2754, 0.0674, 0.0879, 0.0716, 0.1483, 0.1604],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,693][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0132, 0.1286, 0.0386, 0.2110, 0.0851, 0.1238, 0.0554, 0.1190, 0.2253],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,695][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.7389, 0.0175, 0.0237, 0.0404, 0.0276, 0.0143, 0.0853, 0.0338, 0.0184],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,696][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0017, 0.1349, 0.0536, 0.1722, 0.0546, 0.1289, 0.1081, 0.1315, 0.2145],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,698][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0203, 0.1752, 0.1162, 0.1696, 0.0823, 0.0794, 0.1564, 0.0992, 0.1014],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,700][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1444, 0.1912, 0.0344, 0.2600, 0.0640, 0.0640, 0.0481, 0.0643, 0.1295],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,702][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0650, 0.1207, 0.0567, 0.1810, 0.1065, 0.1194, 0.1200, 0.1078, 0.1230],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,703][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([1.0000e+00, 1.1465e-09, 1.5121e-10, 5.3787e-11, 2.5517e-11, 6.5117e-11,
        5.1146e-09, 9.1992e-12, 7.4553e-13], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,705][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0284, 0.2980, 0.0731, 0.2376, 0.1002, 0.0401, 0.1371, 0.0478, 0.0376],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,706][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([9.9995e-01, 2.8628e-06, 1.3590e-06, 1.2072e-06, 9.4891e-06, 4.6399e-06,
        1.8848e-05, 6.4505e-06, 4.9949e-06], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,706][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0005, 0.1471, 0.0207, 0.0377, 0.0990, 0.1577, 0.0584, 0.1455, 0.3334],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the]
[2024-07-24 10:17:42,707][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0291, 0.0396, 0.0648, 0.0551, 0.1031, 0.1480, 0.0593, 0.1111, 0.2200,
        0.1699], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,708][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0049, 0.1232, 0.0406, 0.2164, 0.0644, 0.1052, 0.0679, 0.1383, 0.1886,
        0.0505], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,709][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0019, 0.1383, 0.0449, 0.1577, 0.0631, 0.1306, 0.0633, 0.1254, 0.2008,
        0.0740], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,710][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.2128, 0.0420, 0.0374, 0.0516, 0.0620, 0.0573, 0.1186, 0.1005, 0.0872,
        0.2306], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,712][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0010, 0.1643, 0.0325, 0.1314, 0.0429, 0.1211, 0.0907, 0.1225, 0.2097,
        0.0838], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,714][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0021, 0.1692, 0.0778, 0.1238, 0.0632, 0.0898, 0.1285, 0.0875, 0.1259,
        0.1322], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,715][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0140, 0.1964, 0.0225, 0.2321, 0.0525, 0.1001, 0.0438, 0.0861, 0.2038,
        0.0487], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,717][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0105, 0.1085, 0.0448, 0.1497, 0.0730, 0.1298, 0.1016, 0.1150, 0.1635,
        0.1037], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,718][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([1.0000e+00, 2.0775e-09, 3.2603e-10, 1.1854e-10, 7.3547e-11, 2.3109e-10,
        7.6537e-09, 4.1915e-11, 3.3990e-12, 1.7206e-10], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,720][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0022, 0.2798, 0.0384, 0.2175, 0.0828, 0.0679, 0.1250, 0.0609, 0.0577,
        0.0678], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,721][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([9.9982e-01, 5.9489e-06, 2.4914e-06, 1.7580e-06, 1.3053e-05, 9.8130e-06,
        2.4064e-05, 1.0806e-05, 8.4595e-06, 9.8909e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,723][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0005, 0.1418, 0.0205, 0.0372, 0.0906, 0.1472, 0.0648, 0.1449, 0.2767,
        0.0758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station]
[2024-07-24 10:17:42,724][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0358, 0.0307, 0.0663, 0.0577, 0.1045, 0.1254, 0.0549, 0.1063, 0.1708,
        0.1455, 0.1020], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,724][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0103, 0.0928, 0.0353, 0.2003, 0.0624, 0.0819, 0.0645, 0.1369, 0.1525,
        0.0583, 0.1048], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,725][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0029, 0.1067, 0.0361, 0.1578, 0.0672, 0.1170, 0.0491, 0.1162, 0.1778,
        0.0709, 0.0984], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,726][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.3140, 0.0375, 0.0339, 0.0596, 0.0574, 0.0311, 0.0999, 0.0693, 0.0474,
        0.1615, 0.0885], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,728][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0008, 0.1251, 0.0335, 0.1169, 0.0421, 0.1140, 0.0789, 0.1177, 0.1910,
        0.0742, 0.1059], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,729][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0038, 0.1041, 0.0680, 0.1155, 0.0582, 0.0700, 0.1014, 0.0856, 0.1036,
        0.1229, 0.1667], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,731][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0246, 0.1404, 0.0231, 0.2202, 0.0530, 0.0852, 0.0342, 0.0794, 0.1717,
        0.0425, 0.1258], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,733][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0145, 0.0720, 0.0384, 0.1268, 0.0686, 0.0986, 0.0848, 0.1013, 0.1335,
        0.0944, 0.1671], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,734][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([1.0000e+00, 3.2896e-10, 3.4757e-11, 9.5303e-12, 4.1773e-12, 1.1586e-11,
        1.3058e-09, 3.4308e-12, 1.3494e-13, 9.2933e-12, 4.5819e-13],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,736][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0045, 0.2213, 0.0395, 0.2144, 0.0849, 0.0484, 0.1064, 0.0538, 0.0457,
        0.0706, 0.1105], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,737][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([9.9964e-01, 1.0671e-05, 3.4396e-06, 3.4545e-06, 3.1398e-05, 1.4690e-05,
        6.5402e-05, 2.0067e-05, 1.5796e-05, 1.7561e-04, 1.7963e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,739][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0007, 0.1172, 0.0186, 0.0372, 0.0811, 0.1294, 0.0594, 0.1453, 0.2539,
        0.0726, 0.0848], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station,]
[2024-07-24 10:17:42,740][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Sean] are: tensor([0.0371, 0.0265, 0.0615, 0.0478, 0.0779, 0.1190, 0.0374, 0.0920, 0.1769,
        0.1174, 0.0929, 0.1138], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,741][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Sean] are: tensor([0.0092, 0.1054, 0.0295, 0.1840, 0.0540, 0.0808, 0.0535, 0.1015, 0.1294,
        0.0444, 0.0826, 0.1258], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,742][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Sean] are: tensor([0.0024, 0.0881, 0.0304, 0.1159, 0.0486, 0.0900, 0.0426, 0.0808, 0.1344,
        0.0593, 0.0692, 0.2385], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,743][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Sean] are: tensor([0.2166, 0.0331, 0.0264, 0.0428, 0.0475, 0.0321, 0.0905, 0.0518, 0.0414,
        0.1705, 0.1030, 0.1443], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,743][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Sean] are: tensor([0.0009, 0.0974, 0.0263, 0.1084, 0.0359, 0.1076, 0.0609, 0.0997, 0.1774,
        0.0631, 0.0770, 0.1455], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,745][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Sean] are: tensor([0.0014, 0.0806, 0.0464, 0.0804, 0.0403, 0.0633, 0.0778, 0.0628, 0.0895,
        0.0879, 0.1387, 0.2307], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,747][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Sean] are: tensor([0.0204, 0.1241, 0.0166, 0.1647, 0.0389, 0.0634, 0.0329, 0.0600, 0.1233,
        0.0352, 0.0877, 0.2328], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,749][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Sean] are: tensor([0.0114, 0.0844, 0.0356, 0.1214, 0.0677, 0.0954, 0.0807, 0.0762, 0.0988,
        0.0695, 0.1118, 0.1471], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,750][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Sean] are: tensor([1.0000e+00, 3.0868e-09, 7.6411e-10, 2.4531e-10, 1.9828e-10, 8.4415e-10,
        1.9176e-08, 1.7649e-10, 2.6976e-11, 7.7967e-10, 3.0886e-11, 2.0922e-10],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,753][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Sean] are: tensor([0.0034, 0.1714, 0.0215, 0.1469, 0.0553, 0.0433, 0.0924, 0.0426, 0.0403,
        0.0535, 0.0764, 0.2530], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,754][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Sean] are: tensor([9.9979e-01, 5.4264e-06, 1.5455e-06, 1.0721e-06, 1.2223e-05, 9.4008e-06,
        2.4186e-05, 8.3942e-06, 8.3119e-06, 8.4157e-05, 8.5167e-06, 4.8598e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,756][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Sean] are: tensor([0.0005, 0.1186, 0.0145, 0.0301, 0.0677, 0.1276, 0.0586, 0.1198, 0.2330,
        0.0691, 0.0768, 0.0836], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean]
[2024-07-24 10:17:42,757][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0239, 0.0218, 0.0586, 0.0343, 0.0696, 0.1087, 0.0336, 0.0790, 0.1426,
        0.1045, 0.0796, 0.0923, 0.1518], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,758][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0015, 0.0739, 0.0305, 0.1398, 0.0449, 0.0800, 0.0425, 0.1287, 0.1649,
        0.0405, 0.0907, 0.1090, 0.0532], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,759][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0004, 0.0720, 0.0213, 0.0907, 0.0418, 0.0929, 0.0311, 0.0737, 0.1384,
        0.0481, 0.0672, 0.2443, 0.0780], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,760][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0677, 0.0233, 0.0273, 0.0386, 0.0458, 0.0451, 0.0677, 0.0645, 0.0593,
        0.1688, 0.1270, 0.1579, 0.1072], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,761][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0003, 0.0977, 0.0267, 0.1180, 0.0343, 0.0880, 0.0551, 0.0878, 0.1628,
        0.0544, 0.0732, 0.1410, 0.0608], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,762][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0006, 0.0727, 0.0454, 0.0742, 0.0383, 0.0576, 0.0617, 0.0491, 0.0771,
        0.0736, 0.1101, 0.1858, 0.1539], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,764][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0035, 0.1015, 0.0176, 0.1678, 0.0318, 0.0587, 0.0257, 0.0549, 0.1203,
        0.0357, 0.0842, 0.2333, 0.0649], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,765][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0016, 0.0550, 0.0302, 0.0803, 0.0484, 0.0874, 0.0602, 0.0749, 0.1054,
        0.0738, 0.1063, 0.1387, 0.1377], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,767][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([1.0000e+00, 5.6105e-09, 1.9162e-09, 8.3694e-10, 7.6761e-10, 2.2838e-09,
        5.0834e-08, 5.5932e-10, 7.3030e-11, 3.2061e-09, 9.6671e-11, 3.9463e-10,
        3.7756e-09], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,768][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0010, 0.1468, 0.0251, 0.1454, 0.0626, 0.0414, 0.0796, 0.0401, 0.0342,
        0.0515, 0.0729, 0.2280, 0.0715], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,770][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([9.9982e-01, 4.5815e-06, 1.9966e-06, 1.1752e-06, 1.2698e-05, 9.2887e-06,
        1.6636e-05, 6.6108e-06, 8.0989e-06, 5.3934e-05, 8.1795e-06, 2.5261e-05,
        2.9557e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,771][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([2.0924e-04, 9.9772e-02, 1.2538e-02, 2.2485e-02, 6.3334e-02, 1.2667e-01,
        4.2383e-02, 1.1446e-01, 2.4818e-01, 5.7326e-02, 7.1685e-02, 7.6222e-02,
        6.4735e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided]
[2024-07-24 10:17:42,773][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0423, 0.0160, 0.0566, 0.0429, 0.0652, 0.0902, 0.0277, 0.0802, 0.1156,
        0.0787, 0.0653, 0.1063, 0.1271, 0.0859], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,774][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0030, 0.0580, 0.0223, 0.1586, 0.0383, 0.0561, 0.0309, 0.0835, 0.0976,
        0.0367, 0.0644, 0.1064, 0.0450, 0.1993], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,775][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0007, 0.0447, 0.0140, 0.0932, 0.0389, 0.0784, 0.0228, 0.0741, 0.1265,
        0.0443, 0.0536, 0.1743, 0.0747, 0.1598], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,776][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1976, 0.0193, 0.0277, 0.0430, 0.0357, 0.0206, 0.0781, 0.0478, 0.0329,
        0.1371, 0.0946, 0.1371, 0.0763, 0.0523], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,777][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0004, 0.0748, 0.0253, 0.0895, 0.0335, 0.0816, 0.0605, 0.0695, 0.1188,
        0.0597, 0.0621, 0.1232, 0.0542, 0.1470], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,778][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0027, 0.0875, 0.0481, 0.0711, 0.0314, 0.0428, 0.0700, 0.0388, 0.0463,
        0.0686, 0.0935, 0.1603, 0.1028, 0.1362], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,780][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0115, 0.0657, 0.0143, 0.1325, 0.0298, 0.0434, 0.0207, 0.0445, 0.0992,
        0.0305, 0.0812, 0.1713, 0.0565, 0.1989], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,782][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0042, 0.0407, 0.0292, 0.0880, 0.0461, 0.0635, 0.0511, 0.0543, 0.0656,
        0.0621, 0.0896, 0.1159, 0.1202, 0.1695], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,783][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.0000e+00, 3.8161e-09, 1.2811e-09, 4.3172e-10, 2.4932e-10, 6.0554e-10,
        3.0466e-08, 1.8639e-10, 1.5174e-11, 8.2017e-10, 3.5559e-11, 1.3384e-10,
        9.1536e-10, 3.0884e-10], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,785][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0070, 0.2000, 0.0305, 0.1496, 0.0635, 0.0315, 0.0687, 0.0336, 0.0259,
        0.0487, 0.0881, 0.1499, 0.0562, 0.0468], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,787][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.9970e-01, 6.8784e-06, 2.1324e-06, 1.8640e-06, 1.7603e-05, 8.3697e-06,
        3.1536e-05, 9.2801e-06, 8.4397e-06, 8.8754e-05, 8.9189e-06, 4.8962e-05,
        4.0360e-05, 2.7810e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,789][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0002, 0.0941, 0.0121, 0.0231, 0.0594, 0.1020, 0.0345, 0.0968, 0.2108,
        0.0462, 0.0585, 0.0647, 0.0576, 0.1401], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to]
[2024-07-24 10:17:42,790][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0307, 0.0203, 0.0440, 0.0339, 0.0614, 0.0996, 0.0269, 0.0732, 0.1200,
        0.0754, 0.0586, 0.0903, 0.1120, 0.0880, 0.0659], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,792][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0006, 0.0656, 0.0220, 0.1355, 0.0328, 0.0572, 0.0265, 0.0878, 0.1216,
        0.0240, 0.0628, 0.0865, 0.0288, 0.1873, 0.0611], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,792][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([1.2384e-04, 5.3237e-02, 1.1740e-02, 7.4615e-02, 2.8358e-02, 7.7086e-02,
        2.1129e-02, 7.3130e-02, 1.4700e-01, 3.5816e-02, 4.8813e-02, 1.6649e-01,
        5.5705e-02, 1.5665e-01, 5.0105e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,793][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0390, 0.0213, 0.0298, 0.0387, 0.0358, 0.0342, 0.0673, 0.0612, 0.0522,
        0.1524, 0.1050, 0.1438, 0.0818, 0.0761, 0.0616], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,794][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([1.1872e-04, 7.6408e-02, 1.6826e-02, 8.6300e-02, 2.6303e-02, 7.1253e-02,
        4.2813e-02, 6.4190e-02, 1.3522e-01, 4.3437e-02, 4.7203e-02, 1.0567e-01,
        4.1546e-02, 1.6115e-01, 8.1556e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,795][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0005, 0.0925, 0.0318, 0.0627, 0.0267, 0.0434, 0.0519, 0.0333, 0.0531,
        0.0480, 0.0825, 0.1615, 0.0871, 0.1323, 0.0928], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,797][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0015, 0.0680, 0.0074, 0.1119, 0.0227, 0.0450, 0.0170, 0.0448, 0.1104,
        0.0249, 0.0613, 0.1631, 0.0392, 0.2078, 0.0751], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,799][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0007, 0.0454, 0.0209, 0.0746, 0.0348, 0.0692, 0.0401, 0.0552, 0.0854,
        0.0458, 0.0713, 0.1014, 0.0894, 0.1897, 0.0758], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,800][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([1.0000e+00, 4.6676e-08, 2.2442e-08, 7.2827e-09, 8.0315e-09, 2.8354e-08,
        3.3896e-07, 7.3576e-09, 1.1917e-09, 2.2681e-08, 1.4150e-09, 4.6448e-09,
        3.8694e-08, 9.7219e-09, 1.7714e-08], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,802][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0011, 0.2164, 0.0223, 0.1402, 0.0529, 0.0364, 0.0576, 0.0375, 0.0376,
        0.0362, 0.0682, 0.1599, 0.0440, 0.0513, 0.0384], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,804][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([9.9960e-01, 8.4053e-06, 2.4361e-06, 2.0959e-06, 1.7887e-05, 1.3937e-05,
        4.1552e-05, 1.0946e-05, 1.2861e-05, 1.1146e-04, 1.4931e-05, 5.2436e-05,
        4.4981e-05, 4.6322e-05, 2.3064e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,805][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([1.0917e-04, 9.8116e-02, 9.2701e-03, 1.7775e-02, 5.1978e-02, 9.4013e-02,
        3.2596e-02, 9.3498e-02, 1.9960e-01, 4.3226e-02, 5.3440e-02, 5.8655e-02,
        4.8161e-02, 1.3217e-01, 6.7396e-02], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give]
[2024-07-24 10:17:42,807][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0520, 0.0147, 0.0431, 0.0343, 0.0571, 0.0788, 0.0262, 0.0652, 0.0869,
        0.0719, 0.0523, 0.0991, 0.1267, 0.0675, 0.0551, 0.0691],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,809][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0007, 0.0409, 0.0126, 0.1104, 0.0225, 0.0465, 0.0197, 0.0813, 0.1039,
        0.0244, 0.0543, 0.0838, 0.0306, 0.1898, 0.0600, 0.1186],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,809][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0002, 0.0348, 0.0095, 0.0629, 0.0253, 0.0597, 0.0184, 0.0636, 0.1176,
        0.0370, 0.0388, 0.1484, 0.0610, 0.1453, 0.0504, 0.1269],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,810][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.1343, 0.0151, 0.0204, 0.0359, 0.0260, 0.0217, 0.0712, 0.0583, 0.0345,
        0.1349, 0.0851, 0.1346, 0.0581, 0.0612, 0.0515, 0.0572],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,811][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0002, 0.0519, 0.0134, 0.0673, 0.0230, 0.0623, 0.0372, 0.0571, 0.1083,
        0.0434, 0.0446, 0.0941, 0.0398, 0.1468, 0.0743, 0.1363],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,812][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0009, 0.0719, 0.0309, 0.0541, 0.0194, 0.0324, 0.0432, 0.0282, 0.0401,
        0.0462, 0.0736, 0.1464, 0.0809, 0.1168, 0.0822, 0.1330],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,814][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0040, 0.0429, 0.0070, 0.0767, 0.0169, 0.0327, 0.0143, 0.0394, 0.0918,
        0.0242, 0.0616, 0.1412, 0.0413, 0.1876, 0.0656, 0.1529],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,816][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0012, 0.0349, 0.0156, 0.0633, 0.0295, 0.0503, 0.0352, 0.0451, 0.0600,
        0.0423, 0.0717, 0.1047, 0.0931, 0.1681, 0.0699, 0.1152],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,818][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([1.0000e+00, 1.3972e-07, 9.8752e-08, 3.8751e-08, 4.9067e-08, 1.4514e-07,
        1.0608e-06, 4.2559e-08, 8.3999e-09, 1.1665e-07, 1.2686e-08, 4.2433e-08,
        1.6103e-07, 9.2402e-08, 1.2449e-07, 1.7105e-07], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,819][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0027, 0.1869, 0.0209, 0.1335, 0.0512, 0.0276, 0.0486, 0.0342, 0.0287,
        0.0362, 0.0701, 0.1493, 0.0503, 0.0478, 0.0402, 0.0718],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,821][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([9.9971e-01, 4.5046e-06, 2.0337e-06, 1.5791e-06, 1.2607e-05, 8.7072e-06,
        2.9793e-05, 8.2504e-06, 7.3280e-06, 8.6921e-05, 9.9318e-06, 3.8577e-05,
        2.8522e-05, 2.4335e-05, 1.6387e-05, 1.2228e-05], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,822][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([1.5945e-04, 7.1709e-02, 7.0149e-03, 1.3432e-02, 4.0719e-02, 7.9471e-02,
        2.6450e-02, 7.9119e-02, 1.7571e-01, 3.8533e-02, 4.3137e-02, 5.1345e-02,
        4.3179e-02, 1.1481e-01, 5.9849e-02, 1.5536e-01], device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it]
[2024-07-24 10:17:42,824][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0385, 0.0132, 0.0461, 0.0371, 0.0544, 0.0672, 0.0219, 0.0614, 0.0854,
        0.0700, 0.0503, 0.0963, 0.1247, 0.0650, 0.0512, 0.0640, 0.0531],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,826][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0024, 0.0434, 0.0137, 0.1063, 0.0263, 0.0421, 0.0227, 0.0605, 0.0729,
        0.0279, 0.0492, 0.0835, 0.0342, 0.1402, 0.0572, 0.0886, 0.1290],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,827][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0007, 0.0371, 0.0112, 0.0701, 0.0295, 0.0585, 0.0176, 0.0513, 0.0956,
        0.0333, 0.0400, 0.1438, 0.0578, 0.1156, 0.0408, 0.0980, 0.0992],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,827][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1854, 0.0154, 0.0191, 0.0327, 0.0255, 0.0160, 0.0698, 0.0412, 0.0272,
        0.1337, 0.0736, 0.1237, 0.0624, 0.0432, 0.0482, 0.0447, 0.0383],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,828][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0003, 0.0492, 0.0185, 0.0673, 0.0241, 0.0624, 0.0439, 0.0542, 0.0866,
        0.0444, 0.0472, 0.0891, 0.0399, 0.1080, 0.0573, 0.1013, 0.1062],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,829][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0027, 0.0665, 0.0335, 0.0569, 0.0247, 0.0360, 0.0506, 0.0305, 0.0368,
        0.0484, 0.0699, 0.1152, 0.0704, 0.0973, 0.0700, 0.1042, 0.0864],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,831][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0087, 0.0508, 0.0113, 0.0872, 0.0205, 0.0314, 0.0165, 0.0330, 0.0722,
        0.0213, 0.0612, 0.1264, 0.0385, 0.1403, 0.0514, 0.1108, 0.1185],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,833][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0032, 0.0346, 0.0203, 0.0609, 0.0318, 0.0465, 0.0391, 0.0421, 0.0510,
        0.0522, 0.0763, 0.0964, 0.0947, 0.1259, 0.0514, 0.0919, 0.0815],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,835][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.0000e+00, 3.5511e-09, 1.1319e-09, 3.2656e-10, 1.9777e-10, 4.1706e-10,
        2.3504e-08, 1.1717e-10, 8.4379e-12, 4.6871e-10, 1.9243e-11, 1.1281e-10,
        7.9155e-10, 1.9973e-10, 9.3313e-10, 1.1945e-09, 6.2409e-10],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,836][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0048, 0.1627, 0.0253, 0.1244, 0.0525, 0.0254, 0.0586, 0.0283, 0.0226,
        0.0457, 0.0797, 0.1403, 0.0552, 0.0407, 0.0361, 0.0548, 0.0427],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,838][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.9968e-01, 6.4410e-06, 2.1043e-06, 1.6292e-06, 1.5040e-05, 7.5226e-06,
        2.7645e-05, 8.7002e-06, 7.3548e-06, 8.2033e-05, 8.6095e-06, 4.2605e-05,
        3.3777e-05, 2.4134e-05, 1.7282e-05, 1.1122e-05, 2.4047e-05],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,840][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0003, 0.0610, 0.0074, 0.0156, 0.0400, 0.0707, 0.0238, 0.0683, 0.1487,
        0.0336, 0.0401, 0.0459, 0.0408, 0.0976, 0.0521, 0.1290, 0.1249],
       device='cuda:0') for source tokens [When Sean and Megan got a necklace at the station, Sean decided to give it to]
[2024-07-24 10:17:42,843][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:17:42,845][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[3118],
        [ 170],
        [ 147],
        [   7],
        [  19],
        [  33],
        [  21],
        [   4],
        [   5],
        [ 174],
        [   3],
        [   5],
        [  17],
        [   2],
        [  10],
        [  25],
        [   1]], device='cuda:0')
[2024-07-24 10:17:42,846][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[3006],
        [ 236],
        [ 188],
        [  14],
        [  42],
        [  96],
        [  35],
        [   6],
        [  14],
        [ 236],
        [   5],
        [   6],
        [  25],
        [   5],
        [  24],
        [  47],
        [   3]], device='cuda:0')
[2024-07-24 10:17:42,848][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 9680],
        [ 9292],
        [13711],
        [11878],
        [13214],
        [11146],
        [10155],
        [10011],
        [ 9850],
        [ 8713],
        [ 9757],
        [ 8897],
        [ 9590],
        [ 9585],
        [ 9634],
        [ 9401],
        [ 9326]], device='cuda:0')
[2024-07-24 10:17:42,850][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 7455],
        [ 2107],
        [16726],
        [ 4497],
        [ 9841],
        [18186],
        [17732],
        [22242],
        [22702],
        [29465],
        [30139],
        [30766],
        [31417],
        [34675],
        [35855],
        [36592],
        [41547]], device='cuda:0')
[2024-07-24 10:17:42,852][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 6090],
        [15783],
        [ 8321],
        [    6],
        [    8],
        [    7],
        [  150],
        [   86],
        [  141],
        [  221],
        [  252],
        [  278],
        [  462],
        [  482],
        [  654],
        [  963],
        [ 1057]], device='cuda:0')
[2024-07-24 10:17:42,854][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[3752],
        [1990],
        [2624],
        [2117],
        [2479],
        [1974],
        [2960],
        [3186],
        [3453],
        [8315],
        [8649],
        [5970],
        [5077],
        [6534],
        [6986],
        [6996],
        [6777]], device='cuda:0')
[2024-07-24 10:17:42,856][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[19669],
        [ 8554],
        [ 5008],
        [ 2667],
        [ 2981],
        [ 2630],
        [ 2781],
        [ 2430],
        [ 2411],
        [ 2509],
        [ 2344],
        [ 2123],
        [ 2002],
        [ 1943],
        [ 1873],
        [ 1660],
        [ 1641]], device='cuda:0')
[2024-07-24 10:17:42,858][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[22211],
        [30339],
        [30675],
        [31172],
        [31370],
        [31438],
        [31935],
        [31895],
        [31743],
        [32681],
        [32067],
        [33470],
        [34194],
        [33578],
        [33901],
        [32821],
        [32339]], device='cuda:0')
[2024-07-24 10:17:42,860][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[20515],
        [20438],
        [20587],
        [34552],
        [32882],
        [29062],
        [24069],
        [27202],
        [24533],
        [21293],
        [21776],
        [22356],
        [22704],
        [19185],
        [18211],
        [17641],
        [17853]], device='cuda:0')
[2024-07-24 10:17:42,862][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[17802],
        [13482],
        [10508],
        [ 3707],
        [ 4198],
        [ 4788],
        [ 5177],
        [ 4719],
        [ 4675],
        [ 5250],
        [ 4180],
        [ 5066],
        [ 4729],
        [ 4660],
        [ 5765],
        [ 5734],
        [ 5794]], device='cuda:0')
[2024-07-24 10:17:42,863][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7197],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196],
        [7196]], device='cuda:0')
[2024-07-24 10:17:42,865][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[28669],
        [27719],
        [27783],
        [31952],
        [30814],
        [30978],
        [31544],
        [31305],
        [30902],
        [29967],
        [28920],
        [24554],
        [24063],
        [25121],
        [24497],
        [23512],
        [23629]], device='cuda:0')
[2024-07-24 10:17:42,867][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[45320],
        [44625],
        [41148],
        [21361],
        [31488],
        [41572],
        [43955],
        [45241],
        [47525],
        [48247],
        [48166],
        [48527],
        [49210],
        [48982],
        [49262],
        [49174],
        [49187]], device='cuda:0')
[2024-07-24 10:17:42,869][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[10121],
        [  561],
        [ 4934],
        [  788],
        [  780],
        [  585],
        [ 1166],
        [ 2159],
        [ 4818],
        [ 9495],
        [ 9065],
        [ 9258],
        [ 6482],
        [ 7669],
        [11772],
        [12965],
        [13413]], device='cuda:0')
[2024-07-24 10:17:42,870][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[20934],
        [19373],
        [23448],
        [23456],
        [21858],
        [21739],
        [20743],
        [20753],
        [21659],
        [22251],
        [22132],
        [22187],
        [20917],
        [22242],
        [22570],
        [24362],
        [22656]], device='cuda:0')
[2024-07-24 10:17:42,872][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[34274],
        [18601],
        [13013],
        [12344],
        [ 9037],
        [ 9883],
        [10186],
        [ 9704],
        [10555],
        [11549],
        [11495],
        [12125],
        [12138],
        [12171],
        [12072],
        [12099],
        [12246]], device='cuda:0')
[2024-07-24 10:17:42,875][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[15017],
        [19674],
        [29654],
        [28364],
        [29147],
        [30205],
        [30622],
        [29477],
        [28922],
        [28191],
        [27283],
        [25072],
        [24789],
        [24991],
        [25701],
        [24801],
        [24657]], device='cuda:0')
[2024-07-24 10:17:42,877][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[32995],
        [30201],
        [32265],
        [31514],
        [30616],
        [29245],
        [27230],
        [26837],
        [23929],
        [23438],
        [23054],
        [23617],
        [23328],
        [20395],
        [20164],
        [18558],
        [17790]], device='cuda:0')
[2024-07-24 10:17:42,879][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[34083],
        [26775],
        [22619],
        [23571],
        [24301],
        [25008],
        [23834],
        [23899],
        [25832],
        [22381],
        [22469],
        [21781],
        [20993],
        [21474],
        [21199],
        [21544],
        [21751]], device='cuda:0')
[2024-07-24 10:17:42,880][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[32654],
        [ 8999],
        [ 9293],
        [14441],
        [14448],
        [15575],
        [15172],
        [15527],
        [14753],
        [13895],
        [13340],
        [13344],
        [13440],
        [13115],
        [13116],
        [12903],
        [12971]], device='cuda:0')
[2024-07-24 10:17:42,881][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[6527],
        [2685],
        [1494],
        [1796],
        [2051],
        [2147],
        [2295],
        [2387],
        [2551],
        [2891],
        [3079],
        [3379],
        [3312],
        [3309],
        [3327],
        [3302],
        [3259]], device='cuda:0')
[2024-07-24 10:17:42,883][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[14340],
        [28162],
        [28483],
        [25852],
        [25995],
        [24594],
        [24307],
        [25043],
        [24739],
        [24590],
        [23397],
        [22857],
        [22341],
        [24459],
        [25250],
        [25625],
        [25982]], device='cuda:0')
[2024-07-24 10:17:42,885][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[43447],
        [37987],
        [41151],
        [41076],
        [41055],
        [41081],
        [41745],
        [42128],
        [42864],
        [43039],
        [43178],
        [43337],
        [43051],
        [43609],
        [43505],
        [43294],
        [43247]], device='cuda:0')
[2024-07-24 10:17:42,887][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[23188],
        [23190],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188],
        [23188]], device='cuda:0')
[2024-07-24 10:17:42,889][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[4888],
        [9206],
        [8482],
        [7102],
        [6824],
        [6707],
        [6811],
        [7094],
        [7135],
        [7624],
        [8000],
        [8968],
        [8830],
        [8633],
        [8790],
        [8991],
        [8903]], device='cuda:0')
[2024-07-24 10:17:42,890][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[9154],
        [9155],
        [9155],
        [9154],
        [9147],
        [9149],
        [9150],
        [9153],
        [9155],
        [9155],
        [9152],
        [9154],
        [9153],
        [9152],
        [9151],
        [9151],
        [9150]], device='cuda:0')
[2024-07-24 10:17:42,893][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12698],
        [ 5315],
        [ 4556],
        [ 4866],
        [ 5080],
        [ 4530],
        [ 4394],
        [ 3959],
        [ 3688],
        [ 3645],
        [ 3512],
        [ 3424],
        [ 3432],
        [ 3368],
        [ 3397],
        [ 3353],
        [ 3424]], device='cuda:0')
[2024-07-24 10:17:42,894][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[17317],
        [25932],
        [26697],
        [26224],
        [26272],
        [26121],
        [26775],
        [26441],
        [26182],
        [27386],
        [27745],
        [27553],
        [28352],
        [28318],
        [28231],
        [28574],
        [28530]], device='cuda:0')
[2024-07-24 10:17:42,896][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[27246],
        [28111],
        [22773],
        [23982],
        [24823],
        [24660],
        [25854],
        [26271],
        [25138],
        [24784],
        [25372],
        [24746],
        [26004],
        [24116],
        [23844],
        [22274],
        [23687]], device='cuda:0')
[2024-07-24 10:17:42,898][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595],
        [18595]], device='cuda:0')
