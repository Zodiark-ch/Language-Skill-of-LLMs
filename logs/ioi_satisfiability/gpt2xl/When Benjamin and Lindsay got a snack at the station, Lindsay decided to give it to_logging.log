[2024-07-24 10:21:26,339][explain_satisfiability.py][line:287][INFO] ############ CASE TEXT isWhen Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:288][INFO] ############ CASE Prediction is  Benjamin
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:289][INFO] ############ Refined Forward Graph
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:290][INFO] ****** Layer 1
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 0
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,339][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 1
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 2
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 3
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,340][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 4
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 5
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 6
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,341][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 7
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 8
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 9
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 10
[2024-07-24 10:21:26,342][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 11
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 12
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 13
[2024-07-24 10:21:26,343][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 14
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 15
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 16
[2024-07-24 10:21:26,344][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit24', 'circuit27']
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 17
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 18
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 19
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,345][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 20
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 21
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 22
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 23
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit26']
[2024-07-24 10:21:26,346][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 24
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 25
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 26
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 27
[2024-07-24 10:21:26,347][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 28
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 0
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 1
[2024-07-24 10:21:26,348][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 2
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 3
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,349][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 4
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit8']
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 5
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 6
[2024-07-24 10:21:26,350][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit9', 'circuit27']
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 7
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 8
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit26']
[2024-07-24 10:21:26,351][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 9
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 10
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 11
[2024-07-24 10:21:26,352][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 12
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit9']
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 13
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,353][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 14
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 15
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,354][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 16
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit13', 'circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 17
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 18
[2024-07-24 10:21:26,355][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 19
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 20
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,356][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 21
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 22
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit18', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 23
[2024-07-24 10:21:26,357][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 24
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit19']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 25
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit9', 'circuit11', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,358][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 26
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 27
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 28
[2024-07-24 10:21:26,359][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 0
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit3', 'circuit6', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 1
[2024-07-24 10:21:26,360][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit26']
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 2
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,361][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 3
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 4
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit24']
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,362][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 5
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 6
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit20', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit8', 'circuit15']
[2024-07-24 10:21:26,363][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit26']
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 7
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 8
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit22']
[2024-07-24 10:21:26,364][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 9
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21']
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 10
[2024-07-24 10:21:26,365][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 11
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit26']
[2024-07-24 10:21:26,366][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 12
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21']
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 13
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,367][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 14
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24']
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit23']
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 15
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit19']
[2024-07-24 10:21:26,368][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit22']
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 16
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit2', 'circuit4', 'circuit8']
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17']
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 17
[2024-07-24 10:21:26,369][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 18
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,370][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 19
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 20
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,371][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 21
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 22
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,372][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 23
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 24
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,373][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14']
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit7']
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 25
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 26
[2024-07-24 10:21:26,374][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 27
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,375][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 28
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 0
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,376][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 1
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22']
[2024-07-24 10:21:26,377][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 2
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit2', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20']
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 3
[2024-07-24 10:21:26,378][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit18', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 4
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit15']
[2024-07-24 10:21:26,379][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 5
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit7']
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,380][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 6
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23']
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 7
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:21:26,381][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 8
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit26']
[2024-07-24 10:21:26,382][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 9
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit27']
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit26']
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit5', 'circuit6']
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 10
[2024-07-24 10:21:26,383][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 11
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit24']
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,384][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 12
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,385][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 13
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 14
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,386][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 15
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,387][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 16
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 17
[2024-07-24 10:21:26,388][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 18
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,389][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 19
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,390][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 20
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 21
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,391][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 22
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,392][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 23
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 24
[2024-07-24 10:21:26,393][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 25
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,394][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 26
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,395][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 27
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 28
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,396][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 0
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,397][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 1
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit18']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit26']
[2024-07-24 10:21:26,398][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 2
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit19', 'circuit24']
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17']
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,399][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 3
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit22', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit3', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,400][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 4
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 5
[2024-07-24 10:21:26,401][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit9', 'circuit11']
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit26']
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 6
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,402][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit24']
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 7
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,403][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit2', 'circuit3', 'circuit13', 'circuit14']
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 8
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,404][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit24', 'circuit27']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit24']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 9
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit5', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,405][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 10
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit6', 'circuit12', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,406][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 11
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit3', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13']
[2024-07-24 10:21:26,407][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 12
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit3', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit20', 'circuit21']
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18']
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit20']
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 13
[2024-07-24 10:21:26,408][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 14
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit16', 'circuit24']
[2024-07-24 10:21:26,409][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 15
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,410][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 16
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,411][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 17
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,412][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 18
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,413][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 19
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 20
[2024-07-24 10:21:26,414][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 21
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,415][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 22
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,416][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 23
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,417][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 24
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,418][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 25
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,419][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 26
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 27
[2024-07-24 10:21:26,420][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 28
[2024-07-24 10:21:26,421][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 0
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,422][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 1
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:21:26,423][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit20']
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit23']
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14']
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 2
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,424][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit25']
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 3
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,425][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit19', 'circuit20', 'circuit23']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 4
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,426][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit14', 'circuit16', 'circuit21']
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit27']
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 5
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit6', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,427][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 6
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,428][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit8', 'circuit12', 'circuit14', 'circuit15', 'circuit16']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 7
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,429][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 8
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17']
[2024-07-24 10:21:26,430][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit12', 'circuit13', 'circuit26']
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 9
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,431][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 10
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,432][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit25', 'circuit27']
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit19', 'circuit21', 'circuit23']
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 11
[2024-07-24 10:21:26,433][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 12
[2024-07-24 10:21:26,434][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 13
[2024-07-24 10:21:26,435][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 14
[2024-07-24 10:21:26,436][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 15
[2024-07-24 10:21:26,437][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 16
[2024-07-24 10:21:26,438][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 17
[2024-07-24 10:21:26,439][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 18
[2024-07-24 10:21:26,440][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 19
[2024-07-24 10:21:26,441][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 20
[2024-07-24 10:21:26,442][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 21
[2024-07-24 10:21:26,443][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 22
[2024-07-24 10:21:26,444][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 23
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,445][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 24
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,446][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 25
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,447][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 26
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,448][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 27
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,449][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 28
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,450][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 0
[2024-07-24 10:21:26,451][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit4', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,452][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 1
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit16', 'circuit24']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,453][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 2
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,454][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit23']
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 3
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,455][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit27']
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 4
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14']
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:21:26,456][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 5
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit26']
[2024-07-24 10:21:26,457][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit18', 'circuit19', 'circuit22']
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 6
[2024-07-24 10:21:26,458][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit22']
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 7
[2024-07-24 10:21:26,459][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit6', 'circuit13', 'circuit16', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit3', 'circuit5', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:21:26,460][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 8
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit13', 'circuit26']
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,461][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 9
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,462][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit26']
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 10
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20']
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3']
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,463][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 11
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,464][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit10']
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 12
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22']
[2024-07-24 10:21:26,465][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 13
[2024-07-24 10:21:26,466][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit10', 'circuit11', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,467][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 14
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,468][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 15
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,469][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 16
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,470][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 17
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,471][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 18
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,472][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 19
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,473][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 20
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,474][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 21
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,475][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 22
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,476][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 23
[2024-07-24 10:21:26,477][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,478][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 24
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,479][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 25
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,480][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 26
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,481][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 27
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,482][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 28
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,483][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,484][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 0
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,485][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit26']
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 1
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,486][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 2
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,487][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 3
[2024-07-24 10:21:26,488][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,489][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 4
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,490][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 5
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,491][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 6
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,492][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,493][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 7
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,494][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 8
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,495][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 9
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,496][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 10
[2024-07-24 10:21:26,497][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,498][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 11
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,499][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 12
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,500][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 13
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,501][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit13', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit26']
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17']
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,502][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 14
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,503][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 15
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,504][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 16
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,505][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 17
[2024-07-24 10:21:26,506][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,507][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 18
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,508][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 19
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,509][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 20
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,510][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,511][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 21
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,512][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 22
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,513][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 23
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,514][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 24
[2024-07-24 10:21:26,515][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,516][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 25
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,517][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 26
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,518][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 27
[2024-07-24 10:21:26,519][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,520][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 28
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,521][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 0
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit22', 'circuit26']
[2024-07-24 10:21:26,522][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit13', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit9', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit3', 'circuit7', 'circuit11', 'circuit13', 'circuit18', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,523][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 1
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,524][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 2
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,525][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 3
[2024-07-24 10:21:26,526][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,527][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 4
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,528][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 5
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,529][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,530][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 6
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,531][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 7
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,532][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,533][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 8
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,534][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 9
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,535][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 10
[2024-07-24 10:21:26,536][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,537][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 11
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,538][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 12
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,539][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,540][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 13
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,541][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit25']
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 14
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,542][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,543][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 15
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,544][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 16
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,545][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,546][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 17
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,547][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 18
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,548][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 19
[2024-07-24 10:21:26,549][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,550][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 20
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,551][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 21
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,552][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,553][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 22
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,554][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 23
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,555][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,556][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 24
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,557][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 25
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,558][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 26
[2024-07-24 10:21:26,559][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,560][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 27
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,561][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 28
[2024-07-24 10:21:26,562][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,563][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 0
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,564][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit17', 'circuit19', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit21', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit13', 'circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit18', 'circuit22']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit18', 'circuit19', 'circuit24']
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 1
[2024-07-24 10:21:26,565][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,566][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 2
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,567][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,568][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 3
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,569][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 4
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,570][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,571][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 5
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,572][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 6
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,573][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,574][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 7
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,575][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 8
[2024-07-24 10:21:26,576][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,577][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 9
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,578][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,579][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 10
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,580][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 11
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,581][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,582][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 12
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,583][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 13
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,584][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,585][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 14
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,586][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,587][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 15
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,588][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 16
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,589][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,590][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 17
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,591][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 18
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,592][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,593][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 19
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,594][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 20
[2024-07-24 10:21:26,595][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,596][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 21
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,597][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,598][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 22
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,599][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 23
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,600][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,601][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 24
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,602][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 25
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,603][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,604][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 26
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,605][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 27
[2024-07-24 10:21:26,606][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,607][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 28
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,608][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,609][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 0
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,610][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit18', 'circuit19', 'circuit20', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit26']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit18', 'circuit22', 'circuit23']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit25']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit18', 'circuit25']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:21:26,611][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 1
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,612][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 2
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,613][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,614][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 3
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,615][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15']
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit8']
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit10', 'circuit15']
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit7', 'circuit13']
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit11', 'circuit13', 'circuit18', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,616][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 4
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,617][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 5
[2024-07-24 10:21:26,618][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit25']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:21:26,619][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 6
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,620][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,621][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 7
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22']
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,622][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit23']
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit19']
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,623][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 8
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,624][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 9
[2024-07-24 10:21:26,625][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit26']
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,626][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 10
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,627][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,628][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 11
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,629][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,630][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 12
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,631][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit20', 'circuit24']
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit25']
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 13
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:21:26,632][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,633][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit19']
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 14
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,634][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,635][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 15
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,636][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 16
[2024-07-24 10:21:26,637][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,638][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 17
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,639][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,640][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 18
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,641][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,642][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 19
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,643][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 20
[2024-07-24 10:21:26,644][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,645][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 21
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,646][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,647][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 22
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,648][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,649][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 23
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,650][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 24
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,651][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,652][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 25
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:21:26,653][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:21:26,654][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 26
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,655][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 27
[2024-07-24 10:21:26,656][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,657][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 28
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,658][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,659][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,660][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,660][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:26,660][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:21:28,153][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:28,154][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,155][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,157][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,158][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,159][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,160][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,161][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,162][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,162][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,163][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,165][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,166][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,169][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.8190, 0.1810], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,169][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([4.3868e-04, 9.9956e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,171][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.8836, 0.1164], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,172][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.1070, 0.8930], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,174][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.3585, 0.6415], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,175][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.0251, 0.9749], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,176][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.6306, 0.3694], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,178][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.9302, 0.0698], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,179][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.7381, 0.2619], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,180][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.9480, 0.0520], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,182][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.6711, 0.3289], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,183][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.7439, 0.2561], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,185][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.8127, 0.1250, 0.0623], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,185][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ and] are: tensor([4.2472e-03, 4.6803e-04, 9.9528e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,187][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.3997, 0.0622, 0.5380], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,188][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2673, 0.0117, 0.7210], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,189][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.6703, 0.1204, 0.2093], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,191][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.2163, 0.0137, 0.7700], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,192][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5570, 0.4194, 0.0236], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,194][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.5133, 0.2425, 0.2442], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,195][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2499, 0.0423, 0.7078], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,196][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6141, 0.1271, 0.2588], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,198][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.6463, 0.1042, 0.2494], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,199][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.5392, 0.1174, 0.3434], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,200][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.5038, 0.1172, 0.3195, 0.0594], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,201][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([7.0346e-05, 2.2541e-04, 2.5500e-04, 9.9945e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,203][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.5684, 0.0922, 0.1130, 0.2263], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,204][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([1.3774e-02, 2.3289e-03, 1.2616e-04, 9.8377e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,205][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0324, 0.0218, 0.0021, 0.9437], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,206][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([3.6536e-03, 1.9428e-05, 4.8830e-07, 9.9633e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,206][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.4312, 0.2078, 0.1921, 0.1689], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,207][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.3813, 0.2544, 0.3033, 0.0610], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,207][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.4581, 0.2827, 0.2008, 0.0583], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,207][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.5108, 0.2402, 0.2223, 0.0267], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,208][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.4235, 0.0985, 0.1842, 0.2938], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,208][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.3136, 0.1698, 0.3738, 0.1428], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,208][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.4474, 0.0637, 0.1016, 0.1881, 0.1992], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,209][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ got] are: tensor([4.9936e-04, 4.8600e-04, 9.1839e-04, 4.4170e-04, 9.9765e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,209][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.5436, 0.0846, 0.1830, 0.0677, 0.1212], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,210][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0574, 0.0026, 0.0131, 0.0062, 0.9207], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,211][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.3927, 0.0367, 0.0474, 0.0244, 0.4988], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,212][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ got] are: tensor([1.2127e-02, 6.6715e-05, 9.5214e-05, 4.0820e-05, 9.8767e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,213][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.2484, 0.1719, 0.0292, 0.4863, 0.0643], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,215][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.2915, 0.0664, 0.2406, 0.1537, 0.2479], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,216][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.3768, 0.0868, 0.3695, 0.0590, 0.1078], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,217][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.4393, 0.1231, 0.2351, 0.0839, 0.1187], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,219][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.3938, 0.0884, 0.2007, 0.0363, 0.2807], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,220][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.4732, 0.1047, 0.2291, 0.0815, 0.1114], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,221][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.5167, 0.1154, 0.0502, 0.1264, 0.1545, 0.0368], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,223][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0034, 0.0027, 0.0060, 0.0020, 0.0014, 0.9844], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,224][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.4177, 0.0690, 0.2433, 0.1062, 0.1136, 0.0503], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,226][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0710, 0.0102, 0.0385, 0.0276, 0.1007, 0.7520], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,227][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2154, 0.0488, 0.0422, 0.0303, 0.4792, 0.1840], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,228][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1214, 0.0213, 0.1327, 0.0021, 0.0322, 0.6903], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,230][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.2633, 0.3004, 0.0113, 0.2661, 0.1473, 0.0117], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,231][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1576, 0.0499, 0.1233, 0.1907, 0.2105, 0.2681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,233][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0528, 0.0112, 0.2307, 0.0088, 0.0571, 0.6394], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,234][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.3760, 0.0861, 0.2069, 0.0825, 0.0962, 0.1524], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,236][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.3458, 0.0938, 0.2040, 0.0517, 0.0656, 0.2390], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,237][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.3495, 0.1114, 0.1773, 0.1057, 0.1231, 0.1331], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,238][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.2247, 0.1418, 0.1655, 0.1435, 0.0607, 0.1139, 0.1499],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,239][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([3.8405e-04, 2.5761e-03, 4.3679e-04, 2.0781e-03, 1.2098e-03, 1.0753e-04,
        9.9321e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,241][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.4054, 0.1111, 0.1271, 0.0627, 0.0584, 0.1782, 0.0571],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,242][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([1.6144e-02, 1.4541e-03, 7.1402e-05, 4.3317e-03, 9.2576e-04, 2.7933e-04,
        9.7679e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,243][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0804, 0.0205, 0.0034, 0.0183, 0.0137, 0.0090, 0.8548],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,244][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([9.4604e-04, 3.6293e-04, 1.0198e-07, 1.2271e-05, 5.0086e-06, 4.7798e-08,
        9.9867e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,245][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.2643, 0.1408, 0.1366, 0.1190, 0.0213, 0.0840, 0.2338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,247][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0989, 0.0286, 0.0941, 0.0250, 0.3558, 0.2896, 0.1081],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,248][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.3354, 0.1625, 0.1534, 0.0554, 0.0872, 0.1243, 0.0819],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,250][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.3925, 0.1315, 0.1639, 0.0814, 0.0896, 0.1156, 0.0254],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,251][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.2673, 0.0979, 0.1441, 0.0371, 0.0589, 0.0860, 0.3088],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,253][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.2221, 0.1106, 0.1684, 0.0587, 0.1454, 0.1207, 0.1740],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,254][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.2135, 0.0676, 0.0348, 0.0600, 0.1277, 0.0486, 0.4257, 0.0222],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,255][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ at] are: tensor([1.6117e-03, 1.4750e-03, 5.4656e-03, 3.6795e-03, 1.2684e-04, 1.8733e-03,
        1.2397e-04, 9.8564e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,256][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.3150, 0.0778, 0.1655, 0.0366, 0.1264, 0.0587, 0.0797, 0.1402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,257][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ at] are: tensor([4.0007e-03, 8.3742e-04, 2.7532e-03, 3.1116e-03, 1.2689e-02, 3.7476e-02,
        2.7309e-02, 9.1182e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,259][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1366, 0.0121, 0.0208, 0.0396, 0.2032, 0.0796, 0.1833, 0.3247],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,260][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ at] are: tensor([7.7339e-02, 6.0379e-03, 1.4683e-02, 3.6152e-03, 7.9399e-03, 1.8338e-02,
        8.2652e-04, 8.7122e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,261][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1305, 0.1706, 0.0148, 0.2030, 0.0839, 0.0174, 0.3608, 0.0191],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,263][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0845, 0.0152, 0.0550, 0.0455, 0.0998, 0.1990, 0.1917, 0.3093],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,264][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0629, 0.0115, 0.3096, 0.0105, 0.0668, 0.3355, 0.0413, 0.1619],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,264][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.2735, 0.0828, 0.1716, 0.0544, 0.0890, 0.1293, 0.0603, 0.1392],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,265][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.2291, 0.0724, 0.1704, 0.0521, 0.0565, 0.1215, 0.0482, 0.2498],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,265][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.3159, 0.0686, 0.1222, 0.0760, 0.0765, 0.0645, 0.1187, 0.1576],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,265][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.4116, 0.0798, 0.0291, 0.1051, 0.1014, 0.0233, 0.2015, 0.0224, 0.0257],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,266][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.2882e-03, 1.0579e-03, 5.7319e-02, 1.2968e-03, 1.7140e-04, 4.4543e-02,
        2.2362e-05, 5.6535e-02, 8.2977e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,266][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3091, 0.0533, 0.1341, 0.0451, 0.1311, 0.0393, 0.0453, 0.2051, 0.0376],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,267][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0284, 0.0032, 0.0061, 0.0054, 0.0167, 0.0705, 0.0354, 0.1868, 0.6476],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,267][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2000, 0.0293, 0.0271, 0.0193, 0.1441, 0.0624, 0.1285, 0.2144, 0.1749],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,268][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1258, 0.0318, 0.1211, 0.0137, 0.0592, 0.2330, 0.0050, 0.0740, 0.3367],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,269][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1494, 0.2075, 0.0039, 0.2000, 0.0679, 0.0040, 0.3583, 0.0067, 0.0023],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,271][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0743, 0.0123, 0.0325, 0.0554, 0.0552, 0.0852, 0.1182, 0.2791, 0.2878],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,272][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0196, 0.0051, 0.1378, 0.0037, 0.0293, 0.3241, 0.0188, 0.0642, 0.3974],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,274][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2699, 0.0663, 0.1389, 0.0607, 0.0756, 0.1041, 0.0529, 0.1177, 0.1138],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,275][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2787, 0.0616, 0.1472, 0.0530, 0.0454, 0.1317, 0.0325, 0.0848, 0.1650],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,276][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2138, 0.0714, 0.1023, 0.0992, 0.0698, 0.1065, 0.1188, 0.1010, 0.1172],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,278][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.1811, 0.1570, 0.0888, 0.0859, 0.0514, 0.0767, 0.1619, 0.0590, 0.0697,
        0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,279][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ station] are: tensor([5.2177e-04, 3.8082e-03, 3.4478e-03, 1.8950e-03, 3.4469e-04, 9.4573e-05,
        1.1427e-03, 4.2261e-03, 3.6613e-04, 9.8415e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,280][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.1827, 0.1548, 0.0643, 0.1220, 0.0658, 0.0658, 0.0612, 0.0789, 0.0557,
        0.1487], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,281][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ station] are: tensor([8.0337e-03, 1.4468e-04, 5.3090e-05, 8.4627e-04, 4.7653e-04, 1.5979e-04,
        7.0140e-03, 1.2964e-03, 8.8245e-04, 9.8109e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,283][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0425, 0.0065, 0.0034, 0.0062, 0.0208, 0.0070, 0.0424, 0.0246, 0.0170,
        0.8297], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,284][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ station] are: tensor([1.1860e-02, 8.6550e-04, 1.9749e-05, 7.4416e-04, 1.3637e-05, 4.2345e-06,
        5.7644e-05, 2.3343e-05, 3.0132e-06, 9.8641e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,285][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.1519, 0.2299, 0.0283, 0.1029, 0.0182, 0.0303, 0.2355, 0.0116, 0.0226,
        0.1687], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,286][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0447, 0.0025, 0.0256, 0.0139, 0.0946, 0.0881, 0.0120, 0.2227, 0.3358,
        0.1600], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,288][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.1637, 0.0791, 0.1144, 0.0149, 0.0453, 0.0810, 0.2022, 0.0841, 0.0789,
        0.1365], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,289][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.2109, 0.1578, 0.1102, 0.0626, 0.0648, 0.0945, 0.0989, 0.0869, 0.0994,
        0.0139], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,291][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.1267, 0.0682, 0.1140, 0.0545, 0.0373, 0.0786, 0.0798, 0.1020, 0.0783,
        0.2605], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,292][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.1864, 0.0621, 0.0850, 0.0762, 0.0654, 0.0554, 0.1675, 0.0932, 0.0398,
        0.1691], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,294][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.3384, 0.0555, 0.0125, 0.0764, 0.0878, 0.0223, 0.2648, 0.0149, 0.0401,
        0.0783, 0.0091], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,295][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [,] are: tensor([4.0965e-03, 2.6898e-04, 2.0234e-02, 1.5540e-04, 2.6252e-04, 3.2647e-04,
        6.1341e-05, 2.4369e-03, 9.4248e-04, 8.9537e-05, 9.7113e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,296][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2937, 0.0235, 0.1141, 0.0171, 0.0280, 0.0102, 0.0201, 0.0545, 0.0127,
        0.0112, 0.4150], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,297][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [,] are: tensor([3.4656e-02, 5.1730e-04, 2.6972e-03, 1.0463e-03, 9.4753e-03, 1.2221e-02,
        1.1216e-02, 5.0661e-02, 1.3013e-01, 6.7453e-02, 6.7993e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,299][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.3918, 0.0268, 0.0120, 0.0085, 0.0361, 0.0150, 0.0453, 0.0806, 0.0388,
        0.1011, 0.2439], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,300][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1195, 0.0352, 0.1734, 0.0070, 0.0377, 0.1300, 0.0154, 0.1102, 0.1223,
        0.0159, 0.2334], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,302][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2005, 0.1951, 0.0074, 0.1568, 0.0550, 0.0086, 0.2204, 0.0120, 0.0051,
        0.1342, 0.0048], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,303][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0356, 0.0114, 0.0151, 0.0368, 0.0292, 0.0435, 0.0598, 0.1092, 0.1726,
        0.1489, 0.3380], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,305][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0291, 0.0060, 0.1520, 0.0049, 0.0155, 0.1430, 0.0122, 0.0616, 0.2185,
        0.0119, 0.3452], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,306][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.2322, 0.0731, 0.1205, 0.0493, 0.0683, 0.0837, 0.0476, 0.0968, 0.0897,
        0.0440, 0.0949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,307][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.1865, 0.0686, 0.1432, 0.0505, 0.0626, 0.0910, 0.0615, 0.1039, 0.0885,
        0.0377, 0.1060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,309][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2035, 0.0634, 0.0970, 0.0706, 0.0703, 0.0593, 0.0928, 0.1069, 0.0505,
        0.0791, 0.1065], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,310][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.2045, 0.0482, 0.1159, 0.0291, 0.0181, 0.0555, 0.2243, 0.0715, 0.0691,
        0.0375, 0.0982, 0.0282], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,311][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([3.7561e-05, 7.5845e-05, 8.9030e-05, 5.6123e-01, 4.9667e-05, 3.9072e-05,
        1.9531e-04, 6.3287e-04, 2.7881e-05, 6.9447e-05, 1.7252e-06, 4.3755e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,313][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.2051, 0.0434, 0.0499, 0.1302, 0.0323, 0.0670, 0.1211, 0.0417, 0.0840,
        0.0574, 0.0414, 0.1264], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,314][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([5.8114e-04, 1.2310e-05, 2.9959e-07, 4.5568e-03, 4.2840e-06, 2.8844e-06,
        9.9339e-05, 2.0350e-05, 2.6940e-05, 1.3270e-03, 8.2943e-05, 9.9329e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,315][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([4.1262e-03, 1.4190e-03, 1.6313e-04, 7.0355e-02, 6.5467e-04, 2.4181e-04,
        6.1824e-03, 1.6455e-03, 5.2814e-04, 2.1138e-03, 1.6283e-03, 9.1094e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,316][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([7.5523e-04, 5.6705e-06, 9.0018e-08, 6.7782e-01, 7.5504e-07, 9.4313e-09,
        5.5385e-07, 1.4960e-07, 1.3861e-08, 3.0859e-06, 1.6065e-09, 3.2141e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,317][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.2046, 0.1116, 0.1017, 0.1141, 0.0231, 0.0559, 0.0538, 0.0239, 0.0818,
        0.0425, 0.0873, 0.0996], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,319][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0940, 0.0253, 0.0268, 0.0067, 0.0245, 0.0567, 0.0045, 0.1489, 0.1618,
        0.0717, 0.3136, 0.0655], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,320][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.1992, 0.1571, 0.1030, 0.0390, 0.0515, 0.0890, 0.0706, 0.0599, 0.0838,
        0.0303, 0.0798, 0.0367], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,322][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.2200, 0.1459, 0.1090, 0.0146, 0.0493, 0.0747, 0.0648, 0.0724, 0.0844,
        0.0632, 0.0888, 0.0129], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,322][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.1155, 0.0413, 0.0749, 0.2577, 0.0336, 0.0590, 0.0300, 0.0577, 0.0552,
        0.0220, 0.0514, 0.2018], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,322][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0969, 0.0643, 0.1019, 0.0615, 0.0895, 0.0791, 0.0657, 0.0993, 0.0759,
        0.0713, 0.1116, 0.0829], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,323][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.3184, 0.0339, 0.0354, 0.0695, 0.0670, 0.0318, 0.0683, 0.0374, 0.0388,
        0.0364, 0.0430, 0.0871, 0.1330], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,323][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([4.2182e-04, 5.0818e-04, 2.0358e-04, 1.6453e-04, 1.2764e-02, 4.1493e-04,
        8.8317e-05, 1.3325e-04, 9.4586e-05, 2.4174e-04, 5.3981e-05, 7.9002e-05,
        9.8483e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,324][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.2758, 0.0499, 0.0846, 0.0418, 0.1314, 0.0655, 0.0323, 0.0707, 0.0699,
        0.0438, 0.0317, 0.0385, 0.0641], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,324][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([9.1902e-04, 7.2753e-06, 8.9513e-06, 9.7848e-06, 9.8127e-05, 2.6810e-05,
        1.4240e-04, 1.0288e-04, 1.5227e-04, 5.9376e-04, 9.5916e-04, 8.1208e-04,
        9.9617e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,325][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0388, 0.0039, 0.0049, 0.0026, 0.0076, 0.0069, 0.0115, 0.0232, 0.0143,
        0.0310, 0.0261, 0.0152, 0.8140], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,325][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([1.1286e-02, 1.3683e-04, 4.4803e-05, 1.6294e-05, 2.1540e-03, 5.7190e-06,
        1.7899e-05, 1.9072e-05, 1.8323e-06, 1.0443e-05, 1.9813e-06, 4.4282e-06,
        9.8630e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,327][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.1584, 0.1145, 0.0217, 0.1111, 0.0423, 0.0273, 0.1304, 0.0192, 0.0176,
        0.0997, 0.0172, 0.1311, 0.1094], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,328][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0428, 0.0019, 0.0120, 0.0055, 0.0214, 0.0398, 0.0155, 0.0709, 0.1105,
        0.0813, 0.2872, 0.0874, 0.2238], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,329][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.1511, 0.0283, 0.1191, 0.0276, 0.0817, 0.1205, 0.0182, 0.0984, 0.1566,
        0.0413, 0.0989, 0.0327, 0.0257], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,331][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1949, 0.0654, 0.0874, 0.0525, 0.0726, 0.0663, 0.0690, 0.0692, 0.0725,
        0.0513, 0.0810, 0.0593, 0.0586], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,332][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.1786, 0.0501, 0.0815, 0.0317, 0.0753, 0.0686, 0.0421, 0.0583, 0.0684,
        0.0267, 0.0651, 0.0278, 0.2259], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,334][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.3078, 0.0528, 0.0759, 0.0372, 0.0439, 0.0410, 0.0845, 0.1018, 0.0307,
        0.0491, 0.0822, 0.0370, 0.0560], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,335][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.2098, 0.0378, 0.0204, 0.0599, 0.0687, 0.0198, 0.1543, 0.0179, 0.0272,
        0.0540, 0.0239, 0.0845, 0.2073, 0.0147], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,336][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([5.7891e-03, 1.8372e-04, 3.9635e-02, 1.9617e-04, 4.6667e-04, 1.6733e-03,
        5.6473e-05, 1.9505e-02, 1.9044e-03, 1.9529e-04, 8.6282e-03, 1.2123e-04,
        2.9691e-05, 9.2162e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,338][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2025, 0.0262, 0.1027, 0.0284, 0.0557, 0.0268, 0.0314, 0.1404, 0.0296,
        0.0163, 0.0638, 0.0280, 0.0691, 0.1792], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,339][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([6.1066e-03, 1.7274e-04, 2.8097e-04, 7.9002e-05, 8.1721e-04, 1.6108e-03,
        9.8127e-04, 3.4968e-03, 1.5044e-02, 2.9284e-03, 3.6965e-02, 6.0838e-03,
        9.8946e-02, 8.2649e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,340][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.2058e-02, 1.2349e-03, 1.3477e-03, 6.7070e-04, 1.9540e-02, 3.7262e-03,
        8.5607e-03, 7.8188e-03, 6.2015e-03, 1.3655e-02, 9.1604e-03, 6.1404e-03,
        8.1136e-01, 9.8530e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,341][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([4.3637e-02, 1.5902e-02, 8.9145e-02, 7.7795e-04, 4.9550e-02, 2.1246e-01,
        1.7378e-03, 2.0805e-02, 1.7467e-01, 1.8862e-03, 1.7767e-02, 2.8540e-04,
        5.6972e-03, 3.6568e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,342][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0858, 0.0779, 0.0046, 0.0840, 0.0404, 0.0039, 0.0957, 0.0169, 0.0032,
        0.0712, 0.0039, 0.1369, 0.1236, 0.2522], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,344][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0281, 0.0037, 0.0059, 0.0057, 0.0099, 0.0119, 0.0130, 0.0255, 0.0423,
        0.0305, 0.1120, 0.0914, 0.3469, 0.2733], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,345][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0166, 0.0035, 0.0993, 0.0026, 0.0166, 0.1338, 0.0142, 0.0487, 0.2138,
        0.0122, 0.1790, 0.0037, 0.0076, 0.2483], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,347][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1586, 0.0450, 0.0954, 0.0383, 0.0554, 0.0774, 0.0401, 0.0797, 0.0836,
        0.0402, 0.0796, 0.0404, 0.0585, 0.1077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,348][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1297, 0.0361, 0.1167, 0.0336, 0.0670, 0.0966, 0.0356, 0.0883, 0.0880,
        0.0294, 0.0819, 0.0276, 0.0482, 0.1213], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,349][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1652, 0.0586, 0.0769, 0.0599, 0.0608, 0.0489, 0.0676, 0.0779, 0.0371,
        0.0613, 0.0800, 0.0623, 0.0649, 0.0786], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,351][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.1929, 0.0203, 0.0241, 0.0913, 0.0546, 0.0209, 0.1508, 0.0146, 0.0241,
        0.0558, 0.0210, 0.1194, 0.1499, 0.0220, 0.0383], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,352][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ give] are: tensor([6.2464e-04, 1.0738e-03, 5.1183e-04, 1.8015e-04, 4.3813e-03, 4.6065e-04,
        4.6476e-04, 7.7454e-04, 2.1318e-04, 4.5889e-05, 1.6713e-04, 1.0317e-04,
        1.3254e-03, 5.6342e-04, 9.8911e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,354][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.2063, 0.0295, 0.0562, 0.0225, 0.0950, 0.0446, 0.0434, 0.0843, 0.0549,
        0.0171, 0.0362, 0.0209, 0.1165, 0.1190, 0.0536], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,355][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ give] are: tensor([8.9627e-04, 7.8288e-06, 4.9681e-06, 1.9680e-06, 3.2040e-05, 3.7239e-05,
        5.2650e-05, 6.5410e-05, 2.1141e-04, 8.8583e-05, 9.0922e-04, 1.9007e-04,
        8.8097e-03, 8.6102e-03, 9.8008e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,356][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0441, 0.0083, 0.0063, 0.0031, 0.0098, 0.0096, 0.0160, 0.0234, 0.0161,
        0.0122, 0.0276, 0.0175, 0.1298, 0.1222, 0.5541], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,357][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ give] are: tensor([9.2131e-03, 2.2886e-03, 5.0052e-05, 1.9425e-05, 1.4805e-03, 4.0115e-05,
        4.4480e-04, 3.3073e-05, 2.2188e-05, 3.9029e-07, 3.1369e-06, 5.0144e-06,
        1.6442e-04, 8.3793e-06, 9.8623e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,359][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.1186, 0.0823, 0.0129, 0.1010, 0.0291, 0.0143, 0.2115, 0.0121, 0.0123,
        0.1399, 0.0124, 0.1272, 0.0900, 0.0114, 0.0248], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,360][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0264, 0.0025, 0.0059, 0.0036, 0.0090, 0.0127, 0.0072, 0.0251, 0.0327,
        0.0158, 0.0903, 0.0489, 0.1514, 0.3241, 0.2443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,362][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0784, 0.0164, 0.0929, 0.0209, 0.0770, 0.0762, 0.0554, 0.0675, 0.0970,
        0.0404, 0.1010, 0.0269, 0.0436, 0.1605, 0.0459], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,363][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.1343, 0.0500, 0.0777, 0.0348, 0.0625, 0.0664, 0.0585, 0.0645, 0.0670,
        0.0472, 0.0726, 0.0387, 0.0731, 0.0957, 0.0570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,365][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.1083, 0.0440, 0.0704, 0.0201, 0.0773, 0.0613, 0.0495, 0.0678, 0.0583,
        0.0240, 0.0614, 0.0190, 0.0629, 0.0822, 0.1934], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,366][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.2614, 0.0389, 0.0894, 0.0421, 0.0518, 0.0262, 0.0508, 0.0786, 0.0202,
        0.0525, 0.0630, 0.0387, 0.0493, 0.0619, 0.0752], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,368][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.2696, 0.0470, 0.0197, 0.0590, 0.0469, 0.0123, 0.1025, 0.0160, 0.0181,
        0.0423, 0.0223, 0.0790, 0.1429, 0.0138, 0.0722, 0.0363],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,369][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ it] are: tensor([3.1316e-03, 9.2131e-04, 2.0966e-03, 4.9999e-04, 2.5779e-04, 3.3383e-03,
        4.3261e-05, 6.0132e-04, 1.0975e-02, 8.6496e-05, 7.2888e-04, 3.4593e-04,
        1.1504e-04, 3.1576e-03, 4.1400e-04, 9.7329e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,370][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.1716, 0.0235, 0.0819, 0.0196, 0.1110, 0.0249, 0.0449, 0.0674, 0.0259,
        0.0242, 0.0453, 0.0189, 0.1336, 0.0988, 0.0755, 0.0332],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,371][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ it] are: tensor([8.5273e-04, 3.3332e-06, 1.4939e-05, 1.3792e-05, 5.9196e-05, 1.0301e-04,
        2.9896e-05, 2.5979e-04, 8.7404e-04, 2.9685e-04, 3.2506e-03, 1.3216e-03,
        9.4370e-03, 1.8069e-02, 7.2851e-02, 8.9256e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,373][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0464, 0.0020, 0.0057, 0.0012, 0.0161, 0.0063, 0.0142, 0.0133, 0.0136,
        0.0160, 0.0251, 0.0062, 0.2770, 0.0934, 0.1966, 0.2671],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,374][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ it] are: tensor([2.6436e-02, 7.2256e-03, 4.1482e-03, 3.2213e-03, 8.0425e-03, 6.5319e-03,
        9.4445e-04, 1.8825e-03, 8.2516e-03, 4.4389e-04, 4.4639e-04, 9.2359e-04,
        1.0206e-03, 1.9027e-03, 5.6022e-04, 9.2802e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,375][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.1030, 0.0647, 0.0053, 0.1201, 0.0388, 0.0051, 0.1924, 0.0056, 0.0034,
        0.0914, 0.0043, 0.1811, 0.1240, 0.0072, 0.0468, 0.0067],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,376][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0263, 0.0013, 0.0041, 0.0021, 0.0061, 0.0068, 0.0063, 0.0166, 0.0192,
        0.0121, 0.0620, 0.0260, 0.0705, 0.1768, 0.3016, 0.2622],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,378][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0311, 0.0050, 0.0857, 0.0040, 0.0334, 0.1322, 0.0197, 0.0409, 0.1831,
        0.0158, 0.1220, 0.0050, 0.0147, 0.1119, 0.0259, 0.1698],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,379][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.1607, 0.0438, 0.0784, 0.0353, 0.0523, 0.0597, 0.0402, 0.0623, 0.0667,
        0.0284, 0.0685, 0.0377, 0.0476, 0.0878, 0.0595, 0.0711],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,380][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.1099, 0.0337, 0.0689, 0.0302, 0.0376, 0.0672, 0.0375, 0.0509, 0.0847,
        0.0237, 0.0551, 0.0259, 0.0397, 0.0670, 0.0420, 0.2260],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,380][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.2027, 0.0456, 0.0577, 0.0535, 0.0427, 0.0352, 0.0612, 0.0646, 0.0306,
        0.0583, 0.0685, 0.0534, 0.0540, 0.0605, 0.0637, 0.0478],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,381][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1866, 0.0320, 0.0167, 0.0516, 0.0600, 0.0167, 0.1347, 0.0143, 0.0228,
        0.0459, 0.0193, 0.0720, 0.1796, 0.0119, 0.0704, 0.0512, 0.0142],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,381][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.8636e-03, 7.7269e-05, 1.7566e-02, 9.1481e-05, 2.1393e-04, 7.1493e-04,
        2.7315e-05, 8.8253e-03, 8.9390e-04, 9.7980e-05, 4.6295e-03, 6.2686e-05,
        1.4755e-05, 4.6280e-01, 4.0136e-04, 6.5255e-04, 5.0006e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,382][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1555, 0.0201, 0.0778, 0.0213, 0.0443, 0.0203, 0.0227, 0.1073, 0.0225,
        0.0132, 0.0491, 0.0213, 0.0557, 0.1370, 0.0575, 0.0279, 0.1466],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,382][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([1.6394e-03, 2.7073e-05, 4.0770e-05, 1.1839e-05, 8.5007e-05, 1.4144e-04,
        9.1860e-05, 2.4133e-04, 9.9447e-04, 2.2161e-04, 2.5218e-03, 4.9692e-04,
        6.2112e-03, 4.7909e-02, 4.0161e-01, 9.8652e-02, 4.3911e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,383][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0134, 0.0011, 0.0012, 0.0006, 0.0147, 0.0025, 0.0055, 0.0046, 0.0035,
        0.0078, 0.0050, 0.0035, 0.4310, 0.0499, 0.2337, 0.0621, 0.1600],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,384][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.6770e-02, 1.0595e-02, 6.0953e-02, 5.5493e-04, 3.5507e-02, 1.5350e-01,
        1.1184e-03, 1.5332e-02, 1.2454e-01, 1.3050e-03, 1.1312e-02, 2.0358e-04,
        3.9189e-03, 2.5675e-01, 8.9530e-03, 5.2092e-02, 2.3660e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,385][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0621, 0.0549, 0.0031, 0.0603, 0.0290, 0.0026, 0.0693, 0.0119, 0.0021,
        0.0509, 0.0026, 0.0996, 0.0910, 0.1831, 0.0492, 0.0069, 0.2213],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,387][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0212, 0.0020, 0.0032, 0.0031, 0.0043, 0.0046, 0.0049, 0.0080, 0.0128,
        0.0093, 0.0339, 0.0290, 0.0972, 0.0716, 0.1488, 0.2635, 0.2827],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,388][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0119, 0.0025, 0.0695, 0.0019, 0.0119, 0.0920, 0.0105, 0.0337, 0.1449,
        0.0086, 0.1249, 0.0027, 0.0055, 0.1716, 0.0139, 0.0888, 0.2053],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,390][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1231, 0.0351, 0.0732, 0.0310, 0.0442, 0.0595, 0.0330, 0.0608, 0.0638,
        0.0329, 0.0618, 0.0328, 0.0474, 0.0825, 0.0559, 0.0727, 0.0903],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,391][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0919, 0.0254, 0.0814, 0.0267, 0.0517, 0.0720, 0.0291, 0.0681, 0.0679,
        0.0252, 0.0661, 0.0246, 0.0416, 0.0999, 0.0498, 0.0772, 0.1014],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,393][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1492, 0.0528, 0.0672, 0.0530, 0.0535, 0.0427, 0.0538, 0.0652, 0.0315,
        0.0497, 0.0635, 0.0515, 0.0525, 0.0641, 0.0512, 0.0336, 0.0651],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,404][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:28,406][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,407][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,408][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,409][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,410][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,410][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,411][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,411][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,411][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,412][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,412][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,412][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,412][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.8190, 0.1810], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,413][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([4.3868e-04, 9.9956e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,413][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.8836, 0.1164], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,413][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.1070, 0.8930], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,414][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.3585, 0.6415], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,414][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.0251, 0.9749], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,414][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.6306, 0.3694], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,415][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.9302, 0.0698], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,415][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.7381, 0.2619], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,416][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.9480, 0.0520], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,417][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.6711, 0.3289], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,419][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.7439, 0.2561], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,420][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.8127, 0.1250, 0.0623], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,421][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([4.2472e-03, 4.6803e-04, 9.9528e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,422][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.3997, 0.0622, 0.5380], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,424][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2673, 0.0117, 0.7210], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,425][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.6703, 0.1204, 0.2093], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,427][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.2163, 0.0137, 0.7700], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,428][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5570, 0.4194, 0.0236], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,430][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.5133, 0.2425, 0.2442], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,431][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.2499, 0.0423, 0.7078], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,432][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6141, 0.1271, 0.2588], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,433][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.6463, 0.1042, 0.2494], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,435][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.5392, 0.1174, 0.3434], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,436][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.5038, 0.1172, 0.3195, 0.0594], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,437][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([7.0346e-05, 2.2541e-04, 2.5500e-04, 9.9945e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,438][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.5684, 0.0922, 0.1130, 0.2263], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,439][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([1.3774e-02, 2.3289e-03, 1.2616e-04, 9.8377e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,441][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0324, 0.0218, 0.0021, 0.9437], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,442][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([3.6536e-03, 1.9428e-05, 4.8830e-07, 9.9633e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,443][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.4312, 0.2078, 0.1921, 0.1689], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,445][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.3813, 0.2544, 0.3033, 0.0610], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,446][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.4581, 0.2827, 0.2008, 0.0583], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,448][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.5108, 0.2402, 0.2223, 0.0267], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,449][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.4235, 0.0985, 0.1842, 0.2938], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,450][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.3136, 0.1698, 0.3738, 0.1428], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,452][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.4474, 0.0637, 0.1016, 0.1881, 0.1992], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,452][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([4.9936e-04, 4.8600e-04, 9.1839e-04, 4.4170e-04, 9.9765e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,453][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.5436, 0.0846, 0.1830, 0.0677, 0.1212], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,453][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0574, 0.0026, 0.0131, 0.0062, 0.9207], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,453][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.3927, 0.0367, 0.0474, 0.0244, 0.4988], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,454][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([1.2127e-02, 6.6715e-05, 9.5214e-05, 4.0820e-05, 9.8767e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,454][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.2484, 0.1719, 0.0292, 0.4863, 0.0643], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,454][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.2915, 0.0664, 0.2406, 0.1537, 0.2479], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,455][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.3768, 0.0868, 0.3695, 0.0590, 0.1078], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,455][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.4393, 0.1231, 0.2351, 0.0839, 0.1187], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,456][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.3938, 0.0884, 0.2007, 0.0363, 0.2807], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,457][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.4732, 0.1047, 0.2291, 0.0815, 0.1114], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,458][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.5167, 0.1154, 0.0502, 0.1264, 0.1545, 0.0368], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,460][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0034, 0.0027, 0.0060, 0.0020, 0.0014, 0.9844], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,461][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4177, 0.0690, 0.2433, 0.1062, 0.1136, 0.0503], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,462][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0710, 0.0102, 0.0385, 0.0276, 0.1007, 0.7520], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,463][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2154, 0.0488, 0.0422, 0.0303, 0.4792, 0.1840], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,465][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1214, 0.0213, 0.1327, 0.0021, 0.0322, 0.6903], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,466][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2633, 0.3004, 0.0113, 0.2661, 0.1473, 0.0117], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,468][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1576, 0.0499, 0.1233, 0.1907, 0.2105, 0.2681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,469][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0528, 0.0112, 0.2307, 0.0088, 0.0571, 0.6394], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,470][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.3760, 0.0861, 0.2069, 0.0825, 0.0962, 0.1524], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,472][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3458, 0.0938, 0.2040, 0.0517, 0.0656, 0.2390], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,473][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.3495, 0.1114, 0.1773, 0.1057, 0.1231, 0.1331], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,475][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.2247, 0.1418, 0.1655, 0.1435, 0.0607, 0.1139, 0.1499],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,475][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([3.8405e-04, 2.5761e-03, 4.3679e-04, 2.0781e-03, 1.2098e-03, 1.0753e-04,
        9.9321e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,477][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.4054, 0.1111, 0.1271, 0.0627, 0.0584, 0.1782, 0.0571],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,478][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([1.6144e-02, 1.4541e-03, 7.1402e-05, 4.3317e-03, 9.2576e-04, 2.7933e-04,
        9.7679e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,479][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0804, 0.0205, 0.0034, 0.0183, 0.0137, 0.0090, 0.8548],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,480][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([9.4604e-04, 3.6293e-04, 1.0198e-07, 1.2271e-05, 5.0086e-06, 4.7798e-08,
        9.9867e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,482][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.2643, 0.1408, 0.1366, 0.1190, 0.0213, 0.0840, 0.2338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,483][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0989, 0.0286, 0.0941, 0.0250, 0.3558, 0.2896, 0.1081],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,485][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.3354, 0.1625, 0.1534, 0.0554, 0.0872, 0.1243, 0.0819],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,486][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.3925, 0.1315, 0.1639, 0.0814, 0.0896, 0.1156, 0.0254],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,487][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.2673, 0.0979, 0.1441, 0.0371, 0.0589, 0.0860, 0.3088],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,489][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.2221, 0.1106, 0.1684, 0.0587, 0.1454, 0.1207, 0.1740],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,490][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.2135, 0.0676, 0.0348, 0.0600, 0.1277, 0.0486, 0.4257, 0.0222],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,491][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([1.6117e-03, 1.4750e-03, 5.4656e-03, 3.6795e-03, 1.2684e-04, 1.8733e-03,
        1.2397e-04, 9.8564e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,493][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.3150, 0.0778, 0.1655, 0.0366, 0.1264, 0.0587, 0.0797, 0.1402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,493][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([4.0007e-03, 8.3742e-04, 2.7532e-03, 3.1116e-03, 1.2689e-02, 3.7476e-02,
        2.7309e-02, 9.1182e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,495][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1366, 0.0121, 0.0208, 0.0396, 0.2032, 0.0796, 0.1833, 0.3247],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,496][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([7.7339e-02, 6.0379e-03, 1.4683e-02, 3.6152e-03, 7.9399e-03, 1.8338e-02,
        8.2652e-04, 8.7122e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,497][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1305, 0.1706, 0.0148, 0.2030, 0.0839, 0.0174, 0.3608, 0.0191],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,499][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0845, 0.0152, 0.0550, 0.0455, 0.0998, 0.1990, 0.1917, 0.3093],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,500][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0629, 0.0115, 0.3096, 0.0105, 0.0668, 0.3355, 0.0413, 0.1619],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,502][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.2735, 0.0828, 0.1716, 0.0544, 0.0890, 0.1293, 0.0603, 0.1392],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,503][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.2291, 0.0724, 0.1704, 0.0521, 0.0565, 0.1215, 0.0482, 0.2498],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,505][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.3159, 0.0686, 0.1222, 0.0760, 0.0765, 0.0645, 0.1187, 0.1576],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,506][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.4116, 0.0798, 0.0291, 0.1051, 0.1014, 0.0233, 0.2015, 0.0224, 0.0257],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,507][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([9.2882e-03, 1.0579e-03, 5.7319e-02, 1.2968e-03, 1.7140e-04, 4.4543e-02,
        2.2362e-05, 5.6535e-02, 8.2977e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,508][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3091, 0.0533, 0.1341, 0.0451, 0.1311, 0.0393, 0.0453, 0.2051, 0.0376],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,510][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0284, 0.0032, 0.0061, 0.0054, 0.0167, 0.0705, 0.0354, 0.1868, 0.6476],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,510][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2000, 0.0293, 0.0271, 0.0193, 0.1441, 0.0624, 0.1285, 0.2144, 0.1749],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,511][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1258, 0.0318, 0.1211, 0.0137, 0.0592, 0.2330, 0.0050, 0.0740, 0.3367],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,511][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1494, 0.2075, 0.0039, 0.2000, 0.0679, 0.0040, 0.3583, 0.0067, 0.0023],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,511][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0743, 0.0123, 0.0325, 0.0554, 0.0552, 0.0852, 0.1182, 0.2791, 0.2878],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,512][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0196, 0.0051, 0.1378, 0.0037, 0.0293, 0.3241, 0.0188, 0.0642, 0.3974],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,512][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2699, 0.0663, 0.1389, 0.0607, 0.0756, 0.1041, 0.0529, 0.1177, 0.1138],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,512][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2787, 0.0616, 0.1472, 0.0530, 0.0454, 0.1317, 0.0325, 0.0848, 0.1650],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,513][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2138, 0.0714, 0.1023, 0.0992, 0.0698, 0.1065, 0.1188, 0.1010, 0.1172],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,513][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.1811, 0.1570, 0.0888, 0.0859, 0.0514, 0.0767, 0.1619, 0.0590, 0.0697,
        0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,513][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([5.2177e-04, 3.8082e-03, 3.4478e-03, 1.8950e-03, 3.4469e-04, 9.4573e-05,
        1.1427e-03, 4.2261e-03, 3.6613e-04, 9.8415e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,515][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.1827, 0.1548, 0.0643, 0.1220, 0.0658, 0.0658, 0.0612, 0.0789, 0.0557,
        0.1487], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,516][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([8.0337e-03, 1.4468e-04, 5.3090e-05, 8.4627e-04, 4.7653e-04, 1.5979e-04,
        7.0140e-03, 1.2964e-03, 8.8245e-04, 9.8109e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,517][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0425, 0.0065, 0.0034, 0.0062, 0.0208, 0.0070, 0.0424, 0.0246, 0.0170,
        0.8297], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,518][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([1.1860e-02, 8.6550e-04, 1.9749e-05, 7.4416e-04, 1.3637e-05, 4.2345e-06,
        5.7644e-05, 2.3343e-05, 3.0132e-06, 9.8641e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,519][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.1519, 0.2299, 0.0283, 0.1029, 0.0182, 0.0303, 0.2355, 0.0116, 0.0226,
        0.1687], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,520][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0447, 0.0025, 0.0256, 0.0139, 0.0946, 0.0881, 0.0120, 0.2227, 0.3358,
        0.1600], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,522][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.1637, 0.0791, 0.1144, 0.0149, 0.0453, 0.0810, 0.2022, 0.0841, 0.0789,
        0.1365], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,523][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.2109, 0.1578, 0.1102, 0.0626, 0.0648, 0.0945, 0.0989, 0.0869, 0.0994,
        0.0139], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,524][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.1267, 0.0682, 0.1140, 0.0545, 0.0373, 0.0786, 0.0798, 0.1020, 0.0783,
        0.2605], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,526][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.1864, 0.0621, 0.0850, 0.0762, 0.0654, 0.0554, 0.1675, 0.0932, 0.0398,
        0.1691], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,527][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.3384, 0.0555, 0.0125, 0.0764, 0.0878, 0.0223, 0.2648, 0.0149, 0.0401,
        0.0783, 0.0091], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,528][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([4.0965e-03, 2.6898e-04, 2.0234e-02, 1.5540e-04, 2.6252e-04, 3.2647e-04,
        6.1341e-05, 2.4369e-03, 9.4248e-04, 8.9537e-05, 9.7113e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,530][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2937, 0.0235, 0.1141, 0.0171, 0.0280, 0.0102, 0.0201, 0.0545, 0.0127,
        0.0112, 0.4150], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,530][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([3.4656e-02, 5.1730e-04, 2.6972e-03, 1.0463e-03, 9.4753e-03, 1.2221e-02,
        1.1216e-02, 5.0661e-02, 1.3013e-01, 6.7453e-02, 6.7993e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,532][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.3918, 0.0268, 0.0120, 0.0085, 0.0361, 0.0150, 0.0453, 0.0806, 0.0388,
        0.1011, 0.2439], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,533][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1195, 0.0352, 0.1734, 0.0070, 0.0377, 0.1300, 0.0154, 0.1102, 0.1223,
        0.0159, 0.2334], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,535][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.2005, 0.1951, 0.0074, 0.1568, 0.0550, 0.0086, 0.2204, 0.0120, 0.0051,
        0.1342, 0.0048], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,536][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0356, 0.0114, 0.0151, 0.0368, 0.0292, 0.0435, 0.0598, 0.1092, 0.1726,
        0.1489, 0.3380], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,537][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0291, 0.0060, 0.1520, 0.0049, 0.0155, 0.1430, 0.0122, 0.0616, 0.2185,
        0.0119, 0.3452], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,539][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.2322, 0.0731, 0.1205, 0.0493, 0.0683, 0.0837, 0.0476, 0.0968, 0.0897,
        0.0440, 0.0949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,540][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.1865, 0.0686, 0.1432, 0.0505, 0.0626, 0.0910, 0.0615, 0.1039, 0.0885,
        0.0377, 0.1060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,542][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.2035, 0.0634, 0.0970, 0.0706, 0.0703, 0.0593, 0.0928, 0.1069, 0.0505,
        0.0791, 0.1065], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,543][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2045, 0.0482, 0.1159, 0.0291, 0.0181, 0.0555, 0.2243, 0.0715, 0.0691,
        0.0375, 0.0982, 0.0282], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,544][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([3.7561e-05, 7.5845e-05, 8.9030e-05, 5.6123e-01, 4.9667e-05, 3.9072e-05,
        1.9531e-04, 6.3287e-04, 2.7881e-05, 6.9447e-05, 1.7252e-06, 4.3755e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,546][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.2051, 0.0434, 0.0499, 0.1302, 0.0323, 0.0670, 0.1211, 0.0417, 0.0840,
        0.0574, 0.0414, 0.1264], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,547][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([5.8114e-04, 1.2310e-05, 2.9959e-07, 4.5568e-03, 4.2840e-06, 2.8844e-06,
        9.9339e-05, 2.0350e-05, 2.6940e-05, 1.3270e-03, 8.2943e-05, 9.9329e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,547][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([4.1262e-03, 1.4190e-03, 1.6313e-04, 7.0355e-02, 6.5467e-04, 2.4181e-04,
        6.1824e-03, 1.6455e-03, 5.2814e-04, 2.1138e-03, 1.6283e-03, 9.1094e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,548][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([7.5523e-04, 5.6705e-06, 9.0018e-08, 6.7782e-01, 7.5504e-07, 9.4313e-09,
        5.5385e-07, 1.4960e-07, 1.3861e-08, 3.0859e-06, 1.6065e-09, 3.2141e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,550][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.2046, 0.1116, 0.1017, 0.1141, 0.0231, 0.0559, 0.0538, 0.0239, 0.0818,
        0.0425, 0.0873, 0.0996], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,551][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0940, 0.0253, 0.0268, 0.0067, 0.0245, 0.0567, 0.0045, 0.1489, 0.1618,
        0.0717, 0.3136, 0.0655], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,553][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1992, 0.1571, 0.1030, 0.0390, 0.0515, 0.0890, 0.0706, 0.0599, 0.0838,
        0.0303, 0.0798, 0.0367], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,554][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.2200, 0.1459, 0.1090, 0.0146, 0.0493, 0.0747, 0.0648, 0.0724, 0.0844,
        0.0632, 0.0888, 0.0129], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,556][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.1155, 0.0413, 0.0749, 0.2577, 0.0336, 0.0590, 0.0300, 0.0577, 0.0552,
        0.0220, 0.0514, 0.2018], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,557][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0969, 0.0643, 0.1019, 0.0615, 0.0895, 0.0791, 0.0657, 0.0993, 0.0759,
        0.0713, 0.1116, 0.0829], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,558][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.3184, 0.0339, 0.0354, 0.0695, 0.0670, 0.0318, 0.0683, 0.0374, 0.0388,
        0.0364, 0.0430, 0.0871, 0.1330], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,559][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([4.2182e-04, 5.0818e-04, 2.0358e-04, 1.6453e-04, 1.2764e-02, 4.1493e-04,
        8.8317e-05, 1.3325e-04, 9.4586e-05, 2.4174e-04, 5.3981e-05, 7.9002e-05,
        9.8483e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,561][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.2758, 0.0499, 0.0846, 0.0418, 0.1314, 0.0655, 0.0323, 0.0707, 0.0699,
        0.0438, 0.0317, 0.0385, 0.0641], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,562][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([9.1902e-04, 7.2753e-06, 8.9513e-06, 9.7848e-06, 9.8127e-05, 2.6810e-05,
        1.4240e-04, 1.0288e-04, 1.5227e-04, 5.9376e-04, 9.5916e-04, 8.1208e-04,
        9.9617e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,563][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0388, 0.0039, 0.0049, 0.0026, 0.0076, 0.0069, 0.0115, 0.0232, 0.0143,
        0.0310, 0.0261, 0.0152, 0.8140], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,564][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([1.1286e-02, 1.3683e-04, 4.4803e-05, 1.6294e-05, 2.1540e-03, 5.7190e-06,
        1.7899e-05, 1.9072e-05, 1.8323e-06, 1.0443e-05, 1.9813e-06, 4.4282e-06,
        9.8630e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,566][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.1584, 0.1145, 0.0217, 0.1111, 0.0423, 0.0273, 0.1304, 0.0192, 0.0176,
        0.0997, 0.0172, 0.1311, 0.1094], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,567][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0428, 0.0019, 0.0120, 0.0055, 0.0214, 0.0398, 0.0155, 0.0709, 0.1105,
        0.0813, 0.2872, 0.0874, 0.2238], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,568][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.1511, 0.0283, 0.1191, 0.0276, 0.0817, 0.1205, 0.0182, 0.0984, 0.1566,
        0.0413, 0.0989, 0.0327, 0.0257], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,569][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.1949, 0.0654, 0.0874, 0.0525, 0.0726, 0.0663, 0.0690, 0.0692, 0.0725,
        0.0513, 0.0810, 0.0593, 0.0586], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,569][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.1786, 0.0501, 0.0815, 0.0317, 0.0753, 0.0686, 0.0421, 0.0583, 0.0684,
        0.0267, 0.0651, 0.0278, 0.2259], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,569][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.3078, 0.0528, 0.0759, 0.0372, 0.0439, 0.0410, 0.0845, 0.1018, 0.0307,
        0.0491, 0.0822, 0.0370, 0.0560], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,570][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.2098, 0.0378, 0.0204, 0.0599, 0.0687, 0.0198, 0.1543, 0.0179, 0.0272,
        0.0540, 0.0239, 0.0845, 0.2073, 0.0147], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,570][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([5.7891e-03, 1.8372e-04, 3.9635e-02, 1.9617e-04, 4.6667e-04, 1.6733e-03,
        5.6473e-05, 1.9505e-02, 1.9044e-03, 1.9529e-04, 8.6282e-03, 1.2123e-04,
        2.9691e-05, 9.2162e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,571][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.2025, 0.0262, 0.1027, 0.0284, 0.0557, 0.0268, 0.0314, 0.1404, 0.0296,
        0.0163, 0.0638, 0.0280, 0.0691, 0.1792], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,571][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([6.1066e-03, 1.7274e-04, 2.8097e-04, 7.9002e-05, 8.1721e-04, 1.6108e-03,
        9.8127e-04, 3.4968e-03, 1.5044e-02, 2.9284e-03, 3.6965e-02, 6.0838e-03,
        9.8946e-02, 8.2649e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,571][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([1.2058e-02, 1.2349e-03, 1.3477e-03, 6.7070e-04, 1.9540e-02, 3.7262e-03,
        8.5607e-03, 7.8188e-03, 6.2015e-03, 1.3655e-02, 9.1604e-03, 6.1404e-03,
        8.1136e-01, 9.8530e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,572][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([4.3637e-02, 1.5902e-02, 8.9145e-02, 7.7795e-04, 4.9550e-02, 2.1246e-01,
        1.7378e-03, 2.0805e-02, 1.7467e-01, 1.8862e-03, 1.7767e-02, 2.8540e-04,
        5.6972e-03, 3.6568e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,573][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0858, 0.0779, 0.0046, 0.0840, 0.0404, 0.0039, 0.0957, 0.0169, 0.0032,
        0.0712, 0.0039, 0.1369, 0.1236, 0.2522], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,575][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0281, 0.0037, 0.0059, 0.0057, 0.0099, 0.0119, 0.0130, 0.0255, 0.0423,
        0.0305, 0.1120, 0.0914, 0.3469, 0.2733], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,576][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0166, 0.0035, 0.0993, 0.0026, 0.0166, 0.1338, 0.0142, 0.0487, 0.2138,
        0.0122, 0.1790, 0.0037, 0.0076, 0.2483], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,578][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1586, 0.0450, 0.0954, 0.0383, 0.0554, 0.0774, 0.0401, 0.0797, 0.0836,
        0.0402, 0.0796, 0.0404, 0.0585, 0.1077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,578][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1297, 0.0361, 0.1167, 0.0336, 0.0670, 0.0966, 0.0356, 0.0883, 0.0880,
        0.0294, 0.0819, 0.0276, 0.0482, 0.1213], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,580][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1652, 0.0586, 0.0769, 0.0599, 0.0608, 0.0489, 0.0676, 0.0779, 0.0371,
        0.0613, 0.0800, 0.0623, 0.0649, 0.0786], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,581][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.1929, 0.0203, 0.0241, 0.0913, 0.0546, 0.0209, 0.1508, 0.0146, 0.0241,
        0.0558, 0.0210, 0.1194, 0.1499, 0.0220, 0.0383], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,582][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([6.2464e-04, 1.0738e-03, 5.1183e-04, 1.8015e-04, 4.3813e-03, 4.6065e-04,
        4.6476e-04, 7.7454e-04, 2.1318e-04, 4.5889e-05, 1.6713e-04, 1.0317e-04,
        1.3254e-03, 5.6342e-04, 9.8911e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,583][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.2063, 0.0295, 0.0562, 0.0225, 0.0950, 0.0446, 0.0434, 0.0843, 0.0549,
        0.0171, 0.0362, 0.0209, 0.1165, 0.1190, 0.0536], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,584][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([8.9627e-04, 7.8288e-06, 4.9681e-06, 1.9680e-06, 3.2040e-05, 3.7239e-05,
        5.2650e-05, 6.5410e-05, 2.1141e-04, 8.8583e-05, 9.0922e-04, 1.9007e-04,
        8.8097e-03, 8.6102e-03, 9.8008e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,586][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0441, 0.0083, 0.0063, 0.0031, 0.0098, 0.0096, 0.0160, 0.0234, 0.0161,
        0.0122, 0.0276, 0.0175, 0.1298, 0.1222, 0.5541], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,587][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([9.2131e-03, 2.2886e-03, 5.0052e-05, 1.9425e-05, 1.4805e-03, 4.0115e-05,
        4.4480e-04, 3.3073e-05, 2.2188e-05, 3.9029e-07, 3.1369e-06, 5.0144e-06,
        1.6442e-04, 8.3793e-06, 9.8623e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,588][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.1186, 0.0823, 0.0129, 0.1010, 0.0291, 0.0143, 0.2115, 0.0121, 0.0123,
        0.1399, 0.0124, 0.1272, 0.0900, 0.0114, 0.0248], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,590][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0264, 0.0025, 0.0059, 0.0036, 0.0090, 0.0127, 0.0072, 0.0251, 0.0327,
        0.0158, 0.0903, 0.0489, 0.1514, 0.3241, 0.2443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,591][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0784, 0.0164, 0.0929, 0.0209, 0.0770, 0.0762, 0.0554, 0.0675, 0.0970,
        0.0404, 0.1010, 0.0269, 0.0436, 0.1605, 0.0459], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,593][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.1343, 0.0500, 0.0777, 0.0348, 0.0625, 0.0664, 0.0585, 0.0645, 0.0670,
        0.0472, 0.0726, 0.0387, 0.0731, 0.0957, 0.0570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,594][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.1083, 0.0440, 0.0704, 0.0201, 0.0773, 0.0613, 0.0495, 0.0678, 0.0583,
        0.0240, 0.0614, 0.0190, 0.0629, 0.0822, 0.1934], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,596][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.2614, 0.0389, 0.0894, 0.0421, 0.0518, 0.0262, 0.0508, 0.0786, 0.0202,
        0.0525, 0.0630, 0.0387, 0.0493, 0.0619, 0.0752], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,597][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.2696, 0.0470, 0.0197, 0.0590, 0.0469, 0.0123, 0.1025, 0.0160, 0.0181,
        0.0423, 0.0223, 0.0790, 0.1429, 0.0138, 0.0722, 0.0363],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,598][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([3.1316e-03, 9.2131e-04, 2.0966e-03, 4.9999e-04, 2.5779e-04, 3.3383e-03,
        4.3261e-05, 6.0132e-04, 1.0975e-02, 8.6496e-05, 7.2888e-04, 3.4593e-04,
        1.1504e-04, 3.1576e-03, 4.1400e-04, 9.7329e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,600][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.1716, 0.0235, 0.0819, 0.0196, 0.1110, 0.0249, 0.0449, 0.0674, 0.0259,
        0.0242, 0.0453, 0.0189, 0.1336, 0.0988, 0.0755, 0.0332],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,601][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([8.5273e-04, 3.3332e-06, 1.4939e-05, 1.3792e-05, 5.9196e-05, 1.0301e-04,
        2.9896e-05, 2.5979e-04, 8.7404e-04, 2.9685e-04, 3.2506e-03, 1.3216e-03,
        9.4370e-03, 1.8069e-02, 7.2851e-02, 8.9256e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,602][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0464, 0.0020, 0.0057, 0.0012, 0.0161, 0.0063, 0.0142, 0.0133, 0.0136,
        0.0160, 0.0251, 0.0062, 0.2770, 0.0934, 0.1966, 0.2671],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,603][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([2.6436e-02, 7.2256e-03, 4.1482e-03, 3.2213e-03, 8.0425e-03, 6.5319e-03,
        9.4445e-04, 1.8825e-03, 8.2516e-03, 4.4389e-04, 4.4639e-04, 9.2359e-04,
        1.0206e-03, 1.9027e-03, 5.6022e-04, 9.2802e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,604][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.1030, 0.0647, 0.0053, 0.1201, 0.0388, 0.0051, 0.1924, 0.0056, 0.0034,
        0.0914, 0.0043, 0.1811, 0.1240, 0.0072, 0.0468, 0.0067],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,606][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0263, 0.0013, 0.0041, 0.0021, 0.0061, 0.0068, 0.0063, 0.0166, 0.0192,
        0.0121, 0.0620, 0.0260, 0.0705, 0.1768, 0.3016, 0.2622],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,607][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0311, 0.0050, 0.0857, 0.0040, 0.0334, 0.1322, 0.0197, 0.0409, 0.1831,
        0.0158, 0.1220, 0.0050, 0.0147, 0.1119, 0.0259, 0.1698],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,609][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.1607, 0.0438, 0.0784, 0.0353, 0.0523, 0.0597, 0.0402, 0.0623, 0.0667,
        0.0284, 0.0685, 0.0377, 0.0476, 0.0878, 0.0595, 0.0711],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,610][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.1099, 0.0337, 0.0689, 0.0302, 0.0376, 0.0672, 0.0375, 0.0509, 0.0847,
        0.0237, 0.0551, 0.0259, 0.0397, 0.0670, 0.0420, 0.2260],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,612][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.2027, 0.0456, 0.0577, 0.0535, 0.0427, 0.0352, 0.0612, 0.0646, 0.0306,
        0.0583, 0.0685, 0.0534, 0.0540, 0.0605, 0.0637, 0.0478],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,613][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1866, 0.0320, 0.0167, 0.0516, 0.0600, 0.0167, 0.1347, 0.0143, 0.0228,
        0.0459, 0.0193, 0.0720, 0.1796, 0.0119, 0.0704, 0.0512, 0.0142],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,614][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([2.8636e-03, 7.7269e-05, 1.7566e-02, 9.1481e-05, 2.1393e-04, 7.1493e-04,
        2.7315e-05, 8.8253e-03, 8.9390e-04, 9.7980e-05, 4.6295e-03, 6.2686e-05,
        1.4755e-05, 4.6280e-01, 4.0136e-04, 6.5255e-04, 5.0006e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,616][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1555, 0.0201, 0.0778, 0.0213, 0.0443, 0.0203, 0.0227, 0.1073, 0.0225,
        0.0132, 0.0491, 0.0213, 0.0557, 0.1370, 0.0575, 0.0279, 0.1466],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,616][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.6394e-03, 2.7073e-05, 4.0770e-05, 1.1839e-05, 8.5007e-05, 1.4144e-04,
        9.1860e-05, 2.4133e-04, 9.9447e-04, 2.2161e-04, 2.5218e-03, 4.9692e-04,
        6.2112e-03, 4.7909e-02, 4.0161e-01, 9.8652e-02, 4.3911e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,618][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0134, 0.0011, 0.0012, 0.0006, 0.0147, 0.0025, 0.0055, 0.0046, 0.0035,
        0.0078, 0.0050, 0.0035, 0.4310, 0.0499, 0.2337, 0.0621, 0.1600],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,619][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.6770e-02, 1.0595e-02, 6.0953e-02, 5.5493e-04, 3.5507e-02, 1.5350e-01,
        1.1184e-03, 1.5332e-02, 1.2454e-01, 1.3050e-03, 1.1312e-02, 2.0358e-04,
        3.9189e-03, 2.5675e-01, 8.9530e-03, 5.2092e-02, 2.3660e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,620][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0621, 0.0549, 0.0031, 0.0603, 0.0290, 0.0026, 0.0693, 0.0119, 0.0021,
        0.0509, 0.0026, 0.0996, 0.0910, 0.1831, 0.0492, 0.0069, 0.2213],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,622][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0212, 0.0020, 0.0032, 0.0031, 0.0043, 0.0046, 0.0049, 0.0080, 0.0128,
        0.0093, 0.0339, 0.0290, 0.0972, 0.0716, 0.1488, 0.2635, 0.2827],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,623][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0119, 0.0025, 0.0695, 0.0019, 0.0119, 0.0920, 0.0105, 0.0337, 0.1449,
        0.0086, 0.1249, 0.0027, 0.0055, 0.1716, 0.0139, 0.0888, 0.2053],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,625][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.1231, 0.0351, 0.0732, 0.0310, 0.0442, 0.0595, 0.0330, 0.0608, 0.0638,
        0.0329, 0.0618, 0.0328, 0.0474, 0.0825, 0.0559, 0.0727, 0.0903],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,626][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0919, 0.0254, 0.0814, 0.0267, 0.0517, 0.0720, 0.0291, 0.0681, 0.0679,
        0.0252, 0.0661, 0.0246, 0.0416, 0.0999, 0.0498, 0.0772, 0.1014],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,627][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1492, 0.0528, 0.0672, 0.0530, 0.0535, 0.0427, 0.0538, 0.0652, 0.0315,
        0.0497, 0.0635, 0.0515, 0.0525, 0.0641, 0.0512, 0.0336, 0.0651],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,628][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:28,629][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[  813],
        [    1],
        [11340],
        [ 1543],
        [30143],
        [15564],
        [16327],
        [15651],
        [15539],
        [11496],
        [ 2965],
        [ 1053],
        [18346],
        [ 6379],
        [16207],
        [22863],
        [ 7702]], device='cuda:0')
[2024-07-24 10:21:28,630][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[37137],
        [    1],
        [20655],
        [ 2381],
        [35917],
        [15658],
        [24673],
        [21757],
        [11133],
        [21637],
        [18867],
        [ 1986],
        [27141],
        [23672],
        [19873],
        [12395],
        [22292]], device='cuda:0')
[2024-07-24 10:21:28,631][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[1880],
        [ 607],
        [ 915],
        [2715],
        [8610],
        [4484],
        [4367],
        [8633],
        [3690],
        [3021],
        [2819],
        [5322],
        [5342],
        [5222],
        [6853],
        [3799],
        [4927]], device='cuda:0')
[2024-07-24 10:21:28,633][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[41289],
        [ 5622],
        [31415],
        [14246],
        [37031],
        [40124],
        [13713],
        [37080],
        [47779],
        [17725],
        [ 9054],
        [14118],
        [29165],
        [30699],
        [24005],
        [47119],
        [30800]], device='cuda:0')
[2024-07-24 10:21:28,634][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[16691],
        [14233],
        [37652],
        [19733],
        [24768],
        [27053],
        [17998],
        [28077],
        [26625],
        [19381],
        [20935],
        [19955],
        [22114],
        [29958],
        [24660],
        [25585],
        [29219]], device='cuda:0')
[2024-07-24 10:21:28,636][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[28510],
        [ 8832],
        [31926],
        [ 6880],
        [13739],
        [18925],
        [10471],
        [20827],
        [17580],
        [41478],
        [26846],
        [ 3128],
        [25538],
        [16476],
        [31036],
        [18380],
        [19973]], device='cuda:0')
[2024-07-24 10:21:28,637][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[12919],
        [  181],
        [ 6848],
        [ 2930],
        [15002],
        [17073],
        [30013],
        [22161],
        [21750],
        [41837],
        [22637],
        [ 3498],
        [18701],
        [19256],
        [27007],
        [25292],
        [23716]], device='cuda:0')
[2024-07-24 10:21:28,639][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[40137],
        [ 8272],
        [28345],
        [18420],
        [31955],
        [27109],
        [38311],
        [28664],
        [28602],
        [40750],
        [28874],
        [18107],
        [34615],
        [24949],
        [34943],
        [34111],
        [24406]], device='cuda:0')
[2024-07-24 10:21:28,640][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[32533],
        [ 3939],
        [ 2735],
        [13931],
        [12460],
        [ 4931],
        [21848],
        [16463],
        [13501],
        [15757],
        [17130],
        [23034],
        [29041],
        [18108],
        [32438],
        [31458],
        [15052]], device='cuda:0')
[2024-07-24 10:21:28,642][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[15434],
        [14336],
        [ 9101],
        [ 7993],
        [18995],
        [29776],
        [35160],
        [20723],
        [19357],
        [23176],
        [ 7516],
        [ 7769],
        [ 6210],
        [10174],
        [22036],
        [34559],
        [31150]], device='cuda:0')
[2024-07-24 10:21:28,643][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 8050],
        [14303],
        [ 1243],
        [ 9953],
        [ 2470],
        [ 2986],
        [10360],
        [  861],
        [ 2069],
        [17321],
        [ 1334],
        [ 5564],
        [ 1089],
        [ 1360],
        [  691],
        [ 2201],
        [ 1712]], device='cuda:0')
[2024-07-24 10:21:28,644][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[19131],
        [17413],
        [15293],
        [10181],
        [12990],
        [12956],
        [ 9947],
        [13435],
        [13761],
        [ 6904],
        [14219],
        [ 9847],
        [12709],
        [17847],
        [17098],
        [17810],
        [20029]], device='cuda:0')
[2024-07-24 10:21:28,646][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[12433],
        [25082],
        [16544],
        [35873],
        [23815],
        [36088],
        [34295],
        [31546],
        [38057],
        [37886],
        [34373],
        [45425],
        [36212],
        [32416],
        [37673],
        [42360],
        [33478]], device='cuda:0')
[2024-07-24 10:21:28,647][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[35042],
        [31069],
        [21632],
        [14631],
        [17952],
        [15463],
        [ 9165],
        [11503],
        [12986],
        [12604],
        [11300],
        [ 9303],
        [12758],
        [ 8463],
        [ 9635],
        [10009],
        [ 7917]], device='cuda:0')
[2024-07-24 10:21:28,649][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 6292],
        [    4],
        [24127],
        [ 9729],
        [49522],
        [36941],
        [39557],
        [23803],
        [25892],
        [37695],
        [ 9726],
        [ 8883],
        [41858],
        [11918],
        [34013],
        [29183],
        [11842]], device='cuda:0')
[2024-07-24 10:21:28,650][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[34057],
        [35772],
        [32104],
        [23710],
        [20705],
        [23485],
        [26291],
        [25024],
        [24119],
        [25945],
        [21555],
        [21384],
        [17631],
        [16396],
        [18880],
        [18233],
        [17189]], device='cuda:0')
[2024-07-24 10:21:28,652][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 5669],
        [37237],
        [ 7830],
        [36705],
        [18870],
        [ 5171],
        [28219],
        [ 4953],
        [ 4362],
        [28305],
        [13592],
        [37765],
        [31866],
        [ 5049],
        [20860],
        [ 7153],
        [ 4413]], device='cuda:0')
[2024-07-24 10:21:28,653][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[29584],
        [30540],
        [37822],
        [33913],
        [33271],
        [35423],
        [36475],
        [29314],
        [27266],
        [33815],
        [16519],
        [33574],
        [31582],
        [18639],
        [24920],
        [26628],
        [17231]], device='cuda:0')
[2024-07-24 10:21:28,655][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[14281],
        [15455],
        [24697],
        [10175],
        [29195],
        [32361],
        [ 4435],
        [29686],
        [32171],
        [13470],
        [36759],
        [19499],
        [11541],
        [36560],
        [22579],
        [26587],
        [31584]], device='cuda:0')
[2024-07-24 10:21:28,656][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[16903],
        [29882],
        [20660],
        [28544],
        [18504],
        [20025],
        [39485],
        [17470],
        [17135],
        [31654],
        [26034],
        [26098],
        [31627],
        [32170],
        [30771],
        [28787],
        [32078]], device='cuda:0')
[2024-07-24 10:21:28,658][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[22970],
        [ 4287],
        [12135],
        [10635],
        [ 7050],
        [ 5598],
        [ 3553],
        [10122],
        [ 4532],
        [ 1729],
        [ 7581],
        [10314],
        [ 6426],
        [ 8614],
        [ 3693],
        [ 1896],
        [ 9410]], device='cuda:0')
[2024-07-24 10:21:28,659][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[28598],
        [ 4181],
        [ 3063],
        [10017],
        [25119],
        [ 9854],
        [24821],
        [39911],
        [38114],
        [26411],
        [32540],
        [19226],
        [28642],
        [31748],
        [36754],
        [36134],
        [27905]], device='cuda:0')
[2024-07-24 10:21:28,660][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 8597],
        [ 7662],
        [ 7148],
        [ 7230],
        [ 5236],
        [ 7131],
        [ 5576],
        [ 8263],
        [12436],
        [12756],
        [ 7646],
        [ 9292],
        [ 7611],
        [ 5490],
        [ 4357],
        [ 4659],
        [ 3001]], device='cuda:0')
[2024-07-24 10:21:28,662][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[22203],
        [33439],
        [11344],
        [31353],
        [19474],
        [ 9687],
        [25440],
        [ 8286],
        [ 5992],
        [23170],
        [ 4421],
        [20506],
        [10410],
        [ 5713],
        [12335],
        [ 9480],
        [ 7695]], device='cuda:0')
[2024-07-24 10:21:28,663][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[36598],
        [36807],
        [34868],
        [34248],
        [38502],
        [38564],
        [38086],
        [39722],
        [39307],
        [37896],
        [38783],
        [37718],
        [39713],
        [39340],
        [41435],
        [40893],
        [40385]], device='cuda:0')
[2024-07-24 10:21:28,665][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[46884],
        [43499],
        [48598],
        [45817],
        [46656],
        [48133],
        [44977],
        [48379],
        [47897],
        [44822],
        [48000],
        [44303],
        [46661],
        [47999],
        [47271],
        [47294],
        [47976]], device='cuda:0')
[2024-07-24 10:21:28,666][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 8380],
        [11983],
        [ 8468],
        [19157],
        [19047],
        [18991],
        [19605],
        [23740],
        [15336],
        [22557],
        [21060],
        [19091],
        [23518],
        [20100],
        [25346],
        [23856],
        [19220]], device='cuda:0')
[2024-07-24 10:21:28,668][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 9270],
        [12929],
        [13917],
        [10900],
        [10736],
        [12475],
        [ 9980],
        [ 9930],
        [10981],
        [ 8418],
        [10140],
        [11277],
        [11374],
        [13104],
        [10682],
        [11666],
        [13150]], device='cuda:0')
[2024-07-24 10:21:28,669][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[35855],
        [50248],
        [19433],
        [33964],
        [  454],
        [ 8423],
        [ 7960],
        [20005],
        [17221],
        [ 8803],
        [33990],
        [35127],
        [ 5536],
        [31575],
        [11618],
        [16192],
        [31809]], device='cuda:0')
[2024-07-24 10:21:28,671][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016],
        [19016]], device='cuda:0')
[2024-07-24 10:21:28,696][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:28,697][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,698][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,700][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,701][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,702][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,702][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,702][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,703][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,703][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,703][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,704][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,704][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,704][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.9892, 0.0108], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,705][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.7025, 0.2975], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,705][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.6118, 0.3882], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,706][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.7154, 0.2846], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,707][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.2052, 0.7948], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,708][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.9696, 0.0304], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,710][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.9701, 0.0299], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,711][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.8024, 0.1976], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,712][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.9889, 0.0111], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,714][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.9909, 0.0091], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,715][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([2.6311e-04, 9.9974e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,716][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.4765, 0.5235], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,717][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4310, 0.5436, 0.0254], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,719][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0527, 0.9458, 0.0015], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,720][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4276, 0.2748, 0.2976], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,722][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.5160, 0.2199, 0.2641], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,723][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1390, 0.5239, 0.3372], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,724][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.4564, 0.0015, 0.5421], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,725][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.7923, 0.0779, 0.1297], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,727][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.4501, 0.4151, 0.1349], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,728][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.5511, 0.0064, 0.4426], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,730][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.8651, 0.0096, 0.1254], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,731][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0007, 0.5438, 0.4555], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,732][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.5639, 0.0033, 0.4327], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,734][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.2611, 0.6004, 0.0769, 0.0616], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,735][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.4241, 0.4038, 0.1289, 0.0432], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,736][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.3353, 0.2096, 0.2387, 0.2164], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,738][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.4324, 0.1921, 0.2362, 0.1392], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,739][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0833, 0.3630, 0.2527, 0.3010], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,740][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.1673, 0.5704, 0.2371, 0.0252], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,742][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.7596, 0.0286, 0.1775, 0.0344], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,743][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.4303, 0.3272, 0.2016, 0.0409], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,744][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.5481, 0.0060, 0.4283, 0.0176], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,746][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.8087, 0.0137, 0.1677, 0.0100], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,747][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0008, 0.2994, 0.3250, 0.3748], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,748][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0234, 0.0011, 0.0179, 0.9575], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:28,750][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.8941, 0.0303, 0.0543, 0.0124, 0.0090], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,751][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.1996, 0.6428, 0.0201, 0.1158, 0.0217], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,752][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.2681, 0.1722, 0.1815, 0.1730, 0.2052], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,754][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.3743, 0.1582, 0.1925, 0.1112, 0.1637], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,755][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0690, 0.2752, 0.1867, 0.2276, 0.2415], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,756][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.7402, 0.0092, 0.1681, 0.0068, 0.0757], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,758][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.6724, 0.0567, 0.2002, 0.0278, 0.0429], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,759][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.3147, 0.3018, 0.0872, 0.2398, 0.0565], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,761][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.5601, 0.0063, 0.3818, 0.0181, 0.0335], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,761][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.7475, 0.0199, 0.1540, 0.0194, 0.0593], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,761][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0008, 0.1915, 0.1930, 0.2572, 0.3574], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,762][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ got] are: tensor([4.5998e-02, 6.3362e-04, 2.7702e-02, 5.7057e-04, 9.2510e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:28,762][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3283, 0.0715, 0.0249, 0.0685, 0.4712, 0.0356], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,762][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.5848, 0.2992, 0.0232, 0.0293, 0.0624, 0.0011], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,763][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2226, 0.1466, 0.1565, 0.1464, 0.1742, 0.1537], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,763][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.3208, 0.1341, 0.1654, 0.0970, 0.1439, 0.1388], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,763][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0611, 0.2254, 0.1526, 0.1916, 0.2045, 0.1647], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,764][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.5765, 0.0262, 0.1275, 0.0289, 0.1084, 0.1325], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,764][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3288, 0.1110, 0.1963, 0.1298, 0.0976, 0.1365], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,765][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2508, 0.2283, 0.0942, 0.2195, 0.0866, 0.1205], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,767][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.4558, 0.0062, 0.2928, 0.0173, 0.0278, 0.2001], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,768][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.6678, 0.0147, 0.0984, 0.0189, 0.0459, 0.1543], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,769][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0009, 0.1503, 0.1375, 0.2087, 0.2818, 0.2207], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,771][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1055, 0.0017, 0.0282, 0.0041, 0.0076, 0.8529], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:28,772][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.2315, 0.0449, 0.0191, 0.0445, 0.3332, 0.1735, 0.1533],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,773][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.2091, 0.3968, 0.1408, 0.0671, 0.1508, 0.0277, 0.0077],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,775][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.1992, 0.1266, 0.1338, 0.1244, 0.1513, 0.1255, 0.1391],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,776][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.2579, 0.1138, 0.1492, 0.0918, 0.1350, 0.1351, 0.1172],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,778][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0492, 0.1934, 0.1306, 0.1576, 0.1693, 0.1346, 0.1652],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,778][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([5.7947e-02, 3.1439e-04, 7.0571e-03, 1.8400e-03, 4.9130e-02, 7.4120e-02,
        8.0959e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,780][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1466, 0.0034, 0.0333, 0.0054, 0.0112, 0.1080, 0.6921],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,781][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1502, 0.2325, 0.0587, 0.1794, 0.0824, 0.2916, 0.0053],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,783][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.4064, 0.0063, 0.2917, 0.0194, 0.0321, 0.2362, 0.0079],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,784][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.4563, 0.0278, 0.1612, 0.0498, 0.1069, 0.1638, 0.0343],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,786][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0006, 0.1250, 0.1234, 0.1654, 0.2164, 0.1667, 0.2025],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,787][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0723, 0.0016, 0.0481, 0.0043, 0.0086, 0.0144, 0.8507],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:28,788][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.1749, 0.0341, 0.0125, 0.0341, 0.1445, 0.0720, 0.5249, 0.0030],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,790][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0841, 0.5771, 0.0220, 0.0638, 0.1518, 0.0147, 0.0851, 0.0013],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,791][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1714, 0.1110, 0.1152, 0.1141, 0.1323, 0.1120, 0.1257, 0.1181],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,793][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.2360, 0.0957, 0.1204, 0.0720, 0.1074, 0.1059, 0.0962, 0.1664],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,794][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0446, 0.1625, 0.1086, 0.1403, 0.1469, 0.1158, 0.1557, 0.1257],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,795][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ at] are: tensor([8.7563e-02, 5.9038e-04, 7.8493e-03, 1.1541e-03, 2.9095e-03, 3.0419e-02,
        1.9093e-02, 8.5042e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,796][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1430, 0.0169, 0.0792, 0.1054, 0.0239, 0.0738, 0.5333, 0.0245],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,798][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1868, 0.1708, 0.0689, 0.2233, 0.0655, 0.0784, 0.1187, 0.0875],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,799][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.4509, 0.0061, 0.2702, 0.0184, 0.0289, 0.2119, 0.0071, 0.0064],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,801][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.6242, 0.0142, 0.0824, 0.0200, 0.0386, 0.1477, 0.0119, 0.0610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,802][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0011, 0.0937, 0.0857, 0.1278, 0.1778, 0.1270, 0.1821, 0.2047],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,803][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ at] are: tensor([5.4425e-02, 1.8300e-03, 2.3772e-02, 6.2028e-04, 3.8837e-03, 2.7833e-03,
        7.7019e-04, 9.1192e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:28,804][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2486, 0.0142, 0.0153, 0.0703, 0.0983, 0.1264, 0.0816, 0.3084, 0.0370],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,806][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.2859, 0.3049, 0.0532, 0.0407, 0.1354, 0.0077, 0.1204, 0.0506, 0.0011],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,807][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1476, 0.1011, 0.1038, 0.1014, 0.1206, 0.1032, 0.1153, 0.1032, 0.1038],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,809][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2012, 0.0777, 0.1020, 0.0621, 0.0935, 0.0915, 0.0826, 0.1461, 0.1433],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,810][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0391, 0.1444, 0.0939, 0.1235, 0.1321, 0.1015, 0.1439, 0.1065, 0.1150],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,812][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3365, 0.0179, 0.0910, 0.0321, 0.0506, 0.1383, 0.0333, 0.1764, 0.1239],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,813][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1882, 0.1042, 0.1498, 0.0436, 0.0664, 0.1000, 0.1242, 0.1196, 0.1040],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,814][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1529, 0.1092, 0.0573, 0.1944, 0.0478, 0.0727, 0.1551, 0.0805, 0.1300],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,816][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3783, 0.0055, 0.2453, 0.0162, 0.0246, 0.1922, 0.0067, 0.0060, 0.1252],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,817][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.5264, 0.0150, 0.0760, 0.0202, 0.0421, 0.1039, 0.0172, 0.0488, 0.1503],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,818][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0014, 0.0681, 0.0661, 0.1062, 0.1435, 0.1113, 0.1530, 0.1697, 0.1806],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,819][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1035, 0.0020, 0.0395, 0.0039, 0.0054, 0.0240, 0.0100, 0.0165, 0.7953],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:28,819][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0550, 0.0202, 0.0081, 0.0113, 0.2110, 0.0532, 0.3228, 0.1077, 0.2057,
        0.0049], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,819][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.2619, 0.3813, 0.0406, 0.0472, 0.1980, 0.0197, 0.0127, 0.0278, 0.0099,
        0.0009], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,820][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.1496, 0.0902, 0.0967, 0.0876, 0.1051, 0.0894, 0.0988, 0.0919, 0.0886,
        0.1021], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,820][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.1653, 0.0624, 0.0858, 0.0531, 0.0812, 0.0800, 0.0694, 0.1271, 0.1247,
        0.1509], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,820][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0302, 0.1252, 0.0885, 0.1049, 0.1123, 0.0905, 0.1142, 0.0975, 0.1061,
        0.1307], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,821][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.1241, 0.0070, 0.0188, 0.0052, 0.0401, 0.1000, 0.0538, 0.4761, 0.0929,
        0.0819], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,821][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0630, 0.1325, 0.1657, 0.0065, 0.0174, 0.1090, 0.0237, 0.1591, 0.0884,
        0.2347], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,821][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.1818, 0.1625, 0.0922, 0.1061, 0.0378, 0.0820, 0.0738, 0.1206, 0.1097,
        0.0334], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,823][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.3325, 0.0053, 0.2306, 0.0179, 0.0284, 0.2274, 0.0068, 0.0050, 0.1319,
        0.0142], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,824][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.3910, 0.0190, 0.0673, 0.0174, 0.1134, 0.1145, 0.0229, 0.1122, 0.1229,
        0.0194], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,825][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0007, 0.0611, 0.0601, 0.0879, 0.1278, 0.0935, 0.1286, 0.1500, 0.1431,
        0.1473], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,826][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ station] are: tensor([9.2124e-03, 6.8356e-04, 1.1391e-02, 4.9991e-04, 1.4995e-03, 1.3945e-03,
        1.4821e-03, 2.8664e-02, 2.1951e-03, 9.4298e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:28,828][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.2565, 0.0469, 0.0127, 0.1237, 0.1039, 0.0690, 0.1832, 0.0576, 0.0979,
        0.0261, 0.0226], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,829][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.1787, 0.5873, 0.0161, 0.0435, 0.1113, 0.0026, 0.0333, 0.0090, 0.0018,
        0.0154, 0.0010], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,830][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1199, 0.0803, 0.0829, 0.0831, 0.0989, 0.0812, 0.0923, 0.0836, 0.0809,
        0.0966, 0.1003], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,831][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1485, 0.0594, 0.0767, 0.0464, 0.0703, 0.0701, 0.0622, 0.1107, 0.1080,
        0.1282, 0.1195], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,833][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0322, 0.1130, 0.0761, 0.0993, 0.1067, 0.0799, 0.1101, 0.0858, 0.0897,
        0.1203, 0.0868], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,834][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [,] are: tensor([4.5158e-02, 7.3228e-04, 2.7611e-02, 1.1399e-02, 4.8342e-03, 2.1966e-02,
        3.5984e-03, 8.6808e-03, 3.8699e-02, 3.1991e-03, 8.3412e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,835][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2007, 0.0431, 0.0612, 0.0132, 0.0528, 0.0864, 0.1299, 0.0803, 0.0542,
        0.2116, 0.0666], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,836][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1619, 0.0996, 0.0427, 0.1118, 0.0534, 0.0469, 0.1017, 0.0537, 0.0740,
        0.1564, 0.0978], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,838][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.2804, 0.0042, 0.2654, 0.0110, 0.0214, 0.1627, 0.0056, 0.0054, 0.1255,
        0.0104, 0.1080], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,839][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.4240, 0.0156, 0.0699, 0.0244, 0.0418, 0.0898, 0.0223, 0.0529, 0.1288,
        0.0218, 0.1087], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,841][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0013, 0.0464, 0.0459, 0.0733, 0.1045, 0.0730, 0.1076, 0.1218, 0.1190,
        0.1275, 0.1800], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,842][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2307, 0.0215, 0.1326, 0.0235, 0.0578, 0.0473, 0.0346, 0.0616, 0.0641,
        0.0528, 0.2735], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:28,844][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0793, 0.1473, 0.0210, 0.0178, 0.0856, 0.0635, 0.0854, 0.0353, 0.1090,
        0.0307, 0.2721, 0.0530], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,845][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.2683, 0.2771, 0.0769, 0.0268, 0.1872, 0.0195, 0.0542, 0.0406, 0.0064,
        0.0081, 0.0191, 0.0160], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,846][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.1168, 0.0719, 0.0800, 0.0736, 0.0901, 0.0731, 0.0837, 0.0759, 0.0729,
        0.0882, 0.0930, 0.0806], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,848][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.1340, 0.0539, 0.0726, 0.0431, 0.0680, 0.0671, 0.0549, 0.1066, 0.1020,
        0.1172, 0.1141, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,849][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0238, 0.1056, 0.0726, 0.0868, 0.0904, 0.0720, 0.0933, 0.0785, 0.0830,
        0.1054, 0.0826, 0.1060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,851][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0210, 0.3734, 0.1047, 0.0135, 0.0054, 0.0542, 0.0104, 0.0160, 0.0892,
        0.0012, 0.2944, 0.0166], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,852][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0862, 0.0166, 0.0791, 0.0140, 0.0198, 0.0622, 0.2778, 0.0500, 0.0805,
        0.0947, 0.2134, 0.0057], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,854][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0663, 0.0604, 0.0355, 0.0077, 0.0519, 0.0853, 0.0848, 0.0358, 0.1210,
        0.0768, 0.3690, 0.0055], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,855][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.2809, 0.0037, 0.1963, 0.0126, 0.0225, 0.2179, 0.0062, 0.0043, 0.1295,
        0.0142, 0.0907, 0.0212], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,856][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.1289, 0.0066, 0.0281, 0.0030, 0.0419, 0.2433, 0.0058, 0.1862, 0.1989,
        0.0049, 0.1471, 0.0054], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,858][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0013, 0.0357, 0.0401, 0.0587, 0.0874, 0.0678, 0.0842, 0.1065, 0.1092,
        0.1084, 0.1528, 0.1480], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,859][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([7.7463e-04, 4.1416e-04, 5.4670e-04, 5.1986e-01, 9.1788e-05, 4.6840e-04,
        5.9614e-04, 2.5791e-04, 1.7230e-04, 7.1077e-05, 3.0194e-02, 4.4655e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:28,860][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.3805, 0.0131, 0.0113, 0.0236, 0.1631, 0.0466, 0.0544, 0.0633, 0.0344,
        0.0047, 0.1283, 0.0406, 0.0361], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,862][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.1621, 0.6461, 0.0287, 0.0491, 0.0342, 0.0046, 0.0181, 0.0024, 0.0022,
        0.0111, 0.0094, 0.0267, 0.0053], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,863][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.1063, 0.0664, 0.0697, 0.0681, 0.0802, 0.0665, 0.0760, 0.0699, 0.0662,
        0.0805, 0.0841, 0.0751, 0.0911], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,865][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.1368, 0.0505, 0.0654, 0.0381, 0.0605, 0.0589, 0.0506, 0.0955, 0.0919,
        0.1095, 0.1032, 0.0598, 0.0792], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,866][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0238, 0.0949, 0.0633, 0.0786, 0.0847, 0.0642, 0.0941, 0.0723, 0.0746,
        0.0962, 0.0738, 0.0960, 0.0835], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,868][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.1781, 0.0027, 0.0156, 0.0088, 0.1504, 0.3009, 0.0076, 0.0875, 0.1472,
        0.0038, 0.0735, 0.0052, 0.0187], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,869][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.1316, 0.0114, 0.0578, 0.0091, 0.0314, 0.0579, 0.1779, 0.0368, 0.0726,
        0.0316, 0.2954, 0.0028, 0.0838], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,871][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.1742, 0.1257, 0.0298, 0.1069, 0.0565, 0.0400, 0.0494, 0.0362, 0.0640,
        0.1513, 0.0816, 0.0503, 0.0340], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,872][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.3260, 0.0046, 0.1984, 0.0143, 0.0224, 0.1748, 0.0060, 0.0048, 0.1174,
        0.0125, 0.0834, 0.0189, 0.0165], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,873][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1956, 0.0256, 0.0784, 0.0613, 0.0688, 0.0855, 0.0380, 0.0785, 0.1249,
        0.0216, 0.1083, 0.0555, 0.0580], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,875][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0010, 0.0356, 0.0369, 0.0543, 0.0771, 0.0502, 0.0776, 0.0874, 0.0817,
        0.0890, 0.1209, 0.1150, 0.1731], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,876][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([1.5085e-02, 4.4149e-04, 6.9754e-03, 1.1713e-04, 5.6477e-03, 4.2453e-03,
        4.5323e-04, 1.8200e-03, 5.3054e-03, 2.6074e-03, 6.5329e-02, 3.1181e-05,
        8.9194e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:28,876][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([6.3234e-02, 7.3338e-03, 7.0072e-04, 3.6156e-03, 9.3502e-02, 6.1782e-03,
        3.1788e-02, 7.9805e-03, 9.9648e-03, 5.5105e-03, 3.3226e-02, 1.8105e-02,
        7.1805e-01, 8.0989e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,877][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([1.5927e-01, 4.2565e-01, 2.1667e-02, 6.5960e-02, 9.3225e-02, 7.6702e-03,
        5.0813e-02, 9.2974e-03, 6.4245e-03, 2.2601e-02, 4.9928e-03, 3.4447e-02,
        9.7868e-02, 1.2170e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,877][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0919, 0.0618, 0.0634, 0.0638, 0.0762, 0.0632, 0.0711, 0.0646, 0.0626,
        0.0757, 0.0778, 0.0715, 0.0862, 0.0704], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,878][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1264, 0.0449, 0.0615, 0.0347, 0.0572, 0.0558, 0.0459, 0.0910, 0.0868,
        0.1027, 0.0985, 0.0559, 0.0768, 0.0619], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,878][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0249, 0.0853, 0.0570, 0.0760, 0.0800, 0.0626, 0.0822, 0.0656, 0.0698,
        0.0927, 0.0669, 0.0916, 0.0796, 0.0657], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,878][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([4.5754e-02, 5.1443e-04, 1.3208e-02, 3.1137e-03, 4.3247e-03, 2.3484e-02,
        2.0444e-03, 1.2465e-02, 3.3349e-02, 9.9142e-04, 1.7864e-02, 9.5989e-04,
        2.5324e-03, 8.3939e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,879][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1711, 0.0469, 0.0826, 0.0188, 0.0361, 0.0978, 0.1368, 0.0571, 0.0667,
        0.0504, 0.0898, 0.0072, 0.1066, 0.0321], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,879][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1007, 0.0972, 0.0464, 0.0739, 0.0283, 0.0450, 0.0709, 0.0512, 0.0681,
        0.1096, 0.1481, 0.0383, 0.0541, 0.0682], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,880][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2934, 0.0046, 0.2061, 0.0138, 0.0215, 0.1782, 0.0061, 0.0054, 0.1194,
        0.0118, 0.0855, 0.0187, 0.0171, 0.0184], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,881][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3505, 0.0102, 0.0306, 0.0146, 0.0522, 0.1015, 0.0154, 0.0448, 0.1341,
        0.0167, 0.0582, 0.0222, 0.0846, 0.0642], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,883][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0019, 0.0249, 0.0251, 0.0402, 0.0581, 0.0416, 0.0578, 0.0682, 0.0661,
        0.0670, 0.1017, 0.0963, 0.1667, 0.1844], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,883][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([5.8742e-02, 3.8907e-04, 4.6561e-02, 1.4141e-03, 2.4262e-03, 6.0254e-03,
        1.9427e-03, 7.8384e-03, 2.1132e-03, 1.9557e-03, 6.2058e-02, 1.9233e-04,
        4.2206e-03, 8.0412e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:28,885][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0548, 0.0041, 0.0021, 0.0106, 0.0050, 0.0187, 0.0889, 0.0132, 0.0128,
        0.0082, 0.0888, 0.0365, 0.4409, 0.2012, 0.0141], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,886][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.1297, 0.5609, 0.0130, 0.0811, 0.0410, 0.0055, 0.0189, 0.0047, 0.0100,
        0.0084, 0.0029, 0.0361, 0.0853, 0.0007, 0.0019], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,888][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0894, 0.0571, 0.0609, 0.0574, 0.0692, 0.0568, 0.0660, 0.0598, 0.0568,
        0.0697, 0.0729, 0.0636, 0.0818, 0.0643, 0.0743], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,889][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.1206, 0.0411, 0.0553, 0.0319, 0.0517, 0.0500, 0.0434, 0.0808, 0.0783,
        0.0955, 0.0888, 0.0517, 0.0684, 0.0564, 0.0863], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,891][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0208, 0.0795, 0.0540, 0.0658, 0.0722, 0.0556, 0.0772, 0.0628, 0.0641,
        0.0834, 0.0636, 0.0795, 0.0771, 0.0602, 0.0842], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,892][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.1720, 0.0039, 0.0169, 0.0074, 0.0589, 0.1335, 0.1289, 0.0335, 0.1268,
        0.0038, 0.1386, 0.0052, 0.0540, 0.0183, 0.0982], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,894][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0297, 0.0363, 0.0431, 0.0070, 0.0544, 0.0469, 0.3713, 0.0539, 0.0513,
        0.0104, 0.0669, 0.0040, 0.0795, 0.0518, 0.0933], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,895][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0715, 0.0906, 0.0417, 0.0592, 0.0292, 0.0456, 0.0786, 0.0481, 0.0731,
        0.0996, 0.1314, 0.0528, 0.0821, 0.0697, 0.0268], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,897][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.2998, 0.0047, 0.1879, 0.0142, 0.0235, 0.1775, 0.0059, 0.0053, 0.1202,
        0.0120, 0.0806, 0.0185, 0.0175, 0.0173, 0.0150], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,898][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.2246, 0.0079, 0.0284, 0.0099, 0.0424, 0.1243, 0.0070, 0.0566, 0.1673,
        0.0105, 0.0835, 0.0186, 0.0631, 0.0962, 0.0599], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,899][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0011, 0.0230, 0.0240, 0.0361, 0.0539, 0.0359, 0.0529, 0.0609, 0.0562,
        0.0608, 0.0856, 0.0784, 0.1347, 0.1438, 0.1528], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,900][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ give] are: tensor([7.6257e-03, 5.5516e-04, 5.6096e-03, 5.3611e-05, 1.1924e-02, 3.8448e-04,
        1.2483e-03, 8.6768e-04, 3.6164e-04, 8.6745e-04, 2.1333e-02, 9.2300e-06,
        7.4826e-04, 3.5522e-03, 9.4486e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:28,902][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0434, 0.0016, 0.0019, 0.0055, 0.0241, 0.0142, 0.0345, 0.0053, 0.0121,
        0.0123, 0.0416, 0.0141, 0.3491, 0.0381, 0.3979, 0.0042],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,903][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ it] are: tensor([2.0572e-01, 3.8553e-01, 2.4193e-02, 3.6967e-02, 8.4987e-02, 7.9983e-03,
        7.9663e-02, 1.0401e-02, 2.7777e-03, 1.3374e-02, 9.5261e-03, 2.1465e-02,
        8.2710e-02, 2.0912e-03, 3.2405e-02, 1.8656e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,904][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0819, 0.0546, 0.0562, 0.0552, 0.0646, 0.0551, 0.0615, 0.0563, 0.0551,
        0.0647, 0.0686, 0.0616, 0.0750, 0.0597, 0.0698, 0.0600],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,906][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.1076, 0.0371, 0.0505, 0.0294, 0.0479, 0.0467, 0.0397, 0.0745, 0.0721,
        0.0878, 0.0823, 0.0483, 0.0648, 0.0535, 0.0812, 0.0767],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,907][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0203, 0.0771, 0.0501, 0.0647, 0.0686, 0.0533, 0.0710, 0.0571, 0.0608,
        0.0796, 0.0594, 0.0788, 0.0702, 0.0548, 0.0786, 0.0556],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,908][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ it] are: tensor([8.8007e-02, 6.7873e-03, 1.5930e-02, 2.2703e-02, 6.9650e-03, 5.2303e-02,
        2.2020e-02, 6.7806e-03, 7.8533e-02, 2.7836e-04, 1.7414e-01, 4.5703e-03,
        6.3427e-03, 4.0736e-03, 1.7238e-02, 4.9332e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,909][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.1098, 0.0169, 0.0483, 0.0111, 0.0195, 0.1178, 0.0813, 0.1012, 0.1407,
        0.0756, 0.0753, 0.0038, 0.0563, 0.0782, 0.0443, 0.0198],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,911][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0828, 0.0878, 0.0414, 0.1111, 0.0132, 0.0650, 0.0556, 0.0426, 0.0965,
        0.0759, 0.1076, 0.0521, 0.0446, 0.0580, 0.0353, 0.0304],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,912][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.3128, 0.0049, 0.1649, 0.0146, 0.0218, 0.1756, 0.0059, 0.0052, 0.1226,
        0.0127, 0.0747, 0.0193, 0.0174, 0.0171, 0.0150, 0.0155],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,914][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.2587, 0.0100, 0.0333, 0.0131, 0.0383, 0.1097, 0.0110, 0.0439, 0.1437,
        0.0122, 0.0686, 0.0197, 0.0681, 0.0710, 0.0503, 0.0485],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,915][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0020, 0.0190, 0.0190, 0.0295, 0.0414, 0.0317, 0.0431, 0.0486, 0.0499,
        0.0496, 0.0747, 0.0677, 0.1119, 0.1241, 0.1348, 0.1529],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,916][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ it] are: tensor([7.7138e-03, 6.3355e-04, 5.5103e-03, 2.1996e-04, 6.5290e-04, 5.3504e-04,
        6.1403e-04, 1.1365e-03, 2.2722e-03, 1.3412e-03, 2.0291e-02, 9.3558e-05,
        2.0100e-04, 4.8593e-03, 4.7956e-04, 9.5345e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:28,917][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([4.0080e-02, 5.0798e-03, 3.6272e-04, 2.6376e-03, 5.3680e-02, 3.8679e-03,
        2.1192e-02, 4.8211e-03, 5.9761e-03, 3.2917e-03, 1.9168e-02, 1.2358e-02,
        3.5487e-01, 5.1859e-04, 3.7163e-01, 9.9037e-02, 1.4303e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,918][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([1.7725e-01, 4.3683e-01, 2.1447e-02, 5.7544e-02, 8.9296e-02, 7.2492e-03,
        3.9484e-02, 9.0431e-03, 6.4525e-03, 1.5835e-02, 4.9671e-03, 2.8929e-02,
        8.7387e-02, 1.1269e-04, 1.5563e-02, 2.5320e-03, 8.3020e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,920][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0733, 0.0510, 0.0515, 0.0528, 0.0630, 0.0520, 0.0585, 0.0530, 0.0514,
        0.0626, 0.0634, 0.0593, 0.0710, 0.0578, 0.0667, 0.0561, 0.0564],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,921][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0995, 0.0341, 0.0468, 0.0267, 0.0442, 0.0434, 0.0362, 0.0692, 0.0667,
        0.0801, 0.0758, 0.0440, 0.0604, 0.0499, 0.0756, 0.0708, 0.0766],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,923][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0208, 0.0702, 0.0466, 0.0625, 0.0659, 0.0515, 0.0676, 0.0538, 0.0574,
        0.0760, 0.0548, 0.0754, 0.0655, 0.0538, 0.0768, 0.0519, 0.0496],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,924][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.4317e-02, 1.7605e-04, 4.5948e-03, 9.4132e-04, 1.0888e-03, 1.0559e-02,
        8.0972e-04, 5.9082e-03, 1.4964e-02, 4.0306e-04, 8.6681e-03, 2.9043e-04,
        5.0603e-04, 3.9476e-01, 4.4053e-04, 7.3564e-04, 5.4084e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,925][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1012, 0.0315, 0.0732, 0.0133, 0.0301, 0.0793, 0.1093, 0.0534, 0.0586,
        0.0331, 0.0754, 0.0055, 0.0733, 0.0292, 0.1160, 0.0842, 0.0333],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,927][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0753, 0.1049, 0.0435, 0.0722, 0.0155, 0.0324, 0.0600, 0.0442, 0.0530,
        0.0896, 0.1512, 0.0319, 0.0333, 0.0584, 0.0457, 0.0200, 0.0690],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,928][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2641, 0.0042, 0.1866, 0.0131, 0.0205, 0.1809, 0.0061, 0.0053, 0.1207,
        0.0122, 0.0828, 0.0197, 0.0172, 0.0185, 0.0147, 0.0152, 0.0181],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,930][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.2812, 0.0096, 0.0266, 0.0121, 0.0449, 0.0977, 0.0134, 0.0384, 0.1243,
        0.0142, 0.0459, 0.0170, 0.0674, 0.0555, 0.0539, 0.0425, 0.0555],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,931][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0022, 0.0136, 0.0140, 0.0226, 0.0327, 0.0233, 0.0330, 0.0389, 0.0377,
        0.0369, 0.0595, 0.0548, 0.0978, 0.1114, 0.1202, 0.1316, 0.1698],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,932][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([2.9625e-02, 2.9907e-04, 2.3072e-02, 1.0523e-03, 1.4573e-03, 3.8747e-03,
        1.0532e-03, 4.6889e-03, 1.2268e-03, 1.0488e-03, 3.7161e-02, 2.3768e-04,
        2.6185e-03, 4.0287e-01, 6.8654e-04, 4.2919e-04, 4.8860e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:28,955][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:28,956][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,957][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,958][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,960][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,961][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,962][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,963][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,964][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,965][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,966][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,977][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,978][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:28,980][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.5003, 0.4997], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,981][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.0473, 0.9527], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,982][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.3766, 0.6234], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,984][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.6919, 0.3081], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,985][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.2058, 0.7942], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,986][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.6309, 0.3691], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,988][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.9773, 0.0227], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,989][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.3661, 0.6339], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,991][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.9311, 0.0689], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,991][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.0034, 0.9966], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,991][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.0020, 0.9980], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,992][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.4680, 0.5320], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:28,992][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3333, 0.3329, 0.3337], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,992][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0581, 0.7510, 0.1909], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,992][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.1570, 0.3601, 0.4830], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,993][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.3376, 0.1792, 0.4832], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,993][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.1394, 0.5246, 0.3360], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,993][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1355, 0.0442, 0.8203], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,994][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.8178, 0.0185, 0.1637], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,994][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.3205, 0.3708, 0.3088], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,995][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7642, 0.0805, 0.1554], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,996][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0014, 0.9796, 0.0191], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,997][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0066, 0.4716, 0.5218], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:28,999][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1018, 0.0033, 0.8949], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,000][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2501, 0.2498, 0.2504, 0.2497], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,001][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.1886, 0.7235, 0.0871, 0.0009], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,003][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.1091, 0.2334, 0.4400, 0.2176], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,004][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.2956, 0.1228, 0.4179, 0.1638], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,006][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0839, 0.3642, 0.2520, 0.2999], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,007][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.1127, 0.1129, 0.7657, 0.0086], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,008][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.6671, 0.0151, 0.2954, 0.0223], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,010][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.1690, 0.4181, 0.2724, 0.1405], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,011][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.7837, 0.0709, 0.0739, 0.0715], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,012][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([5.1476e-04, 1.7100e-01, 1.4614e-02, 8.1387e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,013][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0032, 0.2780, 0.3878, 0.3309], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,015][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0866, 0.0056, 0.8899, 0.0179], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,016][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.2001, 0.1999, 0.2004, 0.1998, 0.1999], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,018][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0715, 0.7080, 0.1728, 0.0054, 0.0424], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,019][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0840, 0.2411, 0.2545, 0.2027, 0.2177], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,021][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.2797, 0.1097, 0.3351, 0.1652, 0.1102], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,022][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0694, 0.2762, 0.1865, 0.2270, 0.2410], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,023][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.2033, 0.0474, 0.5902, 0.0039, 0.1552], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,024][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.7158, 0.0234, 0.1853, 0.0179, 0.0576], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,026][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0895, 0.1970, 0.1290, 0.1103, 0.4741], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,027][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.4962, 0.1093, 0.0946, 0.1558, 0.1443], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,028][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0007, 0.2586, 0.0012, 0.1516, 0.5879], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,030][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0048, 0.1675, 0.2144, 0.2402, 0.3731], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,031][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([1.2196e-01, 3.2176e-03, 8.5696e-01, 7.5290e-04, 1.7106e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,032][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1668, 0.1666, 0.1670, 0.1665, 0.1666, 0.1665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,034][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1644, 0.7335, 0.0836, 0.0011, 0.0141, 0.0033], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,035][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0547, 0.1786, 0.2093, 0.1397, 0.1493, 0.2684], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,036][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1835, 0.0962, 0.2296, 0.1238, 0.0800, 0.2869], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,038][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0615, 0.2264, 0.1526, 0.1913, 0.2042, 0.1639], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,039][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0819, 0.0300, 0.3873, 0.0036, 0.0433, 0.4539], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,040][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.4124, 0.0138, 0.0784, 0.0170, 0.0341, 0.4443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,042][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1149, 0.1306, 0.1043, 0.0937, 0.3091, 0.2473], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,043][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2615, 0.0879, 0.0452, 0.1161, 0.0689, 0.4205], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,044][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([2.5524e-04, 1.0391e-01, 9.8049e-04, 2.1223e-01, 6.7786e-01, 4.7674e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,046][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0056, 0.1328, 0.1552, 0.1935, 0.2863, 0.2266], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,047][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0781, 0.0023, 0.4540, 0.0006, 0.0021, 0.4629], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,048][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.1430, 0.1428, 0.1431, 0.1427, 0.1428, 0.1427, 0.1429],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,049][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([2.2803e-01, 6.8642e-01, 6.7521e-02, 9.7320e-04, 1.3499e-02, 3.3378e-03,
        2.1413e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,049][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0559, 0.1470, 0.1880, 0.1223, 0.1567, 0.1924, 0.1378],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,049][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.1770, 0.0778, 0.2376, 0.1088, 0.0656, 0.2358, 0.0973],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,050][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0495, 0.1943, 0.1305, 0.1574, 0.1691, 0.1338, 0.1656],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,050][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0856, 0.0072, 0.2043, 0.0016, 0.0706, 0.4143, 0.2164],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,050][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.3759, 0.0046, 0.0654, 0.0056, 0.0241, 0.4203, 0.1041],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,051][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0600, 0.1487, 0.0862, 0.0758, 0.3285, 0.2524, 0.0483],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,051][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.3198, 0.0222, 0.0880, 0.0501, 0.1221, 0.3783, 0.0194],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,051][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([8.6177e-06, 2.2052e-02, 2.6899e-03, 2.3413e-01, 2.1588e-01, 1.0782e-04,
        5.2513e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,052][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0023, 0.1172, 0.1451, 0.1535, 0.2216, 0.1712, 0.1891],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,053][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0995, 0.0036, 0.6791, 0.0007, 0.0017, 0.0431, 0.1724],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,054][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.1251, 0.1249, 0.1252, 0.1249, 0.1249, 0.1248, 0.1250, 0.1252],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,055][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([1.3608e-01, 7.2463e-01, 1.0133e-01, 2.1219e-03, 2.3129e-02, 6.6604e-03,
        4.3809e-04, 5.6139e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,057][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0453, 0.1249, 0.1228, 0.1163, 0.1109, 0.1573, 0.1135, 0.2088],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,058][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1426, 0.0698, 0.1801, 0.0960, 0.0644, 0.1886, 0.0903, 0.1683],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,059][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0449, 0.1633, 0.1086, 0.1402, 0.1468, 0.1152, 0.1560, 0.1251],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,061][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.1514, 0.0181, 0.3043, 0.0027, 0.0300, 0.3496, 0.0694, 0.0745],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,062][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.3763, 0.0077, 0.0800, 0.0198, 0.0317, 0.3719, 0.0789, 0.0337],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,064][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0790, 0.1100, 0.0787, 0.0787, 0.2413, 0.1987, 0.1107, 0.1029],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,065][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.2305, 0.0849, 0.0359, 0.1072, 0.0672, 0.3464, 0.0557, 0.0722],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,066][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([4.8829e-05, 7.3340e-02, 8.1343e-04, 1.0837e-01, 2.1779e-01, 5.7112e-03,
        1.4789e-01, 4.4605e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,067][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0070, 0.0830, 0.0980, 0.1245, 0.1839, 0.1320, 0.1779, 0.1937],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,069][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.1293, 0.0040, 0.4967, 0.0007, 0.0019, 0.0255, 0.0057, 0.3362],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,070][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1112, 0.1110, 0.1113, 0.1110, 0.1110, 0.1109, 0.1111, 0.1112, 0.1112],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,071][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([1.8639e-01, 6.7263e-01, 1.0487e-01, 1.4548e-03, 2.0656e-02, 5.4513e-03,
        3.3074e-04, 5.4564e-03, 2.7592e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,073][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0218, 0.1005, 0.0856, 0.0806, 0.0853, 0.1282, 0.1129, 0.1372, 0.2479],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,074][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1100, 0.0566, 0.1438, 0.0830, 0.0502, 0.1689, 0.0787, 0.1260, 0.1828],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,075][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0394, 0.1452, 0.0940, 0.1234, 0.1321, 0.1012, 0.1444, 0.1061, 0.1143],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,077][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0534, 0.0184, 0.2087, 0.0029, 0.0241, 0.2150, 0.0257, 0.0093, 0.4425],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,078][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2662, 0.0065, 0.0447, 0.0056, 0.0158, 0.2383, 0.0352, 0.0182, 0.3694],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,080][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0699, 0.0772, 0.0638, 0.0637, 0.1979, 0.1737, 0.1074, 0.0925, 0.1539],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,081][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1633, 0.0792, 0.0291, 0.0817, 0.0420, 0.2890, 0.0543, 0.0533, 0.2082],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,082][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([4.7839e-05, 3.1515e-02, 3.5322e-04, 4.5140e-02, 1.8040e-01, 2.6791e-03,
        1.0060e-01, 6.0556e-01, 3.3707e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,083][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0082, 0.0616, 0.0774, 0.1044, 0.1478, 0.1144, 0.1510, 0.1561, 0.1790],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,085][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0924, 0.0023, 0.5147, 0.0006, 0.0017, 0.0783, 0.0075, 0.0193, 0.2834],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,086][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.1000, 0.0999, 0.1002, 0.0999, 0.0999, 0.0998, 0.1000, 0.1001, 0.1001,
        0.1000], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,087][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([1.7738e-01, 6.8555e-01, 9.9839e-02, 1.2901e-03, 2.0565e-02, 4.9015e-03,
        2.7499e-04, 6.7913e-03, 3.0265e-03, 3.8143e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,088][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0299, 0.0885, 0.1177, 0.0725, 0.0801, 0.1014, 0.0778, 0.1504, 0.1562,
        0.1255], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,090][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.1385, 0.0438, 0.1566, 0.0682, 0.0491, 0.1489, 0.0676, 0.1358, 0.1523,
        0.0393], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,091][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0304, 0.1261, 0.0885, 0.1049, 0.1123, 0.0900, 0.1146, 0.0971, 0.1052,
        0.1308], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,093][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0404, 0.0101, 0.1835, 0.0012, 0.0195, 0.1725, 0.0247, 0.0090, 0.5384,
        0.0007], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,094][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.3047, 0.0073, 0.0772, 0.0040, 0.0127, 0.2431, 0.0183, 0.0258, 0.3037,
        0.0033], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,095][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0388, 0.1135, 0.0580, 0.0708, 0.2266, 0.1388, 0.0850, 0.0865, 0.1260,
        0.0560], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,097][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.1389, 0.0380, 0.0338, 0.0646, 0.1115, 0.2280, 0.0287, 0.0389, 0.2862,
        0.0315], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,098][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.9209e-05, 4.1046e-02, 1.6938e-03, 1.2155e-01, 2.3993e-01, 5.1744e-04,
        3.6926e-01, 2.2052e-01, 4.0213e-03, 1.4370e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,099][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0034, 0.0594, 0.0759, 0.0857, 0.1346, 0.0965, 0.1235, 0.1396, 0.1382,
        0.1433], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,100][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([1.1602e-01, 2.8469e-03, 7.9939e-01, 4.5406e-04, 6.9665e-04, 1.5969e-02,
        4.7998e-03, 3.1690e-02, 2.6663e-02, 1.4743e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,102][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0909, 0.0908, 0.0911, 0.0908, 0.0908, 0.0908, 0.0909, 0.0910, 0.0910,
        0.0909, 0.0910], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,102][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([2.2428e-01, 6.5194e-01, 9.3376e-02, 1.0573e-03, 1.6396e-02, 3.9914e-03,
        2.2075e-04, 3.9836e-03, 2.1125e-03, 3.3140e-04, 2.3116e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,104][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0159, 0.0729, 0.0655, 0.0729, 0.0867, 0.0897, 0.0841, 0.1204, 0.1547,
        0.1561, 0.0812], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,105][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0997, 0.0444, 0.1310, 0.0697, 0.0453, 0.1420, 0.0687, 0.1192, 0.1363,
        0.0438, 0.0999], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,106][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0324, 0.1137, 0.0762, 0.0993, 0.1067, 0.0797, 0.1104, 0.0856, 0.0892,
        0.1203, 0.0864], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,106][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0477, 0.0125, 0.2000, 0.0032, 0.0190, 0.1669, 0.0282, 0.0086, 0.3785,
        0.0017, 0.1337], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,107][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.1958, 0.0058, 0.0362, 0.0097, 0.0184, 0.2240, 0.0338, 0.0213, 0.3371,
        0.0079, 0.1099], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,107][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0525, 0.0624, 0.0597, 0.0584, 0.1361, 0.1417, 0.0883, 0.0810, 0.1342,
        0.0727, 0.1129], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,107][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.1881, 0.0473, 0.0616, 0.0609, 0.0497, 0.2421, 0.0497, 0.0383, 0.1850,
        0.0422, 0.0351], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,108][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([6.3464e-05, 1.4781e-02, 3.3730e-04, 4.4869e-03, 1.9226e-02, 1.1254e-03,
        1.0529e-02, 9.4058e-01, 6.4151e-03, 5.1380e-05, 2.4016e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,108][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0071, 0.0403, 0.0521, 0.0723, 0.1063, 0.0731, 0.1046, 0.1114, 0.1161,
        0.1307, 0.1862], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,109][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([5.5410e-02, 2.3766e-03, 2.3817e-01, 7.3982e-04, 1.1826e-03, 2.2348e-02,
        4.5845e-03, 2.2471e-02, 3.8991e-02, 5.3508e-04, 6.1319e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,109][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0834, 0.0833, 0.0835, 0.0832, 0.0833, 0.0832, 0.0833, 0.0834, 0.0834,
        0.0833, 0.0834, 0.0834], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,110][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([2.9716e-01, 6.0182e-01, 7.7129e-02, 6.4795e-04, 1.1802e-02, 2.9810e-03,
        1.8707e-04, 3.6604e-03, 1.9666e-03, 2.4947e-04, 2.2112e-03, 1.8530e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,111][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0219, 0.0581, 0.1026, 0.0536, 0.0795, 0.0843, 0.0648, 0.1170, 0.1420,
        0.1294, 0.0966, 0.0503], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,112][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0979, 0.0387, 0.1340, 0.0521, 0.0467, 0.1328, 0.0588, 0.1080, 0.1308,
        0.0396, 0.1073, 0.0532], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,114][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0240, 0.1064, 0.0728, 0.0869, 0.0904, 0.0718, 0.0937, 0.0782, 0.0825,
        0.1054, 0.0822, 0.1057], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,115][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([2.9718e-02, 2.3145e-02, 1.8659e-01, 1.6748e-03, 1.3778e-02, 1.9983e-01,
        1.7131e-02, 6.8245e-03, 4.2667e-01, 3.5899e-04, 9.3314e-02, 9.7237e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,116][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.1527, 0.0034, 0.0659, 0.0054, 0.0071, 0.1981, 0.0156, 0.0162, 0.3552,
        0.0019, 0.1715, 0.0069], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,117][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0359, 0.0811, 0.0557, 0.0281, 0.1603, 0.1494, 0.0748, 0.0558, 0.1385,
        0.0640, 0.1185, 0.0378], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,119][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1564, 0.0081, 0.0072, 0.0105, 0.0292, 0.3874, 0.0103, 0.0180, 0.2103,
        0.0150, 0.1341, 0.0135], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,120][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([3.0991e-05, 2.3117e-02, 2.7524e-03, 1.0248e-01, 2.4928e-01, 2.4667e-04,
        4.3745e-01, 1.4203e-01, 2.4695e-03, 1.0736e-03, 9.1448e-03, 2.9932e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,121][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0042, 0.0377, 0.0554, 0.0625, 0.0969, 0.0741, 0.0862, 0.1041, 0.1078,
        0.1057, 0.1515, 0.1140], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,122][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([1.2948e-02, 7.9333e-04, 1.3110e-01, 2.1548e-03, 2.7043e-04, 8.0234e-03,
        1.5475e-03, 4.1023e-03, 6.2937e-03, 5.8460e-05, 8.3079e-01, 1.9138e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,123][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0769, 0.0769, 0.0770, 0.0768, 0.0769, 0.0768, 0.0769, 0.0770, 0.0770,
        0.0769, 0.0770, 0.0770, 0.0769], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,124][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([1.6205e-01, 7.0395e-01, 9.5621e-02, 1.2379e-03, 1.7345e-02, 4.4960e-03,
        2.6186e-04, 5.0291e-03, 2.7007e-03, 4.0151e-04, 3.2633e-03, 3.5276e-04,
        3.2978e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,126][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0203, 0.0643, 0.0693, 0.0632, 0.0635, 0.0783, 0.0649, 0.1079, 0.1242,
        0.1394, 0.0845, 0.0613, 0.0589], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,127][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0892, 0.0383, 0.1117, 0.0579, 0.0407, 0.1240, 0.0627, 0.1009, 0.1273,
        0.0389, 0.0925, 0.0594, 0.0565], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,129][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0241, 0.0955, 0.0635, 0.0787, 0.0848, 0.0639, 0.0946, 0.0721, 0.0741,
        0.0963, 0.0735, 0.0958, 0.0833], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,130][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0855, 0.0159, 0.1413, 0.0021, 0.0519, 0.1306, 0.0155, 0.0122, 0.2859,
        0.0013, 0.0768, 0.0014, 0.1795], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,132][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.2496, 0.0048, 0.0623, 0.0050, 0.0207, 0.1703, 0.0242, 0.0206, 0.2800,
        0.0023, 0.1132, 0.0057, 0.0415], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,133][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0358, 0.0654, 0.0428, 0.0434, 0.1674, 0.0932, 0.0732, 0.0649, 0.0967,
        0.0678, 0.0810, 0.0574, 0.1109], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,135][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.1485, 0.0143, 0.0442, 0.0309, 0.0590, 0.2155, 0.0193, 0.0193, 0.2916,
        0.0268, 0.0713, 0.0242, 0.0351], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,136][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([1.9723e-05, 3.8743e-02, 9.4485e-04, 8.4268e-02, 1.0450e-01, 7.8184e-04,
        1.8080e-01, 3.5406e-01, 1.7966e-02, 5.6081e-04, 8.6282e-03, 2.8568e-02,
        1.8016e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,137][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0038, 0.0332, 0.0438, 0.0565, 0.0810, 0.0527, 0.0804, 0.0839, 0.0847,
        0.0967, 0.1296, 0.1064, 0.1474], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,138][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([3.2944e-02, 1.3295e-03, 1.8624e-01, 1.1015e-04, 8.3058e-04, 1.0116e-02,
        1.6910e-03, 4.9899e-03, 1.5637e-02, 1.7644e-04, 7.3484e-01, 1.1621e-04,
        1.0983e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,139][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0714, 0.0714, 0.0715, 0.0713, 0.0714, 0.0713, 0.0714, 0.0715, 0.0715,
        0.0714, 0.0715, 0.0715, 0.0714, 0.0715], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,140][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([3.0518e-01, 6.1293e-01, 6.3696e-02, 6.0134e-04, 9.2248e-03, 2.0263e-03,
        1.1691e-04, 2.0545e-03, 1.0937e-03, 1.4899e-04, 1.2062e-03, 1.2204e-04,
        1.1562e-03, 4.3978e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,142][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0134, 0.0573, 0.0481, 0.0539, 0.0582, 0.0721, 0.0629, 0.0937, 0.1199,
        0.1164, 0.0607, 0.0548, 0.0549, 0.1337], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,143][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0761, 0.0358, 0.0908, 0.0541, 0.0369, 0.1062, 0.0518, 0.0942, 0.1072,
        0.0350, 0.0793, 0.0566, 0.0510, 0.1251], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,145][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0252, 0.0859, 0.0572, 0.0761, 0.0802, 0.0625, 0.0825, 0.0655, 0.0695,
        0.0928, 0.0667, 0.0914, 0.0794, 0.0651], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,146][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0552, 0.0167, 0.1706, 0.0042, 0.0208, 0.1531, 0.0272, 0.0116, 0.3161,
        0.0017, 0.0714, 0.0028, 0.1180, 0.0307], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,148][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.2426, 0.0041, 0.0354, 0.0051, 0.0148, 0.2358, 0.0294, 0.0137, 0.2959,
        0.0039, 0.0720, 0.0058, 0.0143, 0.0273], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,149][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0464, 0.0482, 0.0449, 0.0390, 0.1090, 0.1093, 0.0666, 0.0565, 0.0960,
        0.0507, 0.0891, 0.0505, 0.1149, 0.0788], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,151][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.1199, 0.0787, 0.0178, 0.0807, 0.0353, 0.2768, 0.0441, 0.0412, 0.1150,
        0.0400, 0.0230, 0.0663, 0.0262, 0.0352], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,152][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.8411e-05, 1.6982e-02, 2.2218e-04, 5.5040e-03, 4.1065e-02, 1.2512e-03,
        1.2873e-02, 8.5926e-01, 9.3806e-03, 5.0963e-05, 1.7496e-03, 2.2909e-03,
        4.9071e-02, 2.4064e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,153][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0101, 0.0227, 0.0302, 0.0444, 0.0638, 0.0449, 0.0617, 0.0671, 0.0681,
        0.0731, 0.1111, 0.0908, 0.1428, 0.1692], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,154][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([2.5012e-02, 6.8849e-04, 1.5099e-01, 2.3374e-04, 4.3740e-04, 1.4372e-02,
        1.4168e-03, 1.1352e-02, 1.2755e-02, 1.1828e-04, 4.3482e-01, 2.5341e-04,
        4.5259e-04, 3.4710e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,155][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0665, 0.0667, 0.0667, 0.0667,
        0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0667], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,156][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([2.2335e-01, 6.7507e-01, 8.0297e-02, 5.8952e-04, 1.1079e-02, 2.4190e-03,
        1.1348e-04, 2.4533e-03, 1.2268e-03, 1.5331e-04, 1.2165e-03, 1.1334e-04,
        1.1646e-03, 4.6597e-04, 2.8866e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,158][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0159, 0.0593, 0.0643, 0.0516, 0.0481, 0.0590, 0.0562, 0.0885, 0.0899,
        0.1004, 0.0649, 0.0499, 0.0545, 0.1056, 0.0921], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,159][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0787, 0.0307, 0.0913, 0.0482, 0.0336, 0.0967, 0.0466, 0.0870, 0.1001,
        0.0311, 0.0794, 0.0505, 0.0464, 0.1157, 0.0640], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,161][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0210, 0.0801, 0.0541, 0.0659, 0.0724, 0.0554, 0.0776, 0.0626, 0.0638,
        0.0836, 0.0634, 0.0793, 0.0770, 0.0596, 0.0842], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,162][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0549, 0.0101, 0.1221, 0.0010, 0.0308, 0.1245, 0.0304, 0.0057, 0.2854,
        0.0005, 0.0773, 0.0007, 0.1358, 0.0053, 0.1156], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,164][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.1879, 0.0053, 0.0758, 0.0044, 0.0218, 0.1893, 0.0334, 0.0245, 0.2551,
        0.0017, 0.1157, 0.0050, 0.0128, 0.0436, 0.0239], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,164][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0303, 0.0605, 0.0378, 0.0326, 0.1428, 0.0795, 0.0691, 0.0507, 0.0774,
        0.0492, 0.0611, 0.0422, 0.1500, 0.0568, 0.0598], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,165][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.1357, 0.0214, 0.0211, 0.0345, 0.0495, 0.2626, 0.0187, 0.0332, 0.1966,
        0.0235, 0.0544, 0.0330, 0.0313, 0.0473, 0.0372], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,165][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([5.0995e-05, 3.6082e-02, 5.7003e-04, 1.7731e-02, 5.9740e-02, 2.5855e-03,
        3.9622e-02, 6.0443e-01, 3.7581e-02, 1.8678e-04, 4.0236e-03, 6.7246e-03,
        1.3240e-01, 3.1785e-04, 5.7962e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,166][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0046, 0.0212, 0.0288, 0.0392, 0.0601, 0.0389, 0.0571, 0.0604, 0.0591,
        0.0665, 0.0937, 0.0745, 0.1175, 0.1339, 0.1445], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,166][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([2.9289e-02, 1.4679e-03, 1.5130e-01, 1.1168e-04, 1.1500e-03, 8.7277e-03,
        2.4363e-03, 3.8088e-03, 7.7309e-03, 1.4004e-04, 6.0317e-01, 1.1167e-04,
        6.9883e-04, 1.3239e-01, 5.7460e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,166][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0625, 0.0624, 0.0626, 0.0624, 0.0624, 0.0624, 0.0625, 0.0626, 0.0625,
        0.0625, 0.0625, 0.0625, 0.0625, 0.0626, 0.0625, 0.0626],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,167][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([2.5027e-01, 6.6881e-01, 6.6811e-02, 3.5446e-04, 7.8380e-03, 1.5109e-03,
        6.6346e-05, 1.5397e-03, 6.7763e-04, 7.6926e-05, 7.4302e-04, 5.9940e-05,
        6.7386e-04, 2.8046e-04, 1.7536e-04, 1.1340e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,167][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0110, 0.0498, 0.0423, 0.0405, 0.0382, 0.0539, 0.0475, 0.0691, 0.1001,
        0.0803, 0.0536, 0.0401, 0.0384, 0.0817, 0.0879, 0.1656],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,168][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0665, 0.0311, 0.0810, 0.0463, 0.0298, 0.0934, 0.0449, 0.0727, 0.0967,
        0.0296, 0.0697, 0.0483, 0.0438, 0.0945, 0.0562, 0.0953],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,169][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0205, 0.0777, 0.0502, 0.0648, 0.0687, 0.0532, 0.0713, 0.0570, 0.0605,
        0.0798, 0.0592, 0.0786, 0.0701, 0.0543, 0.0786, 0.0554],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,170][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([3.0655e-02, 7.1643e-03, 9.6082e-02, 1.3354e-03, 1.4821e-02, 1.3006e-01,
        1.2201e-02, 2.2633e-03, 3.4498e-01, 2.7078e-04, 5.9140e-02, 8.6981e-04,
        6.6158e-02, 2.4746e-03, 3.2554e-02, 1.9897e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,172][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.2766, 0.0032, 0.0335, 0.0052, 0.0092, 0.1499, 0.0244, 0.0126, 0.2999,
        0.0032, 0.0794, 0.0064, 0.0099, 0.0283, 0.0129, 0.0455],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,173][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0325, 0.0450, 0.0363, 0.0363, 0.1055, 0.0978, 0.0624, 0.0495, 0.0900,
        0.0445, 0.0705, 0.0487, 0.1049, 0.0562, 0.0568, 0.0631],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,174][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.1034, 0.0308, 0.0143, 0.0486, 0.0334, 0.3045, 0.0224, 0.0327, 0.1677,
        0.0284, 0.0263, 0.0387, 0.0234, 0.0359, 0.0318, 0.0578],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,175][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([4.7503e-05, 2.4509e-02, 3.5350e-04, 2.6152e-02, 1.0761e-01, 1.1412e-03,
        5.8308e-02, 4.8702e-01, 1.4531e-02, 1.8532e-04, 2.4508e-03, 9.8301e-03,
        2.0818e-01, 2.0656e-04, 4.4762e-02, 1.4711e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,176][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0077, 0.0188, 0.0244, 0.0345, 0.0478, 0.0365, 0.0496, 0.0506, 0.0551,
        0.0572, 0.0851, 0.0666, 0.0992, 0.1166, 0.1258, 0.1243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,177][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([3.0765e-02, 1.2944e-03, 1.8260e-01, 1.4009e-04, 4.4730e-04, 1.1041e-02,
        2.0147e-03, 3.7757e-03, 3.3406e-02, 1.1038e-04, 5.4744e-01, 1.4747e-04,
        4.8601e-04, 8.8885e-02, 1.7896e-03, 9.5660e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,179][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0588, 0.0588, 0.0589, 0.0587, 0.0588, 0.0587, 0.0588, 0.0589, 0.0589,
        0.0588, 0.0588, 0.0588, 0.0588, 0.0589, 0.0588, 0.0589, 0.0588],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,180][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([3.0648e-01, 6.0816e-01, 6.8026e-02, 5.1586e-04, 8.9398e-03, 1.8492e-03,
        9.2175e-05, 1.8218e-03, 9.1776e-04, 1.1895e-04, 9.9749e-04, 9.4638e-05,
        8.8619e-04, 3.5904e-04, 2.6764e-04, 1.8398e-04, 2.9287e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,181][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0091, 0.0402, 0.0338, 0.0377, 0.0408, 0.0494, 0.0437, 0.0658, 0.0820,
        0.0792, 0.0417, 0.0383, 0.0377, 0.0914, 0.0831, 0.1260, 0.1002],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,183][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0583, 0.0274, 0.0691, 0.0416, 0.0283, 0.0811, 0.0395, 0.0721, 0.0819,
        0.0270, 0.0609, 0.0437, 0.0391, 0.0962, 0.0532, 0.0801, 0.1005],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,184][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0210, 0.0707, 0.0468, 0.0626, 0.0660, 0.0514, 0.0679, 0.0537, 0.0572,
        0.0761, 0.0547, 0.0752, 0.0654, 0.0534, 0.0768, 0.0518, 0.0492],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,186][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0480, 0.0138, 0.1552, 0.0036, 0.0170, 0.1343, 0.0214, 0.0105, 0.2754,
        0.0015, 0.0644, 0.0024, 0.0919, 0.0276, 0.0439, 0.0561, 0.0329],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,187][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.2107, 0.0035, 0.0310, 0.0044, 0.0125, 0.2191, 0.0243, 0.0113, 0.2728,
        0.0034, 0.0637, 0.0050, 0.0119, 0.0240, 0.0263, 0.0529, 0.0232],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,189][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0410, 0.0377, 0.0369, 0.0317, 0.0866, 0.0913, 0.0572, 0.0470, 0.0813,
        0.0400, 0.0745, 0.0412, 0.0920, 0.0673, 0.0547, 0.0558, 0.0638],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,190][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0813, 0.0639, 0.0087, 0.0756, 0.0266, 0.3295, 0.0321, 0.0335, 0.0909,
        0.0345, 0.0158, 0.0598, 0.0173, 0.0247, 0.0241, 0.0567, 0.0249],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,191][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([7.1989e-05, 1.5682e-02, 1.8987e-04, 3.6340e-03, 3.3558e-02, 1.3670e-03,
        9.3302e-03, 8.5467e-01, 8.5375e-03, 3.8544e-05, 1.4685e-03, 1.6128e-03,
        4.1521e-02, 2.4416e-04, 2.6174e-02, 1.7307e-03, 1.7149e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,193][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0111, 0.0133, 0.0183, 0.0279, 0.0388, 0.0272, 0.0390, 0.0414, 0.0418,
        0.0436, 0.0694, 0.0565, 0.0888, 0.1067, 0.1148, 0.1079, 0.1536],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,194][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.6320e-02, 5.0689e-04, 1.0333e-01, 1.7564e-04, 3.1703e-04, 1.0318e-02,
        9.2434e-04, 8.1058e-03, 9.4737e-03, 8.6929e-05, 2.9618e-01, 1.9247e-04,
        3.5088e-04, 2.3722e-01, 1.0117e-03, 3.4122e-03, 3.1207e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,195][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:29,197][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 4487],
        [  145],
        [ 9387],
        [ 3208],
        [16358],
        [14948],
        [18092],
        [13204],
        [12077],
        [ 9792],
        [ 5593],
        [ 3636],
        [ 7602],
        [ 5889],
        [ 8396],
        [13962],
        [ 6829]], device='cuda:0')
[2024-07-24 10:21:29,198][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[  380],
        [    2],
        [20805],
        [ 3663],
        [35373],
        [18817],
        [17788],
        [19253],
        [16916],
        [ 5945],
        [ 7079],
        [ 3074],
        [23293],
        [ 8085],
        [16826],
        [21176],
        [ 8095]], device='cuda:0')
[2024-07-24 10:21:29,200][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 2634],
        [ 2617],
        [ 5235],
        [ 6489],
        [ 2317],
        [ 7996],
        [ 9773],
        [14048],
        [ 2159],
        [ 4039],
        [ 2818],
        [ 1920],
        [ 2096],
        [ 6483],
        [ 2089],
        [ 8702],
        [ 9842]], device='cuda:0')
[2024-07-24 10:21:29,201][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[32402],
        [30433],
        [19191],
        [26037],
        [21468],
        [31078],
        [28830],
        [24566],
        [27735],
        [31532],
        [24504],
        [30269],
        [20969],
        [23399],
        [21344],
        [25489],
        [24663]], device='cuda:0')
[2024-07-24 10:21:29,203][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 8982],
        [10642],
        [11590],
        [12777],
        [14182],
        [14919],
        [15349],
        [14613],
        [14656],
        [14543],
        [14716],
        [14923],
        [14593],
        [14709],
        [14676],
        [15038],
        [15114]], device='cuda:0')
[2024-07-24 10:21:29,204][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[15148],
        [16128],
        [17848],
        [19076],
        [18990],
        [19710],
        [20914],
        [20267],
        [20450],
        [20711],
        [20694],
        [20896],
        [20820],
        [20930],
        [20599],
        [20527],
        [20316]], device='cuda:0')
[2024-07-24 10:21:29,205][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[26532],
        [19153],
        [21649],
        [19782],
        [19966],
        [21108],
        [19820],
        [20129],
        [21183],
        [20608],
        [21584],
        [21126],
        [20944],
        [21492],
        [22401],
        [22797],
        [23079]], device='cuda:0')
[2024-07-24 10:21:29,207][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[20923],
        [22230],
        [ 5553],
        [28970],
        [11115],
        [13325],
        [24535],
        [23065],
        [16759],
        [21298],
        [ 8894],
        [20829],
        [13602],
        [11701],
        [14878],
        [16391],
        [13335]], device='cuda:0')
[2024-07-24 10:21:29,208][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[37141],
        [37211],
        [39556],
        [41400],
        [41720],
        [44666],
        [37356],
        [41163],
        [45075],
        [43408],
        [44360],
        [44643],
        [44711],
        [45234],
        [42048],
        [45939],
        [45182]], device='cuda:0')
[2024-07-24 10:21:29,210][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[2625],
        [  10],
        [  10],
        [  24],
        [  37],
        [  56],
        [  65],
        [ 167],
        [ 602],
        [ 271],
        [2474],
        [8491],
        [1028],
        [2356],
        [2310],
        [1020],
        [1482]], device='cuda:0')
[2024-07-24 10:21:29,211][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[4008],
        [4034],
        [4549],
        [4449],
        [4293],
        [3847],
        [3731],
        [3713],
        [3134],
        [2931],
        [2772],
        [2445],
        [2460],
        [2391],
        [2340],
        [2303],
        [2242]], device='cuda:0')
[2024-07-24 10:21:29,213][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[31807],
        [31467],
        [26176],
        [23139],
        [22266],
        [21849],
        [14017],
        [20718],
        [17483],
        [14200],
        [13496],
        [ 9169],
        [ 7900],
        [15093],
        [12498],
        [12531],
        [13615]], device='cuda:0')
[2024-07-24 10:21:29,214][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[11758],
        [12972],
        [11092],
        [14967],
        [15116],
        [14192],
        [14348],
        [11244],
        [11909],
        [11899],
        [12462],
        [13242],
        [12181],
        [12144],
        [11818],
        [11619],
        [11565]], device='cuda:0')
[2024-07-24 10:21:29,215][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[ 4269],
        [12560],
        [ 3591],
        [33394],
        [12688],
        [21811],
        [21674],
        [21280],
        [29019],
        [15006],
        [ 7795],
        [44263],
        [19688],
        [ 5021],
        [10829],
        [16351],
        [ 4387]], device='cuda:0')
[2024-07-24 10:21:29,217][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 6593],
        [ 2680],
        [ 5565],
        [ 5696],
        [26166],
        [20076],
        [21676],
        [14177],
        [15139],
        [34258],
        [15533],
        [11463],
        [ 8577],
        [10660],
        [ 7900],
        [ 9222],
        [ 8435]], device='cuda:0')
[2024-07-24 10:21:29,218][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[25545],
        [25590],
        [25617],
        [25616],
        [25622],
        [25624],
        [25631],
        [25645],
        [25632],
        [25627],
        [25632],
        [25635],
        [25623],
        [25622],
        [25620],
        [25614],
        [25618]], device='cuda:0')
[2024-07-24 10:21:29,220][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 8938],
        [11905],
        [13856],
        [12008],
        [14716],
        [12578],
        [12012],
        [13464],
        [13222],
        [13263],
        [12744],
        [11960],
        [13371],
        [11668],
        [12326],
        [11837],
        [11676]], device='cuda:0')
[2024-07-24 10:21:29,221][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[1940],
        [2023],
        [2455],
        [2647],
        [2742],
        [2762],
        [2915],
        [2715],
        [2741],
        [2778],
        [2912],
        [2893],
        [2925],
        [3045],
        [2954],
        [3388],
        [3472]], device='cuda:0')
[2024-07-24 10:21:29,223][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[12192],
        [ 6173],
        [ 7502],
        [ 5766],
        [ 4469],
        [ 4265],
        [ 4188],
        [ 3675],
        [ 3872],
        [ 3821],
        [ 3977],
        [ 3896],
        [ 3520],
        [ 3855],
        [ 3696],
        [ 3796],
        [ 4137]], device='cuda:0')
[2024-07-24 10:21:29,224][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[18689],
        [20180],
        [19060],
        [18904],
        [19242],
        [19073],
        [19112],
        [19079],
        [18843],
        [18880],
        [18512],
        [18316],
        [18304],
        [18296],
        [18031],
        [17951],
        [17902]], device='cuda:0')
[2024-07-24 10:21:29,225][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[46823],
        [44130],
        [46905],
        [46572],
        [47270],
        [46287],
        [46015],
        [46529],
        [45431],
        [45202],
        [45776],
        [45450],
        [46663],
        [46400],
        [46417],
        [46406],
        [46614]], device='cuda:0')
[2024-07-24 10:21:29,226][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[30971],
        [30914],
        [31161],
        [31111],
        [30699],
        [27873],
        [28137],
        [28495],
        [29221],
        [29548],
        [29246],
        [29430],
        [29448],
        [29300],
        [29782],
        [29759],
        [29477]], device='cuda:0')
[2024-07-24 10:21:29,227][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[44399],
        [48378],
        [46287],
        [46922],
        [45720],
        [44435],
        [44227],
        [43460],
        [42163],
        [42060],
        [39556],
        [39743],
        [38756],
        [37847],
        [37661],
        [37441],
        [36784]], device='cuda:0')
[2024-07-24 10:21:29,228][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[8293],
        [7305],
        [6236],
        [5198],
        [3211],
        [2700],
        [1679],
        [2963],
        [2316],
        [1857],
        [2064],
        [1905],
        [1648],
        [3269],
        [1999],
        [2641],
        [3475]], device='cuda:0')
[2024-07-24 10:21:29,230][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 8772],
        [28357],
        [29339],
        [36516],
        [20005],
        [18695],
        [38759],
        [37257],
        [37368],
        [39516],
        [37759],
        [40591],
        [41509],
        [37911],
        [38014],
        [37708],
        [37647]], device='cuda:0')
[2024-07-24 10:21:29,231][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[5978],
        [2354],
        [3868],
        [4323],
        [4316],
        [4063],
        [4229],
        [4178],
        [4229],
        [3970],
        [4301],
        [4516],
        [4950],
        [5139],
        [5250],
        [5148],
        [5304]], device='cuda:0')
[2024-07-24 10:21:29,233][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[11976],
        [11165],
        [14485],
        [14275],
        [14284],
        [12548],
        [11782],
        [12152],
        [12690],
        [13891],
        [14257],
        [15088],
        [14797],
        [13893],
        [14012],
        [13802],
        [13783]], device='cuda:0')
[2024-07-24 10:21:29,234][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[20031],
        [30045],
        [27186],
        [28837],
        [33781],
        [31416],
        [31285],
        [32788],
        [32190],
        [31742],
        [34374],
        [31710],
        [33279],
        [33527],
        [34680],
        [33039],
        [32193]], device='cuda:0')
[2024-07-24 10:21:29,236][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[36941],
        [36206],
        [38635],
        [21403],
        [14875],
        [21984],
        [23871],
        [25644],
        [25382],
        [13774],
        [23819],
        [16065],
        [26267],
        [29882],
        [30196],
        [31800],
        [34174]], device='cuda:0')
[2024-07-24 10:21:29,237][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123],
        [34123]], device='cuda:0')
[2024-07-24 10:21:29,264][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:29,266][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,267][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,268][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,269][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,271][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,272][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,272][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,273][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,273][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,273][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,273][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,274][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,274][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.4747, 0.5253], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,274][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.4529, 0.5471], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,275][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.4099, 0.5901], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,275][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.0453, 0.9547], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,275][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.8934, 0.1066], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,276][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.0269, 0.9731], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,276][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.2984, 0.7016], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,276][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.4373, 0.5627], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,277][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.9840, 0.0160], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,277][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.0036, 0.9964], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,277][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.3450, 0.6550], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,278][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.0793, 0.9207], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,278][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3121, 0.3734, 0.3146], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,278][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.2744, 0.6312, 0.0943], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,279][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0256, 0.9711, 0.0034], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,279][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0073, 0.4207, 0.5721], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,279][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.5721, 0.1033, 0.3246], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,280][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0170, 0.1294, 0.8536], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,280][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2139, 0.3877, 0.3984], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,280][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.2071, 0.6476, 0.1453], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,281][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.9152, 0.0369, 0.0479], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,281][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ and] are: tensor([4.1593e-04, 3.7713e-01, 6.2245e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,281][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1239, 0.3328, 0.5433], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,282][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0600, 0.7081, 0.2319], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,282][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.1648, 0.3070, 0.2910, 0.2371], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,283][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.7767, 0.0380, 0.1336, 0.0517], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,283][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0579, 0.8553, 0.0740, 0.0127], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,284][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([5.9842e-04, 3.5891e-02, 1.7331e-01, 7.9020e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,285][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.4955, 0.0680, 0.2945, 0.1420], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,287][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0035, 0.1048, 0.6381, 0.2536], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,288][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.1632, 0.2780, 0.2803, 0.2785], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,289][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.2387, 0.3746, 0.1774, 0.2093], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,290][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.7851, 0.0380, 0.1736, 0.0032], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,291][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([3.9304e-05, 3.7239e-02, 3.9943e-01, 5.6329e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,293][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0451, 0.1522, 0.3327, 0.4700], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,294][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0511, 0.4991, 0.2086, 0.2412], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,294][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.1366, 0.2144, 0.2409, 0.2522, 0.1560], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,294][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0009, 0.0752, 0.0293, 0.0817, 0.8129], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,295][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0341, 0.6215, 0.2139, 0.1221, 0.0083], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,295][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ got] are: tensor([3.6287e-04, 1.5320e-02, 7.8246e-02, 7.0692e-01, 1.9915e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,295][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.4812, 0.0251, 0.3563, 0.0597, 0.0777], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,295][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0006, 0.0701, 0.3504, 0.1947, 0.3842], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,296][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1304, 0.2111, 0.2214, 0.2163, 0.2208], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,296][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.1581, 0.3120, 0.1356, 0.1957, 0.1987], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,296][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.5587, 0.0998, 0.2779, 0.0274, 0.0362], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,297][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ got] are: tensor([4.3371e-06, 6.5773e-03, 4.0461e-02, 6.9656e-01, 2.5640e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,297][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0305, 0.0779, 0.2427, 0.2660, 0.3829], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,299][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0464, 0.3766, 0.1754, 0.1779, 0.2237], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,300][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1293, 0.1655, 0.1997, 0.2083, 0.1593, 0.1379], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,301][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0052, 0.2784, 0.0286, 0.4108, 0.2754, 0.0016], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,303][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0756, 0.2769, 0.3131, 0.1298, 0.2042, 0.0004], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,304][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([2.0547e-04, 8.8927e-03, 3.3887e-02, 3.6872e-01, 2.9378e-01, 2.9451e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,305][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2658, 0.0257, 0.2649, 0.1304, 0.1660, 0.1473], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,306][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([1.0978e-04, 9.8559e-04, 1.1371e-02, 4.6802e-03, 4.0767e-02, 9.4209e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,307][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1144, 0.1637, 0.1690, 0.1663, 0.1708, 0.2158], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,309][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0774, 0.3065, 0.0779, 0.1573, 0.2144, 0.1664], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,310][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2614, 0.0378, 0.1809, 0.0466, 0.4657, 0.0076], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,311][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([3.9271e-06, 2.4769e-03, 1.6385e-02, 2.4525e-01, 4.1408e-01, 3.2180e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,312][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0267, 0.0652, 0.1543, 0.1874, 0.2604, 0.3060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,314][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0355, 0.2834, 0.1331, 0.1526, 0.2012, 0.1941], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,315][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0891, 0.1496, 0.1619, 0.1783, 0.1679, 0.0997, 0.1534],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,316][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0360, 0.0438, 0.0463, 0.0392, 0.5133, 0.0317, 0.2897],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,318][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0429, 0.1137, 0.1740, 0.0145, 0.5903, 0.0613, 0.0034],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,319][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([3.4785e-05, 2.1391e-03, 9.9081e-03, 8.3059e-02, 9.7326e-02, 4.3281e-01,
        3.7472e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,320][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.2652, 0.0162, 0.2070, 0.0743, 0.1845, 0.1675, 0.0853],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,321][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([4.6168e-04, 1.9536e-02, 6.5936e-02, 2.9941e-02, 1.0407e-01, 7.0477e-01,
        7.5291e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,323][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0989, 0.1415, 0.1462, 0.1422, 0.1452, 0.1798, 0.1462],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,324][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1066, 0.1904, 0.1183, 0.1050, 0.1701, 0.1439, 0.1658],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,326][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.2175, 0.0400, 0.0821, 0.0629, 0.5458, 0.0153, 0.0364],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,326][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([3.0666e-06, 1.0522e-03, 7.2291e-03, 9.4057e-02, 1.3666e-01, 1.5806e-01,
        6.0294e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,328][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0156, 0.0358, 0.1228, 0.1056, 0.1641, 0.2403, 0.3158],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,329][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0427, 0.2253, 0.1439, 0.1237, 0.1622, 0.1843, 0.1180],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,331][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0677, 0.1031, 0.1106, 0.1637, 0.1156, 0.0965, 0.1610, 0.1817],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,332][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0833, 0.0593, 0.1439, 0.1696, 0.3661, 0.0989, 0.0631, 0.0156],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,334][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0375, 0.0920, 0.0724, 0.1680, 0.0553, 0.0518, 0.5222, 0.0007],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,335][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ at] are: tensor([4.1338e-05, 1.2536e-03, 3.3004e-03, 7.4021e-02, 3.9960e-02, 1.4621e-01,
        4.0150e-01, 3.3371e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,336][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1734, 0.0241, 0.2007, 0.1116, 0.1169, 0.1591, 0.1384, 0.0758],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,337][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ at] are: tensor([1.0157e-04, 5.4812e-03, 1.4990e-02, 8.0998e-03, 2.8882e-02, 2.8115e-01,
        5.0928e-02, 6.1036e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,338][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0762, 0.1172, 0.1201, 0.1202, 0.1254, 0.1557, 0.1231, 0.1621],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,340][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0984, 0.1935, 0.0586, 0.1203, 0.1650, 0.1173, 0.1704, 0.0764],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,341][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0480, 0.0088, 0.0303, 0.0489, 0.7666, 0.0658, 0.0267, 0.0049],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,342][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ at] are: tensor([4.2855e-07, 1.3992e-04, 9.2319e-04, 2.0979e-02, 1.8241e-02, 5.1213e-02,
        5.8587e-01, 3.2264e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,344][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0157, 0.0319, 0.0747, 0.0869, 0.1281, 0.1425, 0.2308, 0.2893],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,345][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0380, 0.2130, 0.1030, 0.1128, 0.1438, 0.1452, 0.1118, 0.1324],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,347][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0610, 0.0880, 0.0992, 0.1189, 0.0864, 0.0753, 0.1661, 0.2248, 0.0803],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,348][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0024, 0.2087, 0.0142, 0.1191, 0.5070, 0.0007, 0.1401, 0.0064, 0.0014],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,349][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.8943e-02, 9.6665e-02, 1.1062e-01, 1.3394e-01, 2.8088e-01, 4.2261e-03,
        1.3858e-01, 2.0601e-01, 1.3834e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,350][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([8.6510e-06, 2.8492e-04, 1.0522e-03, 1.5994e-02, 8.8345e-03, 4.2589e-02,
        1.8131e-01, 3.1353e-01, 4.3640e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,351][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1989, 0.0385, 0.1624, 0.0624, 0.0753, 0.1422, 0.1188, 0.1066, 0.0949],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,352][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.7766e-06, 3.6268e-05, 1.7099e-04, 8.8236e-05, 5.5603e-04, 7.5752e-03,
        1.7894e-03, 4.6184e-02, 9.4359e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,352][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0597, 0.0982, 0.0971, 0.1002, 0.1039, 0.1293, 0.1038, 0.1384, 0.1694],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,352][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0595, 0.1652, 0.0533, 0.0894, 0.1605, 0.1097, 0.1793, 0.0888, 0.0943],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,353][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0593, 0.0116, 0.1032, 0.0319, 0.2019, 0.0157, 0.0971, 0.4782, 0.0012],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,353][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.3516e-07, 5.9051e-05, 2.3746e-04, 6.5630e-03, 6.8862e-03, 4.7403e-03,
        1.5499e-01, 2.3642e-01, 5.9010e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,353][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0101, 0.0243, 0.0560, 0.0668, 0.0909, 0.1051, 0.1724, 0.2030, 0.2714],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,354][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0335, 0.1804, 0.0840, 0.0950, 0.1256, 0.1167, 0.0916, 0.1103, 0.1629],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,354][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0466, 0.0872, 0.0722, 0.0929, 0.0739, 0.0565, 0.1711, 0.1890, 0.0732,
        0.1374], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,354][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0222, 0.0577, 0.0211, 0.0398, 0.1790, 0.0140, 0.2696, 0.0123, 0.0041,
        0.3802], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,355][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0061, 0.1728, 0.0518, 0.0413, 0.0691, 0.0354, 0.3484, 0.2546, 0.0197,
        0.0009], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,356][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ station] are: tensor([1.3225e-06, 4.6032e-05, 2.6051e-04, 3.3678e-03, 2.9404e-03, 9.4706e-03,
        6.9783e-02, 1.2169e-01, 4.7809e-01, 3.1436e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,357][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.1251, 0.0103, 0.1356, 0.0339, 0.1343, 0.1483, 0.1265, 0.0846, 0.1703,
        0.0311], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,358][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ station] are: tensor([1.1326e-05, 4.7963e-03, 6.2645e-03, 4.1432e-03, 8.4965e-03, 6.6029e-02,
        1.4763e-02, 1.0372e-01, 7.5747e-01, 3.4299e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,360][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0497, 0.0890, 0.0904, 0.0906, 0.0950, 0.1173, 0.0933, 0.1209, 0.1461,
        0.1078], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,361][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0805, 0.1646, 0.0626, 0.0745, 0.1203, 0.0759, 0.1125, 0.0585, 0.0565,
        0.1942], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,363][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.2801, 0.0160, 0.0406, 0.0082, 0.0367, 0.0256, 0.0454, 0.5309, 0.0146,
        0.0019], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,363][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ station] are: tensor([3.8007e-08, 1.0364e-05, 1.0112e-04, 2.1029e-03, 1.6947e-03, 3.4542e-03,
        3.3876e-02, 1.3826e-01, 5.3899e-01, 2.8151e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,365][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0073, 0.0129, 0.0366, 0.0427, 0.0631, 0.0797, 0.1319, 0.1627, 0.2336,
        0.2294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,366][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0276, 0.1227, 0.0653, 0.0644, 0.0910, 0.0953, 0.0810, 0.0919, 0.1483,
        0.2125], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,368][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0463, 0.0666, 0.0724, 0.1046, 0.0785, 0.0665, 0.0976, 0.1534, 0.0715,
        0.1485, 0.0941], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,369][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0408, 0.0931, 0.0248, 0.0930, 0.1720, 0.0263, 0.1016, 0.0450, 0.0543,
        0.3398, 0.0094], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,370][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [,] are: tensor([2.5083e-02, 1.6384e-01, 5.3274e-02, 1.7732e-01, 2.6508e-01, 6.3137e-02,
        9.2577e-02, 6.0743e-02, 1.8187e-02, 8.0655e-02, 1.0118e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,371][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [,] are: tensor([4.5586e-06, 7.0876e-05, 2.1941e-04, 2.6738e-03, 2.7687e-03, 9.1462e-03,
        2.1936e-02, 6.7940e-02, 2.5230e-01, 1.5975e-01, 4.8319e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,373][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1356, 0.0384, 0.0749, 0.0702, 0.0516, 0.1295, 0.1531, 0.0573, 0.1474,
        0.0747, 0.0673], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,373][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [,] are: tensor([6.9053e-07, 9.4315e-07, 2.2787e-06, 1.5791e-06, 6.2472e-06, 4.4674e-05,
        2.8100e-05, 4.3901e-04, 8.8616e-03, 3.6970e-04, 9.9025e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,375][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0504, 0.0738, 0.0721, 0.0746, 0.0762, 0.0944, 0.0781, 0.1034, 0.1275,
        0.0937, 0.1558], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,376][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0514, 0.1058, 0.0371, 0.0665, 0.1388, 0.0744, 0.1259, 0.0655, 0.0613,
        0.2183, 0.0550], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,378][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.2808, 0.0075, 0.0593, 0.0098, 0.1166, 0.0644, 0.1172, 0.3028, 0.0272,
        0.0068, 0.0076], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,379][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [,] are: tensor([1.8985e-08, 4.8376e-06, 2.3374e-05, 5.7019e-04, 6.1913e-04, 1.5781e-03,
        1.4249e-02, 2.5469e-02, 1.3621e-01, 2.9178e-01, 5.2950e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,380][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0068, 0.0127, 0.0353, 0.0365, 0.0538, 0.0581, 0.0944, 0.1098, 0.1443,
        0.1426, 0.3057], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,382][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0273, 0.1296, 0.0606, 0.0649, 0.0856, 0.0785, 0.0618, 0.0727, 0.1074,
        0.1713, 0.1404], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,383][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0341, 0.0636, 0.0597, 0.0496, 0.0596, 0.0598, 0.1042, 0.1602, 0.0754,
        0.1435, 0.0999, 0.0905], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,384][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0732, 0.0120, 0.0404, 0.0109, 0.3392, 0.0833, 0.0347, 0.0144, 0.1215,
        0.2138, 0.0415, 0.0152], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,386][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0316, 0.4382, 0.0355, 0.0055, 0.1040, 0.0692, 0.1385, 0.0027, 0.0746,
        0.0939, 0.0027, 0.0036], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,387][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([2.0649e-07, 3.8949e-06, 1.5740e-05, 1.2561e-04, 2.2469e-04, 1.0647e-03,
        4.5293e-03, 1.1805e-02, 3.1284e-02, 3.3202e-02, 1.3147e-01, 7.8628e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,388][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0996, 0.0120, 0.0630, 0.0270, 0.0698, 0.1311, 0.0631, 0.0390, 0.1356,
        0.2008, 0.1286, 0.0304], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,389][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([1.6454e-05, 3.9252e-04, 8.7401e-04, 4.1797e-04, 1.1310e-03, 6.0835e-03,
        1.5945e-03, 9.3162e-03, 6.6054e-02, 5.0425e-03, 9.0402e-01, 5.0562e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,390][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0412, 0.0704, 0.0692, 0.0698, 0.0723, 0.0886, 0.0735, 0.0935, 0.1125,
        0.0841, 0.1338, 0.0912], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,392][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0811, 0.1393, 0.0661, 0.0666, 0.1050, 0.0849, 0.0873, 0.0509, 0.0587,
        0.1405, 0.0502, 0.0693], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,394][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.1850, 0.0089, 0.0382, 0.0007, 0.0329, 0.0236, 0.0493, 0.4988, 0.0531,
        0.0874, 0.0216, 0.0006], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,394][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([8.1729e-09, 7.2100e-07, 7.0013e-06, 1.8556e-05, 1.7260e-04, 3.5741e-04,
        2.0979e-03, 7.7708e-03, 2.9363e-02, 3.9845e-02, 2.9243e-01, 6.2794e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,396][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0024, 0.0098, 0.0246, 0.0288, 0.0333, 0.0413, 0.0604, 0.0814, 0.1139,
        0.0948, 0.2586, 0.2507], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,397][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0202, 0.1068, 0.0502, 0.0487, 0.0651, 0.0670, 0.0421, 0.0625, 0.0958,
        0.1357, 0.1386, 0.1672], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,399][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0354, 0.0483, 0.0596, 0.0670, 0.0554, 0.0501, 0.0759, 0.1580, 0.0610,
        0.1392, 0.0960, 0.1138, 0.0403], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,400][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([1.0380e-03, 3.4072e-01, 3.6269e-02, 3.1099e-02, 3.8456e-01, 1.2546e-04,
        1.8531e-02, 7.1307e-03, 4.1944e-04, 1.8508e-02, 2.0245e-03, 2.4622e-02,
        1.3495e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,401][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0234, 0.1137, 0.1861, 0.0873, 0.1082, 0.0311, 0.1908, 0.0123, 0.1187,
        0.0351, 0.0157, 0.0748, 0.0027], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,402][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([1.4764e-07, 1.7991e-06, 6.4585e-06, 1.0043e-04, 4.8427e-05, 1.9377e-04,
        8.3092e-04, 2.2512e-03, 5.6454e-03, 1.1047e-02, 5.9092e-02, 8.4967e-01,
        7.1113e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,403][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.2011, 0.0057, 0.0997, 0.0188, 0.0712, 0.0877, 0.0908, 0.0612, 0.1309,
        0.0590, 0.1328, 0.0189, 0.0222], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,404][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([3.8270e-06, 7.7332e-04, 6.9719e-04, 5.9069e-04, 6.4014e-04, 2.9517e-03,
        1.2951e-03, 5.3416e-03, 3.9353e-02, 2.4595e-03, 9.1952e-01, 7.2347e-03,
        1.9135e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,406][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0422, 0.0643, 0.0635, 0.0637, 0.0640, 0.0798, 0.0653, 0.0834, 0.1012,
        0.0744, 0.1212, 0.0804, 0.0967], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,407][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0485, 0.1084, 0.0550, 0.0598, 0.0667, 0.0630, 0.0926, 0.0467, 0.0530,
        0.1560, 0.0649, 0.0702, 0.1152], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,409][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.2336, 0.0863, 0.0463, 0.0328, 0.1049, 0.0601, 0.0390, 0.1363, 0.0463,
        0.0152, 0.0994, 0.0288, 0.0709], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,409][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([1.1482e-09, 1.0772e-07, 5.4531e-07, 1.4599e-05, 7.3730e-06, 3.5336e-05,
        3.5376e-04, 6.1642e-04, 2.8763e-03, 8.7639e-03, 5.8026e-02, 8.7706e-01,
        5.2245e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,410][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0021, 0.0053, 0.0181, 0.0168, 0.0263, 0.0322, 0.0490, 0.0705, 0.0878,
        0.0704, 0.2326, 0.1726, 0.2163], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,410][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0250, 0.0916, 0.0530, 0.0473, 0.0605, 0.0582, 0.0417, 0.0618, 0.0815,
        0.1199, 0.1381, 0.1434, 0.0782], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,410][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0397, 0.0445, 0.0533, 0.0622, 0.0523, 0.0453, 0.0972, 0.1357, 0.0488,
        0.1070, 0.0782, 0.1055, 0.0540, 0.0764], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,411][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0014, 0.0071, 0.0041, 0.0343, 0.0163, 0.0016, 0.0133, 0.0008, 0.0030,
        0.1689, 0.0053, 0.0371, 0.7043, 0.0026], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,411][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([6.7818e-02, 3.7081e-02, 6.9151e-02, 3.6414e-02, 8.8359e-02, 5.1662e-02,
        2.0822e-01, 2.8650e-02, 5.8489e-02, 5.7332e-02, 4.1942e-03, 3.6066e-02,
        2.5656e-01, 1.3285e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,411][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([3.6626e-07, 2.3292e-06, 5.8222e-06, 8.9182e-05, 3.8473e-05, 2.1883e-04,
        8.4057e-04, 2.1573e-03, 7.1449e-03, 9.1554e-03, 3.5413e-02, 4.5181e-01,
        1.3595e-01, 3.5717e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,412][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0558, 0.0197, 0.0824, 0.0758, 0.0689, 0.0675, 0.0927, 0.0594, 0.0684,
        0.0691, 0.0941, 0.0816, 0.1278, 0.0368], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,412][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([3.3940e-07, 2.2319e-07, 2.6206e-07, 2.1742e-07, 4.2036e-07, 1.4265e-06,
        2.6152e-06, 1.6589e-05, 2.4573e-04, 1.9331e-05, 2.3680e-02, 6.1648e-05,
        1.3736e-03, 9.7460e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,413][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0396, 0.0542, 0.0530, 0.0539, 0.0535, 0.0667, 0.0557, 0.0725, 0.0893,
        0.0656, 0.1082, 0.0729, 0.0856, 0.1294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,414][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0402, 0.0932, 0.0280, 0.0478, 0.1039, 0.0531, 0.0879, 0.0495, 0.0461,
        0.1339, 0.0461, 0.0576, 0.1611, 0.0516], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,415][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([2.3536e-02, 8.5935e-03, 2.3326e-02, 9.5084e-03, 2.1215e-01, 1.1317e-02,
        3.0403e-02, 3.1331e-02, 7.7575e-03, 7.4988e-03, 1.7413e-02, 8.5032e-03,
        6.0853e-01, 1.3490e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,416][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([1.5169e-09, 4.5325e-08, 2.9018e-07, 5.3280e-06, 4.4386e-06, 1.5143e-05,
        3.0242e-04, 1.1968e-04, 1.2604e-03, 2.9753e-03, 2.4435e-02, 2.2536e-01,
        8.3782e-02, 6.6174e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,418][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0024, 0.0047, 0.0134, 0.0136, 0.0207, 0.0222, 0.0342, 0.0424, 0.0577,
        0.0482, 0.1269, 0.1130, 0.1695, 0.3311], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,419][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0321, 0.0951, 0.0469, 0.0444, 0.0604, 0.0544, 0.0385, 0.0497, 0.0729,
        0.0982, 0.1027, 0.1131, 0.0827, 0.1090], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,420][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0319, 0.0426, 0.0477, 0.0592, 0.0459, 0.0410, 0.0920, 0.1107, 0.0528,
        0.0980, 0.0654, 0.0986, 0.0783, 0.1016, 0.0343], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,421][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ give] are: tensor([2.5979e-04, 3.6345e-02, 2.9863e-03, 9.9234e-03, 4.6872e-02, 1.5480e-04,
        7.8126e-02, 9.8345e-04, 6.6798e-04, 1.7812e-01, 2.3783e-03, 1.4399e-02,
        5.4049e-01, 6.4685e-04, 8.7647e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,423][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0357, 0.0620, 0.0583, 0.0198, 0.0239, 0.0199, 0.0601, 0.0161, 0.0375,
        0.0371, 0.0096, 0.0150, 0.4061, 0.1978, 0.0009], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,424][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ give] are: tensor([1.3784e-07, 8.0918e-07, 3.2877e-06, 3.6121e-05, 1.5170e-05, 1.0305e-04,
        6.6774e-04, 7.7648e-04, 3.9837e-03, 4.3412e-03, 1.6795e-02, 2.1314e-01,
        6.2861e-02, 2.3875e-01, 4.5853e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,425][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.1304, 0.0098, 0.1212, 0.0360, 0.0469, 0.0808, 0.0730, 0.0597, 0.1013,
        0.0286, 0.1685, 0.0374, 0.0506, 0.0498, 0.0060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,426][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ give] are: tensor([1.8869e-06, 2.1840e-04, 1.1026e-04, 1.0852e-04, 8.9153e-05, 2.6531e-04,
        2.1607e-04, 6.2043e-04, 3.4682e-03, 3.4882e-04, 7.7213e-02, 1.1697e-03,
        3.8557e-03, 8.9894e-01, 1.3377e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,428][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0348, 0.0510, 0.0505, 0.0503, 0.0510, 0.0631, 0.0516, 0.0665, 0.0799,
        0.0596, 0.0967, 0.0650, 0.0774, 0.1146, 0.0878], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,429][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0539, 0.0899, 0.0409, 0.0448, 0.0743, 0.0564, 0.0723, 0.0388, 0.0390,
        0.1092, 0.0384, 0.0498, 0.1267, 0.0465, 0.1194], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,431][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0681, 0.0281, 0.0143, 0.0191, 0.0105, 0.0144, 0.0797, 0.0184, 0.0259,
        0.0102, 0.0144, 0.0175, 0.6369, 0.0360, 0.0064], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,432][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ give] are: tensor([9.3404e-10, 2.7658e-08, 1.7231e-07, 2.7875e-06, 1.6923e-06, 8.8128e-06,
        7.1205e-05, 1.8349e-04, 7.8767e-04, 1.0698e-03, 1.2650e-02, 1.0509e-01,
        2.3383e-02, 5.0319e-01, 3.5356e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,433][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0012, 0.0026, 0.0094, 0.0077, 0.0128, 0.0155, 0.0224, 0.0334, 0.0415,
        0.0304, 0.0948, 0.0709, 0.1103, 0.2646, 0.2824], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,435][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0286, 0.0765, 0.0444, 0.0341, 0.0514, 0.0502, 0.0315, 0.0485, 0.0651,
        0.0788, 0.0977, 0.0930, 0.0680, 0.1076, 0.1246], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,436][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0266, 0.0467, 0.0473, 0.0620, 0.0517, 0.0385, 0.0780, 0.1069, 0.0422,
        0.0906, 0.0689, 0.1018, 0.0600, 0.0894, 0.0622, 0.0271],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,437][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ it] are: tensor([1.1227e-03, 2.8269e-02, 8.2732e-04, 2.9113e-02, 2.4874e-01, 4.0322e-04,
        4.1308e-02, 2.3349e-03, 2.0059e-04, 6.5953e-02, 1.2788e-03, 2.7625e-02,
        4.9679e-01, 8.5359e-04, 5.0002e-02, 5.1846e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,438][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ it] are: tensor([2.1747e-02, 5.9420e-03, 7.2257e-02, 9.1675e-03, 5.5671e-02, 2.1563e-02,
        1.5362e-01, 2.7501e-03, 2.9900e-03, 1.1085e-01, 1.9151e-03, 7.2073e-03,
        3.4530e-01, 9.9861e-03, 1.7903e-01, 7.9204e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,439][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ it] are: tensor([2.8340e-08, 2.1309e-07, 5.0464e-07, 1.1216e-05, 4.3250e-06, 1.9656e-05,
        9.7535e-05, 2.7227e-04, 9.1083e-04, 8.1358e-04, 3.6374e-03, 5.3716e-02,
        1.0514e-02, 1.5421e-01, 3.0464e-01, 4.7116e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,440][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0946, 0.0067, 0.1006, 0.0288, 0.0398, 0.1082, 0.0943, 0.0519, 0.0824,
        0.0743, 0.1341, 0.0266, 0.0710, 0.0589, 0.0186, 0.0093],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,441][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ it] are: tensor([2.1091e-06, 1.5070e-05, 1.2779e-05, 1.1265e-05, 1.2119e-05, 3.4924e-05,
        4.3950e-05, 1.4879e-04, 1.2190e-03, 1.1036e-04, 4.3637e-02, 4.0700e-04,
        2.7359e-03, 7.9963e-01, 9.7302e-03, 1.4225e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,443][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0323, 0.0472, 0.0464, 0.0465, 0.0462, 0.0576, 0.0472, 0.0613, 0.0743,
        0.0543, 0.0893, 0.0596, 0.0702, 0.1039, 0.0798, 0.0838],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,444][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0372, 0.0745, 0.0292, 0.0359, 0.0742, 0.0466, 0.0647, 0.0410, 0.0369,
        0.1031, 0.0425, 0.0432, 0.1138, 0.0457, 0.1187, 0.0928],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,445][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ it] are: tensor([3.1404e-02, 1.5976e-03, 2.4159e-02, 8.4086e-03, 3.9973e-02, 1.0181e-02,
        3.7356e-02, 1.7175e-01, 4.8273e-03, 1.2966e-02, 1.7834e-02, 7.6538e-03,
        3.4107e-01, 2.7275e-02, 2.6343e-01, 1.1829e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,446][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ it] are: tensor([1.3371e-10, 4.3885e-09, 2.9755e-08, 5.1861e-07, 4.5789e-07, 1.1209e-06,
        2.4447e-05, 1.6641e-05, 9.5553e-05, 2.0852e-04, 2.2113e-03, 2.1257e-02,
        6.8188e-03, 1.0092e-01, 4.2000e-01, 4.4845e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,448][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0009, 0.0022, 0.0070, 0.0060, 0.0099, 0.0119, 0.0164, 0.0251, 0.0312,
        0.0221, 0.0635, 0.0531, 0.0767, 0.1912, 0.2076, 0.2750],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,449][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0299, 0.0717, 0.0381, 0.0320, 0.0413, 0.0421, 0.0281, 0.0413, 0.0524,
        0.0614, 0.0829, 0.0794, 0.0569, 0.0900, 0.1091, 0.1434],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,451][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0347, 0.0375, 0.0456, 0.0528, 0.0453, 0.0390, 0.0846, 0.1173, 0.0411,
        0.0924, 0.0663, 0.0895, 0.0469, 0.0658, 0.0491, 0.0313, 0.0608],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,452][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([6.5769e-04, 5.7439e-03, 1.8916e-03, 2.7177e-02, 1.1711e-02, 8.1932e-04,
        9.3030e-03, 6.5968e-04, 1.4573e-03, 1.7123e-01, 3.8812e-03, 2.9208e-02,
        6.8264e-01, 1.4300e-03, 3.9822e-02, 1.0177e-02, 2.1900e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,453][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([5.6533e-02, 2.8095e-02, 5.6160e-02, 2.8584e-02, 6.9543e-02, 4.0824e-02,
        1.6258e-01, 2.3478e-02, 4.8016e-02, 4.3852e-02, 3.5384e-03, 2.8739e-02,
        1.9961e-01, 1.0661e-05, 1.8336e-01, 2.7069e-02, 7.3296e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,454][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([2.6787e-08, 9.1427e-08, 1.9341e-07, 3.2036e-06, 1.0392e-06, 6.6951e-06,
        2.8305e-05, 6.7220e-05, 2.0085e-04, 2.3976e-04, 9.5427e-04, 1.2058e-02,
        2.8134e-03, 8.7071e-03, 1.1835e-01, 4.7419e-01, 3.8239e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,455][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0528, 0.0155, 0.0742, 0.0604, 0.0609, 0.0612, 0.0845, 0.0547, 0.0647,
        0.0625, 0.0892, 0.0656, 0.1096, 0.0342, 0.0304, 0.0510, 0.0285],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,456][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.8201e-07, 4.6399e-08, 2.4957e-08, 2.5369e-08, 2.5061e-08, 3.3490e-08,
        1.6880e-07, 4.3968e-07, 4.1110e-06, 6.7369e-07, 3.1815e-04, 3.5619e-06,
        5.1075e-05, 1.3315e-02, 1.8347e-04, 3.3837e-03, 9.8274e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,458][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0318, 0.0417, 0.0404, 0.0411, 0.0401, 0.0496, 0.0422, 0.0533, 0.0652,
        0.0484, 0.0779, 0.0540, 0.0624, 0.0925, 0.0721, 0.0759, 0.1116],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,459][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0298, 0.0714, 0.0213, 0.0355, 0.0765, 0.0395, 0.0664, 0.0369, 0.0343,
        0.0987, 0.0352, 0.0424, 0.1194, 0.0388, 0.1105, 0.1057, 0.0375],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,460][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([4.5639e-03, 1.7095e-03, 4.4671e-03, 1.9118e-03, 4.3344e-02, 2.1492e-03,
        6.4288e-03, 6.0159e-03, 1.4770e-03, 1.5510e-03, 3.4093e-03, 1.7213e-03,
        1.2169e-01, 2.5526e-05, 7.9916e-01, 3.6032e-04, 1.9401e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,461][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([8.5613e-11, 1.1014e-09, 5.5093e-09, 1.0935e-07, 7.0676e-08, 2.2369e-07,
        5.1169e-06, 1.6627e-06, 1.6315e-05, 3.4942e-05, 3.5518e-04, 3.1228e-03,
        1.0454e-03, 7.5215e-03, 6.7710e-02, 2.3771e-01, 6.8248e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,462][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0009, 0.0017, 0.0049, 0.0049, 0.0074, 0.0078, 0.0124, 0.0149, 0.0201,
        0.0157, 0.0413, 0.0382, 0.0589, 0.1192, 0.1446, 0.1906, 0.3165],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,464][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0303, 0.0678, 0.0338, 0.0296, 0.0408, 0.0364, 0.0251, 0.0329, 0.0472,
        0.0571, 0.0643, 0.0675, 0.0508, 0.0689, 0.0903, 0.1380, 0.1194],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,487][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:29,488][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,489][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,490][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,492][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,493][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,494][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,495][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,496][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,497][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,498][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,500][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,508][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,509][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.6794, 0.3206], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,511][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.9861, 0.0139], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,512][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([9.9988e-01, 1.1882e-04], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,513][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,514][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.0529, 0.9471], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,516][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.9582, 0.0418], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,517][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.4147, 0.5853], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,519][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.2593, 0.7407], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,520][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.9360, 0.0640], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,521][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.2445, 0.7555], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,523][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.0947, 0.9053], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,524][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.6185, 0.3815], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,526][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7551, 0.1112, 0.1337], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,527][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9756, 0.0046, 0.0198], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,527][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([9.9960e-01, 4.4206e-05, 3.5516e-04], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,528][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([9.9336e-01, 5.7472e-04, 6.0665e-03], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,529][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([1.9970e-04, 2.5677e-03, 9.9723e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,531][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9296, 0.0290, 0.0414], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,531][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2758, 0.3702, 0.3540], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,532][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.2977, 0.4718, 0.2305], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,532][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7478, 0.0884, 0.1638], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,532][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0129, 0.9397, 0.0474], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,532][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0881, 0.5853, 0.3265], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,533][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.4661, 0.4399, 0.0940], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,533][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.5422, 0.2101, 0.1239, 0.1238], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,533][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.9355, 0.0083, 0.0483, 0.0078], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,534][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([9.9948e-01, 2.2165e-05, 2.1511e-04, 2.8055e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,534][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.9728, 0.0015, 0.0137, 0.0120], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,534][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([4.0170e-04, 4.6843e-04, 2.5774e-01, 7.4139e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,535][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.8996, 0.0217, 0.0317, 0.0470], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,536][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.2142, 0.2778, 0.2664, 0.2417], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,538][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.3708, 0.3473, 0.1375, 0.1444], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,539][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.6511, 0.0669, 0.2656, 0.0163], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,540][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0288, 0.4737, 0.2359, 0.2616], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,542][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0353, 0.3394, 0.1869, 0.4384], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,543][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.3167, 0.3315, 0.0683, 0.2836], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,545][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.6239, 0.0763, 0.1524, 0.0662, 0.0812], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,546][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.7546, 0.0015, 0.0629, 0.0022, 0.1788], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,547][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([7.6047e-01, 6.7921e-04, 3.8781e-03, 3.6582e-03, 2.3131e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,548][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.9361, 0.0016, 0.0161, 0.0142, 0.0320], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,549][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([3.6398e-04, 3.1072e-04, 1.1561e-01, 2.5909e-01, 6.2462e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,550][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.5607, 0.0564, 0.0764, 0.0876, 0.2189], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,552][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.1626, 0.2264, 0.2176, 0.1988, 0.1946], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,553][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.4070, 0.1479, 0.1242, 0.1537, 0.1672], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,554][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.4661, 0.0887, 0.3029, 0.0486, 0.0937], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,556][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0030, 0.3153, 0.0446, 0.6190, 0.0181], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,557][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0293, 0.2457, 0.1329, 0.2923, 0.2998], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,558][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.2300, 0.2837, 0.1017, 0.2519, 0.1327], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,560][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.4667, 0.1099, 0.1010, 0.0798, 0.1150, 0.1276], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,561][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.8253, 0.0052, 0.0307, 0.0043, 0.0621, 0.0724], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,562][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([5.4208e-01, 1.7465e-04, 1.1645e-03, 1.0217e-03, 1.0172e-01, 3.5384e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,563][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([9.1381e-01, 9.0867e-05, 1.3283e-03, 1.1257e-03, 6.6258e-03, 7.7021e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,564][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([1.4363e-06, 1.7501e-07, 2.0587e-04, 9.0080e-04, 3.3423e-03, 9.9555e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,565][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.4221, 0.0440, 0.0578, 0.0613, 0.1524, 0.2624], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,567][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1371, 0.1871, 0.1806, 0.1634, 0.1610, 0.1708], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,568][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.3304, 0.1357, 0.1258, 0.1182, 0.1882, 0.1018], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,570][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.3019, 0.0615, 0.2091, 0.0724, 0.3319, 0.0232], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,571][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0134, 0.2602, 0.0673, 0.5664, 0.0611, 0.0315], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,572][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0304, 0.1957, 0.1142, 0.2438, 0.2422, 0.1736], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,574][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.2769, 0.2004, 0.1210, 0.1628, 0.1419, 0.0969], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,575][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.3854, 0.0708, 0.1129, 0.0636, 0.1934, 0.1214, 0.0525],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,577][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.5442, 0.0014, 0.0771, 0.0025, 0.1788, 0.0632, 0.1327],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,578][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([5.3341e-01, 1.5104e-04, 9.7916e-04, 7.8503e-04, 8.1848e-02, 2.4471e-01,
        1.3812e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,579][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.6040, 0.0017, 0.0101, 0.0076, 0.0334, 0.2320, 0.1112],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,580][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([5.0251e-06, 7.5022e-07, 4.2246e-04, 1.0760e-03, 5.3421e-03, 9.3950e-01,
        5.3655e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,582][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.3567, 0.0336, 0.0429, 0.0437, 0.1179, 0.2037, 0.2013],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,583][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.1219, 0.1599, 0.1538, 0.1400, 0.1372, 0.1448, 0.1424],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,584][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.1584, 0.1594, 0.1459, 0.1227, 0.2062, 0.1320, 0.0755],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,586][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.2755, 0.0425, 0.1403, 0.0642, 0.3320, 0.0329, 0.1126],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,587][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0257, 0.1334, 0.0708, 0.3568, 0.2123, 0.0862, 0.1148],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,588][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0169, 0.1494, 0.0807, 0.1822, 0.1956, 0.1174, 0.2578],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,590][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.2016, 0.1880, 0.0265, 0.2148, 0.1345, 0.0464, 0.1883],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,591][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.2362, 0.0511, 0.1180, 0.0465, 0.1867, 0.1208, 0.0663, 0.1743],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,593][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.6960, 0.0014, 0.0276, 0.0033, 0.0378, 0.0451, 0.1486, 0.0402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,593][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([9.5275e-02, 1.4925e-04, 7.0496e-04, 6.1211e-04, 2.8425e-02, 9.0666e-02,
        6.2276e-02, 7.2189e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,594][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([4.2633e-01, 3.7083e-04, 2.4675e-03, 1.9262e-03, 1.0343e-02, 8.2901e-02,
        6.6292e-02, 4.0937e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,594][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([6.0613e-06, 4.3766e-07, 3.1737e-04, 6.4409e-04, 4.0602e-03, 8.6089e-01,
        5.7694e-02, 7.6391e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,595][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.2228, 0.0328, 0.0390, 0.0374, 0.0936, 0.1574, 0.1633, 0.2535],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,595][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0982, 0.1398, 0.1346, 0.1234, 0.1210, 0.1285, 0.1240, 0.1305],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,595][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.2702, 0.1557, 0.1025, 0.0980, 0.1478, 0.0964, 0.0637, 0.0658],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,596][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.1706, 0.0288, 0.1019, 0.0748, 0.4114, 0.0753, 0.0873, 0.0500],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,596][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0029, 0.1199, 0.0251, 0.3335, 0.0433, 0.0651, 0.3977, 0.0125],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,596][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0188, 0.1259, 0.0724, 0.1613, 0.1614, 0.1149, 0.2200, 0.1252],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,597][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.2031, 0.1338, 0.0782, 0.1063, 0.0921, 0.0802, 0.1096, 0.1967],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,597][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1192, 0.0554, 0.0691, 0.0383, 0.0575, 0.1032, 0.1328, 0.3234, 0.1011],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,598][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.5504, 0.0051, 0.0663, 0.0034, 0.0841, 0.1065, 0.0521, 0.0364, 0.0957],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,599][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([5.1166e-02, 1.5321e-05, 9.1796e-05, 8.3796e-05, 7.5118e-03, 2.6859e-02,
        1.8587e-02, 3.7023e-01, 5.2546e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,599][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.1932e-01, 5.8454e-06, 4.9781e-05, 4.0034e-05, 2.3347e-04, 2.3817e-03,
        3.3053e-03, 3.1068e-02, 8.4359e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,600][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.2252e-07, 1.0846e-08, 7.3828e-06, 1.7512e-05, 1.0827e-04, 3.2827e-02,
        2.3299e-03, 1.0206e-02, 9.5450e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,602][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2295, 0.0188, 0.0224, 0.0219, 0.0602, 0.1027, 0.1106, 0.1914, 0.2423],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,603][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0854, 0.1226, 0.1182, 0.1081, 0.1064, 0.1130, 0.1095, 0.1156, 0.1212],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,605][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.3583, 0.1590, 0.0734, 0.0670, 0.1020, 0.0688, 0.0512, 0.0467, 0.0736],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,606][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1113, 0.0297, 0.1292, 0.0501, 0.1400, 0.0272, 0.1450, 0.3600, 0.0074],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,607][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0102, 0.1213, 0.0393, 0.3308, 0.0463, 0.0210, 0.3147, 0.0920, 0.0243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,609][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0180, 0.1132, 0.0678, 0.1426, 0.1445, 0.1034, 0.1918, 0.1084, 0.1102],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,610][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2103, 0.0866, 0.0848, 0.0686, 0.0958, 0.0734, 0.0684, 0.2405, 0.0716],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,612][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.1786, 0.0674, 0.0472, 0.0429, 0.0383, 0.0448, 0.1650, 0.2167, 0.0448,
        0.1544], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,613][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.2332, 0.0020, 0.0385, 0.0028, 0.0808, 0.0771, 0.2152, 0.0926, 0.1456,
        0.1122], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,614][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([5.1362e-02, 2.8655e-05, 1.6192e-04, 1.3584e-04, 8.7481e-03, 2.7689e-02,
        2.0118e-02, 3.3161e-01, 4.5840e-01, 1.0174e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,615][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([1.1849e-02, 7.3951e-05, 2.3628e-04, 2.1381e-04, 1.0992e-03, 6.6731e-03,
        6.9144e-03, 4.7627e-02, 7.5457e-01, 1.7075e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,616][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([1.1388e-05, 1.6204e-07, 8.3586e-05, 5.7058e-05, 6.2391e-04, 8.5331e-02,
        6.9427e-03, 1.7673e-02, 8.7601e-01, 1.3271e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,617][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.1765, 0.0195, 0.0227, 0.0212, 0.0539, 0.0885, 0.0909, 0.1505, 0.1940,
        0.1824], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,619][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0765, 0.1112, 0.1065, 0.0978, 0.0968, 0.1021, 0.0990, 0.1032, 0.1072,
        0.0998], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,620][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.3867, 0.0516, 0.0543, 0.0681, 0.1013, 0.0576, 0.0380, 0.0441, 0.0651,
        0.1331], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,621][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.2181, 0.0218, 0.0674, 0.0170, 0.0665, 0.0336, 0.1244, 0.4185, 0.0211,
        0.0116], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,623][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0012, 0.1046, 0.0141, 0.2512, 0.0445, 0.0600, 0.3634, 0.0491, 0.0940,
        0.0178], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,624][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0098, 0.0881, 0.0498, 0.1083, 0.1255, 0.0777, 0.1682, 0.0819, 0.0834,
        0.2072], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,626][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.1825, 0.1221, 0.0318, 0.0976, 0.0815, 0.0514, 0.1583, 0.1087, 0.0794,
        0.0867], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,627][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0868, 0.0395, 0.0493, 0.0189, 0.0533, 0.0552, 0.0325, 0.1410, 0.0583,
        0.2227, 0.2426], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,629][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.3635, 0.0056, 0.1000, 0.0022, 0.0863, 0.0939, 0.0427, 0.0213, 0.1359,
        0.0332, 0.1154], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,630][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([4.5149e-02, 7.1108e-06, 3.7339e-05, 3.6214e-05, 3.2597e-03, 8.9379e-03,
        7.5921e-03, 1.6270e-01, 1.9568e-01, 3.9068e-02, 5.3753e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,631][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([9.4219e-03, 1.3854e-07, 8.8269e-07, 6.6024e-07, 2.8659e-06, 1.5214e-05,
        4.1785e-05, 2.5313e-04, 7.2528e-03, 2.4347e-03, 9.8058e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,632][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([1.1113e-07, 9.0601e-10, 7.2736e-07, 3.5857e-06, 1.1763e-05, 4.5822e-03,
        4.7249e-04, 1.8649e-03, 2.6487e-01, 4.5993e-03, 7.2359e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,633][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1761, 0.0113, 0.0125, 0.0119, 0.0345, 0.0570, 0.0615, 0.1091, 0.1361,
        0.1329, 0.2571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,634][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0693, 0.0977, 0.0947, 0.0875, 0.0852, 0.0906, 0.0887, 0.0938, 0.0986,
        0.0900, 0.1039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,636][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.4472, 0.0983, 0.0401, 0.0455, 0.0472, 0.0358, 0.0327, 0.0300, 0.0445,
        0.0905, 0.0883], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,637][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.2139, 0.0187, 0.0853, 0.0225, 0.0958, 0.0524, 0.1652, 0.2618, 0.0342,
        0.0227, 0.0274], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,639][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0042, 0.0774, 0.0272, 0.2536, 0.0255, 0.0687, 0.2184, 0.0731, 0.1015,
        0.1123, 0.0380], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,640][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0144, 0.0863, 0.0513, 0.1096, 0.1081, 0.0777, 0.1448, 0.0812, 0.0835,
        0.1788, 0.0643], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,642][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.2271, 0.0694, 0.0498, 0.0653, 0.0744, 0.0566, 0.0712, 0.1421, 0.0749,
        0.0615, 0.1077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,643][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2302, 0.0919, 0.0765, 0.0547, 0.0336, 0.0470, 0.0277, 0.1038, 0.0551,
        0.0518, 0.1104, 0.1174], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,645][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.1897, 0.0020, 0.1249, 0.0026, 0.0812, 0.0736, 0.0756, 0.0889, 0.0930,
        0.0549, 0.2114, 0.0023], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,645][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([6.6753e-02, 6.7361e-06, 3.8956e-05, 3.3388e-05, 4.1193e-03, 9.3406e-03,
        6.4593e-03, 1.8063e-01, 2.1333e-01, 3.8344e-02, 4.6530e-01, 1.5643e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,647][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([2.3003e-03, 4.5272e-06, 1.8491e-05, 1.4066e-05, 6.7723e-05, 3.5982e-04,
        4.4314e-04, 2.5212e-03, 2.8137e-02, 1.0911e-02, 9.4718e-01, 8.0431e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,647][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([1.1207e-05, 3.7413e-08, 1.4534e-05, 1.6410e-05, 1.3524e-04, 3.0439e-02,
        1.6251e-03, 5.9145e-03, 3.9655e-01, 2.0276e-02, 4.6222e-01, 8.2800e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,649][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.1493, 0.0114, 0.0134, 0.0132, 0.0336, 0.0551, 0.0573, 0.0984, 0.1236,
        0.1177, 0.2242, 0.1027], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,650][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0634, 0.0915, 0.0875, 0.0801, 0.0797, 0.0838, 0.0818, 0.0859, 0.0893,
        0.0833, 0.0927, 0.0810], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,652][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.3386, 0.0553, 0.0343, 0.0456, 0.0563, 0.0369, 0.0287, 0.0312, 0.0473,
        0.0766, 0.1000, 0.1492], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,652][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1485, 0.0169, 0.0616, 0.0039, 0.0552, 0.0293, 0.0844, 0.4098, 0.0541,
        0.0865, 0.0458, 0.0041], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,652][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0043, 0.0454, 0.0316, 0.0322, 0.0702, 0.0828, 0.1875, 0.1237, 0.1459,
        0.1292, 0.1012, 0.0461], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,653][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0075, 0.0697, 0.0402, 0.0905, 0.0979, 0.0616, 0.1331, 0.0678, 0.0677,
        0.1665, 0.0556, 0.1419], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,653][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.1963, 0.0992, 0.0243, 0.0819, 0.0899, 0.0471, 0.0762, 0.0762, 0.0685,
        0.0675, 0.0655, 0.1074], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,653][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0448, 0.0125, 0.0342, 0.0095, 0.0955, 0.0539, 0.0351, 0.0985, 0.0667,
        0.1826, 0.2787, 0.0310, 0.0570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,654][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0928, 0.0005, 0.0434, 0.0004, 0.2265, 0.0397, 0.0708, 0.0392, 0.1306,
        0.0266, 0.1310, 0.0005, 0.1981], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,654][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([6.3905e-02, 1.2625e-05, 6.0609e-05, 5.4314e-05, 3.9562e-03, 8.5505e-03,
        7.1552e-03, 1.5071e-01, 1.6446e-01, 3.3811e-02, 4.1119e-01, 2.1396e-02,
        1.3473e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,655][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([3.1042e-03, 1.0643e-05, 3.5257e-05, 2.8088e-05, 9.4064e-05, 3.7076e-04,
        4.2050e-04, 2.1086e-03, 2.2574e-02, 8.7670e-03, 9.0804e-01, 1.1398e-02,
        4.3053e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,656][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([8.6644e-06, 1.0791e-07, 3.1581e-05, 5.0963e-05, 4.0436e-04, 2.9501e-02,
        4.2037e-03, 8.4490e-03, 3.2863e-01, 1.3403e-02, 4.5994e-01, 1.2704e-01,
        2.8337e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,657][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.1408, 0.0118, 0.0131, 0.0122, 0.0290, 0.0459, 0.0484, 0.0818, 0.0993,
        0.0942, 0.1831, 0.0860, 0.1544], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,659][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0594, 0.0837, 0.0810, 0.0745, 0.0733, 0.0776, 0.0755, 0.0791, 0.0825,
        0.0766, 0.0865, 0.0749, 0.0754], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,660][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.2656, 0.0460, 0.0390, 0.0474, 0.0554, 0.0339, 0.0263, 0.0273, 0.0396,
        0.0778, 0.1081, 0.1511, 0.0826], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,661][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.1815, 0.0485, 0.0622, 0.0325, 0.0832, 0.0425, 0.0735, 0.1594, 0.0410,
        0.0300, 0.0840, 0.0352, 0.1266], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,663][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0015, 0.0350, 0.0162, 0.1674, 0.0178, 0.0429, 0.1333, 0.0551, 0.0607,
        0.1079, 0.0693, 0.2580, 0.0348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,664][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0085, 0.0679, 0.0378, 0.0807, 0.0849, 0.0582, 0.1148, 0.0618, 0.0620,
        0.1441, 0.0509, 0.1177, 0.1107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,666][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.1394, 0.0802, 0.0264, 0.0854, 0.0591, 0.0264, 0.0794, 0.0849, 0.0448,
        0.0940, 0.0730, 0.1103, 0.0968], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,667][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0426, 0.0102, 0.0251, 0.0037, 0.0644, 0.0277, 0.0298, 0.0584, 0.0299,
        0.0858, 0.2119, 0.0144, 0.1453, 0.2508], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,669][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.1373, 0.0011, 0.0651, 0.0005, 0.0686, 0.0694, 0.0301, 0.0359, 0.1283,
        0.0122, 0.1767, 0.0008, 0.2394, 0.0346], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,670][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([4.4881e-02, 8.9981e-06, 3.6989e-05, 3.4635e-05, 2.3849e-03, 4.5825e-03,
        4.3462e-03, 8.2737e-02, 8.6699e-02, 2.1727e-02, 2.1248e-01, 1.3166e-02,
        8.4939e-02, 4.4197e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,671][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([2.9989e-03, 2.7467e-08, 1.0414e-07, 7.8529e-08, 1.7292e-07, 6.7923e-07,
        3.0285e-06, 1.0804e-05, 1.9750e-04, 8.6004e-05, 2.4579e-02, 2.0193e-04,
        2.7856e-03, 9.6914e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,671][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([5.0850e-08, 1.0334e-10, 7.1217e-08, 4.1657e-07, 1.5638e-06, 4.7038e-04,
        7.0050e-05, 2.9868e-04, 3.8590e-02, 8.8602e-04, 1.4775e-01, 1.4496e-02,
        1.3381e-02, 7.8405e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,673][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.1298, 0.0088, 0.0090, 0.0084, 0.0222, 0.0336, 0.0372, 0.0642, 0.0734,
        0.0720, 0.1314, 0.0640, 0.1211, 0.2248], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,674][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0555, 0.0763, 0.0741, 0.0685, 0.0657, 0.0702, 0.0697, 0.0733, 0.0772,
        0.0704, 0.0813, 0.0697, 0.0693, 0.0789], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,676][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.4517, 0.0462, 0.0209, 0.0268, 0.0315, 0.0204, 0.0181, 0.0169, 0.0255,
        0.0593, 0.0573, 0.0897, 0.0532, 0.0825], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,677][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0586, 0.0195, 0.0494, 0.0234, 0.1470, 0.0230, 0.0960, 0.0733, 0.0167,
        0.0191, 0.0398, 0.0211, 0.4092, 0.0039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,679][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0029, 0.0420, 0.0122, 0.1404, 0.0130, 0.0411, 0.2314, 0.0147, 0.0606,
        0.0720, 0.0457, 0.1848, 0.1355, 0.0039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,680][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0103, 0.0612, 0.0382, 0.0785, 0.0807, 0.0567, 0.1034, 0.0597, 0.0611,
        0.1293, 0.0491, 0.1145, 0.1016, 0.0558], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,682][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1178, 0.0564, 0.0373, 0.0405, 0.0493, 0.0472, 0.0415, 0.1305, 0.0691,
        0.0323, 0.1107, 0.0575, 0.1162, 0.0937], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,683][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0348, 0.0071, 0.0164, 0.0060, 0.0252, 0.0251, 0.0192, 0.0505, 0.0400,
        0.0761, 0.1262, 0.0234, 0.1698, 0.3558, 0.0246], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,685][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0591, 0.0017, 0.0761, 0.0004, 0.1205, 0.0523, 0.0863, 0.0321, 0.0897,
        0.0220, 0.1530, 0.0003, 0.1675, 0.0195, 0.1194], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,686][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([2.9068e-02, 7.6183e-06, 3.0637e-05, 2.7306e-05, 1.5392e-03, 3.1719e-03,
        3.1189e-03, 5.4209e-02, 5.7803e-02, 1.4555e-02, 1.4752e-01, 9.7840e-03,
        6.0670e-02, 3.4436e-01, 2.7413e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,687][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([5.6035e-04, 1.1039e-06, 2.6707e-06, 1.8039e-06, 4.8056e-06, 1.8010e-05,
        3.7664e-05, 1.0839e-04, 1.2859e-03, 4.7833e-04, 5.0347e-02, 6.2943e-04,
        5.1402e-03, 9.1784e-01, 2.3543e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,688][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([3.6151e-06, 1.5142e-08, 4.2763e-06, 9.0900e-06, 4.9676e-05, 8.3348e-03,
        7.4657e-04, 2.2607e-03, 1.3005e-01, 4.4992e-03, 2.1290e-01, 3.8239e-02,
        2.1808e-02, 5.7494e-01, 6.1519e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,689][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.1079, 0.0086, 0.0090, 0.0081, 0.0197, 0.0298, 0.0314, 0.0535, 0.0623,
        0.0595, 0.1111, 0.0532, 0.0982, 0.1885, 0.1592], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,690][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0497, 0.0718, 0.0693, 0.0639, 0.0624, 0.0663, 0.0647, 0.0680, 0.0713,
        0.0658, 0.0749, 0.0647, 0.0645, 0.0728, 0.0700], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,692][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.2796, 0.0281, 0.0220, 0.0304, 0.0373, 0.0218, 0.0178, 0.0190, 0.0271,
        0.0596, 0.0729, 0.1078, 0.0680, 0.1015, 0.1071], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,693][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.1111, 0.0285, 0.0414, 0.0273, 0.0253, 0.0215, 0.1226, 0.0593, 0.0295,
        0.0205, 0.0330, 0.0248, 0.3963, 0.0398, 0.0191], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,695][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0036, 0.0439, 0.0113, 0.1595, 0.0114, 0.0380, 0.1722, 0.0679, 0.0585,
        0.0665, 0.0316, 0.2095, 0.1148, 0.0070, 0.0043], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,696][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0077, 0.0568, 0.0328, 0.0675, 0.0746, 0.0504, 0.0992, 0.0548, 0.0532,
        0.1211, 0.0434, 0.0990, 0.0996, 0.0484, 0.0915], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,698][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.1218, 0.0707, 0.0226, 0.0574, 0.0547, 0.0349, 0.0693, 0.0862, 0.0530,
        0.0527, 0.0589, 0.0762, 0.1227, 0.0595, 0.0592], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,699][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0566, 0.0100, 0.0151, 0.0075, 0.0361, 0.0230, 0.0200, 0.0697, 0.0165,
        0.0657, 0.0914, 0.0258, 0.0614, 0.2798, 0.1814, 0.0401],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,701][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.2537, 0.0054, 0.0457, 0.0010, 0.0648, 0.0635, 0.0423, 0.0173, 0.0804,
        0.0294, 0.0687, 0.0011, 0.1107, 0.0187, 0.0646, 0.1329],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,702][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([2.4266e-02, 7.5433e-06, 2.8308e-05, 2.6756e-05, 1.3810e-03, 2.6435e-03,
        2.7883e-03, 4.0643e-02, 4.3396e-02, 1.1941e-02, 1.0647e-01, 7.9771e-03,
        4.5224e-02, 2.3756e-01, 2.0400e-01, 2.7165e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,703][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([1.8569e-03, 3.3078e-07, 9.2057e-07, 7.5237e-07, 1.7439e-06, 4.9711e-06,
        1.6458e-05, 4.7493e-05, 6.2489e-04, 2.4605e-04, 3.4267e-02, 4.8788e-04,
        4.2405e-03, 8.2687e-01, 2.7873e-02, 1.0347e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,703][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([7.6328e-07, 3.2054e-09, 9.5815e-07, 3.6555e-06, 1.4435e-05, 2.6384e-03,
        4.0492e-04, 1.1232e-03, 7.4259e-02, 3.2427e-03, 1.7539e-01, 3.1082e-02,
        1.8689e-02, 6.7369e-01, 9.6472e-03, 9.8215e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,705][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0997, 0.0075, 0.0078, 0.0072, 0.0174, 0.0255, 0.0277, 0.0464, 0.0524,
        0.0504, 0.0925, 0.0456, 0.0818, 0.1562, 0.1324, 0.1495],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,706][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0471, 0.0672, 0.0647, 0.0598, 0.0580, 0.0617, 0.0605, 0.0635, 0.0667,
        0.0611, 0.0701, 0.0603, 0.0600, 0.0680, 0.0652, 0.0661],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,708][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.3152, 0.0206, 0.0170, 0.0239, 0.0277, 0.0173, 0.0134, 0.0148, 0.0215,
        0.0500, 0.0590, 0.0892, 0.0512, 0.0848, 0.0889, 0.1056],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,709][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0724, 0.0066, 0.0493, 0.0174, 0.0496, 0.0195, 0.0830, 0.2152, 0.0138,
        0.0227, 0.0411, 0.0155, 0.2396, 0.0378, 0.1154, 0.0012],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,709][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0015, 0.0425, 0.0143, 0.1139, 0.0181, 0.0388, 0.2150, 0.0277, 0.0408,
        0.0854, 0.0639, 0.1590, 0.1101, 0.0125, 0.0433, 0.0131],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,710][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0081, 0.0541, 0.0319, 0.0649, 0.0685, 0.0490, 0.0875, 0.0537, 0.0530,
        0.1149, 0.0414, 0.0958, 0.0894, 0.0473, 0.0817, 0.0588],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,710][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.1427, 0.0434, 0.0374, 0.0342, 0.0450, 0.0459, 0.0334, 0.1222, 0.0490,
        0.0294, 0.0970, 0.0429, 0.0804, 0.0884, 0.0712, 0.0373],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,711][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0301, 0.0062, 0.0153, 0.0025, 0.0414, 0.0162, 0.0202, 0.0344, 0.0156,
        0.0534, 0.1589, 0.0103, 0.0887, 0.1384, 0.1366, 0.0543, 0.1775],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,711][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0843, 0.0008, 0.0530, 0.0004, 0.0507, 0.0506, 0.0139, 0.0350, 0.1002,
        0.0074, 0.1446, 0.0006, 0.2090, 0.0256, 0.1190, 0.0913, 0.0138],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,711][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([2.3453e-02, 4.8232e-06, 1.5825e-05, 1.5789e-05, 8.3809e-04, 1.2607e-03,
        1.4728e-03, 2.3141e-02, 2.1489e-02, 6.1450e-03, 5.2820e-02, 3.9805e-03,
        2.3965e-02, 1.1933e-01, 1.1014e-01, 1.5583e-01, 4.5610e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,712][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([7.7035e-04, 5.1626e-09, 1.2380e-08, 9.9117e-09, 1.4093e-08, 3.3787e-08,
        2.6028e-07, 5.6792e-07, 7.4492e-06, 4.2439e-06, 8.1734e-04, 1.3920e-05,
        1.6019e-04, 3.2256e-02, 1.2456e-03, 6.1579e-03, 9.5857e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,712][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([3.0309e-08, 6.0144e-11, 2.8836e-08, 3.0958e-07, 9.1059e-07, 2.1788e-04,
        4.1776e-05, 1.8187e-04, 2.1437e-02, 6.9580e-04, 8.1715e-02, 1.2353e-02,
        1.0894e-02, 4.7339e-01, 4.1682e-03, 1.0369e-02, 3.8453e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,714][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0935, 0.0060, 0.0060, 0.0055, 0.0139, 0.0199, 0.0221, 0.0378, 0.0412,
        0.0405, 0.0720, 0.0362, 0.0666, 0.1235, 0.1093, 0.1232, 0.1828],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,715][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0453, 0.0622, 0.0602, 0.0560, 0.0533, 0.0567, 0.0567, 0.0593, 0.0624,
        0.0570, 0.0657, 0.0566, 0.0561, 0.0636, 0.0612, 0.0617, 0.0658],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,716][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.3789, 0.0296, 0.0127, 0.0174, 0.0213, 0.0140, 0.0126, 0.0115, 0.0180,
        0.0451, 0.0405, 0.0633, 0.0373, 0.0618, 0.0682, 0.0817, 0.0861],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,718][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0351, 0.0130, 0.0293, 0.0153, 0.0998, 0.0136, 0.0715, 0.0471, 0.0101,
        0.0132, 0.0247, 0.0138, 0.2745, 0.0024, 0.3308, 0.0041, 0.0016],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,719][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0027, 0.0354, 0.0108, 0.1233, 0.0121, 0.0393, 0.2218, 0.0128, 0.0548,
        0.0639, 0.0412, 0.1633, 0.1129, 0.0033, 0.0307, 0.0692, 0.0025],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,720][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0085, 0.0504, 0.0311, 0.0647, 0.0662, 0.0464, 0.0851, 0.0489, 0.0501,
        0.1063, 0.0400, 0.0946, 0.0836, 0.0457, 0.0789, 0.0575, 0.0422],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,722][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0961, 0.0409, 0.0320, 0.0300, 0.0368, 0.0370, 0.0314, 0.1039, 0.0559,
        0.0237, 0.0917, 0.0420, 0.0852, 0.0779, 0.0605, 0.0652, 0.0899],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:29,723][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:29,725][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 8228],
        [  194],
        [ 4582],
        [ 4540],
        [22112],
        [18189],
        [23715],
        [15838],
        [14341],
        [10823],
        [ 6361],
        [ 5113],
        [ 7541],
        [12134],
        [ 4590],
        [23342],
        [13635]], device='cuda:0')
[2024-07-24 10:21:29,726][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 5925],
        [ 1071],
        [30786],
        [ 9914],
        [38818],
        [32443],
        [37319],
        [27612],
        [27172],
        [22852],
        [22600],
        [ 8025],
        [21373],
        [22019],
        [22026],
        [35183],
        [26228]], device='cuda:0')
[2024-07-24 10:21:29,728][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[11763],
        [ 9937],
        [ 9739],
        [12317],
        [14239],
        [13460],
        [14012],
        [12598],
        [11573],
        [11758],
        [10970],
        [11119],
        [11545],
        [11599],
        [11599],
        [11466],
        [11121]], device='cuda:0')
[2024-07-24 10:21:29,730][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[30167],
        [  307],
        [  262],
        [20343],
        [29732],
        [25290],
        [30172],
        [30381],
        [23510],
        [35449],
        [33203],
        [32967],
        [11827],
        [29767],
        [25942],
        [27767],
        [28343]], device='cuda:0')
[2024-07-24 10:21:29,731][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 9121],
        [ 5973],
        [ 7880],
        [ 7791],
        [ 7928],
        [ 7213],
        [ 8285],
        [21557],
        [11100],
        [17539],
        [ 5678],
        [ 6316],
        [10106],
        [ 9247],
        [ 6898],
        [10942],
        [11647]], device='cuda:0')
[2024-07-24 10:21:29,732][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 3618],
        [36168],
        [35318],
        [30832],
        [29020],
        [26881],
        [25565],
        [24762],
        [27933],
        [38255],
        [32973],
        [24243],
        [22638],
        [25006],
        [21016],
        [26402],
        [29425]], device='cuda:0')
[2024-07-24 10:21:29,734][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[33155],
        [32066],
        [26493],
        [27611],
        [25937],
        [23045],
        [22623],
        [22788],
        [22826],
        [20483],
        [20892],
        [17841],
        [21697],
        [21296],
        [21809],
        [20277],
        [21327]], device='cuda:0')
[2024-07-24 10:21:29,735][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[14000],
        [23362],
        [10428],
        [15899],
        [18557],
        [10585],
        [11946],
        [16398],
        [18928],
        [18496],
        [17055],
        [17130],
        [17188],
        [16776],
        [16722],
        [16853],
        [15086]], device='cuda:0')
[2024-07-24 10:21:29,737][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[32769],
        [31671],
        [30205],
        [31194],
        [31963],
        [32030],
        [33222],
        [33393],
        [32544],
        [31875],
        [31860],
        [32149],
        [33121],
        [33177],
        [33639],
        [33785],
        [33877]], device='cuda:0')
[2024-07-24 10:21:29,738][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[12647],
        [20795],
        [23059],
        [20682],
        [23320],
        [25251],
        [22701],
        [22423],
        [22365],
        [20971],
        [20947],
        [20916],
        [22246],
        [23532],
        [23164],
        [23647],
        [23973]], device='cuda:0')
[2024-07-24 10:21:29,740][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[21639],
        [21797],
        [21792],
        [21202],
        [20502],
        [ 9425],
        [ 7735],
        [ 5966],
        [ 2851],
        [ 6777],
        [ 8806],
        [ 6784],
        [14139],
        [ 1768],
        [ 2937],
        [ 8729],
        [38052]], device='cuda:0')
[2024-07-24 10:21:29,741][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[25458],
        [31905],
        [24432],
        [25957],
        [20697],
        [13923],
        [17538],
        [19880],
        [25447],
        [27102],
        [18751],
        [24365],
        [28145],
        [20447],
        [18273],
        [17002],
        [20454]], device='cuda:0')
[2024-07-24 10:21:29,742][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[19653],
        [21700],
        [22992],
        [22506],
        [30430],
        [33937],
        [31402],
        [34179],
        [34047],
        [32812],
        [24869],
        [20329],
        [24088],
        [26966],
        [31313],
        [35376],
        [33614]], device='cuda:0')
[2024-07-24 10:21:29,744][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[11515],
        [ 5632],
        [ 5142],
        [ 5726],
        [ 8280],
        [ 8241],
        [ 8368],
        [ 8489],
        [ 7562],
        [ 7808],
        [ 7890],
        [ 8129],
        [ 8837],
        [ 8371],
        [10335],
        [10452],
        [ 9878]], device='cuda:0')
[2024-07-24 10:21:29,745][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 8623],
        [  384],
        [ 2405],
        [ 4552],
        [ 9154],
        [14044],
        [ 5482],
        [ 7695],
        [10347],
        [ 4585],
        [ 3215],
        [ 9603],
        [ 4549],
        [ 7715],
        [ 2283],
        [ 6534],
        [ 7766]], device='cuda:0')
[2024-07-24 10:21:29,747][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 8228],
        [10324],
        [ 8662],
        [11953],
        [ 9697],
        [12175],
        [11047],
        [12187],
        [12461],
        [16551],
        [11659],
        [13931],
        [ 9673],
        [11212],
        [13566],
        [12211],
        [ 9868]], device='cuda:0')
[2024-07-24 10:21:29,748][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[11602],
        [10031],
        [ 9647],
        [ 8998],
        [37134],
        [24791],
        [31491],
        [21603],
        [28847],
        [17324],
        [26477],
        [24015],
        [34135],
        [32109],
        [28532],
        [27636],
        [27046]], device='cuda:0')
[2024-07-24 10:21:29,750][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[11171],
        [11170],
        [11168],
        [11170],
        [ 9788],
        [ 7290],
        [ 7411],
        [11735],
        [ 8341],
        [ 8296],
        [ 6962],
        [ 7075],
        [ 7866],
        [ 6806],
        [ 7255],
        [ 7953],
        [ 6060]], device='cuda:0')
[2024-07-24 10:21:29,751][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 921],
        [ 917],
        [ 918],
        [ 901],
        [ 862],
        [ 881],
        [ 781],
        [ 855],
        [ 568],
        [ 438],
        [1010],
        [ 971],
        [ 948],
        [1101],
        [1065],
        [ 987],
        [1174]], device='cuda:0')
[2024-07-24 10:21:29,753][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[3960],
        [4209],
        [6923],
        [6003],
        [6294],
        [1421],
        [1589],
        [1597],
        [1313],
        [1312],
        [1910],
        [1649],
        [1654],
        [ 798],
        [ 964],
        [ 879],
        [ 867]], device='cuda:0')
[2024-07-24 10:21:29,754][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[2556],
        [2555],
        [2559],
        [2558],
        [2622],
        [2696],
        [2756],
        [2807],
        [2770],
        [2745],
        [2785],
        [2793],
        [2762],
        [2753],
        [2720],
        [2675],
        [2680]], device='cuda:0')
[2024-07-24 10:21:29,755][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[2305],
        [1274],
        [1515],
        [1431],
        [1559],
        [1563],
        [1585],
        [1676],
        [1763],
        [1806],
        [1891],
        [1889],
        [1908],
        [1974],
        [2038],
        [2075],
        [2122]], device='cuda:0')
[2024-07-24 10:21:29,757][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 8903],
        [ 7980],
        [12892],
        [14577],
        [21353],
        [22271],
        [21077],
        [20637],
        [19420],
        [22788],
        [23050],
        [27205],
        [29063],
        [28705],
        [30588],
        [31876],
        [31648]], device='cuda:0')
[2024-07-24 10:21:29,758][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 9247],
        [ 9605],
        [11338],
        [12316],
        [13602],
        [13596],
        [13209],
        [14135],
        [14598],
        [13179],
        [13329],
        [13707],
        [13804],
        [16642],
        [15845],
        [16104],
        [24974]], device='cuda:0')
[2024-07-24 10:21:29,760][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[11398],
        [15507],
        [14985],
        [17482],
        [12801],
        [14764],
        [18017],
        [19983],
        [19051],
        [20918],
        [20627],
        [20424],
        [19090],
        [17439],
        [17573],
        [16606],
        [16891]], device='cuda:0')
[2024-07-24 10:21:29,761][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 3753],
        [10467],
        [ 7662],
        [ 8687],
        [ 8011],
        [ 6754],
        [ 7337],
        [ 6580],
        [ 6281],
        [ 7215],
        [ 7319],
        [ 7865],
        [ 8536],
        [ 8330],
        [ 8273],
        [ 8055],
        [ 7988]], device='cuda:0')
[2024-07-24 10:21:29,763][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 9219],
        [ 9689],
        [10177],
        [ 7340],
        [ 9267],
        [ 7870],
        [12977],
        [ 5822],
        [ 5369],
        [ 9630],
        [ 6967],
        [ 8719],
        [10328],
        [ 7006],
        [ 9954],
        [ 7814],
        [ 7807]], device='cuda:0')
[2024-07-24 10:21:29,764][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[41948],
        [41811],
        [41348],
        [40991],
        [38827],
        [41153],
        [40753],
        [40796],
        [41340],
        [42280],
        [39526],
        [38603],
        [38648],
        [38753],
        [38836],
        [39167],
        [38569]], device='cuda:0')
[2024-07-24 10:21:29,766][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[42009],
        [43914],
        [44403],
        [41197],
        [37473],
        [39612],
        [39920],
        [33493],
        [41782],
        [32968],
        [37103],
        [37101],
        [36168],
        [39639],
        [34440],
        [45251],
        [34661]], device='cuda:0')
[2024-07-24 10:21:29,767][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919],
        [49919]], device='cuda:0')
[2024-07-24 10:21:29,799][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:29,799][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,800][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,800][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,801][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,803][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,804][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,805][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,806][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,807][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,808][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,809][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,810][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:29,812][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.5408, 0.4592], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,813][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.8917, 0.1083], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,814][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.7592, 0.2408], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,816][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.3017, 0.6983], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,817][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.6221, 0.3779], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,818][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.5137, 0.4863], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,820][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.4876, 0.5124], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,821][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([7.3723e-05, 9.9993e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,822][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.2997, 0.7003], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,823][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.5316, 0.4684], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,825][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.2771, 0.7229], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,826][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.1245, 0.8755], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:29,827][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3315, 0.3123, 0.3562], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,829][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.7613, 0.0173, 0.2214], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,830][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4946, 0.1446, 0.3608], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,832][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0749, 0.7271, 0.1980], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,833][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.4342, 0.2758, 0.2900], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,834][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.3534, 0.3341, 0.3125], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,836][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.3439, 0.3584, 0.2978], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,837][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ and] are: tensor([1.3874e-04, 2.0276e-02, 9.7959e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,837][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0244, 0.0443, 0.9313], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,838][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.5919, 0.0425, 0.3656], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,838][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1605, 0.4179, 0.4217], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,838][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0086, 0.0955, 0.8959], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:29,838][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.2690, 0.2247, 0.2429, 0.2634], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,839][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.7743, 0.0099, 0.1354, 0.0804], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,839][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.5380, 0.0117, 0.1169, 0.3334], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,839][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0811, 0.4936, 0.3578, 0.0675], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,840][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.3457, 0.2255, 0.2340, 0.1947], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,840][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.2666, 0.2558, 0.2380, 0.2396], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,840][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.2568, 0.2616, 0.2367, 0.2449], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,840][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([3.7940e-05, 2.9321e-02, 9.3536e-01, 3.5278e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,841][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0775, 0.0133, 0.2827, 0.6265], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,842][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.5018, 0.0260, 0.1286, 0.3436], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,844][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.1124, 0.2908, 0.2933, 0.3034], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,845][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0400, 0.0335, 0.2236, 0.7028], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:29,846][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.2501, 0.1935, 0.2102, 0.2139, 0.1323], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,848][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.6280, 0.0013, 0.1322, 0.0242, 0.2143], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,849][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0833, 0.0050, 0.0383, 0.5674, 0.3061], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,850][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.1057, 0.2942, 0.3919, 0.1155, 0.0928], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,852][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.2812, 0.1839, 0.1903, 0.1822, 0.1624], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,853][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.2031, 0.1982, 0.1862, 0.1932, 0.2192], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,854][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1949, 0.1951, 0.1737, 0.1826, 0.2536], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,855][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ got] are: tensor([7.9233e-06, 1.4348e-02, 8.5654e-01, 4.9686e-02, 7.9415e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,856][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0044, 0.0034, 0.1102, 0.4874, 0.3947], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,858][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.3533, 0.0034, 0.0793, 0.0521, 0.5119], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,859][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0917, 0.2253, 0.2278, 0.2356, 0.2196], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,861][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0056, 0.0199, 0.2210, 0.4199, 0.3335], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:29,862][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1931, 0.1609, 0.1744, 0.1824, 0.1292, 0.1601], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,863][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.2771, 0.0013, 0.0679, 0.0352, 0.2989, 0.3196], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,865][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0250, 0.0007, 0.0119, 0.2029, 0.6559, 0.1036], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,866][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0401, 0.1386, 0.1506, 0.1635, 0.2619, 0.2454], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,867][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2384, 0.1520, 0.1621, 0.1458, 0.1439, 0.1578], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,869][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1743, 0.1687, 0.1534, 0.1622, 0.1805, 0.1609], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,870][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1605, 0.1628, 0.1370, 0.1501, 0.2033, 0.1863], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,871][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([3.6557e-06, 8.1638e-03, 6.8512e-01, 4.0951e-02, 9.5477e-02, 1.7028e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,872][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0005, 0.0025, 0.0560, 0.4792, 0.3937, 0.0682], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,874][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0823, 0.0040, 0.0426, 0.0454, 0.6185, 0.2072], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,875][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0736, 0.1861, 0.1878, 0.1942, 0.1811, 0.1773], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,877][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0018, 0.0295, 0.1154, 0.4162, 0.2773, 0.1598], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:29,878][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.1666, 0.1369, 0.1441, 0.1536, 0.1150, 0.1383, 0.1453],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,879][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.1721, 0.0024, 0.0558, 0.0879, 0.2156, 0.3909, 0.0752],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,881][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0106, 0.0005, 0.0087, 0.0829, 0.4339, 0.3855, 0.0779],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,882][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0412, 0.0797, 0.1478, 0.0269, 0.0846, 0.5015, 0.1184],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,883][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.2178, 0.1382, 0.1430, 0.1308, 0.1209, 0.1345, 0.1147],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,885][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.1417, 0.1398, 0.1330, 0.1374, 0.1590, 0.1385, 0.1505],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,886][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1276, 0.1265, 0.1161, 0.1190, 0.1766, 0.1629, 0.1712],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,887][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([1.2159e-05, 1.0216e-02, 5.4337e-01, 2.5103e-02, 1.2961e-01, 2.4598e-01,
        4.5706e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,888][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([3.8232e-04, 4.6809e-03, 6.4911e-02, 4.5605e-01, 4.0911e-01, 6.0532e-02,
        4.3304e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,889][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0687, 0.0034, 0.0568, 0.0668, 0.2547, 0.3722, 0.1774],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,891][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0627, 0.1572, 0.1590, 0.1646, 0.1535, 0.1499, 0.1532],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,892][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0043, 0.0301, 0.1213, 0.3365, 0.2883, 0.1943, 0.0253],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:29,893][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.1523, 0.1260, 0.1346, 0.1418, 0.0918, 0.1189, 0.1275, 0.1071],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,895][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0811, 0.0007, 0.0380, 0.0191, 0.3034, 0.3756, 0.1246, 0.0575],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,895][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ at] are: tensor([3.6333e-03, 1.9176e-04, 5.1357e-03, 1.1465e-01, 1.6102e-01, 4.1420e-01,
        2.8775e-01, 1.3431e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,896][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0097, 0.2159, 0.0803, 0.0593, 0.1322, 0.2172, 0.2008, 0.0847],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,896][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1886, 0.1220, 0.1263, 0.1176, 0.1114, 0.1222, 0.1097, 0.1022],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,896][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.1337, 0.1255, 0.1158, 0.1222, 0.1360, 0.1199, 0.1329, 0.1140],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,897][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1193, 0.1157, 0.0988, 0.1061, 0.1440, 0.1321, 0.1379, 0.1462],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,897][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ at] are: tensor([2.2640e-05, 8.2829e-03, 4.7980e-01, 2.7936e-02, 9.5030e-02, 1.2748e-01,
        1.1618e-01, 1.4526e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,897][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ at] are: tensor([2.0907e-04, 9.4256e-04, 7.2389e-02, 3.2211e-01, 4.7359e-01, 7.5320e-02,
        2.6080e-02, 2.9357e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,898][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0515, 0.0006, 0.0329, 0.0362, 0.3619, 0.2730, 0.2242, 0.0198],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,898][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0552, 0.1350, 0.1365, 0.1411, 0.1316, 0.1290, 0.1318, 0.1398],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,899][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0010, 0.0143, 0.1051, 0.2638, 0.3795, 0.1475, 0.0295, 0.0593],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:29,901][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1355, 0.1120, 0.1171, 0.1249, 0.0838, 0.1067, 0.1133, 0.0971, 0.1094],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,902][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0400, 0.0007, 0.0301, 0.0281, 0.3416, 0.2650, 0.0995, 0.0724, 0.1224],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,903][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.5913e-03, 8.2174e-05, 2.8859e-03, 5.6240e-02, 4.0476e-01, 1.7071e-01,
        1.7016e-01, 1.6015e-01, 3.3426e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,904][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0199, 0.0918, 0.0730, 0.0736, 0.1261, 0.1398, 0.1790, 0.1591, 0.1376],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,906][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1674, 0.1055, 0.1141, 0.1003, 0.0981, 0.1104, 0.1005, 0.0912, 0.1126],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,907][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1157, 0.1120, 0.1024, 0.1088, 0.1185, 0.1063, 0.1182, 0.1027, 0.1153],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,908][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1009, 0.1004, 0.0849, 0.0928, 0.1226, 0.1133, 0.1240, 0.1277, 0.1334],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,909][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.5078e-06, 3.9224e-03, 4.7608e-01, 2.5037e-02, 4.6163e-02, 8.0360e-02,
        1.0372e-01, 1.1851e-01, 1.4620e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,910][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.0901e-04, 6.4477e-04, 4.0438e-02, 2.4288e-01, 4.3725e-01, 4.0764e-02,
        1.9926e-02, 3.3631e-02, 1.8435e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,912][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0180, 0.0015, 0.0234, 0.0285, 0.4287, 0.1845, 0.1675, 0.0867, 0.0612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,913][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0489, 0.1190, 0.1200, 0.1243, 0.1164, 0.1139, 0.1167, 0.1227, 0.1180],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,915][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0005, 0.0065, 0.0435, 0.3758, 0.2203, 0.1380, 0.0314, 0.0629, 0.1212],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:29,916][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.1282, 0.1015, 0.1084, 0.1132, 0.0742, 0.0951, 0.0992, 0.0876, 0.0978,
        0.0947], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,917][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0641, 0.0010, 0.0310, 0.0427, 0.1331, 0.3192, 0.0959, 0.0692, 0.1977,
        0.0460], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,918][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ station] are: tensor([4.0244e-03, 2.2892e-04, 3.2637e-03, 4.8272e-02, 1.1673e-01, 3.2062e-01,
        1.2761e-01, 1.1061e-01, 2.2307e-01, 4.5574e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,920][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0247, 0.1138, 0.0597, 0.0308, 0.0466, 0.2494, 0.1340, 0.1462, 0.1661,
        0.0286], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,921][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.1547, 0.0974, 0.1042, 0.0981, 0.0941, 0.0984, 0.0910, 0.0815, 0.0989,
        0.0816], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,922][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.1005, 0.0980, 0.0919, 0.0964, 0.1119, 0.0967, 0.1042, 0.0912, 0.1054,
        0.1038], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,924][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0859, 0.0782, 0.0726, 0.0741, 0.1107, 0.1023, 0.1074, 0.1160, 0.1207,
        0.1320], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,925][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ station] are: tensor([3.8902e-07, 4.4461e-03, 1.3293e-01, 5.1763e-03, 5.4776e-02, 1.6128e-01,
        1.6124e-02, 9.2011e-02, 5.3290e-01, 3.4552e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,925][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ station] are: tensor([2.2274e-04, 6.5247e-04, 3.0460e-02, 1.8191e-01, 3.6828e-01, 2.8238e-02,
        9.1732e-03, 4.2768e-02, 2.4221e-01, 9.6091e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,927][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0183, 0.0022, 0.0282, 0.0215, 0.1621, 0.3319, 0.0469, 0.2518, 0.0996,
        0.0374], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,928][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0432, 0.1061, 0.1070, 0.1108, 0.1033, 0.1010, 0.1034, 0.1089, 0.1045,
        0.1120], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,930][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0022, 0.0084, 0.0403, 0.2743, 0.1597, 0.1323, 0.0233, 0.1112, 0.2147,
        0.0336], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:29,931][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.1160, 0.0948, 0.0981, 0.1048, 0.0664, 0.0866, 0.0930, 0.0780, 0.0874,
        0.0856, 0.0893], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,933][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0666, 0.0004, 0.0261, 0.0096, 0.0956, 0.1687, 0.0608, 0.0564, 0.1113,
        0.0702, 0.3343], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,933][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [,] are: tensor([2.9346e-03, 8.2972e-05, 2.6629e-03, 5.0953e-02, 2.3842e-01, 2.0182e-01,
        8.3976e-02, 8.3635e-02, 2.0263e-01, 6.0445e-02, 7.2441e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,935][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0154, 0.1034, 0.0372, 0.0457, 0.0573, 0.2149, 0.1521, 0.0529, 0.2014,
        0.0759, 0.0438], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,936][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1383, 0.0852, 0.0924, 0.0848, 0.0810, 0.0899, 0.0842, 0.0752, 0.0919,
        0.0807, 0.0963], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,938][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0961, 0.0897, 0.0836, 0.0871, 0.0978, 0.0879, 0.0951, 0.0828, 0.0975,
        0.0970, 0.0853], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,939][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0807, 0.0795, 0.0660, 0.0737, 0.0979, 0.0892, 0.0973, 0.0994, 0.1043,
        0.1171, 0.0948], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,940][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [,] are: tensor([5.9631e-06, 3.6313e-03, 3.1260e-01, 1.9109e-02, 4.3506e-02, 5.7956e-02,
        7.0938e-02, 1.1063e-01, 1.4554e-01, 1.3540e-03, 2.3472e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,941][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [,] are: tensor([1.0797e-04, 3.3303e-04, 1.6096e-02, 6.2065e-02, 3.5997e-01, 4.4918e-02,
        1.4250e-02, 2.4935e-02, 1.9252e-01, 1.4124e-01, 1.4356e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,942][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1070, 0.0027, 0.0170, 0.0594, 0.1096, 0.2222, 0.1775, 0.0385, 0.0775,
        0.0397, 0.1489], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,944][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0386, 0.0956, 0.0967, 0.1000, 0.0935, 0.0917, 0.0940, 0.0989, 0.0950,
        0.1017, 0.0945], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,945][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0003, 0.0036, 0.0302, 0.1028, 0.1774, 0.1574, 0.0212, 0.0525, 0.1483,
        0.0569, 0.2494], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:29,947][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.1035, 0.0813, 0.0845, 0.0928, 0.0612, 0.0771, 0.0806, 0.0711, 0.0798,
        0.0790, 0.0823, 0.1068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,948][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.1017, 0.0009, 0.0259, 0.0129, 0.0868, 0.1060, 0.0557, 0.0637, 0.0997,
        0.0460, 0.3622, 0.0385], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,949][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([3.3809e-02, 1.8244e-04, 7.3125e-03, 2.2103e-02, 1.2745e-01, 1.1893e-01,
        1.1609e-01, 3.7953e-02, 5.0291e-02, 2.5872e-02, 3.1890e-01, 1.4110e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,951][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0144, 0.0833, 0.0699, 0.0137, 0.0352, 0.1972, 0.1402, 0.1025, 0.1221,
        0.1040, 0.0975, 0.0200], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,952][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.1362, 0.0849, 0.0898, 0.0728, 0.0753, 0.0814, 0.0737, 0.0699, 0.0843,
        0.0761, 0.0902, 0.0653], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,953][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0850, 0.0794, 0.0741, 0.0751, 0.0904, 0.0817, 0.0872, 0.0782, 0.0923,
        0.0934, 0.0804, 0.0827], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,953][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0759, 0.0656, 0.0578, 0.0596, 0.0879, 0.0790, 0.0829, 0.0896, 0.0958,
        0.1027, 0.0926, 0.1106], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,953][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([2.1779e-05, 2.5072e-03, 1.6761e-01, 5.2152e-03, 3.5709e-02, 4.7703e-02,
        1.7634e-02, 5.9984e-02, 1.2270e-01, 7.5466e-04, 4.6860e-01, 7.1561e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,954][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([2.0580e-03, 4.4713e-04, 2.4891e-02, 5.4546e-02, 1.9013e-01, 1.0147e-02,
        4.7245e-03, 8.7415e-03, 5.4119e-02, 8.0452e-02, 1.0479e-01, 4.6495e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,954][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.1169, 0.0025, 0.0316, 0.0531, 0.0715, 0.1320, 0.0481, 0.0324, 0.0654,
        0.0173, 0.2671, 0.1621], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,954][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0351, 0.0875, 0.0885, 0.0915, 0.0853, 0.0832, 0.0853, 0.0897, 0.0862,
        0.0921, 0.0856, 0.0901], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,955][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0102, 0.0042, 0.0314, 0.1244, 0.1120, 0.0539, 0.0113, 0.0185, 0.0626,
        0.0342, 0.0752, 0.4621], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:29,955][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0985, 0.0747, 0.0812, 0.0852, 0.0573, 0.0718, 0.0752, 0.0662, 0.0719,
        0.0697, 0.0754, 0.0933, 0.0796], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,955][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([3.6531e-02, 1.6790e-04, 1.7561e-02, 3.4980e-03, 7.7992e-02, 7.8624e-02,
        5.1336e-02, 2.2871e-02, 6.4658e-02, 3.9884e-02, 5.0548e-01, 2.0853e-02,
        8.0543e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,956][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([6.3617e-04, 5.5389e-05, 9.4799e-04, 3.4338e-02, 7.3049e-02, 3.9814e-02,
        4.9702e-02, 2.0452e-02, 3.8364e-02, 2.3337e-02, 1.7880e-01, 4.9890e-01,
        4.1607e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,957][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0153, 0.1068, 0.0631, 0.0429, 0.0260, 0.1350, 0.1240, 0.1466, 0.1052,
        0.0527, 0.0904, 0.0729, 0.0193], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,959][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.1234, 0.0755, 0.0801, 0.0744, 0.0709, 0.0771, 0.0717, 0.0642, 0.0780,
        0.0697, 0.0806, 0.0685, 0.0659], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,960][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0756, 0.0729, 0.0691, 0.0724, 0.0816, 0.0727, 0.0824, 0.0703, 0.0810,
        0.0857, 0.0715, 0.0810, 0.0839], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,962][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0711, 0.0602, 0.0549, 0.0548, 0.0780, 0.0705, 0.0740, 0.0813, 0.0846,
        0.0895, 0.0813, 0.0975, 0.1023], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,962][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([1.9255e-06, 2.7410e-03, 1.2388e-01, 8.3169e-03, 2.0676e-02, 5.2557e-02,
        2.8201e-02, 6.0865e-02, 1.5743e-01, 9.2913e-04, 4.0890e-01, 1.2778e-01,
        7.7246e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,963][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([2.5326e-05, 2.4413e-04, 7.5627e-03, 4.9526e-02, 4.3106e-02, 1.3884e-02,
        4.8618e-03, 4.5941e-03, 4.4880e-02, 3.8342e-02, 9.5162e-02, 6.9629e-01,
        1.5220e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,965][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0252, 0.0004, 0.0201, 0.0255, 0.1680, 0.0873, 0.0414, 0.0284, 0.0438,
        0.0126, 0.3831, 0.0909, 0.0732], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,966][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0324, 0.0798, 0.0808, 0.0835, 0.0781, 0.0765, 0.0785, 0.0825, 0.0794,
        0.0849, 0.0789, 0.0826, 0.0820], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,967][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([8.1720e-05, 1.4987e-03, 1.6733e-02, 5.6790e-02, 3.9651e-02, 3.9956e-02,
        1.0387e-02, 1.5245e-02, 3.1852e-02, 2.2787e-02, 1.1478e-01, 6.2813e-01,
        2.2110e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:29,969][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0839, 0.0726, 0.0767, 0.0814, 0.0513, 0.0670, 0.0721, 0.0596, 0.0674,
        0.0670, 0.0687, 0.0912, 0.0760, 0.0651], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,969][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([7.3396e-02, 1.7647e-04, 1.0076e-02, 6.7874e-03, 5.1259e-02, 7.7999e-02,
        4.2164e-02, 1.6762e-02, 4.0720e-02, 1.6866e-02, 3.0152e-01, 2.8204e-02,
        1.4324e-01, 1.9083e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,970][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([4.2090e-03, 1.2249e-05, 5.2045e-04, 1.1516e-02, 7.3985e-02, 3.6636e-02,
        5.4742e-02, 1.1273e-02, 2.8588e-02, 2.0668e-02, 1.2620e-01, 1.8222e-01,
        1.6229e-01, 2.8714e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,972][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0126, 0.0618, 0.0388, 0.0392, 0.0895, 0.1495, 0.1439, 0.0387, 0.1282,
        0.1004, 0.0538, 0.0570, 0.0757, 0.0109], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,973][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.1093, 0.0696, 0.0740, 0.0675, 0.0658, 0.0724, 0.0692, 0.0612, 0.0735,
        0.0646, 0.0766, 0.0633, 0.0626, 0.0705], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,975][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0759, 0.0702, 0.0653, 0.0682, 0.0753, 0.0685, 0.0751, 0.0650, 0.0755,
        0.0755, 0.0677, 0.0741, 0.0767, 0.0671], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,976][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0678, 0.0604, 0.0508, 0.0537, 0.0726, 0.0665, 0.0693, 0.0737, 0.0788,
        0.0814, 0.0728, 0.0918, 0.0926, 0.0676], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,977][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([9.7535e-07, 1.8866e-03, 1.6068e-01, 7.1719e-03, 3.0617e-02, 3.4854e-02,
        7.2991e-02, 4.6440e-02, 1.1684e-01, 7.8203e-04, 3.2959e-01, 1.6799e-01,
        9.3664e-03, 2.0791e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,978][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([6.3886e-05, 2.2630e-04, 1.0147e-02, 4.1987e-02, 1.1308e-01, 1.2879e-02,
        4.4995e-03, 5.7088e-03, 1.1009e-01, 6.9614e-02, 1.6947e-01, 4.4659e-01,
        7.1846e-03, 8.4702e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,979][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([3.1644e-02, 2.4212e-04, 7.6283e-03, 4.6657e-03, 1.0109e-01, 5.7059e-02,
        3.7138e-02, 4.0804e-03, 1.8414e-02, 9.0799e-03, 1.5997e-01, 2.4396e-02,
        4.4878e-01, 9.5812e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,980][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0306, 0.0738, 0.0747, 0.0773, 0.0722, 0.0708, 0.0726, 0.0762, 0.0732,
        0.0784, 0.0728, 0.0763, 0.0755, 0.0755], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,981][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([4.1425e-05, 1.1208e-03, 9.3704e-03, 3.8664e-02, 6.1655e-02, 3.0283e-02,
        6.0756e-03, 2.1900e-02, 5.6609e-02, 1.6906e-02, 1.0194e-01, 5.7906e-01,
        6.4577e-02, 1.1805e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:29,983][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0819, 0.0678, 0.0730, 0.0764, 0.0492, 0.0624, 0.0663, 0.0564, 0.0623,
        0.0610, 0.0648, 0.0826, 0.0693, 0.0608, 0.0657], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,984][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ give] are: tensor([5.2751e-02, 9.6328e-05, 7.4985e-03, 4.3729e-03, 2.7446e-02, 4.7910e-02,
        2.3510e-02, 1.1272e-02, 2.3256e-02, 1.4483e-02, 2.0415e-01, 2.0462e-02,
        1.6757e-01, 1.8753e-01, 2.0770e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,985][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ give] are: tensor([1.0175e-03, 6.3802e-06, 1.4850e-04, 5.8074e-03, 4.6813e-03, 9.4255e-03,
        8.9012e-03, 4.8160e-03, 4.8529e-03, 2.0466e-03, 1.4887e-02, 7.5936e-02,
        1.7388e-02, 7.4303e-01, 1.0706e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,986][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0154, 0.0784, 0.0876, 0.0257, 0.0240, 0.1266, 0.1099, 0.1371, 0.0935,
        0.0611, 0.0970, 0.0373, 0.0402, 0.0553, 0.0107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,987][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.1046, 0.0666, 0.0706, 0.0638, 0.0619, 0.0671, 0.0660, 0.0564, 0.0688,
        0.0609, 0.0708, 0.0590, 0.0620, 0.0652, 0.0563], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,989][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0676, 0.0647, 0.0610, 0.0633, 0.0723, 0.0633, 0.0713, 0.0607, 0.0690,
        0.0706, 0.0621, 0.0696, 0.0732, 0.0627, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,990][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0593, 0.0508, 0.0458, 0.0467, 0.0664, 0.0608, 0.0635, 0.0691, 0.0726,
        0.0769, 0.0687, 0.0847, 0.0888, 0.0649, 0.0810], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,991][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ give] are: tensor([4.0660e-06, 1.8926e-03, 1.3818e-01, 7.1193e-03, 1.6071e-02, 4.0024e-02,
        3.3246e-02, 4.6559e-02, 1.0960e-01, 8.3754e-04, 3.8739e-01, 1.3593e-01,
        8.7862e-03, 4.7395e-02, 2.6959e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,992][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ give] are: tensor([5.1482e-05, 1.5506e-04, 6.9936e-03, 4.0695e-02, 9.9172e-02, 5.4867e-03,
        2.3409e-03, 3.6319e-03, 3.8181e-02, 5.1221e-02, 1.1894e-01, 5.7353e-01,
        1.6205e-02, 1.9080e-02, 2.4319e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,993][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ give] are: tensor([9.1065e-03, 1.5076e-04, 3.9192e-03, 4.2660e-03, 2.3369e-02, 4.0825e-02,
        2.0891e-02, 8.3184e-03, 9.5602e-03, 4.7272e-03, 8.6583e-02, 1.6491e-02,
        3.6775e-01, 3.4430e-01, 5.9740e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,994][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0278, 0.0687, 0.0696, 0.0718, 0.0672, 0.0658, 0.0674, 0.0708, 0.0681,
        0.0729, 0.0676, 0.0708, 0.0701, 0.0701, 0.0712], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,995][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ give] are: tensor([9.8009e-05, 9.3683e-04, 7.0871e-03, 3.1477e-02, 3.8666e-02, 1.9332e-02,
        2.7527e-03, 2.0028e-02, 5.1175e-02, 1.7725e-02, 1.1672e-01, 5.7584e-01,
        5.8623e-02, 3.0133e-02, 2.9407e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:29,997][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0747, 0.0630, 0.0678, 0.0706, 0.0460, 0.0589, 0.0622, 0.0532, 0.0594,
        0.0582, 0.0615, 0.0785, 0.0664, 0.0576, 0.0624, 0.0593],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,998][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ it] are: tensor([6.7552e-02, 6.7483e-05, 6.7879e-03, 3.4278e-03, 1.0865e-02, 2.6742e-02,
        1.5501e-02, 5.9763e-03, 1.5361e-02, 5.2042e-03, 1.4248e-01, 1.4567e-02,
        5.4639e-02, 1.3011e-01, 1.2713e-01, 3.7359e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:29,999][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ it] are: tensor([2.0811e-03, 1.1167e-06, 8.5313e-05, 1.6308e-03, 7.9907e-03, 2.7522e-03,
        1.5159e-02, 6.6413e-03, 1.4841e-03, 2.1773e-03, 7.5049e-03, 1.7297e-02,
        2.4008e-02, 3.2331e-01, 3.6884e-01, 2.1904e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,000][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0167, 0.0368, 0.0606, 0.0224, 0.0812, 0.1346, 0.1110, 0.0830, 0.0735,
        0.1120, 0.0639, 0.0303, 0.0677, 0.0406, 0.0499, 0.0159],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,001][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0991, 0.0610, 0.0666, 0.0584, 0.0567, 0.0648, 0.0583, 0.0521, 0.0661,
        0.0576, 0.0694, 0.0545, 0.0570, 0.0610, 0.0537, 0.0637],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,003][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0635, 0.0603, 0.0571, 0.0581, 0.0644, 0.0601, 0.0634, 0.0568, 0.0674,
        0.0668, 0.0592, 0.0642, 0.0677, 0.0584, 0.0634, 0.0693],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,004][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0562, 0.0490, 0.0424, 0.0449, 0.0613, 0.0567, 0.0596, 0.0638, 0.0671,
        0.0713, 0.0624, 0.0804, 0.0822, 0.0612, 0.0764, 0.0652],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,005][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ it] are: tensor([1.1889e-06, 1.9937e-03, 1.4474e-01, 8.9350e-03, 2.1187e-02, 3.9215e-02,
        4.9471e-02, 5.2972e-02, 1.0334e-01, 8.2932e-04, 2.8605e-01, 1.9735e-01,
        8.8383e-03, 4.6853e-02, 3.1781e-02, 6.4375e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,006][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ it] are: tensor([2.9942e-05, 1.2103e-04, 3.8514e-03, 4.1489e-02, 4.2607e-02, 2.7181e-03,
        8.2417e-04, 3.8919e-03, 1.1824e-02, 1.8696e-02, 6.9167e-02, 7.2031e-01,
        5.4952e-03, 1.3283e-02, 6.2418e-02, 3.2726e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,007][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ it] are: tensor([2.4608e-02, 1.4711e-04, 2.0034e-03, 6.7743e-03, 2.8602e-02, 1.2857e-02,
        2.4303e-02, 4.3742e-03, 5.3204e-03, 1.1649e-02, 2.5568e-02, 2.2020e-02,
        7.8606e-02, 8.5635e-02, 5.1446e-01, 1.5307e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,009][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0262, 0.0640, 0.0648, 0.0670, 0.0627, 0.0613, 0.0629, 0.0661, 0.0634,
        0.0680, 0.0629, 0.0661, 0.0654, 0.0652, 0.0664, 0.0675],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,010][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ it] are: tensor([1.2317e-04, 1.1698e-03, 1.0181e-02, 5.2470e-02, 5.2384e-02, 2.4925e-02,
        2.4039e-03, 1.4068e-02, 3.5800e-02, 7.9555e-03, 7.0706e-02, 6.0220e-01,
        3.9206e-02, 3.2395e-02, 4.3732e-02, 1.0285e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,011][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0666, 0.0599, 0.0633, 0.0676, 0.0437, 0.0558, 0.0602, 0.0497, 0.0562,
        0.0565, 0.0573, 0.0756, 0.0638, 0.0539, 0.0594, 0.0552, 0.0555],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,011][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.1508e-02, 2.0922e-05, 1.2781e-03, 1.1089e-03, 5.9243e-03, 9.6523e-03,
        5.4334e-03, 1.9543e-03, 4.5144e-03, 1.9518e-03, 3.8054e-02, 4.1461e-03,
        1.6554e-02, 2.3593e-02, 6.9307e-02, 7.1623e-01, 7.8775e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,011][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([4.4064e-04, 2.0236e-07, 1.0722e-05, 2.5943e-04, 1.8115e-03, 7.2288e-04,
        1.6627e-03, 2.0276e-04, 4.6056e-04, 4.6076e-04, 2.6302e-03, 3.7354e-03,
        2.8660e-03, 6.2803e-03, 2.3466e-01, 6.7034e-01, 7.3455e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,012][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0107, 0.0516, 0.0310, 0.0329, 0.0727, 0.1412, 0.1323, 0.0306, 0.1170,
        0.0907, 0.0452, 0.0494, 0.0642, 0.0088, 0.0354, 0.0807, 0.0056],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,012][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0904, 0.0579, 0.0617, 0.0565, 0.0547, 0.0602, 0.0578, 0.0510, 0.0612,
        0.0535, 0.0635, 0.0528, 0.0521, 0.0587, 0.0507, 0.0583, 0.0590],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,013][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0619, 0.0574, 0.0534, 0.0557, 0.0613, 0.0559, 0.0615, 0.0534, 0.0618,
        0.0621, 0.0554, 0.0606, 0.0623, 0.0550, 0.0602, 0.0651, 0.0568],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,013][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0563, 0.0499, 0.0409, 0.0440, 0.0587, 0.0544, 0.0563, 0.0596, 0.0640,
        0.0654, 0.0589, 0.0739, 0.0752, 0.0555, 0.0700, 0.0615, 0.0552],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,014][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.1524e-06, 1.1791e-03, 1.5523e-01, 8.1134e-03, 1.8266e-02, 2.3124e-02,
        8.7295e-02, 3.8044e-02, 7.4359e-02, 6.4777e-04, 2.4902e-01, 2.3474e-01,
        6.7618e-03, 1.2773e-02, 3.7900e-02, 5.1448e-03, 4.7395e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,015][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([7.5802e-05, 1.6095e-04, 1.0899e-02, 4.1709e-02, 1.3025e-01, 5.4601e-03,
        1.8212e-03, 4.9610e-03, 6.0860e-02, 4.4113e-02, 1.2559e-01, 4.4839e-01,
        8.2655e-03, 6.7844e-03, 6.9099e-02, 1.8698e-02, 2.2868e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,015][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([9.0731e-03, 3.4517e-05, 1.0849e-03, 7.3366e-04, 1.2875e-02, 7.9145e-03,
        6.3536e-03, 5.4828e-04, 2.3684e-03, 1.4905e-03, 2.1380e-02, 3.5501e-03,
        5.4533e-02, 1.1579e-02, 4.7241e-01, 3.5683e-01, 3.7243e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,017][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0251, 0.0601, 0.0607, 0.0629, 0.0588, 0.0575, 0.0590, 0.0618, 0.0594,
        0.0635, 0.0589, 0.0620, 0.0612, 0.0611, 0.0621, 0.0632, 0.0628],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,018][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([7.4669e-05, 8.2684e-04, 6.6647e-03, 3.4826e-02, 6.1053e-02, 2.3744e-02,
        3.2842e-03, 1.7368e-02, 7.2511e-02, 9.3608e-03, 8.1391e-02, 5.3940e-01,
        5.5644e-02, 1.1906e-02, 5.3121e-02, 9.5445e-03, 1.9284e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,047][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:30,048][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,050][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,051][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,052][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,053][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,054][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,056][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,057][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,058][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,059][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,060][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,061][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,062][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.9684, 0.0316], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,064][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.8709, 0.1291], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,065][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.0623, 0.9377], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,067][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.0833, 0.9167], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,067][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.9937, 0.0063], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,068][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.5560, 0.4440], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,068][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.6618, 0.3382], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,068][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.2403, 0.7597], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,068][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.4845, 0.5155], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,069][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.5316, 0.4684], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,069][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.0607, 0.9393], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,069][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.8408, 0.1592], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,070][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.8915, 0.0162, 0.0923], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,070][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.8180, 0.1322, 0.0499], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,070][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0304, 0.4700, 0.4995], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,071][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0235, 0.0229, 0.9536], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,072][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.9566, 0.0044, 0.0390], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,073][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3227, 0.3248, 0.3526], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,075][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5858, 0.0982, 0.3160], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,076][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0878, 0.0284, 0.8837], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,077][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.1957, 0.0158, 0.7885], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,078][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.5919, 0.0425, 0.3656], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,080][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0239, 0.1619, 0.8142], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,081][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.7395, 0.0213, 0.2392], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,083][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.9213, 0.0060, 0.0176, 0.0550], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,084][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.6154, 0.1511, 0.2207, 0.0128], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,085][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0199, 0.2518, 0.3135, 0.4147], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,087][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0249, 0.0039, 0.1077, 0.8635], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,088][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.9375, 0.0078, 0.0527, 0.0020], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,089][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.2296, 0.2314, 0.2466, 0.2924], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,091][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.4627, 0.0506, 0.0691, 0.4175], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,092][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0648, 0.0137, 0.1970, 0.7244], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,094][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.4015, 0.0053, 0.2825, 0.3107], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,095][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.5018, 0.0260, 0.1286, 0.3436], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,096][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0343, 0.1098, 0.2538, 0.6022], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,098][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.8118, 0.0098, 0.0562, 0.1222], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,099][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.9836, 0.0015, 0.0079, 0.0019, 0.0051], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,100][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.5874, 0.0548, 0.2685, 0.0204, 0.0689], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,102][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0121, 0.1788, 0.2133, 0.3280, 0.2678], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,102][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([3.5602e-03, 9.6885e-05, 2.8531e-02, 1.4635e-01, 8.2147e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,104][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.8847, 0.0081, 0.0381, 0.0190, 0.0501], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,105][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.1155, 0.1417, 0.1837, 0.2588, 0.3003], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,107][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0657, 0.0077, 0.0597, 0.5516, 0.3153], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,108][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([7.6890e-03, 2.7197e-04, 3.0102e-02, 5.0389e-01, 4.5804e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,109][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.1191, 0.0008, 0.1330, 0.2292, 0.5179], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,110][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.3533, 0.0034, 0.0793, 0.0521, 0.5119], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,112][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0101, 0.0175, 0.0823, 0.1850, 0.7051], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,113][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.3199, 0.0011, 0.0504, 0.0482, 0.5805], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,115][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.8344, 0.0120, 0.0272, 0.0097, 0.0054, 0.1112], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,116][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.4383, 0.1101, 0.1164, 0.0531, 0.1177, 0.1643], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,117][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0097, 0.1450, 0.1788, 0.2810, 0.2535, 0.1320], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,118][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([2.9897e-04, 1.3652e-05, 3.0863e-03, 4.7669e-02, 9.0105e-01, 4.7878e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,120][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.8857, 0.0070, 0.0285, 0.0167, 0.0457, 0.0164], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,121][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1210, 0.1480, 0.1260, 0.2021, 0.2062, 0.1968], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,122][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0453, 0.0045, 0.0371, 0.2858, 0.4584, 0.1689], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,123][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([1.0468e-03, 1.0207e-04, 3.9877e-03, 1.2412e-01, 7.3233e-01, 1.3842e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,124][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([1.8079e-02, 2.9871e-04, 3.2465e-02, 9.3081e-02, 6.8256e-01, 1.7352e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,125][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0823, 0.0040, 0.0426, 0.0454, 0.6185, 0.2072], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,126][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0052, 0.0231, 0.0675, 0.1933, 0.4194, 0.2915], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,126][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.2003, 0.0018, 0.0348, 0.0406, 0.5754, 0.1471], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,126][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.8683, 0.0041, 0.0144, 0.0060, 0.0058, 0.0167, 0.0846],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,127][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.1862, 0.0735, 0.1591, 0.0326, 0.0822, 0.2878, 0.1787],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,127][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0100, 0.1160, 0.1559, 0.2257, 0.2227, 0.1305, 0.1392],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,127][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([7.5565e-04, 2.9941e-05, 4.1802e-03, 4.8634e-02, 7.5145e-01, 9.1414e-02,
        1.0354e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,128][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.9098, 0.0035, 0.0263, 0.0078, 0.0392, 0.0105, 0.0029],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,128][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0836, 0.1015, 0.1217, 0.1426, 0.2448, 0.1843, 0.1216],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,129][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0308, 0.0052, 0.0338, 0.2000, 0.3262, 0.2327, 0.1713],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,130][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([7.5797e-04, 1.2232e-04, 6.6017e-03, 1.1510e-01, 5.2986e-01, 3.0075e-01,
        4.6812e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,131][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0127, 0.0007, 0.0304, 0.0933, 0.6351, 0.2111, 0.0168],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,133][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0687, 0.0034, 0.0568, 0.0668, 0.2547, 0.3722, 0.1774],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,134][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0258, 0.0120, 0.0982, 0.1275, 0.4606, 0.1827, 0.0933],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,136][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0934, 0.0025, 0.0477, 0.0350, 0.4234, 0.2303, 0.1676],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,137][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.7078, 0.0021, 0.0193, 0.0054, 0.0024, 0.0149, 0.0032, 0.2449],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,138][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.3197, 0.0497, 0.0746, 0.0475, 0.1221, 0.1831, 0.1892, 0.0139],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,140][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0098, 0.1113, 0.1400, 0.2114, 0.1770, 0.1193, 0.1387, 0.0925],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,141][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([1.6774e-04, 1.9087e-05, 1.7300e-03, 2.4612e-02, 6.1421e-01, 2.5662e-02,
        3.1054e-01, 2.3063e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,142][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.8127, 0.0101, 0.0286, 0.0415, 0.0697, 0.0112, 0.0169, 0.0092],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,144][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0793, 0.0939, 0.0943, 0.1634, 0.2019, 0.1614, 0.1280, 0.0777],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,145][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0202, 0.0018, 0.0146, 0.1746, 0.4072, 0.1787, 0.1627, 0.0401],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,146][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([1.8459e-04, 1.6473e-05, 1.2066e-03, 2.6871e-02, 2.7606e-01, 8.2038e-02,
        5.7886e-01, 3.4768e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,147][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([7.8239e-03, 6.9478e-05, 1.7510e-02, 3.6889e-02, 4.8025e-01, 2.6553e-01,
        1.3202e-01, 5.9917e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,148][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0515, 0.0006, 0.0329, 0.0362, 0.3619, 0.2730, 0.2242, 0.0198],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,150][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0055, 0.0115, 0.0679, 0.0860, 0.3676, 0.1496, 0.0333, 0.2785],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,151][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([2.0000e-02, 3.1603e-04, 1.3584e-02, 9.5016e-03, 6.4446e-01, 9.8586e-02,
        1.8991e-01, 2.3637e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,152][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.7559, 0.0082, 0.0349, 0.0105, 0.0057, 0.0438, 0.0052, 0.0366, 0.0992],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,153][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2169, 0.0344, 0.0454, 0.0209, 0.0504, 0.0434, 0.4671, 0.0126, 0.1089],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,155][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0092, 0.0948, 0.1205, 0.1726, 0.1708, 0.1024, 0.1245, 0.0939, 0.1113],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,156][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([3.4802e-05, 5.5394e-06, 8.4375e-04, 1.8260e-02, 4.7426e-01, 4.2925e-02,
        1.6197e-01, 2.8841e-01, 1.3281e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,157][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.8794, 0.0060, 0.0276, 0.0125, 0.0319, 0.0145, 0.0096, 0.0070, 0.0115],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,159][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0694, 0.0936, 0.0861, 0.1461, 0.1342, 0.1324, 0.1052, 0.0774, 0.1556],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,160][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0107, 0.0016, 0.0110, 0.1451, 0.2308, 0.2196, 0.2563, 0.0911, 0.0338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,161][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([1.5824e-04, 1.7371e-05, 8.8608e-04, 4.3367e-02, 5.0261e-01, 1.3786e-01,
        2.1007e-01, 7.9674e-02, 2.5362e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,162][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([3.2806e-03, 1.0376e-04, 1.0946e-02, 6.8366e-02, 3.3096e-01, 1.9323e-01,
        1.0415e-01, 2.2449e-01, 6.4481e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,163][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0180, 0.0015, 0.0234, 0.0285, 0.4287, 0.1845, 0.1675, 0.0867, 0.0612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,165][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0020, 0.0129, 0.0325, 0.1155, 0.3507, 0.1737, 0.0333, 0.1427, 0.1367],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,166][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0198, 0.0005, 0.0120, 0.0353, 0.3153, 0.1777, 0.3397, 0.0725, 0.0271],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,167][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([9.6349e-01, 1.4150e-03, 5.7090e-03, 1.5728e-03, 3.9387e-04, 4.1140e-03,
        8.5928e-04, 8.7374e-03, 7.4652e-03, 6.2443e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,169][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.1405, 0.0247, 0.0890, 0.0081, 0.0627, 0.1629, 0.1181, 0.0708, 0.2981,
        0.0251], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,170][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0095, 0.0814, 0.1027, 0.1468, 0.1367, 0.0909, 0.1095, 0.0846, 0.1150,
        0.1229], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,171][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([2.2788e-04, 2.2286e-05, 3.2792e-03, 4.7907e-02, 3.9238e-01, 1.3137e-01,
        2.1880e-01, 1.4877e-01, 3.4436e-02, 2.2806e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,173][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.7008, 0.0035, 0.0365, 0.0334, 0.1766, 0.0158, 0.0090, 0.0113, 0.0123,
        0.0008], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,174][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0472, 0.0659, 0.0776, 0.1072, 0.1822, 0.1289, 0.0918, 0.0766, 0.1593,
        0.0635], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,175][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0105, 0.0020, 0.0174, 0.1321, 0.2319, 0.2529, 0.1373, 0.1378, 0.0517,
        0.0264], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,176][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([2.6859e-04, 3.7457e-05, 2.1426e-03, 8.3866e-02, 4.1319e-01, 1.6866e-01,
        1.4059e-01, 1.0525e-01, 7.5161e-02, 1.0829e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,177][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([3.4809e-03, 1.3244e-04, 8.9011e-03, 5.0521e-02, 4.3564e-01, 1.8787e-01,
        3.1985e-02, 1.9952e-01, 6.4207e-02, 1.7735e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,179][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0183, 0.0022, 0.0282, 0.0215, 0.1621, 0.3319, 0.0469, 0.2518, 0.0996,
        0.0374], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,180][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0049, 0.0116, 0.0461, 0.1073, 0.3149, 0.1151, 0.0562, 0.1694, 0.0841,
        0.0903], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,182][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0447, 0.0011, 0.0183, 0.0311, 0.1664, 0.2242, 0.2265, 0.2020, 0.0647,
        0.0209], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,183][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.7400, 0.0040, 0.0362, 0.0107, 0.0034, 0.0239, 0.0040, 0.0648, 0.0409,
        0.0096, 0.0624], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,183][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.3507, 0.0282, 0.0517, 0.0059, 0.0248, 0.0758, 0.1294, 0.0125, 0.2589,
        0.0201, 0.0420], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,183][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0050, 0.0662, 0.0919, 0.1337, 0.1324, 0.0784, 0.0894, 0.0731, 0.0997,
        0.1111, 0.1190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,184][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([3.3058e-04, 6.7255e-06, 6.9120e-04, 3.3313e-02, 2.1931e-01, 8.6358e-02,
        1.8661e-01, 8.1746e-02, 3.8370e-02, 1.4398e-01, 2.0928e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,184][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.7729, 0.0039, 0.0499, 0.0206, 0.0631, 0.0127, 0.0102, 0.0096, 0.0121,
        0.0028, 0.0421], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,184][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0615, 0.0602, 0.0721, 0.1091, 0.1275, 0.1172, 0.0823, 0.0579, 0.1599,
        0.0753, 0.0770], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,185][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0129, 0.0014, 0.0089, 0.1469, 0.1652, 0.1886, 0.1018, 0.0763, 0.0773,
        0.0778, 0.1428], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,185][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([2.7000e-04, 5.7211e-06, 4.9263e-04, 2.8974e-02, 2.0843e-01, 6.3887e-02,
        1.8891e-01, 6.4083e-02, 3.2185e-02, 1.6355e-01, 2.4921e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,186][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([4.5338e-03, 2.2369e-05, 5.1944e-03, 7.5574e-03, 2.7216e-01, 2.4498e-01,
        3.1280e-02, 5.6826e-02, 9.1217e-02, 4.3522e-02, 2.4270e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,186][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.1070, 0.0027, 0.0170, 0.0594, 0.1096, 0.2222, 0.1775, 0.0385, 0.0775,
        0.0397, 0.1489], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,188][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0019, 0.0087, 0.0372, 0.0894, 0.2807, 0.1201, 0.0260, 0.1169, 0.0842,
        0.0363, 0.1986], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,189][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([1.7138e-02, 1.6344e-04, 9.8603e-03, 7.0086e-03, 2.8639e-01, 1.3434e-01,
        1.9003e-01, 6.1314e-02, 3.5997e-02, 6.3926e-02, 1.9383e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,190][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.8135, 0.0034, 0.0103, 0.0450, 0.0017, 0.0081, 0.0033, 0.0160, 0.0062,
        0.0041, 0.0139, 0.0746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,192][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.1462, 0.0398, 0.0588, 0.0043, 0.0308, 0.1264, 0.1886, 0.0450, 0.2342,
        0.0617, 0.0584, 0.0059], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,193][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0056, 0.0560, 0.0827, 0.1054, 0.1139, 0.0705, 0.0817, 0.0581, 0.0871,
        0.0952, 0.1075, 0.1364], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,194][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([1.9411e-03, 2.7672e-05, 1.9073e-03, 1.9488e-02, 1.4039e-01, 5.2662e-02,
        2.2088e-02, 2.9526e-02, 1.6104e-02, 2.6215e-02, 3.1810e-01, 3.7156e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,195][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.5664, 0.0219, 0.1423, 0.0076, 0.1003, 0.0383, 0.0069, 0.0297, 0.0268,
        0.0056, 0.0493, 0.0048], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,197][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0446, 0.0436, 0.0462, 0.0526, 0.1119, 0.1159, 0.0744, 0.0670, 0.1958,
        0.0797, 0.0926, 0.0757], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,198][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0694, 0.0035, 0.0074, 0.0714, 0.0494, 0.0841, 0.0665, 0.0315, 0.0162,
        0.0345, 0.1439, 0.4222], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,199][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([1.1353e-03, 3.1806e-05, 1.2226e-03, 7.2392e-03, 2.2069e-01, 9.8361e-02,
        9.6129e-02, 2.5346e-02, 2.4373e-02, 8.2409e-02, 3.5083e-01, 9.2241e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,200][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([2.3098e-02, 9.3125e-05, 9.5199e-03, 1.3630e-02, 1.6953e-01, 7.9127e-02,
        4.4637e-02, 5.5486e-02, 3.2508e-02, 6.6491e-02, 4.2093e-01, 8.4948e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,202][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.1169, 0.0025, 0.0316, 0.0531, 0.0715, 0.1320, 0.0481, 0.0324, 0.0654,
        0.0173, 0.2671, 0.1621], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,203][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0068, 0.0174, 0.0346, 0.1083, 0.1569, 0.0587, 0.0145, 0.0451, 0.0566,
        0.0251, 0.1410, 0.3352], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,204][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.1249, 0.0006, 0.0107, 0.0146, 0.2124, 0.1148, 0.1471, 0.0476, 0.0437,
        0.0682, 0.1709, 0.0446], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,205][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([9.3491e-01, 8.8636e-04, 6.7820e-03, 8.8097e-04, 1.7271e-03, 5.4291e-03,
        1.2293e-03, 7.8725e-03, 1.0049e-02, 1.7436e-03, 1.1169e-02, 1.1877e-03,
        1.6130e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,207][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.2023, 0.0088, 0.0599, 0.0023, 0.0122, 0.0788, 0.1146, 0.0140, 0.4078,
        0.0148, 0.0634, 0.0042, 0.0167], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,208][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0038, 0.0505, 0.0662, 0.1069, 0.0991, 0.0578, 0.0721, 0.0504, 0.0739,
        0.0842, 0.0942, 0.1587, 0.0823], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,209][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([1.8367e-04, 3.7043e-06, 1.2892e-03, 1.2063e-02, 9.1925e-02, 1.2707e-02,
        6.4228e-02, 1.9800e-02, 9.9595e-03, 3.3555e-02, 5.4941e-01, 1.7625e-01,
        2.8625e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,211][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.6101, 0.0061, 0.0579, 0.0149, 0.1659, 0.0230, 0.0171, 0.0098, 0.0173,
        0.0104, 0.0427, 0.0079, 0.0170], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,212][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0387, 0.0424, 0.0567, 0.0711, 0.0842, 0.0747, 0.0755, 0.0504, 0.1052,
        0.0725, 0.0651, 0.1029, 0.1607], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,214][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0023, 0.0008, 0.0053, 0.0656, 0.0412, 0.0630, 0.0415, 0.0226, 0.0174,
        0.0270, 0.2347, 0.4252, 0.0536], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,215][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([7.2303e-05, 3.2394e-06, 5.0004e-04, 2.3294e-02, 3.7975e-02, 3.2396e-02,
        4.7409e-02, 3.1678e-02, 8.4704e-03, 3.8570e-02, 2.9339e-01, 4.5689e-01,
        2.9358e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,215][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([1.8804e-03, 1.8623e-05, 6.3710e-03, 2.7723e-02, 9.3391e-02, 7.5594e-02,
        3.4700e-02, 2.0103e-02, 1.9921e-02, 2.4104e-02, 5.1856e-01, 1.5544e-01,
        2.2190e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,217][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0252, 0.0004, 0.0201, 0.0255, 0.1680, 0.0873, 0.0414, 0.0284, 0.0438,
        0.0126, 0.3831, 0.0909, 0.0732], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,219][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0028, 0.0041, 0.0216, 0.0464, 0.1757, 0.0578, 0.0202, 0.0392, 0.0350,
        0.0196, 0.1026, 0.1362, 0.3387], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,220][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([1.7582e-02, 7.0692e-05, 9.0338e-03, 1.0397e-02, 1.0734e-01, 9.1426e-02,
        2.6107e-01, 4.2267e-02, 1.9845e-02, 4.9862e-02, 2.4700e-01, 3.1041e-02,
        1.1307e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,221][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.6533, 0.0067, 0.0241, 0.0133, 0.0068, 0.0262, 0.0053, 0.0428, 0.0291,
        0.0142, 0.0320, 0.0169, 0.0044, 0.1250], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,222][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.1301, 0.0301, 0.0266, 0.0114, 0.0277, 0.0443, 0.2233, 0.0114, 0.1145,
        0.0140, 0.0425, 0.0173, 0.2962, 0.0106], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,224][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0046, 0.0492, 0.0655, 0.0966, 0.0967, 0.0572, 0.0692, 0.0498, 0.0754,
        0.0850, 0.0842, 0.1244, 0.0780, 0.0642], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,225][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.5493e-04, 8.8490e-08, 4.3246e-05, 3.7948e-04, 9.9203e-02, 4.2408e-03,
        1.6753e-02, 1.8565e-03, 1.0444e-03, 8.7660e-03, 2.6526e-02, 1.1405e-02,
        3.2100e-01, 5.0863e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,226][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.6743, 0.0039, 0.0453, 0.0159, 0.1264, 0.0146, 0.0114, 0.0109, 0.0114,
        0.0025, 0.0252, 0.0078, 0.0368, 0.0138], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,228][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0426, 0.0476, 0.0479, 0.0792, 0.0843, 0.0832, 0.0674, 0.0391, 0.1052,
        0.0482, 0.0631, 0.1004, 0.1570, 0.0348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,229][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([1.0823e-02, 1.7999e-04, 2.2217e-03, 2.6311e-02, 4.2122e-02, 3.4899e-02,
        4.4759e-02, 1.2560e-02, 8.9674e-03, 2.3434e-02, 1.0644e-01, 1.6057e-01,
        9.5046e-02, 4.3167e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,230][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([2.4743e-04, 4.8487e-07, 6.4301e-05, 2.2990e-03, 1.6790e-02, 7.6800e-03,
        4.6666e-02, 4.9361e-03, 2.2618e-03, 3.0197e-02, 8.7592e-02, 4.7137e-02,
        9.9063e-02, 6.5507e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,231][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([4.4752e-03, 2.9275e-06, 1.0927e-03, 1.9902e-03, 6.1776e-02, 1.1056e-02,
        1.0923e-02, 4.8248e-03, 5.9093e-03, 4.7946e-03, 1.2741e-01, 1.2543e-02,
        7.8251e-02, 6.7495e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,232][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([3.1644e-02, 2.4212e-04, 7.6283e-03, 4.6657e-03, 1.0109e-01, 5.7059e-02,
        3.7138e-02, 4.0804e-03, 1.8414e-02, 9.0799e-03, 1.5997e-01, 2.4396e-02,
        4.4878e-01, 9.5812e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,233][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0015, 0.0030, 0.0089, 0.0334, 0.1037, 0.0307, 0.0092, 0.0392, 0.0181,
        0.0126, 0.0613, 0.1034, 0.2178, 0.3572], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,234][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.7326e-02, 2.9669e-05, 3.0048e-03, 1.9545e-03, 9.6863e-02, 2.5650e-02,
        7.2546e-02, 1.9915e-02, 9.4293e-03, 6.8384e-03, 1.0098e-01, 8.5052e-03,
        4.1314e-01, 2.2382e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,236][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.7118, 0.0015, 0.0097, 0.0011, 0.0032, 0.0134, 0.0022, 0.0161, 0.0089,
        0.0028, 0.0226, 0.0016, 0.0008, 0.0655, 0.1389], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,237][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0899, 0.0128, 0.0418, 0.0047, 0.0232, 0.0755, 0.0874, 0.0231, 0.2395,
        0.0112, 0.0718, 0.0066, 0.2534, 0.0298, 0.0294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,239][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0046, 0.0458, 0.0614, 0.0950, 0.0832, 0.0542, 0.0638, 0.0499, 0.0675,
        0.0724, 0.0744, 0.1225, 0.0694, 0.0756, 0.0602], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,240][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([7.7053e-05, 1.7072e-07, 8.1372e-05, 6.8546e-04, 4.3334e-03, 9.6282e-04,
        6.7682e-03, 1.2746e-03, 4.0329e-04, 1.4241e-03, 2.8136e-02, 9.6931e-03,
        1.7925e-02, 7.2962e-01, 1.9862e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,241][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.6183, 0.0039, 0.0446, 0.0106, 0.1184, 0.0153, 0.0252, 0.0120, 0.0145,
        0.0065, 0.0275, 0.0057, 0.0657, 0.0160, 0.0157], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,241][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0289, 0.0416, 0.0492, 0.0661, 0.0893, 0.0682, 0.0692, 0.0386, 0.0806,
        0.0476, 0.0540, 0.0859, 0.1619, 0.0392, 0.0797], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,241][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([3.7740e-03, 7.1892e-05, 1.1387e-03, 1.4824e-02, 1.1252e-02, 1.4250e-02,
        1.1901e-02, 7.8236e-03, 3.3840e-03, 1.0444e-02, 4.9616e-02, 8.7588e-02,
        4.8820e-02, 6.4776e-01, 8.7358e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,242][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([1.9642e-04, 6.0841e-07, 1.1013e-04, 2.4720e-03, 4.5765e-03, 8.1826e-03,
        2.4239e-02, 3.4869e-03, 1.3228e-03, 1.0099e-02, 8.2940e-02, 3.2950e-02,
        2.2669e-02, 5.5270e-01, 2.5405e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,242][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([1.1456e-03, 2.1678e-06, 6.6471e-04, 1.8882e-03, 2.1052e-02, 5.3247e-03,
        6.3324e-03, 4.0319e-03, 2.0731e-03, 5.8692e-03, 5.7731e-02, 9.8913e-03,
        9.6455e-02, 7.1312e-01, 7.4418e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,243][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([9.1065e-03, 1.5076e-04, 3.9192e-03, 4.2660e-03, 2.3369e-02, 4.0825e-02,
        2.0891e-02, 8.3184e-03, 9.5602e-03, 4.7272e-03, 8.6583e-02, 1.6491e-02,
        3.6775e-01, 3.4430e-01, 5.9740e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,243][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0029, 0.0031, 0.0113, 0.0297, 0.1113, 0.0427, 0.0129, 0.0278, 0.0196,
        0.0120, 0.0438, 0.0876, 0.1700, 0.1273, 0.2979], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,244][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([1.5613e-02, 3.4739e-05, 2.8835e-03, 3.1592e-03, 4.3316e-02, 2.3099e-02,
        4.2736e-02, 1.9069e-02, 5.9561e-03, 9.5042e-03, 8.6496e-02, 1.0725e-02,
        2.3619e-01, 3.7237e-01, 1.2884e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,245][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.7609, 0.0028, 0.0140, 0.0026, 0.0020, 0.0172, 0.0013, 0.0077, 0.0358,
        0.0046, 0.0158, 0.0034, 0.0015, 0.0219, 0.0032, 0.1055],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,246][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0510, 0.0070, 0.0266, 0.0036, 0.0124, 0.0168, 0.0568, 0.0057, 0.1116,
        0.0078, 0.0449, 0.0054, 0.0439, 0.0082, 0.0235, 0.5749],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,248][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0039, 0.0390, 0.0560, 0.0818, 0.0828, 0.0479, 0.0624, 0.0470, 0.0598,
        0.0698, 0.0675, 0.1005, 0.0671, 0.0663, 0.0633, 0.0850],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,249][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([2.2001e-05, 6.5933e-09, 6.0902e-06, 2.8689e-05, 5.6306e-03, 2.3905e-04,
        1.1979e-03, 6.7228e-04, 6.4272e-05, 5.7973e-04, 2.1727e-03, 7.1589e-04,
        1.8545e-02, 1.7398e-01, 6.6302e-01, 1.3313e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,250][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.8523, 0.0031, 0.0221, 0.0051, 0.0420, 0.0080, 0.0030, 0.0032, 0.0064,
        0.0048, 0.0162, 0.0026, 0.0110, 0.0044, 0.0067, 0.0089],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,251][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0336, 0.0389, 0.0469, 0.0592, 0.0615, 0.0792, 0.0428, 0.0379, 0.1067,
        0.0477, 0.0572, 0.0770, 0.1313, 0.0341, 0.0621, 0.0840],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,252][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([2.0029e-02, 1.0899e-04, 1.1454e-03, 1.3390e-02, 1.1482e-02, 1.4968e-02,
        3.3676e-02, 8.7232e-03, 2.4363e-03, 1.5620e-02, 3.5310e-02, 6.8697e-02,
        3.7870e-02, 3.9045e-01, 1.8030e-01, 1.6580e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,253][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([1.7406e-04, 1.8870e-07, 1.1178e-05, 6.8090e-04, 4.1463e-03, 1.3238e-03,
        6.6556e-03, 9.6002e-04, 2.9043e-04, 2.2162e-03, 7.8086e-03, 1.2950e-02,
        1.5239e-02, 1.4403e-01, 4.3728e-01, 3.6624e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,254][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([3.1799e-03, 1.8198e-06, 2.3470e-04, 1.5360e-03, 9.5842e-03, 1.9994e-03,
        2.4550e-03, 3.0537e-03, 4.3020e-04, 1.4691e-03, 1.6266e-02, 1.0172e-02,
        1.4746e-02, 4.1024e-01, 3.8366e-01, 1.4097e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,255][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([2.4608e-02, 1.4711e-04, 2.0034e-03, 6.7743e-03, 2.8602e-02, 1.2857e-02,
        2.4303e-02, 4.3742e-03, 5.3204e-03, 1.1649e-02, 2.5568e-02, 2.2020e-02,
        7.8606e-02, 8.5635e-02, 5.1446e-01, 1.5307e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,257][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0032, 0.0028, 0.0073, 0.0254, 0.0618, 0.0281, 0.0082, 0.0249, 0.0207,
        0.0128, 0.0407, 0.0697, 0.1377, 0.1430, 0.2230, 0.1906],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,258][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([2.8162e-02, 2.1568e-05, 1.7328e-03, 2.8756e-03, 2.9336e-02, 1.5480e-02,
        3.5736e-02, 7.4492e-03, 2.5137e-03, 3.8344e-03, 3.9121e-02, 9.9445e-03,
        7.3169e-02, 2.5202e-01, 3.1847e-01, 1.8014e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,259][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.6117, 0.0048, 0.0195, 0.0089, 0.0055, 0.0229, 0.0043, 0.0335, 0.0245,
        0.0103, 0.0267, 0.0111, 0.0036, 0.1000, 0.0166, 0.0120, 0.0839],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,261][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0315, 0.0071, 0.0067, 0.0026, 0.0067, 0.0094, 0.0531, 0.0026, 0.0268,
        0.0034, 0.0103, 0.0039, 0.0685, 0.0024, 0.0256, 0.7373, 0.0021],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,262][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0060, 0.0401, 0.0545, 0.0734, 0.0767, 0.0473, 0.0550, 0.0424, 0.0612,
        0.0672, 0.0655, 0.0842, 0.0590, 0.0496, 0.0576, 0.0846, 0.0758],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,263][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.4368e-05, 1.4838e-09, 7.5641e-07, 8.2188e-06, 2.3427e-03, 8.9002e-05,
        3.7682e-04, 3.3925e-05, 2.0113e-05, 1.6989e-04, 4.2763e-04, 2.3144e-04,
        5.6039e-03, 1.1369e-02, 2.2460e-01, 5.6298e-01, 1.9174e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,265][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.6724, 0.0030, 0.0374, 0.0135, 0.1013, 0.0123, 0.0113, 0.0101, 0.0098,
        0.0023, 0.0217, 0.0067, 0.0313, 0.0117, 0.0238, 0.0214, 0.0100],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,266][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0368, 0.0397, 0.0392, 0.0636, 0.0682, 0.0665, 0.0570, 0.0322, 0.0846,
        0.0392, 0.0500, 0.0805, 0.1284, 0.0283, 0.0722, 0.0863, 0.0272],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,267][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([4.9923e-03, 2.3140e-05, 3.0231e-04, 4.1521e-03, 7.6117e-03, 5.0088e-03,
        8.5721e-03, 1.8611e-03, 1.0470e-03, 3.6181e-03, 1.3851e-02, 2.2588e-02,
        1.3263e-02, 6.2323e-02, 6.5178e-02, 4.2874e-01, 3.5687e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,268][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([1.8232e-05, 6.4242e-09, 1.0025e-06, 3.8117e-05, 3.1051e-04, 1.2465e-04,
        9.1909e-04, 7.2147e-05, 3.1938e-05, 4.5299e-04, 1.2475e-03, 7.1821e-04,
        1.7511e-03, 1.1111e-02, 1.4477e-01, 6.5260e-01, 1.8583e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,269][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.5062e-03, 2.1428e-07, 9.0527e-05, 2.1272e-04, 5.7385e-03, 8.7678e-04,
        9.9300e-04, 4.1164e-04, 4.2003e-04, 4.2923e-04, 9.2856e-03, 1.1613e-03,
        5.3312e-03, 5.9011e-02, 1.1063e-01, 3.4500e-01, 4.5890e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,270][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.0731e-03, 3.4517e-05, 1.0849e-03, 7.3366e-04, 1.2875e-02, 7.9145e-03,
        6.3536e-03, 5.4828e-04, 2.3684e-03, 1.4905e-03, 2.1380e-02, 3.5501e-03,
        5.4533e-02, 1.1579e-02, 4.7241e-01, 3.5683e-01, 3.7243e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,271][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0012, 0.0017, 0.0043, 0.0175, 0.0466, 0.0154, 0.0045, 0.0189, 0.0088,
        0.0064, 0.0287, 0.0512, 0.0924, 0.1582, 0.2314, 0.1082, 0.2045],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,272][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.1767e-02, 8.6893e-06, 8.4052e-04, 7.7625e-04, 2.6854e-02, 7.8986e-03,
        2.3595e-02, 4.9020e-03, 2.4865e-03, 1.7871e-03, 2.6813e-02, 3.2660e-03,
        1.1074e-01, 5.7646e-02, 3.2760e-01, 1.8507e-01, 2.0796e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,273][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:30,275][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 8405],
        [  769],
        [ 3660],
        [ 4845],
        [24881],
        [12955],
        [20987],
        [ 7893],
        [ 5249],
        [ 2871],
        [ 3111],
        [ 3011],
        [ 5032],
        [ 4619],
        [ 3145],
        [12689],
        [ 4363]], device='cuda:0')
[2024-07-24 10:21:30,277][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8406],
        [  216],
        [ 7162],
        [ 6270],
        [29702],
        [22143],
        [29488],
        [14980],
        [12581],
        [ 8629],
        [ 7395],
        [ 5368],
        [ 7323],
        [12534],
        [ 3542],
        [21056],
        [12888]], device='cuda:0')
[2024-07-24 10:21:30,278][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[36462],
        [35526],
        [33841],
        [33161],
        [32796],
        [31962],
        [31978],
        [31761],
        [31431],
        [31083],
        [31163],
        [30862],
        [31042],
        [31093],
        [31243],
        [31164],
        [31257]], device='cuda:0')
[2024-07-24 10:21:30,279][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[42151],
        [42414],
        [34894],
        [35264],
        [33949],
        [29842],
        [27250],
        [25953],
        [24357],
        [23563],
        [25018],
        [24151],
        [26307],
        [24624],
        [24683],
        [23130],
        [21824]], device='cuda:0')
[2024-07-24 10:21:30,281][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[19717],
        [27527],
        [24465],
        [30042],
        [33107],
        [24008],
        [19729],
        [18530],
        [17248],
        [19566],
        [19961],
        [20614],
        [33530],
        [26331],
        [22309],
        [28367],
        [30006]], device='cuda:0')
[2024-07-24 10:21:30,282][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[30721],
        [40369],
        [39549],
        [36017],
        [32305],
        [29540],
        [31556],
        [34814],
        [32926],
        [33912],
        [33659],
        [33448],
        [33215],
        [32900],
        [33284],
        [33502],
        [34804]], device='cuda:0')
[2024-07-24 10:21:30,284][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[11205],
        [ 9172],
        [ 9183],
        [ 9036],
        [ 8547],
        [ 7757],
        [ 7326],
        [ 7747],
        [ 7806],
        [ 8089],
        [ 8421],
        [ 8592],
        [ 8747],
        [ 8840],
        [ 8653],
        [ 8491],
        [ 8611]], device='cuda:0')
[2024-07-24 10:21:30,285][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[12227],
        [18027],
        [18756],
        [20427],
        [19880],
        [19800],
        [18684],
        [19026],
        [19747],
        [19496],
        [19331],
        [20146],
        [19604],
        [19712],
        [19752],
        [19583],
        [19678]], device='cuda:0')
[2024-07-24 10:21:30,287][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[30611],
        [32765],
        [32153],
        [32676],
        [32767],
        [32476],
        [33055],
        [32610],
        [32286],
        [32365],
        [31992],
        [31838],
        [31864],
        [31627],
        [31679],
        [31731],
        [31541]], device='cuda:0')
[2024-07-24 10:21:30,288][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[19988],
        [32617],
        [29833],
        [30098],
        [30366],
        [30324],
        [30797],
        [31644],
        [30930],
        [28393],
        [31087],
        [31142],
        [31249],
        [31756],
        [31344],
        [31521],
        [32088]], device='cuda:0')
[2024-07-24 10:21:30,290][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[5882],
        [8113],
        [4422],
        [5109],
        [3139],
        [2871],
        [2814],
        [2406],
        [1686],
        [1413],
        [1364],
        [1110],
        [1079],
        [1005],
        [1023],
        [1049],
        [1014]], device='cuda:0')
[2024-07-24 10:21:30,291][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[   50],
        [  613],
        [  772],
        [  222],
        [ 7141],
        [10560],
        [ 6806],
        [12419],
        [13122],
        [ 4970],
        [ 4120],
        [ 1351],
        [ 2346],
        [ 5224],
        [ 4778],
        [ 4059],
        [ 5300]], device='cuda:0')
[2024-07-24 10:21:30,292][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[19721],
        [19842],
        [21458],
        [20541],
        [20878],
        [21877],
        [22173],
        [22606],
        [22606],
        [22326],
        [22500],
        [22372],
        [22678],
        [22700],
        [22819],
        [22711],
        [22655]], device='cuda:0')
[2024-07-24 10:21:30,294][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[34053],
        [ 6239],
        [ 6464],
        [12153],
        [ 9858],
        [ 7854],
        [ 6957],
        [ 7165],
        [ 7333],
        [ 6386],
        [ 4470],
        [ 6133],
        [ 5824],
        [ 5351],
        [ 5165],
        [ 5533],
        [ 5095]], device='cuda:0')
[2024-07-24 10:21:30,295][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 5163],
        [31477],
        [11150],
        [17296],
        [19264],
        [11409],
        [ 4316],
        [22708],
        [ 7130],
        [ 2128],
        [12786],
        [16500],
        [21156],
        [11103],
        [28966],
        [18195],
        [ 6140]], device='cuda:0')
[2024-07-24 10:21:30,297][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[29534],
        [29474],
        [29620],
        [29422],
        [29507],
        [29028],
        [28903],
        [29074],
        [29254],
        [29496],
        [29589],
        [29246],
        [29332],
        [29283],
        [28051],
        [29260],
        [28968]], device='cuda:0')
[2024-07-24 10:21:30,298][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[16145],
        [19752],
        [20908],
        [25204],
        [24067],
        [27308],
        [28545],
        [27672],
        [29564],
        [22775],
        [25813],
        [26327],
        [22901],
        [28249],
        [25160],
        [29236],
        [31575]], device='cuda:0')
[2024-07-24 10:21:30,300][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[34088],
        [ 7981],
        [ 8547],
        [ 9084],
        [ 8549],
        [ 8983],
        [ 9795],
        [ 9894],
        [ 9856],
        [10511],
        [ 9665],
        [ 9427],
        [ 8712],
        [ 8695],
        [ 8352],
        [ 7680],
        [ 7705]], device='cuda:0')
[2024-07-24 10:21:30,301][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[15142],
        [18931],
        [14533],
        [15384],
        [29073],
        [30525],
        [28107],
        [26308],
        [24488],
        [18620],
        [10299],
        [12896],
        [10400],
        [32672],
        [18052],
        [18450],
        [ 8944]], device='cuda:0')
[2024-07-24 10:21:30,302][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[29680],
        [29656],
        [29657],
        [29601],
        [28765],
        [28862],
        [29172],
        [26194],
        [28333],
        [26261],
        [29411],
        [29713],
        [27731],
        [27316],
        [25546],
        [29274],
        [27401]], device='cuda:0')
[2024-07-24 10:21:30,303][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[20393],
        [28169],
        [29312],
        [29523],
        [32400],
        [33086],
        [33649],
        [32923],
        [33186],
        [33095],
        [32321],
        [31853],
        [32927],
        [33048],
        [33087],
        [32982],
        [32975]], device='cuda:0')
[2024-07-24 10:21:30,304][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 3611],
        [ 5092],
        [ 3909],
        [ 1786],
        [ 7038],
        [17261],
        [19363],
        [19670],
        [17947],
        [14729],
        [11812],
        [ 3060],
        [ 3499],
        [ 4846],
        [ 5469],
        [ 9930],
        [ 6270]], device='cuda:0')
[2024-07-24 10:21:30,305][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[11943],
        [ 5125],
        [18653],
        [ 8454],
        [ 3932],
        [ 3450],
        [ 3118],
        [   55],
        [  887],
        [ 1611],
        [  500],
        [ 2363],
        [ 5046],
        [ 8467],
        [ 4974],
        [ 1643],
        [ 7910]], device='cuda:0')
[2024-07-24 10:21:30,307][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[27253],
        [18838],
        [14357],
        [16972],
        [12772],
        [ 6886],
        [ 6683],
        [ 3638],
        [ 3218],
        [ 4181],
        [ 6658],
        [13623],
        [19479],
        [ 7780],
        [ 5460],
        [ 1982],
        [ 3702]], device='cuda:0')
[2024-07-24 10:21:30,308][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 3767],
        [14681],
        [ 2375],
        [ 3571],
        [ 8097],
        [ 7886],
        [ 5042],
        [ 6872],
        [ 7696],
        [ 6470],
        [ 5615],
        [ 5536],
        [ 3301],
        [ 6053],
        [ 5221],
        [ 7105],
        [ 9531]], device='cuda:0')
[2024-07-24 10:21:30,310][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[20243],
        [23530],
        [ 9444],
        [12349],
        [ 9071],
        [ 8964],
        [ 8489],
        [ 8571],
        [ 8808],
        [ 8767],
        [ 8665],
        [ 9228],
        [ 9056],
        [ 9015],
        [ 9386],
        [ 9324],
        [ 9619]], device='cuda:0')
[2024-07-24 10:21:30,311][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 6145],
        [ 7614],
        [ 7200],
        [ 6432],
        [12760],
        [10187],
        [ 7020],
        [ 9139],
        [ 5208],
        [ 4483],
        [ 4948],
        [ 4920],
        [ 3336],
        [ 4427],
        [ 5805],
        [ 7181],
        [ 7280]], device='cuda:0')
[2024-07-24 10:21:30,312][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[26653],
        [24369],
        [27217],
        [27826],
        [22493],
        [22783],
        [24794],
        [29121],
        [28799],
        [31107],
        [31617],
        [30971],
        [30729],
        [25610],
        [30345],
        [31429],
        [29489]], device='cuda:0')
[2024-07-24 10:21:30,314][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[17969],
        [ 8579],
        [19694],
        [15013],
        [17269],
        [20421],
        [28498],
        [21372],
        [24404],
        [24778],
        [23294],
        [24577],
        [22021],
        [20202],
        [23992],
        [17048],
        [16993]], device='cuda:0')
[2024-07-24 10:21:30,315][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366],
        [42366]], device='cuda:0')
[2024-07-24 10:21:30,358][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:30,359][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,359][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,360][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,360][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,360][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,361][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,361][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,361][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,362][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,362][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,363][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,364][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,366][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.8765, 0.1235], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,367][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.1971, 0.8029], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,368][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.9100, 0.0900], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,370][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.0948, 0.9052], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,371][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([9.9937e-01, 6.3286e-04], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,372][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.1994, 0.8006], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,373][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.9548, 0.0452], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,375][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.4305, 0.5695], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,376][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.4592, 0.5408], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,377][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.5785, 0.4215], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,379][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.2154, 0.7846], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,380][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.9872, 0.0128], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,381][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.6243, 0.0976, 0.2782], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,383][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0842, 0.4368, 0.4790], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,384][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.7969, 0.0594, 0.1438], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,386][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0073, 0.3991, 0.5936], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,387][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9955, 0.0013, 0.0032], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,389][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1834, 0.4750, 0.3416], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,390][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9053, 0.0225, 0.0722], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,391][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.2686, 0.3554, 0.3760], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,393][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.2611, 0.3684, 0.3705], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,394][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.5758, 0.1732, 0.2510], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,395][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1217, 0.4295, 0.4488], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,397][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0141, 0.0315, 0.9544], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,398][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.4818, 0.1017, 0.2388, 0.1778], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,399][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0629, 0.2946, 0.3271, 0.3153], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,401][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.7301, 0.0479, 0.1325, 0.0895], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,402][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0816, 0.2001, 0.0845, 0.6339], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,403][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([9.9508e-01, 9.7250e-04, 2.6240e-03, 1.3226e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,404][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.1054, 0.2341, 0.2088, 0.4516], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,406][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.8425, 0.0168, 0.0769, 0.0638], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,407][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.1985, 0.2583, 0.2699, 0.2733], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,409][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.1729, 0.2729, 0.2464, 0.3078], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,410][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.4140, 0.1560, 0.2229, 0.2072], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,411][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0866, 0.3016, 0.3279, 0.2839], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,412][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0219, 0.1199, 0.8079, 0.0502], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,414][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.5469, 0.0325, 0.1200, 0.0589, 0.2417], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,415][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0492, 0.2302, 0.2441, 0.2498, 0.2266], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,417][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.6193, 0.0387, 0.1006, 0.0779, 0.1636], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,417][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ got] are: tensor([4.4741e-04, 5.2600e-02, 2.2661e-01, 6.4089e-01, 7.9460e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,417][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.9930, 0.0015, 0.0025, 0.0016, 0.0014], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,418][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0949, 0.1121, 0.1050, 0.2163, 0.4717], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,418][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.7978, 0.0150, 0.0636, 0.0540, 0.0695], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,418][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.1532, 0.2031, 0.2151, 0.2159, 0.2126], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,419][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0667, 0.2553, 0.2171, 0.2494, 0.2115], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,419][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.3917, 0.1082, 0.1635, 0.1558, 0.1808], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,419][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0578, 0.2186, 0.2392, 0.1907, 0.2937], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,420][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ got] are: tensor([5.4668e-05, 3.3305e-04, 9.3315e-02, 8.9143e-01, 1.4871e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,420][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3172, 0.0267, 0.0883, 0.0507, 0.2138, 0.3033], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,421][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0410, 0.1868, 0.1974, 0.2017, 0.1840, 0.1891], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,422][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.5014, 0.0373, 0.0919, 0.0756, 0.1405, 0.1533], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,423][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0009, 0.0541, 0.0954, 0.6135, 0.1620, 0.0741], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,424][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([9.9620e-01, 7.0273e-04, 1.5421e-03, 8.1686e-04, 4.9077e-04, 2.4587e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,425][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0811, 0.0621, 0.0440, 0.1200, 0.3173, 0.3756], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,427][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.7324, 0.0098, 0.0424, 0.0351, 0.0544, 0.1260], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,428][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1281, 0.1671, 0.1759, 0.1763, 0.1731, 0.1794], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,429][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0638, 0.2084, 0.1693, 0.1985, 0.1703, 0.1897], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,431][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.2123, 0.0975, 0.1581, 0.1567, 0.1744, 0.2010], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,432][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0520, 0.1782, 0.1840, 0.1608, 0.2320, 0.1930], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,433][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([1.9424e-05, 8.9626e-06, 1.1301e-03, 4.4603e-03, 9.9398e-01, 3.9856e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,434][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.1897, 0.0263, 0.0905, 0.0494, 0.2176, 0.3854, 0.0411],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,436][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0347, 0.1535, 0.1645, 0.1674, 0.1555, 0.1595, 0.1649],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,437][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.3038, 0.0579, 0.1165, 0.0900, 0.1715, 0.1636, 0.0966],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,439][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0032, 0.0413, 0.0786, 0.4145, 0.1681, 0.0497, 0.2446],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,439][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([9.8947e-01, 1.1030e-03, 3.7503e-03, 1.6035e-03, 1.3334e-03, 7.4728e-04,
        1.9944e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,441][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0288, 0.0271, 0.0366, 0.0609, 0.3617, 0.2777, 0.2073],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,442][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.6242, 0.0076, 0.0361, 0.0251, 0.0341, 0.0861, 0.1868],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,444][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1076, 0.1418, 0.1499, 0.1506, 0.1478, 0.1535, 0.1488],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,445][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0708, 0.1770, 0.1532, 0.1761, 0.1463, 0.1562, 0.1204],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,447][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.1774, 0.0655, 0.1221, 0.1140, 0.1479, 0.1863, 0.1868],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,448][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0380, 0.1391, 0.1458, 0.1296, 0.1878, 0.1614, 0.1983],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,449][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([1.2261e-07, 8.3875e-06, 3.2384e-03, 6.7758e-03, 9.2594e-01, 6.3952e-02,
        8.3547e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,450][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.2326, 0.0172, 0.0666, 0.0360, 0.1726, 0.2773, 0.0394, 0.1583],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,452][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0331, 0.1340, 0.1415, 0.1454, 0.1317, 0.1354, 0.1414, 0.1375],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,453][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.5110, 0.0265, 0.0602, 0.0528, 0.0915, 0.0844, 0.0536, 0.1200],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,454][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ at] are: tensor([3.8949e-05, 2.4062e-02, 8.7348e-02, 3.9047e-01, 1.0783e-01, 1.2857e-01,
        2.5242e-01, 9.2671e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,455][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ at] are: tensor([9.9352e-01, 7.8504e-04, 2.1717e-03, 1.2732e-03, 6.6135e-04, 3.8046e-04,
        5.5574e-04, 6.5118e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,456][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0352, 0.0213, 0.0228, 0.0542, 0.1808, 0.2026, 0.2406, 0.2424],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,458][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.6102, 0.0050, 0.0214, 0.0155, 0.0228, 0.0519, 0.1253, 0.1480],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,459][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0940, 0.1219, 0.1296, 0.1296, 0.1282, 0.1326, 0.1290, 0.1351],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,460][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0387, 0.1661, 0.1407, 0.1546, 0.1294, 0.1486, 0.1155, 0.1063],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,462][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.1536, 0.0628, 0.1079, 0.1027, 0.1223, 0.1410, 0.1543, 0.1555],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,463][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0330, 0.1227, 0.1268, 0.1151, 0.1628, 0.1294, 0.1790, 0.1313],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,464][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ at] are: tensor([3.0095e-06, 6.3242e-07, 1.2646e-03, 6.7372e-02, 9.1069e-01, 4.1816e-04,
        2.0128e-02, 1.1975e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,466][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1085, 0.0135, 0.0543, 0.0272, 0.1424, 0.2452, 0.0360, 0.1891, 0.1837],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,467][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0255, 0.1165, 0.1229, 0.1262, 0.1166, 0.1189, 0.1268, 0.1226, 0.1240],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,469][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.4167, 0.0260, 0.0627, 0.0482, 0.0852, 0.0870, 0.0502, 0.1189, 0.1051],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,470][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([2.2422e-05, 1.4153e-02, 3.0872e-02, 1.2674e-01, 1.0513e-01, 6.2418e-02,
        6.4073e-01, 1.8322e-02, 1.6136e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,470][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([9.9582e-01, 6.2879e-04, 1.6450e-03, 5.2181e-04, 3.4013e-04, 1.9422e-04,
        2.8442e-04, 3.4072e-04, 2.2306e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,472][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0214, 0.0193, 0.0183, 0.0458, 0.1165, 0.1536, 0.1906, 0.2888, 0.1459],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,473][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4897, 0.0037, 0.0196, 0.0134, 0.0222, 0.0488, 0.1323, 0.1505, 0.1199],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,474][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0828, 0.1076, 0.1139, 0.1143, 0.1131, 0.1170, 0.1137, 0.1187, 0.1190],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,475][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0393, 0.1433, 0.1198, 0.1371, 0.1165, 0.1312, 0.1014, 0.0944, 0.1170],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,475][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1345, 0.0537, 0.0892, 0.0894, 0.1025, 0.1137, 0.1305, 0.1463, 0.1402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,475][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0285, 0.1067, 0.1099, 0.0977, 0.1417, 0.1132, 0.1489, 0.1131, 0.1404],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,476][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([2.1219e-04, 5.9284e-06, 1.2392e-03, 1.5183e-02, 9.7086e-01, 9.4395e-04,
        3.7907e-03, 4.4997e-03, 3.2676e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,476][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0758, 0.0187, 0.0554, 0.0311, 0.1273, 0.2300, 0.0331, 0.1779, 0.2196,
        0.0310], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,476][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0243, 0.1005, 0.1054, 0.1103, 0.1035, 0.1062, 0.1116, 0.1101, 0.1114,
        0.1166], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,477][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.2259, 0.0240, 0.0665, 0.0496, 0.0945, 0.1101, 0.0556, 0.1618, 0.1373,
        0.0749], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,477][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ station] are: tensor([3.9233e-04, 4.5624e-02, 3.5750e-02, 5.4319e-01, 1.3045e-01, 4.5586e-02,
        1.3138e-01, 8.7518e-03, 2.2330e-03, 5.6643e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,477][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ station] are: tensor([9.9513e-01, 5.2052e-04, 1.6399e-03, 5.0067e-04, 2.0498e-04, 2.1012e-04,
        3.6650e-04, 5.6136e-04, 2.7134e-04, 5.8979e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,478][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0072, 0.0164, 0.0217, 0.0336, 0.1148, 0.1540, 0.1582, 0.2799, 0.1559,
        0.0583], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,479][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.4607, 0.0035, 0.0175, 0.0107, 0.0164, 0.0390, 0.0884, 0.1155, 0.0933,
        0.1550], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,481][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0739, 0.0962, 0.1018, 0.1021, 0.1009, 0.1045, 0.1017, 0.1061, 0.1061,
        0.1066], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,482][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0481, 0.1282, 0.1029, 0.1235, 0.1028, 0.1194, 0.0862, 0.0872, 0.1087,
        0.0930], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,484][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.1450, 0.0455, 0.0702, 0.0748, 0.0825, 0.1034, 0.1084, 0.1233, 0.1218,
        0.1251], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,484][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0237, 0.0901, 0.0920, 0.0811, 0.1187, 0.1026, 0.1227, 0.1092, 0.1288,
        0.1311], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,485][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ station] are: tensor([9.4052e-06, 2.9202e-05, 2.0817e-03, 9.2964e-02, 8.2207e-01, 3.0352e-02,
        1.3901e-02, 2.2759e-03, 2.5355e-02, 1.0959e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,487][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.1187, 0.0103, 0.0406, 0.0195, 0.0998, 0.1776, 0.0217, 0.1266, 0.1327,
        0.0162, 0.2365], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,488][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0188, 0.0917, 0.0991, 0.0996, 0.0922, 0.0965, 0.1013, 0.0993, 0.1003,
        0.1071, 0.0942], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,489][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.3009, 0.0242, 0.0563, 0.0441, 0.0764, 0.0784, 0.0392, 0.0938, 0.0875,
        0.0655, 0.1337], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,490][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [,] are: tensor([1.4175e-06, 3.2765e-02, 6.6115e-02, 2.7658e-01, 7.0108e-02, 1.8580e-01,
        2.1358e-01, 2.9191e-02, 6.9919e-03, 1.1818e-01, 6.9170e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,491][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [,] are: tensor([9.8956e-01, 1.0510e-03, 3.0447e-03, 1.2758e-03, 4.8266e-04, 3.5373e-04,
        4.1242e-04, 5.1937e-04, 3.4744e-04, 5.6254e-04, 2.3878e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,493][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0255, 0.0179, 0.0163, 0.0422, 0.0842, 0.1361, 0.1567, 0.2304, 0.1156,
        0.0570, 0.1181], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,494][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.4553, 0.0020, 0.0110, 0.0070, 0.0112, 0.0235, 0.0618, 0.0791, 0.0620,
        0.1108, 0.1763], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,496][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0677, 0.0869, 0.0921, 0.0923, 0.0912, 0.0941, 0.0915, 0.0955, 0.0956,
        0.0959, 0.0972], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,497][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0277, 0.1254, 0.1020, 0.1146, 0.0977, 0.1089, 0.0861, 0.0781, 0.0961,
        0.0817, 0.0817], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,498][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1351, 0.0388, 0.0669, 0.0635, 0.0748, 0.0898, 0.0896, 0.1130, 0.1078,
        0.1061, 0.1145], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,500][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0204, 0.0834, 0.0831, 0.0761, 0.1118, 0.0888, 0.1112, 0.0855, 0.1103,
        0.1147, 0.1147], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,501][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [,] are: tensor([5.5008e-06, 6.1218e-06, 2.8311e-04, 1.8377e-01, 6.5702e-02, 7.6122e-04,
        1.1463e-02, 4.8215e-04, 1.7191e-03, 7.0423e-01, 3.1579e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,502][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0874, 0.0095, 0.0350, 0.0162, 0.0833, 0.1815, 0.0147, 0.1187, 0.1378,
        0.0148, 0.2486, 0.0525], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,503][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0185, 0.0835, 0.0902, 0.0876, 0.0849, 0.0872, 0.0888, 0.0901, 0.0898,
        0.0951, 0.0863, 0.0980], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,505][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.1800, 0.0219, 0.0549, 0.0399, 0.0801, 0.0807, 0.0459, 0.1206, 0.0993,
        0.0679, 0.1652, 0.0436], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,506][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0024, 0.0277, 0.0248, 0.0938, 0.0716, 0.0681, 0.3894, 0.0094, 0.0026,
        0.1502, 0.0012, 0.1588], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,507][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([9.8737e-01, 1.2960e-03, 3.9164e-03, 1.5815e-03, 3.8575e-04, 3.0670e-04,
        3.7262e-04, 4.5952e-04, 3.2638e-04, 3.3770e-04, 2.0035e-03, 1.6400e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,509][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0305, 0.0095, 0.0112, 0.0253, 0.0755, 0.1040, 0.0762, 0.1709, 0.1086,
        0.0300, 0.1491, 0.2092], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,510][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.1831, 0.0028, 0.0128, 0.0094, 0.0148, 0.0346, 0.0833, 0.0988, 0.0878,
        0.1561, 0.2644, 0.0519], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,512][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0616, 0.0794, 0.0835, 0.0842, 0.0827, 0.0856, 0.0833, 0.0868, 0.0872,
        0.0873, 0.0882, 0.0903], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,513][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0475, 0.1096, 0.0896, 0.1098, 0.0922, 0.0935, 0.0702, 0.0715, 0.0855,
        0.0733, 0.0749, 0.0824], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,515][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.1676, 0.0315, 0.0505, 0.0464, 0.0640, 0.0788, 0.0756, 0.0932, 0.0945,
        0.0904, 0.0988, 0.1088], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,516][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0192, 0.0737, 0.0824, 0.0697, 0.1025, 0.0822, 0.0971, 0.0837, 0.1008,
        0.0969, 0.1009, 0.0909], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,517][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([3.9991e-06, 9.1661e-06, 3.5845e-04, 1.6698e-04, 4.2107e-02, 3.6395e-03,
        2.0773e-04, 8.8609e-04, 2.2335e-02, 1.5574e-02, 9.1442e-01, 2.8912e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,518][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.1054, 0.0082, 0.0366, 0.0150, 0.0780, 0.1417, 0.0153, 0.1131, 0.1186,
        0.0140, 0.2255, 0.0426, 0.0860], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,520][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0175, 0.0736, 0.0802, 0.0797, 0.0760, 0.0804, 0.0798, 0.0829, 0.0842,
        0.0865, 0.0805, 0.0890, 0.0899], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,521][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.1762, 0.0219, 0.0519, 0.0398, 0.0705, 0.0796, 0.0394, 0.1153, 0.0901,
        0.0560, 0.1473, 0.0451, 0.0666], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,522][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([2.7975e-05, 2.8825e-02, 1.5086e-01, 2.4027e-01, 1.0969e-01, 1.1310e-01,
        1.3330e-01, 2.2227e-02, 3.0994e-03, 7.6397e-02, 9.5762e-04, 1.1078e-01,
        1.0457e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,523][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([9.8106e-01, 2.5189e-03, 4.6272e-03, 1.4992e-03, 1.2613e-03, 6.1866e-04,
        6.7995e-04, 5.8422e-04, 5.1293e-04, 6.5858e-04, 2.8510e-03, 1.5967e-03,
        1.5329e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,525][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0128, 0.0169, 0.0159, 0.0355, 0.0656, 0.1040, 0.0679, 0.1276, 0.0874,
        0.0363, 0.1337, 0.2349, 0.0614], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,526][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.3112, 0.0025, 0.0116, 0.0076, 0.0110, 0.0258, 0.0621, 0.0789, 0.0650,
        0.1028, 0.1868, 0.0431, 0.0915], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,527][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0558, 0.0728, 0.0772, 0.0775, 0.0766, 0.0790, 0.0769, 0.0799, 0.0801,
        0.0803, 0.0815, 0.0829, 0.0796], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,529][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0341, 0.0965, 0.0809, 0.0956, 0.0872, 0.0915, 0.0666, 0.0672, 0.0831,
        0.0687, 0.0698, 0.0752, 0.0836], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,530][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1658, 0.0301, 0.0507, 0.0472, 0.0552, 0.0673, 0.0678, 0.0863, 0.0825,
        0.0716, 0.0881, 0.1017, 0.0857], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,532][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0166, 0.0669, 0.0707, 0.0570, 0.0894, 0.0707, 0.0932, 0.0718, 0.0874,
        0.0892, 0.0893, 0.0755, 0.1222], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,532][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([2.6498e-07, 6.2233e-06, 3.6306e-03, 2.1995e-01, 1.2097e-01, 1.0749e-02,
        3.4166e-03, 1.1032e-02, 1.5321e-02, 1.1104e-01, 3.6822e-01, 1.3347e-01,
        2.1927e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,533][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1139, 0.0061, 0.0279, 0.0138, 0.0680, 0.0990, 0.0149, 0.0721, 0.0648,
        0.0091, 0.1317, 0.0266, 0.0710, 0.2811], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,533][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0164, 0.0689, 0.0736, 0.0746, 0.0711, 0.0751, 0.0771, 0.0752, 0.0768,
        0.0815, 0.0725, 0.0829, 0.0855, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,533][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2542, 0.0170, 0.0422, 0.0329, 0.0602, 0.0592, 0.0334, 0.0798, 0.0723,
        0.0558, 0.1123, 0.0372, 0.0564, 0.0872], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,534][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([5.6177e-06, 7.3507e-03, 5.9712e-02, 9.6597e-02, 8.6328e-02, 9.5095e-02,
        4.3836e-01, 3.3997e-02, 2.0489e-03, 7.4158e-02, 4.5154e-04, 6.6600e-02,
        2.1368e-02, 1.7925e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,534][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.9158e-01, 6.7686e-04, 1.8971e-03, 6.9981e-04, 3.1957e-04, 2.4423e-04,
        2.7214e-04, 3.3560e-04, 2.4340e-04, 3.5786e-04, 1.6414e-03, 8.2209e-04,
        4.5213e-04, 4.5375e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,535][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0079, 0.0119, 0.0129, 0.0306, 0.0888, 0.1073, 0.0839, 0.1152, 0.0706,
        0.0318, 0.0884, 0.2058, 0.0881, 0.0568], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,535][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.3213, 0.0022, 0.0090, 0.0063, 0.0105, 0.0227, 0.0620, 0.0660, 0.0542,
        0.1028, 0.1585, 0.0328, 0.0846, 0.0672], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,536][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0518, 0.0672, 0.0711, 0.0714, 0.0706, 0.0728, 0.0711, 0.0740, 0.0740,
        0.0742, 0.0752, 0.0765, 0.0736, 0.0764], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,538][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0217, 0.0929, 0.0746, 0.0864, 0.0739, 0.0850, 0.0699, 0.0619, 0.0752,
        0.0716, 0.0641, 0.0782, 0.0830, 0.0617], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,539][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1469, 0.0248, 0.0459, 0.0416, 0.0514, 0.0562, 0.0532, 0.0734, 0.0719,
        0.0681, 0.0831, 0.0851, 0.0918, 0.1066], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,541][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0176, 0.0631, 0.0655, 0.0555, 0.0818, 0.0639, 0.0822, 0.0638, 0.0772,
        0.0834, 0.0780, 0.0685, 0.1101, 0.0894], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,541][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([4.3589e-06, 1.2551e-07, 1.4248e-05, 3.9186e-03, 3.0901e-02, 2.6785e-05,
        1.9075e-03, 1.6519e-05, 8.1523e-05, 8.4500e-02, 1.1849e-02, 2.9096e-03,
        1.0657e-01, 7.5730e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,543][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.1212, 0.0048, 0.0229, 0.0094, 0.0472, 0.0760, 0.0093, 0.0576, 0.0593,
        0.0072, 0.1192, 0.0220, 0.0593, 0.3081, 0.0766], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,544][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0171, 0.0636, 0.0694, 0.0690, 0.0661, 0.0690, 0.0695, 0.0691, 0.0707,
        0.0743, 0.0688, 0.0745, 0.0794, 0.0660, 0.0735], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,546][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.1986, 0.0167, 0.0369, 0.0301, 0.0499, 0.0558, 0.0324, 0.0837, 0.0686,
        0.0494, 0.1119, 0.0341, 0.0511, 0.0847, 0.0962], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,547][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ give] are: tensor([4.9760e-05, 1.5761e-02, 1.0694e-01, 1.5650e-01, 1.0524e-01, 3.7647e-02,
        3.3279e-01, 3.2202e-02, 9.7320e-04, 6.9442e-02, 5.7423e-04, 9.2171e-02,
        1.6196e-02, 1.5101e-02, 1.8403e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,548][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ give] are: tensor([9.8419e-01, 9.8579e-04, 3.6478e-03, 1.4646e-03, 6.7491e-04, 5.2557e-04,
        5.5841e-04, 5.5201e-04, 4.1149e-04, 8.8836e-04, 2.3384e-03, 1.6846e-03,
        7.6720e-04, 7.5346e-04, 5.5309e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,549][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0083, 0.0113, 0.0123, 0.0263, 0.0555, 0.0822, 0.0651, 0.1189, 0.0718,
        0.0297, 0.1253, 0.2098, 0.0734, 0.0796, 0.0305], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,551][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.2886, 0.0019, 0.0092, 0.0057, 0.0084, 0.0199, 0.0485, 0.0631, 0.0508,
        0.0883, 0.1552, 0.0318, 0.0776, 0.0653, 0.0856], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,552][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0474, 0.0621, 0.0662, 0.0663, 0.0655, 0.0677, 0.0658, 0.0687, 0.0688,
        0.0690, 0.0701, 0.0711, 0.0683, 0.0710, 0.0719], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,553][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0277, 0.0826, 0.0676, 0.0813, 0.0730, 0.0812, 0.0614, 0.0589, 0.0731,
        0.0640, 0.0612, 0.0710, 0.0732, 0.0612, 0.0628], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,555][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.1589, 0.0208, 0.0362, 0.0332, 0.0397, 0.0481, 0.0427, 0.0629, 0.0622,
        0.0606, 0.0718, 0.0755, 0.0854, 0.1057, 0.0963], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,556][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0147, 0.0579, 0.0593, 0.0490, 0.0733, 0.0596, 0.0770, 0.0598, 0.0741,
        0.0763, 0.0759, 0.0632, 0.1011, 0.0820, 0.0769], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,557][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ give] are: tensor([6.8801e-08, 5.5290e-09, 3.2464e-05, 3.8195e-04, 1.1634e-03, 1.5012e-06,
        4.6654e-06, 3.2499e-05, 7.3883e-06, 1.4264e-03, 3.0091e-03, 1.1713e-04,
        1.2117e-03, 9.8907e-01, 3.5428e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,559][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0271, 0.0056, 0.0218, 0.0100, 0.0475, 0.0688, 0.0129, 0.0627, 0.0617,
        0.0112, 0.1269, 0.0313, 0.0703, 0.2706, 0.1030, 0.0688],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,560][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0162, 0.0588, 0.0638, 0.0649, 0.0616, 0.0635, 0.0661, 0.0647, 0.0655,
        0.0687, 0.0628, 0.0719, 0.0742, 0.0614, 0.0704, 0.0654],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,562][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.1575, 0.0146, 0.0363, 0.0276, 0.0505, 0.0529, 0.0302, 0.0838, 0.0681,
        0.0419, 0.1119, 0.0335, 0.0507, 0.0848, 0.0927, 0.0630],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,563][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ it] are: tensor([4.4987e-05, 3.8227e-03, 1.2274e-02, 3.8721e-02, 6.7026e-02, 6.4490e-02,
        2.6599e-01, 1.5569e-02, 1.3119e-03, 8.8062e-02, 5.1077e-04, 6.2055e-02,
        1.8707e-02, 4.3038e-02, 5.7010e-02, 2.6137e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,564][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ it] are: tensor([9.9143e-01, 5.4295e-04, 1.7711e-03, 5.4964e-04, 5.1820e-04, 2.2451e-04,
        3.3997e-04, 2.5912e-04, 2.4896e-04, 3.6987e-04, 1.5174e-03, 6.2469e-04,
        4.6058e-04, 3.5932e-04, 2.9907e-04, 4.8890e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,565][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0065, 0.0125, 0.0153, 0.0323, 0.0736, 0.0901, 0.0656, 0.1181, 0.0653,
        0.0270, 0.1038, 0.2029, 0.0612, 0.0736, 0.0316, 0.0208],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,567][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.2145, 0.0018, 0.0080, 0.0055, 0.0086, 0.0194, 0.0516, 0.0603, 0.0478,
        0.0942, 0.1489, 0.0305, 0.0804, 0.0625, 0.0908, 0.0751],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,568][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0448, 0.0580, 0.0613, 0.0616, 0.0610, 0.0631, 0.0614, 0.0640, 0.0641,
        0.0643, 0.0649, 0.0661, 0.0635, 0.0660, 0.0667, 0.0694],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,570][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0211, 0.0757, 0.0607, 0.0740, 0.0641, 0.0776, 0.0584, 0.0545, 0.0705,
        0.0633, 0.0596, 0.0695, 0.0702, 0.0546, 0.0589, 0.0673],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,571][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.1359, 0.0211, 0.0343, 0.0324, 0.0409, 0.0436, 0.0431, 0.0583, 0.0568,
        0.0532, 0.0637, 0.0690, 0.0710, 0.0858, 0.0871, 0.1037],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,572][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0137, 0.0509, 0.0532, 0.0465, 0.0699, 0.0562, 0.0682, 0.0593, 0.0702,
        0.0728, 0.0696, 0.0584, 0.0916, 0.0779, 0.0722, 0.0694],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,573][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ it] are: tensor([1.4508e-06, 5.1796e-10, 2.7508e-07, 4.3611e-06, 4.9435e-03, 9.5720e-07,
        6.3745e-06, 5.5678e-06, 2.7476e-06, 1.8637e-04, 8.7842e-04, 3.0134e-06,
        4.2120e-03, 2.6521e-01, 6.1747e-01, 1.0707e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,575][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1557, 0.0034, 0.0170, 0.0076, 0.0363, 0.0576, 0.0072, 0.0412, 0.0413,
        0.0046, 0.0780, 0.0148, 0.0391, 0.1511, 0.0546, 0.0433, 0.2473],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,576][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0155, 0.0557, 0.0598, 0.0602, 0.0587, 0.0616, 0.0630, 0.0614, 0.0625,
        0.0664, 0.0589, 0.0660, 0.0689, 0.0555, 0.0648, 0.0625, 0.0587],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,578][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1920, 0.0125, 0.0333, 0.0255, 0.0490, 0.0472, 0.0267, 0.0710, 0.0595,
        0.0438, 0.0955, 0.0284, 0.0441, 0.0733, 0.0809, 0.0552, 0.0621],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,579][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([8.0300e-06, 2.7504e-03, 1.9524e-02, 4.4128e-02, 5.1321e-02, 4.5334e-02,
        2.8612e-01, 1.5080e-02, 7.8704e-04, 3.7662e-02, 1.6605e-04, 3.0503e-02,
        9.1352e-03, 8.7677e-03, 2.2769e-02, 3.9191e-01, 3.4025e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,580][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.9173e-01, 5.3989e-04, 1.8828e-03, 5.9248e-04, 3.1843e-04, 2.4184e-04,
        2.3895e-04, 2.6781e-04, 2.0314e-04, 2.5074e-04, 1.4089e-03, 6.5765e-04,
        3.4589e-04, 3.6110e-04, 2.5887e-04, 3.0099e-04, 3.9619e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,581][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0101, 0.0092, 0.0098, 0.0273, 0.0732, 0.0934, 0.0675, 0.0948, 0.0654,
        0.0269, 0.0810, 0.1973, 0.0757, 0.0544, 0.0360, 0.0232, 0.0545],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,583][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.2672, 0.0019, 0.0074, 0.0051, 0.0085, 0.0182, 0.0471, 0.0520, 0.0434,
        0.0747, 0.1219, 0.0251, 0.0657, 0.0533, 0.0775, 0.0693, 0.0617],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,584][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0415, 0.0544, 0.0575, 0.0578, 0.0572, 0.0591, 0.0577, 0.0599, 0.0601,
        0.0602, 0.0608, 0.0621, 0.0596, 0.0619, 0.0625, 0.0651, 0.0627],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,586][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0194, 0.0743, 0.0624, 0.0704, 0.0617, 0.0695, 0.0580, 0.0524, 0.0622,
        0.0605, 0.0542, 0.0629, 0.0682, 0.0518, 0.0566, 0.0614, 0.0539],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,587][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.1393, 0.0180, 0.0326, 0.0297, 0.0363, 0.0384, 0.0353, 0.0504, 0.0488,
        0.0475, 0.0562, 0.0571, 0.0632, 0.0737, 0.0756, 0.0949, 0.1033],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,589][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0145, 0.0493, 0.0517, 0.0460, 0.0657, 0.0526, 0.0638, 0.0529, 0.0631,
        0.0656, 0.0630, 0.0557, 0.0836, 0.0697, 0.0657, 0.0622, 0.0748],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,589][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([3.5812e-08, 1.6424e-10, 1.5834e-08, 5.0748e-06, 1.6112e-04, 8.2918e-08,
        7.4101e-06, 2.7401e-08, 1.3069e-07, 1.8264e-04, 1.8656e-05, 3.6030e-06,
        2.0172e-04, 1.9875e-03, 1.0305e-02, 9.5684e-01, 3.0282e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,628][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:30,629][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,630][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,632][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,633][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,634][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,635][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,636][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,637][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,638][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,640][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,641][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,642][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,643][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.5105, 0.4895], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,645][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.4645, 0.5355], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,646][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.9496, 0.0504], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,647][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.5756, 0.4244], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,647][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([9.9938e-01, 6.2056e-04], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,647][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.5900, 0.4100], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,647][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.4295, 0.5705], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,648][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.0492, 0.9508], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,648][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.9925, 0.0075], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,648][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.8816, 0.1184], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,649][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,649][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.9872, 0.0128], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,649][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3223, 0.2499, 0.4278], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,650][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.1847, 0.2335, 0.5818], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,651][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.7905, 0.0410, 0.1685], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,653][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0380, 0.3118, 0.6503], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,654][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.9957, 0.0013, 0.0030], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,655][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.2900, 0.3226, 0.3874], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,657][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.1642, 0.2299, 0.6059], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,658][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0105, 0.1782, 0.8113], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,659][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.9658, 0.0122, 0.0220], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,661][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.7733, 0.1112, 0.1156], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,662][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9743, 0.0037, 0.0220], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,664][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0141, 0.0315, 0.9544], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,665][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2728, 0.1014, 0.2286, 0.3972], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,666][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.2201, 0.0824, 0.5010, 0.1964], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,668][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.8265, 0.0263, 0.1143, 0.0329], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,669][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.1677, 0.1791, 0.2503, 0.4029], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,670][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([9.9513e-01, 9.6617e-04, 2.5864e-03, 1.3166e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,672][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.3696, 0.1947, 0.2272, 0.2086], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,673][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.1248, 0.2057, 0.3649, 0.3046], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,674][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0128, 0.2668, 0.1353, 0.5851], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,676][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.9360, 0.0178, 0.0220, 0.0242], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,677][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.8308, 0.0505, 0.0706, 0.0480], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,678][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.9806, 0.0024, 0.0146, 0.0024], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,680][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0219, 0.1199, 0.8079, 0.0502], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,681][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.1111, 0.0233, 0.1347, 0.1418, 0.5891], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,683][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0716, 0.0828, 0.2071, 0.3787, 0.2598], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,684][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.4688, 0.0530, 0.1806, 0.0787, 0.2188], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,685][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0082, 0.0830, 0.2520, 0.2979, 0.3589], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,687][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.9934, 0.0014, 0.0024, 0.0015, 0.0013], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,688][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.1222, 0.1135, 0.3371, 0.2284, 0.1989], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,689][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0470, 0.0888, 0.2364, 0.3270, 0.3008], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,691][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0054, 0.0481, 0.2438, 0.1760, 0.5269], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,692][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.9254, 0.0127, 0.0281, 0.0161, 0.0177], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,694][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.4430, 0.0930, 0.1185, 0.1135, 0.2321], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,695][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.9546, 0.0050, 0.0324, 0.0029, 0.0051], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,696][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([5.4668e-05, 3.3305e-04, 9.3315e-02, 8.9143e-01, 1.4871e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:30,697][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0337, 0.0172, 0.0495, 0.0978, 0.5836, 0.2182], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,698][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0646, 0.0491, 0.1703, 0.1915, 0.2123, 0.3121], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,700][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.5562, 0.0294, 0.1337, 0.0400, 0.1268, 0.1139], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,701][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0208, 0.0440, 0.1465, 0.1732, 0.3517, 0.2638], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,702][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([9.9639e-01, 6.7591e-04, 1.4581e-03, 7.9474e-04, 4.5315e-04, 2.3089e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,703][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.2077, 0.0883, 0.1581, 0.1480, 0.1887, 0.2091], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,704][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0501, 0.0586, 0.1655, 0.2083, 0.3040, 0.2134], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,704][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0076, 0.0847, 0.1130, 0.1700, 0.3074, 0.3173], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,705][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.9227, 0.0104, 0.0143, 0.0124, 0.0110, 0.0292], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,705][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.3482, 0.0576, 0.0792, 0.0826, 0.2993, 0.1330], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,705][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.9699, 0.0033, 0.0129, 0.0028, 0.0044, 0.0067], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,706][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.9424e-05, 8.9626e-06, 1.1301e-03, 4.4603e-03, 9.9398e-01, 3.9856e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:30,706][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0020, 0.0066, 0.0319, 0.0564, 0.6073, 0.2668, 0.0290],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,706][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0062, 0.0294, 0.1068, 0.1323, 0.2592, 0.4212, 0.0449],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,707][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.3125, 0.0609, 0.1432, 0.0678, 0.1939, 0.1751, 0.0467],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,707][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0045, 0.0341, 0.1056, 0.1353, 0.3449, 0.3024, 0.0731],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,708][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([9.8863e-01, 1.1285e-03, 3.7872e-03, 1.5944e-03, 1.3818e-03, 7.6049e-04,
        2.7168e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,709][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0189, 0.0322, 0.1163, 0.0814, 0.3244, 0.3548, 0.0720],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,710][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0126, 0.0587, 0.1739, 0.2191, 0.2295, 0.2781, 0.0281],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,712][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0010, 0.0402, 0.1344, 0.1422, 0.2595, 0.3338, 0.0889],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,713][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.7661, 0.0153, 0.0321, 0.0241, 0.0251, 0.0634, 0.0740],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,714][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0616, 0.0444, 0.0966, 0.0863, 0.3369, 0.2458, 0.1284],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,715][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.9108, 0.0060, 0.0289, 0.0043, 0.0089, 0.0130, 0.0282],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,716][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([1.2261e-07, 8.3875e-06, 3.2384e-03, 6.7758e-03, 9.2594e-01, 6.3952e-02,
        8.3547e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:30,718][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0057, 0.0032, 0.0189, 0.0361, 0.6841, 0.1509, 0.0759, 0.0251],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,719][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0214, 0.0262, 0.1222, 0.1096, 0.2902, 0.3201, 0.0670, 0.0432],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,720][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.4445, 0.0304, 0.1166, 0.0461, 0.1601, 0.1065, 0.0261, 0.0697],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,722][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0020, 0.0191, 0.0853, 0.1173, 0.2730, 0.3193, 0.0737, 0.1105],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,723][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([9.9353e-01, 7.7240e-04, 2.1202e-03, 1.2313e-03, 6.5557e-04, 3.6902e-04,
        6.7708e-04, 6.4427e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,724][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0337, 0.0343, 0.1230, 0.0699, 0.2501, 0.3344, 0.1012, 0.0536],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,726][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0202, 0.0339, 0.1311, 0.1538, 0.2471, 0.2398, 0.0539, 0.1201],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,727][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0031, 0.0168, 0.1094, 0.0746, 0.2686, 0.2186, 0.0810, 0.2280],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,728][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.8064, 0.0136, 0.0262, 0.0126, 0.0252, 0.0422, 0.0481, 0.0257],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,730][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0896, 0.0278, 0.0534, 0.0445, 0.2848, 0.1942, 0.1535, 0.1523],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,731][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.9427, 0.0026, 0.0203, 0.0030, 0.0045, 0.0068, 0.0108, 0.0094],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,732][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([3.0095e-06, 6.3242e-07, 1.2646e-03, 6.7372e-02, 9.1069e-01, 4.1816e-04,
        2.0128e-02, 1.1975e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:30,734][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0117, 0.0064, 0.0247, 0.0568, 0.4560, 0.1611, 0.0684, 0.0610, 0.1538],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,735][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0205, 0.0209, 0.0893, 0.0944, 0.2060, 0.2812, 0.0589, 0.0582, 0.1704],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,736][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4410, 0.0259, 0.0942, 0.0351, 0.1390, 0.1085, 0.0209, 0.0721, 0.0632],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,738][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0065, 0.0207, 0.0736, 0.0833, 0.2308, 0.2158, 0.1173, 0.1407, 0.1113],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,739][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.9604e-01, 6.1259e-04, 1.5663e-03, 4.9883e-04, 3.1379e-04, 1.8114e-04,
        2.7830e-04, 3.1207e-04, 2.0090e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,740][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0747, 0.0360, 0.1016, 0.1023, 0.1634, 0.2389, 0.0893, 0.0696, 0.1242],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,742][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0203, 0.0316, 0.1012, 0.1355, 0.2098, 0.2063, 0.0613, 0.1282, 0.1058],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,743][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0056, 0.0367, 0.0499, 0.1044, 0.1868, 0.2082, 0.0647, 0.1102, 0.2335],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,745][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.8515, 0.0112, 0.0176, 0.0106, 0.0151, 0.0299, 0.0293, 0.0169, 0.0179],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,746][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1139, 0.0262, 0.0408, 0.0468, 0.2626, 0.1220, 0.1190, 0.1956, 0.0731],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,747][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.9560, 0.0027, 0.0133, 0.0020, 0.0044, 0.0059, 0.0044, 0.0060, 0.0052],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,748][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([2.1219e-04, 5.9284e-06, 1.2392e-03, 1.5183e-02, 9.7086e-01, 9.4395e-04,
        3.7907e-03, 4.4997e-03, 3.2676e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:30,750][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0056, 0.0066, 0.0234, 0.0796, 0.3009, 0.2208, 0.0350, 0.0869, 0.1977,
        0.0436], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,751][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0096, 0.0116, 0.0462, 0.0756, 0.1757, 0.2378, 0.0507, 0.0591, 0.1940,
        0.1396], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,752][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.3160, 0.0366, 0.0978, 0.0497, 0.1604, 0.1212, 0.0343, 0.0807, 0.0732,
        0.0302], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,754][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0058, 0.0312, 0.0545, 0.1560, 0.1886, 0.1974, 0.0548, 0.0948, 0.1179,
        0.0989], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,755][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([9.9521e-01, 5.1304e-04, 1.5972e-03, 4.7882e-04, 1.8908e-04, 2.0145e-04,
        3.5363e-04, 5.3599e-04, 2.5662e-04, 6.6830e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,756][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0376, 0.0261, 0.0871, 0.0825, 0.1127, 0.3000, 0.0708, 0.0630, 0.1640,
        0.0563], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,757][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0166, 0.0433, 0.0927, 0.1614, 0.1819, 0.1541, 0.0680, 0.1148, 0.1230,
        0.0441], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,759][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0016, 0.0290, 0.0632, 0.0961, 0.1395, 0.1955, 0.0649, 0.1144, 0.1902,
        0.1056], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,760][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.8583, 0.0123, 0.0116, 0.0071, 0.0072, 0.0285, 0.0275, 0.0180, 0.0178,
        0.0117], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,762][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0818, 0.0290, 0.0365, 0.0511, 0.1560, 0.1499, 0.1167, 0.1816, 0.0912,
        0.1062], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,762][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.9209, 0.0040, 0.0153, 0.0027, 0.0064, 0.0075, 0.0068, 0.0092, 0.0063,
        0.0210], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,763][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([9.4052e-06, 2.9202e-05, 2.0817e-03, 9.2964e-02, 8.2207e-01, 3.0352e-02,
        1.3901e-02, 2.2759e-03, 2.5355e-02, 1.0959e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:30,763][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0178, 0.0056, 0.0180, 0.0431, 0.2700, 0.1562, 0.0547, 0.0501, 0.1676,
        0.0783, 0.1385], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,763][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0137, 0.0103, 0.0478, 0.0487, 0.0911, 0.1661, 0.0373, 0.0360, 0.1046,
        0.1992, 0.2451], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,764][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.2762, 0.0239, 0.0969, 0.0339, 0.1546, 0.1004, 0.0198, 0.0697, 0.0666,
        0.0255, 0.1325], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,764][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0021, 0.0147, 0.0435, 0.0730, 0.1207, 0.2124, 0.0485, 0.1009, 0.1201,
        0.1283, 0.1357], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,764][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([9.8986e-01, 1.0323e-03, 2.9386e-03, 1.1956e-03, 4.5036e-04, 3.3256e-04,
        4.0426e-04, 4.7887e-04, 3.2176e-04, 5.9365e-04, 2.3924e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,765][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0560, 0.0330, 0.1005, 0.0727, 0.0736, 0.2240, 0.0694, 0.0544, 0.1398,
        0.0569, 0.1198], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,767][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0146, 0.0238, 0.0825, 0.1025, 0.1668, 0.1593, 0.0353, 0.0988, 0.1100,
        0.0535, 0.1529], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,768][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0026, 0.0223, 0.0810, 0.0770, 0.1574, 0.1421, 0.0338, 0.0976, 0.1328,
        0.0488, 0.2046], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,769][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.6566, 0.0200, 0.0339, 0.0188, 0.0315, 0.0535, 0.0448, 0.0279, 0.0334,
        0.0217, 0.0580], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,771][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0696, 0.0181, 0.0325, 0.0342, 0.1811, 0.1356, 0.0890, 0.1785, 0.0764,
        0.0831, 0.1019], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,772][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.8899, 0.0042, 0.0220, 0.0045, 0.0087, 0.0108, 0.0091, 0.0073, 0.0090,
        0.0071, 0.0275], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,773][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([5.5008e-06, 6.1218e-06, 2.8311e-04, 1.8377e-01, 6.5702e-02, 7.6122e-04,
        1.1463e-02, 4.8215e-04, 1.7191e-03, 7.0423e-01, 3.1579e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:30,774][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0083, 0.0059, 0.0214, 0.0334, 0.1746, 0.2643, 0.0315, 0.0537, 0.1479,
        0.0639, 0.1590, 0.0363], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,776][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0075, 0.0089, 0.0650, 0.0228, 0.1185, 0.1668, 0.0257, 0.0424, 0.1026,
        0.1147, 0.2899, 0.0353], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,777][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.3019, 0.0237, 0.0897, 0.0274, 0.1197, 0.0989, 0.0220, 0.0735, 0.0634,
        0.0226, 0.1245, 0.0326], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,779][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0076, 0.0162, 0.0293, 0.0448, 0.1443, 0.1786, 0.0615, 0.0849, 0.1015,
        0.1260, 0.1451, 0.0603], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,780][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([9.8725e-01, 1.3141e-03, 3.9278e-03, 1.5624e-03, 3.8181e-04, 3.0233e-04,
        3.8952e-04, 4.3949e-04, 3.1812e-04, 3.6034e-04, 2.0725e-03, 1.6814e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,781][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0414, 0.0262, 0.0613, 0.0509, 0.0877, 0.2176, 0.0389, 0.0807, 0.1417,
        0.0394, 0.1498, 0.0645], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,783][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0110, 0.0366, 0.0856, 0.0768, 0.1560, 0.1277, 0.0403, 0.1017, 0.1065,
        0.0519, 0.1519, 0.0541], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,784][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0017, 0.0407, 0.0219, 0.1150, 0.0825, 0.1488, 0.0370, 0.0663, 0.1767,
        0.0490, 0.1127, 0.1476], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,786][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.7457, 0.0170, 0.0247, 0.0177, 0.0143, 0.0398, 0.0243, 0.0199, 0.0229,
        0.0192, 0.0371, 0.0175], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,787][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0988, 0.0212, 0.0396, 0.0299, 0.1846, 0.1102, 0.0902, 0.1196, 0.0689,
        0.0815, 0.1147, 0.0408], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,789][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.9026, 0.0033, 0.0197, 0.0028, 0.0070, 0.0091, 0.0063, 0.0080, 0.0086,
        0.0074, 0.0217, 0.0034], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,789][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([3.9991e-06, 9.1661e-06, 3.5845e-04, 1.6698e-04, 4.2107e-02, 3.6395e-03,
        2.0773e-04, 8.8609e-04, 2.2335e-02, 1.5574e-02, 9.1442e-01, 2.8912e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:30,791][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0050, 0.0049, 0.0244, 0.0283, 0.3040, 0.1645, 0.0312, 0.0752, 0.1460,
        0.0411, 0.1243, 0.0315, 0.0194], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,792][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0020, 0.0043, 0.0213, 0.0216, 0.0735, 0.1888, 0.0140, 0.0442, 0.1291,
        0.0787, 0.3527, 0.0422, 0.0274], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,794][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.1729, 0.0243, 0.1081, 0.0358, 0.1070, 0.1429, 0.0158, 0.0817, 0.0771,
        0.0189, 0.1355, 0.0345, 0.0454], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,795][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0006, 0.0143, 0.0591, 0.0832, 0.1099, 0.1735, 0.0348, 0.1089, 0.0972,
        0.0933, 0.1259, 0.0641, 0.0352], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,796][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([9.8122e-01, 2.5809e-03, 4.5700e-03, 1.4638e-03, 1.2294e-03, 5.9769e-04,
        6.5469e-04, 5.2565e-04, 4.7369e-04, 6.5625e-04, 2.8086e-03, 1.5576e-03,
        1.6591e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,798][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0108, 0.0198, 0.0644, 0.0792, 0.0752, 0.2732, 0.0262, 0.0350, 0.1118,
        0.0295, 0.1566, 0.0710, 0.0474], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,799][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0042, 0.0192, 0.0645, 0.0873, 0.1368, 0.1693, 0.0261, 0.0986, 0.0955,
        0.0475, 0.1531, 0.0689, 0.0288], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,800][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0008, 0.0111, 0.0601, 0.0538, 0.1810, 0.1266, 0.0325, 0.0819, 0.1197,
        0.0379, 0.1394, 0.0452, 0.1100], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,802][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.5369, 0.0157, 0.0382, 0.0186, 0.0406, 0.0804, 0.0379, 0.0355, 0.0378,
        0.0245, 0.0759, 0.0198, 0.0383], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,803][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0294, 0.0222, 0.0457, 0.0447, 0.1687, 0.1170, 0.1011, 0.1586, 0.0793,
        0.0479, 0.1032, 0.0419, 0.0403], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,805][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.8430, 0.0074, 0.0321, 0.0027, 0.0086, 0.0126, 0.0124, 0.0133, 0.0117,
        0.0092, 0.0354, 0.0038, 0.0077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,806][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([2.6498e-07, 6.2233e-06, 3.6306e-03, 2.1995e-01, 1.2097e-01, 1.0749e-02,
        3.4166e-03, 1.1032e-02, 1.5321e-02, 1.1104e-01, 3.6822e-01, 1.3347e-01,
        2.1927e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:30,807][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0129, 0.0028, 0.0187, 0.0365, 0.3217, 0.0972, 0.0503, 0.0214, 0.0966,
        0.0572, 0.0956, 0.0393, 0.0623, 0.0875], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,809][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0129, 0.0053, 0.0228, 0.0217, 0.1014, 0.1144, 0.0211, 0.0235, 0.0742,
        0.1089, 0.2120, 0.0339, 0.0657, 0.1823], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,810][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.3771, 0.0146, 0.0536, 0.0253, 0.0842, 0.0710, 0.0175, 0.0461, 0.0479,
        0.0154, 0.0954, 0.0293, 0.0536, 0.0690], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,812][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0020, 0.0072, 0.0321, 0.0352, 0.1038, 0.1169, 0.0485, 0.0789, 0.0616,
        0.0862, 0.1117, 0.0328, 0.0620, 0.2212], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,813][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.9177e-01, 6.7626e-04, 1.8245e-03, 6.6849e-04, 3.0033e-04, 2.3217e-04,
        2.6458e-04, 3.0549e-04, 2.2288e-04, 3.5642e-04, 1.6269e-03, 8.1064e-04,
        4.9100e-04, 4.4658e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,814][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0541, 0.0165, 0.0702, 0.0480, 0.0870, 0.1728, 0.0547, 0.0315, 0.0848,
        0.0430, 0.1268, 0.0519, 0.0817, 0.0770], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,815][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0183, 0.0140, 0.0617, 0.0687, 0.1207, 0.1036, 0.0369, 0.0831, 0.0761,
        0.0379, 0.1244, 0.0510, 0.0540, 0.1496], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,817][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0017, 0.0151, 0.0423, 0.0486, 0.1072, 0.0896, 0.0341, 0.0534, 0.0812,
        0.0327, 0.1175, 0.0461, 0.0720, 0.2584], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,819][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.7511, 0.0112, 0.0159, 0.0102, 0.0161, 0.0303, 0.0269, 0.0169, 0.0156,
        0.0100, 0.0277, 0.0100, 0.0277, 0.0304], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,819][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0943, 0.0107, 0.0281, 0.0249, 0.1348, 0.0716, 0.0534, 0.1081, 0.0502,
        0.0492, 0.1052, 0.0281, 0.0738, 0.1676], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,820][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.9078, 0.0030, 0.0151, 0.0020, 0.0055, 0.0067, 0.0054, 0.0082, 0.0052,
        0.0047, 0.0158, 0.0025, 0.0032, 0.0150], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,820][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([4.3589e-06, 1.2551e-07, 1.4248e-05, 3.9186e-03, 3.0901e-02, 2.6785e-05,
        1.9075e-03, 1.6519e-05, 8.1523e-05, 8.4500e-02, 1.1849e-02, 2.9096e-03,
        1.0657e-01, 7.5730e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:30,821][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0109, 0.0015, 0.0274, 0.0194, 0.2317, 0.0889, 0.0211, 0.0310, 0.0798,
        0.0263, 0.1399, 0.0181, 0.0473, 0.1564, 0.1004], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,821][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0042, 0.0037, 0.0181, 0.0222, 0.0638, 0.0772, 0.0135, 0.0157, 0.0584,
        0.0987, 0.2319, 0.0342, 0.0582, 0.2491, 0.0511], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,821][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.2488, 0.0223, 0.0593, 0.0321, 0.0695, 0.0835, 0.0228, 0.0624, 0.0520,
        0.0216, 0.0981, 0.0333, 0.0528, 0.0817, 0.0598], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,822][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0009, 0.0089, 0.0413, 0.0481, 0.0901, 0.0889, 0.0413, 0.0965, 0.0481,
        0.0800, 0.1051, 0.0397, 0.0457, 0.1984, 0.0670], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,822][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([9.8427e-01, 9.8285e-04, 3.5102e-03, 1.3918e-03, 6.4932e-04, 4.9958e-04,
        6.2146e-04, 5.3198e-04, 3.8787e-04, 8.8376e-04, 2.3825e-03, 1.6932e-03,
        8.5323e-04, 7.7608e-04, 5.6895e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,822][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0273, 0.0170, 0.0638, 0.0484, 0.0605, 0.1444, 0.0334, 0.0291, 0.0654,
        0.0292, 0.1525, 0.0452, 0.0844, 0.1493, 0.0501], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,824][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0088, 0.0130, 0.0540, 0.0625, 0.1073, 0.1225, 0.0230, 0.0688, 0.0830,
        0.0348, 0.1158, 0.0437, 0.0432, 0.1476, 0.0720], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,825][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0011, 0.0067, 0.0503, 0.0340, 0.0927, 0.0696, 0.0203, 0.0472, 0.0725,
        0.0280, 0.1191, 0.0287, 0.0617, 0.1936, 0.1744], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,827][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.6162, 0.0102, 0.0194, 0.0116, 0.0207, 0.0372, 0.0466, 0.0316, 0.0217,
        0.0199, 0.0318, 0.0135, 0.0492, 0.0442, 0.0262], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,828][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0247, 0.0093, 0.0218, 0.0183, 0.0757, 0.0684, 0.0355, 0.1023, 0.0482,
        0.0490, 0.0833, 0.0221, 0.0862, 0.2492, 0.1061], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,830][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.8440, 0.0049, 0.0185, 0.0024, 0.0072, 0.0101, 0.0105, 0.0139, 0.0080,
        0.0065, 0.0228, 0.0028, 0.0053, 0.0236, 0.0196], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,830][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([6.8801e-08, 5.5290e-09, 3.2464e-05, 3.8195e-04, 1.1634e-03, 1.5012e-06,
        4.6654e-06, 3.2499e-05, 7.3883e-06, 1.4264e-03, 3.0091e-03, 1.1713e-04,
        1.2117e-03, 9.8907e-01, 3.5428e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:30,832][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0038, 0.0008, 0.0083, 0.0142, 0.2303, 0.0491, 0.0188, 0.0201, 0.0376,
        0.0305, 0.0557, 0.0130, 0.0386, 0.1100, 0.1797, 0.1893],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,833][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0066, 0.0031, 0.0177, 0.0251, 0.0680, 0.0657, 0.0128, 0.0116, 0.0476,
        0.0584, 0.1570, 0.0331, 0.0396, 0.2161, 0.0884, 0.1491],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,835][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.4017, 0.0141, 0.0411, 0.0148, 0.0805, 0.0603, 0.0118, 0.0403, 0.0422,
        0.0083, 0.0726, 0.0158, 0.0328, 0.0623, 0.0615, 0.0400],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,836][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0029, 0.0029, 0.0136, 0.0145, 0.0586, 0.0773, 0.0284, 0.0550, 0.0400,
        0.0597, 0.0734, 0.0186, 0.0326, 0.2246, 0.0859, 0.2119],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,837][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([9.9180e-01, 5.2979e-04, 1.6687e-03, 5.0959e-04, 4.7081e-04, 2.0762e-04,
        3.2798e-04, 2.3808e-04, 2.2323e-04, 3.6677e-04, 1.4857e-03, 6.1213e-04,
        4.7951e-04, 3.4246e-04, 2.7653e-04, 4.6013e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,839][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0426, 0.0069, 0.0387, 0.0424, 0.0749, 0.1209, 0.0252, 0.0190, 0.0641,
        0.0160, 0.1021, 0.0384, 0.0705, 0.1322, 0.0618, 0.1440],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,840][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0126, 0.0132, 0.0456, 0.0773, 0.0854, 0.0975, 0.0250, 0.0625, 0.0502,
        0.0293, 0.1014, 0.0527, 0.0287, 0.1459, 0.1017, 0.0712],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,842][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0014, 0.0137, 0.0175, 0.0320, 0.0630, 0.0766, 0.0265, 0.0368, 0.0702,
        0.0287, 0.0686, 0.0338, 0.0516, 0.1740, 0.1107, 0.1949],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,843][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.8052, 0.0060, 0.0105, 0.0066, 0.0076, 0.0177, 0.0129, 0.0106, 0.0128,
        0.0082, 0.0209, 0.0070, 0.0169, 0.0234, 0.0145, 0.0192],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,845][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0487, 0.0073, 0.0167, 0.0151, 0.1176, 0.0474, 0.0387, 0.0838, 0.0366,
        0.0237, 0.0652, 0.0187, 0.0442, 0.1862, 0.1602, 0.0897],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,846][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.9060, 0.0014, 0.0121, 0.0014, 0.0047, 0.0043, 0.0046, 0.0080, 0.0038,
        0.0044, 0.0123, 0.0018, 0.0035, 0.0156, 0.0105, 0.0056],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,847][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([1.4508e-06, 5.1796e-10, 2.7508e-07, 4.3611e-06, 4.9435e-03, 9.5720e-07,
        6.3745e-06, 5.5678e-06, 2.7476e-06, 1.8637e-04, 8.7842e-04, 3.0134e-06,
        4.2120e-03, 2.6521e-01, 6.1747e-01, 1.0707e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:30,849][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0059, 0.0009, 0.0081, 0.0138, 0.1624, 0.0485, 0.0225, 0.0089, 0.0429,
        0.0232, 0.0509, 0.0142, 0.0276, 0.0446, 0.1831, 0.2481, 0.0944],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,850][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0100, 0.0032, 0.0125, 0.0142, 0.0685, 0.0714, 0.0133, 0.0118, 0.0435,
        0.0753, 0.1163, 0.0204, 0.0375, 0.1172, 0.0554, 0.1681, 0.1614],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,852][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.3789, 0.0098, 0.0370, 0.0180, 0.0657, 0.0492, 0.0135, 0.0364, 0.0366,
        0.0114, 0.0722, 0.0211, 0.0464, 0.0570, 0.0550, 0.0357, 0.0561],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,853][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0016, 0.0031, 0.0127, 0.0164, 0.0565, 0.0535, 0.0257, 0.0358, 0.0269,
        0.0391, 0.0502, 0.0150, 0.0299, 0.1043, 0.0645, 0.2895, 0.1753],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,854][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.9207e-01, 5.3643e-04, 1.7924e-03, 5.6226e-04, 2.9493e-04, 2.2576e-04,
        2.2828e-04, 2.3850e-04, 1.8275e-04, 2.4438e-04, 1.3699e-03, 6.4191e-04,
        3.6211e-04, 3.4697e-04, 2.4108e-04, 2.8571e-04, 3.8070e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,855][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0360, 0.0095, 0.0336, 0.0307, 0.0545, 0.1091, 0.0324, 0.0155, 0.0535,
        0.0235, 0.0684, 0.0315, 0.0445, 0.0474, 0.0474, 0.2777, 0.0849],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,857][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0163, 0.0080, 0.0377, 0.0416, 0.0806, 0.0695, 0.0249, 0.0555, 0.0497,
        0.0252, 0.0852, 0.0319, 0.0342, 0.1049, 0.0810, 0.1106, 0.1432],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,858][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0012, 0.0090, 0.0195, 0.0280, 0.0640, 0.0546, 0.0234, 0.0319, 0.0468,
        0.0191, 0.0638, 0.0262, 0.0426, 0.1480, 0.0853, 0.1434, 0.1932],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,860][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.7493, 0.0083, 0.0138, 0.0083, 0.0124, 0.0197, 0.0170, 0.0121, 0.0101,
        0.0078, 0.0212, 0.0075, 0.0180, 0.0225, 0.0169, 0.0266, 0.0284],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,861][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0651, 0.0066, 0.0189, 0.0155, 0.0996, 0.0449, 0.0336, 0.0688, 0.0304,
        0.0311, 0.0633, 0.0165, 0.0442, 0.1108, 0.1213, 0.0960, 0.1334],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,863][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.9038, 0.0024, 0.0117, 0.0019, 0.0043, 0.0054, 0.0042, 0.0064, 0.0040,
        0.0033, 0.0120, 0.0021, 0.0023, 0.0112, 0.0087, 0.0062, 0.0100],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,864][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([3.5812e-08, 1.6424e-10, 1.5834e-08, 5.0748e-06, 1.6112e-04, 8.2918e-08,
        7.4101e-06, 2.7401e-08, 1.3069e-07, 1.8264e-04, 1.8656e-05, 3.6030e-06,
        2.0172e-04, 1.9875e-03, 1.0305e-02, 9.5684e-01, 3.0282e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:30,865][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:30,867][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 7866],
        [ 2088],
        [ 4047],
        [ 5046],
        [22325],
        [ 9503],
        [15188],
        [ 5265],
        [ 3707],
        [ 2034],
        [ 3414],
        [ 3708],
        [ 5154],
        [ 4216],
        [ 2805],
        [ 9318],
        [ 2988]], device='cuda:0')
[2024-07-24 10:21:30,868][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8250],
        [ 2715],
        [ 5935],
        [ 7283],
        [28428],
        [15507],
        [24098],
        [ 9211],
        [ 6981],
        [ 3119],
        [ 4695],
        [ 5962],
        [ 6675],
        [ 6503],
        [ 4107],
        [14740],
        [ 5188]], device='cuda:0')
[2024-07-24 10:21:30,870][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 4437],
        [ 5069],
        [ 8862],
        [10337],
        [11254],
        [12450],
        [13022],
        [13610],
        [13797],
        [13632],
        [13826],
        [13748],
        [14422],
        [13914],
        [13632],
        [13944],
        [12678]], device='cuda:0')
[2024-07-24 10:21:30,871][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[12770],
        [ 1294],
        [  830],
        [  821],
        [  690],
        [  606],
        [  637],
        [  585],
        [  524],
        [  478],
        [  447],
        [  453],
        [  455],
        [  433],
        [  401],
        [  415],
        [  405]], device='cuda:0')
[2024-07-24 10:21:30,873][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[23357],
        [26905],
        [24896],
        [27600],
        [27493],
        [27371],
        [29087],
        [27591],
        [27532],
        [27419],
        [27596],
        [27992],
        [27881],
        [27515],
        [27314],
        [26914],
        [26961]], device='cuda:0')
[2024-07-24 10:21:30,874][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[11632],
        [49712],
        [47160],
        [47350],
        [45278],
        [44980],
        [39749],
        [40295],
        [22655],
        [43279],
        [41196],
        [34232],
        [42175],
        [31210],
        [35162],
        [26550],
        [21561]], device='cuda:0')
[2024-07-24 10:21:30,876][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[702],
        [693],
        [701],
        [693],
        [687],
        [695],
        [689],
        [687],
        [695],
        [691],
        [684],
        [680],
        [649],
        [680],
        [659],
        [681],
        [681]], device='cuda:0')
[2024-07-24 10:21:30,877][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[1235],
        [7691],
        [5095],
        [7758],
        [2865],
        [2924],
        [2583],
        [3466],
        [4401],
        [4601],
        [4418],
        [5526],
        [5669],
        [5413],
        [5868],
        [5845],
        [6043]], device='cuda:0')
[2024-07-24 10:21:30,878][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[11029],
        [11360],
        [11083],
        [11357],
        [11475],
        [10525],
        [11565],
        [10686],
        [10739],
        [10907],
        [10788],
        [11097],
        [10778],
        [10528],
        [10240],
        [10253],
        [10293]], device='cuda:0')
[2024-07-24 10:21:30,879][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[5112],
        [4611],
        [4552],
        [4482],
        [4488],
        [4493],
        [4516],
        [4572],
        [4573],
        [4597],
        [4593],
        [4566],
        [4571],
        [4569],
        [4563],
        [4567],
        [4573]], device='cuda:0')
[2024-07-24 10:21:30,880][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[21565],
        [29308],
        [26696],
        [29739],
        [31038],
        [30010],
        [30052],
        [30280],
        [30348],
        [30119],
        [29695],
        [30369],
        [30001],
        [29925],
        [30301],
        [29806],
        [29757]], device='cuda:0')
[2024-07-24 10:21:30,882][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[ 1494],
        [17298],
        [13965],
        [19558],
        [23423],
        [25940],
        [27714],
        [27916],
        [26764],
        [27571],
        [27348],
        [27139],
        [27765],
        [28480],
        [29330],
        [29682],
        [29441]], device='cuda:0')
[2024-07-24 10:21:30,883][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[19947],
        [25490],
        [27309],
        [28890],
        [26838],
        [27136],
        [27723],
        [28457],
        [28769],
        [29126],
        [29566],
        [29727],
        [29193],
        [28787],
        [28507],
        [28419],
        [28324]], device='cuda:0')
[2024-07-24 10:21:30,884][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[29379],
        [30020],
        [17874],
        [25433],
        [42303],
        [ 4353],
        [ 4718],
        [ 5589],
        [ 4580],
        [ 6737],
        [40262],
        [23189],
        [33159],
        [18841],
        [20157],
        [15329],
        [ 8612]], device='cuda:0')
[2024-07-24 10:21:30,886][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[33553],
        [28900],
        [31556],
        [28776],
        [27358],
        [24047],
        [26692],
        [27089],
        [27144],
        [28285],
        [30937],
        [29300],
        [34396],
        [30192],
        [31549],
        [31270],
        [33564]], device='cuda:0')
[2024-07-24 10:21:30,887][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[13892],
        [14408],
        [17914],
        [20032],
        [11833],
        [12046],
        [11951],
        [11607],
        [13736],
        [13814],
        [15268],
        [15352],
        [15603],
        [15834],
        [17302],
        [18298],
        [20031]], device='cuda:0')
[2024-07-24 10:21:30,889][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[18474],
        [10298],
        [16183],
        [17193],
        [14994],
        [19317],
        [21050],
        [21281],
        [21616],
        [21674],
        [25011],
        [26099],
        [28140],
        [30323],
        [32063],
        [30542],
        [30364]], device='cuda:0')
[2024-07-24 10:21:30,890][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[11252],
        [ 9558],
        [13367],
        [12192],
        [ 5889],
        [10733],
        [ 7503],
        [ 7782],
        [ 8617],
        [ 7288],
        [ 8132],
        [ 8397],
        [ 8895],
        [ 8589],
        [ 8151],
        [ 8664],
        [ 8234]], device='cuda:0')
[2024-07-24 10:21:30,892][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 7578],
        [ 6350],
        [12134],
        [11826],
        [ 9261],
        [ 7783],
        [ 8508],
        [ 9584],
        [10719],
        [11318],
        [10685],
        [10601],
        [10769],
        [ 9218],
        [ 8820],
        [ 6591],
        [ 6313]], device='cuda:0')
[2024-07-24 10:21:30,893][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[33342],
        [33274],
        [32657],
        [32599],
        [32217],
        [32743],
        [31627],
        [32204],
        [32669],
        [32550],
        [31894],
        [31599],
        [30685],
        [32196],
        [31069],
        [32150],
        [32191]], device='cuda:0')
[2024-07-24 10:21:30,894][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[14571],
        [11595],
        [12837],
        [11464],
        [11086],
        [10494],
        [11089],
        [11819],
        [12731],
        [14977],
        [16119],
        [14946],
        [13913],
        [14738],
        [14219],
        [15257],
        [17255]], device='cuda:0')
[2024-07-24 10:21:30,896][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 8388],
        [ 7012],
        [ 6046],
        [ 6456],
        [11254],
        [ 9146],
        [ 7324],
        [ 8323],
        [ 8281],
        [ 7984],
        [ 8515],
        [ 8792],
        [ 7993],
        [ 6110],
        [ 5821],
        [ 5083],
        [ 3344]], device='cuda:0')
[2024-07-24 10:21:30,898][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[38436],
        [40420],
        [40932],
        [42751],
        [42645],
        [41664],
        [41046],
        [38811],
        [40056],
        [39676],
        [40697],
        [41276],
        [41361],
        [41505],
        [42149],
        [41341],
        [40920]], device='cuda:0')
[2024-07-24 10:21:30,899][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[12337],
        [11984],
        [12742],
        [11769],
        [13481],
        [14587],
        [15064],
        [15952],
        [16178],
        [15119],
        [19248],
        [16749],
        [23914],
        [20252],
        [24133],
        [20962],
        [24336]], device='cuda:0')
[2024-07-24 10:21:30,900][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[37622],
        [35831],
        [28451],
        [30435],
        [13683],
        [11309],
        [11089],
        [10690],
        [11018],
        [13450],
        [12270],
        [12462],
        [12653],
        [14811],
        [15633],
        [15483],
        [16585]], device='cuda:0')
[2024-07-24 10:21:30,902][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[23252],
        [23200],
        [22673],
        [22826],
        [21975],
        [22609],
        [22036],
        [22419],
        [22460],
        [20798],
        [19253],
        [19697],
        [17081],
        [19370],
        [16268],
        [19142],
        [19001]], device='cuda:0')
[2024-07-24 10:21:30,903][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[49832],
        [49789],
        [43051],
        [42059],
        [35821],
        [46939],
        [46959],
        [47716],
        [47120],
        [47846],
        [46928],
        [40884],
        [43905],
        [41737],
        [39298],
        [45967],
        [45899]], device='cuda:0')
[2024-07-24 10:21:30,905][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[22526],
        [25332],
        [23270],
        [22794],
        [28110],
        [26570],
        [27414],
        [26291],
        [25162],
        [24770],
        [23478],
        [25135],
        [24472],
        [24611],
        [25161],
        [23967],
        [22921]], device='cuda:0')
[2024-07-24 10:21:30,906][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 8344],
        [11613],
        [ 6391],
        [ 8204],
        [ 7807],
        [ 9050],
        [ 7291],
        [ 7627],
        [ 8113],
        [ 8022],
        [ 7231],
        [ 8478],
        [ 5744],
        [ 8031],
        [ 7594],
        [ 7718],
        [ 7477]], device='cuda:0')
[2024-07-24 10:21:30,908][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480],
        [17480]], device='cuda:0')
[2024-07-24 10:21:30,939][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:30,940][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,941][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,942][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,942][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,943][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,944][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,944][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,946][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,947][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,948][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,949][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,950][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:30,952][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.5852, 0.4148], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,954][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.0554, 0.9446], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,955][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.5424, 0.4576], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,957][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.6869, 0.3131], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,958][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.9828, 0.0172], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,960][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.1022, 0.8978], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,961][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.5096, 0.4904], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,963][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.6983, 0.3017], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,964][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.8350, 0.1650], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,966][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.6477, 0.3523], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,968][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.1012, 0.8988], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,969][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.1011, 0.8989], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:30,971][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4405, 0.2874, 0.2721], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,972][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0216, 0.3738, 0.6046], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,974][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.3757, 0.3175, 0.3068], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,975][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.5260, 0.2087, 0.2652], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,977][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9722, 0.0164, 0.0114], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,978][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0506, 0.4665, 0.4829], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,980][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ and] are: tensor([9.6281e-03, 9.9024e-01, 1.3640e-04], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,981][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.4258, 0.1356, 0.4387], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,983][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6772, 0.1440, 0.1788], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,984][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.5391, 0.3959, 0.0649], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,986][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0508, 0.4994, 0.4498], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,988][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0560, 0.4702, 0.4737], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:30,989][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.3268, 0.2238, 0.2110, 0.2384], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,991][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0169, 0.2722, 0.4086, 0.3022], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,992][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.2900, 0.2351, 0.2338, 0.2411], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,994][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.4102, 0.1426, 0.2050, 0.2421], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,995][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.9531, 0.0188, 0.0132, 0.0149], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,997][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0310, 0.3064, 0.3302, 0.3323], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,999][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0252, 0.8216, 0.1416, 0.0117], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:30,999][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.4505, 0.0875, 0.2649, 0.1971], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,000][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.6172, 0.0995, 0.1350, 0.1483], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,001][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.1316, 0.6495, 0.1800, 0.0389], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,002][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0289, 0.3532, 0.3168, 0.3011], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,003][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0344, 0.3355, 0.3339, 0.2961], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,005][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.2678, 0.1817, 0.1731, 0.1926, 0.1847], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,006][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0082, 0.2116, 0.3006, 0.2314, 0.2481], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,008][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.2286, 0.1948, 0.1888, 0.2000, 0.1878], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,009][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.3163, 0.1163, 0.1727, 0.2016, 0.1930], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,011][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.9401, 0.0185, 0.0129, 0.0148, 0.0137], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,013][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0188, 0.2270, 0.2424, 0.2497, 0.2620], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,014][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0156, 0.3491, 0.1607, 0.4691, 0.0055], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,016][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.2945, 0.0495, 0.2148, 0.1718, 0.2694], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,017][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.4466, 0.0997, 0.1257, 0.1617, 0.1663], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,019][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.7152, 0.0758, 0.1135, 0.0421, 0.0535], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,020][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0235, 0.2831, 0.2523, 0.2418, 0.1994], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,022][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0234, 0.2517, 0.2512, 0.2225, 0.2512], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,024][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2154, 0.1548, 0.1469, 0.1642, 0.1558, 0.1628], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,025][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0064, 0.1721, 0.2565, 0.1881, 0.2088, 0.1682], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,027][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2159, 0.1633, 0.1601, 0.1668, 0.1640, 0.1300], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,029][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.2906, 0.1045, 0.1505, 0.1747, 0.1667, 0.1130], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,030][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.9530, 0.0128, 0.0086, 0.0101, 0.0095, 0.0060], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,032][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0170, 0.1841, 0.1969, 0.2019, 0.2184, 0.1816], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,033][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([1.2561e-02, 1.4124e-01, 9.2181e-03, 8.1115e-01, 2.5550e-02, 2.7942e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,034][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1507, 0.0292, 0.1216, 0.0993, 0.1690, 0.4302], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,036][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.4126, 0.0697, 0.1062, 0.1379, 0.1477, 0.1260], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,038][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.8299, 0.0657, 0.0641, 0.0259, 0.0089, 0.0054], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,039][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0182, 0.2299, 0.2089, 0.1988, 0.1657, 0.1785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,041][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0168, 0.2056, 0.2047, 0.1798, 0.2041, 0.1890], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,043][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.1818, 0.1323, 0.1268, 0.1410, 0.1346, 0.1397, 0.1438],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,044][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0052, 0.1462, 0.2300, 0.1604, 0.1827, 0.1457, 0.1297],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,046][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.1577, 0.1491, 0.1437, 0.1525, 0.1430, 0.1202, 0.1338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,048][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.2662, 0.0876, 0.1268, 0.1401, 0.1383, 0.0971, 0.1439],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,049][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.9228, 0.0175, 0.0122, 0.0140, 0.0130, 0.0085, 0.0120],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,051][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0108, 0.1526, 0.1603, 0.1749, 0.1823, 0.1528, 0.1662],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,052][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0080, 0.0237, 0.3868, 0.0601, 0.0899, 0.4287, 0.0028],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,054][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1413, 0.0251, 0.1048, 0.0869, 0.1554, 0.3788, 0.1076],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,056][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.2774, 0.0784, 0.0973, 0.1166, 0.1301, 0.1295, 0.1708],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,057][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.1299, 0.4645, 0.2697, 0.0910, 0.0131, 0.0057, 0.0261],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,057][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0128, 0.2049, 0.1826, 0.1736, 0.1416, 0.1508, 0.1338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,058][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0136, 0.1710, 0.1703, 0.1497, 0.1686, 0.1551, 0.1716],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,059][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.1592, 0.1156, 0.1116, 0.1235, 0.1179, 0.1221, 0.1264, 0.1236],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,060][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0047, 0.1269, 0.1942, 0.1400, 0.1569, 0.1254, 0.1132, 0.1387],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,061][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1510, 0.1317, 0.1270, 0.1345, 0.1265, 0.1037, 0.1182, 0.1075],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,063][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.2273, 0.0780, 0.1112, 0.1225, 0.1237, 0.0869, 0.1260, 0.1243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,065][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.9394, 0.0127, 0.0087, 0.0101, 0.0094, 0.0059, 0.0087, 0.0050],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,066][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0126, 0.1299, 0.1385, 0.1470, 0.1578, 0.1347, 0.1517, 0.1279],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,067][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ at] are: tensor([1.9501e-02, 5.8217e-02, 6.4678e-03, 8.4252e-01, 3.3523e-02, 9.3110e-03,
        3.0405e-02, 5.7650e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,069][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1245, 0.0283, 0.0865, 0.0811, 0.1414, 0.3292, 0.0970, 0.1120],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,071][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.3219, 0.0537, 0.0734, 0.1002, 0.1040, 0.0933, 0.1470, 0.1064],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,072][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.6311, 0.0834, 0.0894, 0.0385, 0.0189, 0.0164, 0.0947, 0.0276],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,074][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0143, 0.1671, 0.1528, 0.1457, 0.1236, 0.1333, 0.1181, 0.1450],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,076][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0116, 0.1456, 0.1473, 0.1281, 0.1436, 0.1331, 0.1450, 0.1457],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,077][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1389, 0.1032, 0.0988, 0.1094, 0.1046, 0.1088, 0.1114, 0.1095, 0.1153],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,079][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0044, 0.1123, 0.1701, 0.1226, 0.1357, 0.1108, 0.0990, 0.1215, 0.1237],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,081][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1474, 0.1141, 0.1116, 0.1176, 0.1138, 0.0919, 0.1074, 0.0982, 0.0978],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,082][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2076, 0.0651, 0.0956, 0.1055, 0.1071, 0.0758, 0.1150, 0.1141, 0.1141],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,084][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.9377, 0.0121, 0.0081, 0.0096, 0.0090, 0.0057, 0.0083, 0.0048, 0.0046],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,085][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0112, 0.1174, 0.1242, 0.1280, 0.1368, 0.1162, 0.1323, 0.1203, 0.1136],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,087][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([1.2016e-02, 1.2156e-01, 1.6500e-03, 5.8913e-01, 2.1200e-02, 2.2749e-04,
        2.5041e-01, 3.5682e-03, 2.4704e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,088][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1120, 0.0265, 0.0817, 0.0608, 0.1189, 0.3411, 0.1040, 0.0972, 0.0579],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,090][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3032, 0.0426, 0.0614, 0.0941, 0.0888, 0.0841, 0.1306, 0.0940, 0.1013],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,092][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.6983, 0.0634, 0.0772, 0.0266, 0.0075, 0.0068, 0.0905, 0.0180, 0.0118],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,093][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0134, 0.1440, 0.1328, 0.1265, 0.1082, 0.1174, 0.1034, 0.1257, 0.1287],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,095][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0098, 0.1283, 0.1284, 0.1114, 0.1253, 0.1158, 0.1268, 0.1274, 0.1268],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,097][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.1274, 0.0921, 0.0890, 0.0987, 0.0941, 0.0980, 0.0998, 0.0983, 0.1034,
        0.0992], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,098][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0043, 0.0962, 0.1568, 0.1075, 0.1222, 0.0969, 0.0863, 0.1100, 0.1117,
        0.1081], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,100][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.1208, 0.1049, 0.1072, 0.1101, 0.1032, 0.0869, 0.0945, 0.0878, 0.0889,
        0.0957], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,102][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.1887, 0.0628, 0.0869, 0.0946, 0.0918, 0.0677, 0.0987, 0.0962, 0.0992,
        0.1134], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,104][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.9172, 0.0145, 0.0099, 0.0117, 0.0109, 0.0070, 0.0101, 0.0059, 0.0056,
        0.0071], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,105][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0107, 0.1013, 0.1085, 0.1106, 0.1191, 0.1053, 0.1157, 0.1055, 0.1021,
        0.1211], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,107][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0081, 0.0873, 0.0308, 0.0419, 0.0306, 0.0790, 0.0499, 0.0369, 0.6317,
        0.0037], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,108][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0916, 0.0232, 0.0754, 0.0633, 0.1104, 0.2936, 0.0977, 0.0881, 0.0472,
        0.1094], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,110][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.2190, 0.0402, 0.0567, 0.0876, 0.0899, 0.0821, 0.1143, 0.0866, 0.0998,
        0.1238], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,112][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.3179, 0.2635, 0.2338, 0.0873, 0.0067, 0.0051, 0.0480, 0.0128, 0.0091,
        0.0158], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,113][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0089, 0.1376, 0.1244, 0.1160, 0.0940, 0.1008, 0.0865, 0.1146, 0.1171,
        0.1002], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,114][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0086, 0.1142, 0.1147, 0.0996, 0.1104, 0.1017, 0.1112, 0.1110, 0.1102,
        0.1184], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,115][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.1081, 0.0848, 0.0817, 0.0891, 0.0851, 0.0885, 0.0909, 0.0894, 0.0943,
        0.0909, 0.0972], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,116][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0029, 0.0896, 0.1445, 0.1000, 0.1124, 0.0888, 0.0776, 0.0990, 0.1011,
        0.0978, 0.0864], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,117][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1205, 0.0929, 0.0934, 0.0962, 0.0942, 0.0765, 0.0878, 0.0810, 0.0808,
        0.0907, 0.0860], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,119][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1713, 0.0505, 0.0727, 0.0790, 0.0798, 0.0594, 0.0896, 0.0861, 0.0899,
        0.1061, 0.1156], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,120][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.9099, 0.0145, 0.0099, 0.0117, 0.0109, 0.0072, 0.0103, 0.0061, 0.0058,
        0.0073, 0.0063], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,122][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0089, 0.0923, 0.0968, 0.1032, 0.1096, 0.0936, 0.1072, 0.0943, 0.0920,
        0.1143, 0.0878], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,123][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [,] are: tensor([2.7722e-03, 2.4079e-01, 1.0982e-04, 4.2303e-01, 3.7219e-02, 4.9023e-03,
        3.9767e-02, 9.0873e-05, 2.1074e-02, 2.3019e-01, 5.6933e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,125][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0775, 0.0222, 0.0646, 0.0515, 0.0984, 0.2787, 0.1009, 0.0809, 0.0523,
        0.1112, 0.0618], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,126][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.2007, 0.0436, 0.0562, 0.0749, 0.0757, 0.0701, 0.1009, 0.0795, 0.0826,
        0.1111, 0.1048], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,128][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.3314, 0.1719, 0.1775, 0.1034, 0.0116, 0.0105, 0.1028, 0.0237, 0.0147,
        0.0369, 0.0156], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,130][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0096, 0.1161, 0.1076, 0.1019, 0.0868, 0.0947, 0.0829, 0.1021, 0.1037,
        0.0919, 0.1028], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,131][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0063, 0.1036, 0.1043, 0.0886, 0.1012, 0.0929, 0.1026, 0.1033, 0.1029,
        0.1106, 0.0839], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,133][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.1036, 0.0765, 0.0737, 0.0816, 0.0776, 0.0808, 0.0822, 0.0809, 0.0852,
        0.0816, 0.0879, 0.0883], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,135][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0038, 0.0817, 0.1320, 0.0906, 0.0994, 0.0812, 0.0724, 0.0914, 0.0932,
        0.0910, 0.0826, 0.0809], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,137][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.1035, 0.0849, 0.0846, 0.0874, 0.0879, 0.0735, 0.0825, 0.0746, 0.0763,
        0.0879, 0.0811, 0.0759], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,138][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.1517, 0.0493, 0.0678, 0.0727, 0.0725, 0.0533, 0.0795, 0.0781, 0.0793,
        0.0913, 0.1005, 0.1039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,140][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.8905, 0.0166, 0.0112, 0.0131, 0.0123, 0.0081, 0.0113, 0.0068, 0.0064,
        0.0081, 0.0071, 0.0085], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,142][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0067, 0.0816, 0.0881, 0.0894, 0.1044, 0.0833, 0.0999, 0.0875, 0.0831,
        0.1056, 0.0818, 0.0887], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,144][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0212, 0.3015, 0.0896, 0.0072, 0.0420, 0.0450, 0.0644, 0.0208, 0.1396,
        0.0267, 0.2321, 0.0099], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,145][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0911, 0.0192, 0.0669, 0.0520, 0.0904, 0.2571, 0.0930, 0.0712, 0.0451,
        0.1050, 0.0528, 0.0562], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,147][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.2119, 0.0342, 0.0468, 0.0585, 0.0639, 0.0600, 0.0907, 0.0665, 0.0709,
        0.0990, 0.0925, 0.1050], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,148][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([1.1232e-02, 7.7816e-01, 1.6783e-01, 2.7111e-02, 1.0583e-03, 2.8670e-04,
        5.2114e-03, 7.9174e-04, 6.6133e-04, 4.2254e-03, 1.0803e-03, 2.3589e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,150][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0085, 0.1110, 0.1000, 0.0949, 0.0787, 0.0843, 0.0734, 0.0929, 0.0947,
        0.0816, 0.0924, 0.0876], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,152][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0060, 0.0947, 0.0955, 0.0819, 0.0928, 0.0853, 0.0940, 0.0945, 0.0937,
        0.1013, 0.0762, 0.0841], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,153][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0955, 0.0707, 0.0675, 0.0748, 0.0712, 0.0742, 0.0761, 0.0744, 0.0784,
        0.0752, 0.0806, 0.0813, 0.0801], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,155][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0030, 0.0808, 0.1186, 0.0865, 0.0939, 0.0741, 0.0666, 0.0816, 0.0844,
        0.0830, 0.0726, 0.0772, 0.0778], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,157][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0903, 0.0808, 0.0802, 0.0835, 0.0794, 0.0683, 0.0742, 0.0686, 0.0707,
        0.0781, 0.0739, 0.0704, 0.0817], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,159][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.1401, 0.0404, 0.0581, 0.0613, 0.0615, 0.0479, 0.0727, 0.0710, 0.0730,
        0.0839, 0.0913, 0.0976, 0.1013], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,160][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.8871, 0.0157, 0.0107, 0.0126, 0.0117, 0.0077, 0.0111, 0.0065, 0.0062,
        0.0079, 0.0068, 0.0083, 0.0077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,162][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0067, 0.0761, 0.0816, 0.0842, 0.0891, 0.0789, 0.0892, 0.0794, 0.0766,
        0.0952, 0.0746, 0.0816, 0.0868], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,164][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0069, 0.0447, 0.0772, 0.0593, 0.0066, 0.0381, 0.0099, 0.0116, 0.4872,
        0.0180, 0.1562, 0.0826, 0.0016], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,166][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0700, 0.0206, 0.0669, 0.0576, 0.0950, 0.2479, 0.0821, 0.0718, 0.0415,
        0.0825, 0.0542, 0.0573, 0.0528], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,167][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.1505, 0.0372, 0.0458, 0.0610, 0.0634, 0.0578, 0.0826, 0.0630, 0.0700,
        0.0941, 0.0905, 0.1089, 0.0752], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,169][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.1162, 0.2819, 0.2941, 0.1202, 0.0059, 0.0038, 0.0393, 0.0123, 0.0068,
        0.0163, 0.0081, 0.0823, 0.0127], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,171][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0078, 0.1003, 0.0909, 0.0861, 0.0736, 0.0785, 0.0687, 0.0861, 0.0871,
        0.0760, 0.0858, 0.0798, 0.0793], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,172][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0058, 0.0871, 0.0877, 0.0751, 0.0849, 0.0782, 0.0860, 0.0863, 0.0857,
        0.0914, 0.0699, 0.0767, 0.0850], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,172][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0876, 0.0648, 0.0623, 0.0691, 0.0655, 0.0683, 0.0707, 0.0692, 0.0727,
        0.0700, 0.0750, 0.0751, 0.0747, 0.0751], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,173][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0023, 0.0699, 0.1112, 0.0791, 0.0882, 0.0702, 0.0624, 0.0781, 0.0811,
        0.0786, 0.0689, 0.0703, 0.0707, 0.0689], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,175][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0920, 0.0740, 0.0727, 0.0756, 0.0742, 0.0612, 0.0701, 0.0641, 0.0643,
        0.0726, 0.0680, 0.0659, 0.0787, 0.0666], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,176][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1251, 0.0358, 0.0527, 0.0543, 0.0574, 0.0441, 0.0656, 0.0647, 0.0660,
        0.0778, 0.0838, 0.0915, 0.0969, 0.0843], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,178][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.8968, 0.0135, 0.0092, 0.0109, 0.0102, 0.0066, 0.0095, 0.0056, 0.0053,
        0.0069, 0.0058, 0.0072, 0.0068, 0.0057], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,179][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0072, 0.0696, 0.0748, 0.0784, 0.0826, 0.0724, 0.0824, 0.0723, 0.0715,
        0.0866, 0.0691, 0.0765, 0.0857, 0.0708], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,181][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([1.5849e-03, 7.2330e-02, 1.4134e-04, 2.2537e-01, 5.9027e-02, 2.5140e-03,
        2.7171e-02, 1.0117e-04, 2.6879e-02, 2.7579e-01, 7.6551e-04, 2.2729e-01,
        8.1009e-02, 3.1877e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,182][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0913, 0.0201, 0.0702, 0.0472, 0.0762, 0.2251, 0.0780, 0.0721, 0.0473,
        0.0911, 0.0608, 0.0457, 0.0499, 0.0248], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,183][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.1634, 0.0285, 0.0384, 0.0548, 0.0531, 0.0496, 0.0758, 0.0579, 0.0612,
        0.0803, 0.0800, 0.0967, 0.0677, 0.0926], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,184][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3395, 0.0893, 0.1255, 0.0652, 0.0076, 0.0064, 0.1044, 0.0151, 0.0107,
        0.0340, 0.0132, 0.1338, 0.0470, 0.0082], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,185][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0078, 0.0871, 0.0821, 0.0780, 0.0680, 0.0738, 0.0657, 0.0791, 0.0795,
        0.0730, 0.0790, 0.0732, 0.0720, 0.0814], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,186][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0052, 0.0799, 0.0811, 0.0687, 0.0785, 0.0726, 0.0800, 0.0808, 0.0801,
        0.0852, 0.0655, 0.0709, 0.0797, 0.0717], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,187][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0820, 0.0606, 0.0579, 0.0643, 0.0611, 0.0639, 0.0656, 0.0644, 0.0675,
        0.0647, 0.0697, 0.0699, 0.0689, 0.0698, 0.0698], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,188][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0022, 0.0671, 0.1047, 0.0742, 0.0816, 0.0654, 0.0582, 0.0745, 0.0753,
        0.0738, 0.0636, 0.0642, 0.0643, 0.0637, 0.0672], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,189][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0810, 0.0711, 0.0697, 0.0736, 0.0678, 0.0585, 0.0635, 0.0598, 0.0610,
        0.0665, 0.0640, 0.0619, 0.0714, 0.0631, 0.0671], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,191][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.1199, 0.0333, 0.0499, 0.0512, 0.0511, 0.0398, 0.0591, 0.0584, 0.0600,
        0.0710, 0.0762, 0.0826, 0.0879, 0.0775, 0.0821], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,193][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.8807, 0.0147, 0.0101, 0.0118, 0.0109, 0.0072, 0.0102, 0.0061, 0.0058,
        0.0074, 0.0064, 0.0077, 0.0072, 0.0062, 0.0076], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,194][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0049, 0.0625, 0.0676, 0.0707, 0.0784, 0.0649, 0.0772, 0.0692, 0.0658,
        0.0815, 0.0636, 0.0704, 0.0826, 0.0668, 0.0739], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,196][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0117, 0.0310, 0.0275, 0.1815, 0.0052, 0.0064, 0.0176, 0.0094, 0.0796,
        0.1288, 0.0499, 0.1777, 0.0298, 0.2403, 0.0037], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,198][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0644, 0.0170, 0.0596, 0.0465, 0.0831, 0.2314, 0.0842, 0.0718, 0.0408,
        0.0868, 0.0526, 0.0494, 0.0482, 0.0241, 0.0402], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,200][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.1438, 0.0280, 0.0385, 0.0491, 0.0462, 0.0459, 0.0681, 0.0543, 0.0535,
        0.0765, 0.0758, 0.0839, 0.0605, 0.0916, 0.0842], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,201][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.2031, 0.2005, 0.2853, 0.1028, 0.0084, 0.0059, 0.0644, 0.0095, 0.0083,
        0.0264, 0.0115, 0.0356, 0.0249, 0.0067, 0.0068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,203][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0060, 0.0852, 0.0786, 0.0741, 0.0629, 0.0663, 0.0584, 0.0746, 0.0744,
        0.0667, 0.0736, 0.0696, 0.0689, 0.0772, 0.0633], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,205][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0046, 0.0743, 0.0757, 0.0639, 0.0735, 0.0679, 0.0752, 0.0755, 0.0749,
        0.0797, 0.0610, 0.0665, 0.0744, 0.0667, 0.0660], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,207][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0766, 0.0561, 0.0540, 0.0604, 0.0572, 0.0596, 0.0615, 0.0606, 0.0632,
        0.0607, 0.0655, 0.0656, 0.0648, 0.0652, 0.0653, 0.0639],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,208][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0022, 0.0619, 0.1006, 0.0689, 0.0773, 0.0623, 0.0546, 0.0698, 0.0705,
        0.0683, 0.0608, 0.0605, 0.0611, 0.0599, 0.0644, 0.0570],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,210][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0776, 0.0656, 0.0652, 0.0685, 0.0644, 0.0530, 0.0604, 0.0556, 0.0566,
        0.0622, 0.0603, 0.0580, 0.0685, 0.0592, 0.0648, 0.0603],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,212][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.1079, 0.0312, 0.0479, 0.0498, 0.0491, 0.0370, 0.0556, 0.0554, 0.0561,
        0.0665, 0.0710, 0.0766, 0.0821, 0.0718, 0.0766, 0.0653],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,214][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.8910, 0.0132, 0.0088, 0.0105, 0.0096, 0.0062, 0.0090, 0.0052, 0.0049,
        0.0064, 0.0054, 0.0067, 0.0062, 0.0053, 0.0066, 0.0049],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,215][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0052, 0.0621, 0.0661, 0.0687, 0.0724, 0.0619, 0.0703, 0.0639, 0.0606,
        0.0735, 0.0609, 0.0663, 0.0723, 0.0638, 0.0723, 0.0599],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,217][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0074, 0.0504, 0.0024, 0.1979, 0.0320, 0.0003, 0.0435, 0.0246, 0.0006,
        0.1657, 0.0070, 0.1917, 0.2081, 0.0075, 0.0596, 0.0013],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,219][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0686, 0.0119, 0.0445, 0.0354, 0.0699, 0.2317, 0.0877, 0.0767, 0.0409,
        0.1021, 0.0574, 0.0461, 0.0481, 0.0253, 0.0384, 0.0152],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,221][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.1233, 0.0233, 0.0326, 0.0496, 0.0457, 0.0422, 0.0627, 0.0498, 0.0523,
        0.0689, 0.0680, 0.0848, 0.0569, 0.0805, 0.0793, 0.0801],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,223][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.1462, 0.1648, 0.2473, 0.0956, 0.0042, 0.0033, 0.1344, 0.0095, 0.0065,
        0.0300, 0.0070, 0.0953, 0.0329, 0.0040, 0.0122, 0.0067],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,224][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0070, 0.0777, 0.0729, 0.0687, 0.0589, 0.0641, 0.0560, 0.0690, 0.0699,
        0.0631, 0.0696, 0.0638, 0.0626, 0.0717, 0.0580, 0.0670],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,226][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0047, 0.0695, 0.0703, 0.0600, 0.0684, 0.0633, 0.0699, 0.0704, 0.0699,
        0.0749, 0.0572, 0.0625, 0.0694, 0.0627, 0.0616, 0.0652],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,228][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0708, 0.0533, 0.0512, 0.0567, 0.0537, 0.0561, 0.0578, 0.0564, 0.0593,
        0.0569, 0.0613, 0.0612, 0.0610, 0.0612, 0.0616, 0.0606, 0.0609],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,229][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0022, 0.0586, 0.0929, 0.0667, 0.0739, 0.0591, 0.0513, 0.0653, 0.0662,
        0.0635, 0.0570, 0.0576, 0.0586, 0.0573, 0.0596, 0.0560, 0.0544],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,231][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0771, 0.0610, 0.0602, 0.0624, 0.0618, 0.0511, 0.0581, 0.0529, 0.0532,
        0.0593, 0.0564, 0.0541, 0.0645, 0.0551, 0.0613, 0.0572, 0.0544],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,233][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1043, 0.0293, 0.0437, 0.0439, 0.0461, 0.0356, 0.0524, 0.0521, 0.0523,
        0.0615, 0.0646, 0.0713, 0.0756, 0.0661, 0.0709, 0.0607, 0.0696],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,234][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.8819, 0.0131, 0.0090, 0.0106, 0.0100, 0.0065, 0.0094, 0.0054, 0.0052,
        0.0067, 0.0056, 0.0069, 0.0065, 0.0055, 0.0069, 0.0052, 0.0055],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,235][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0062, 0.0567, 0.0611, 0.0637, 0.0667, 0.0592, 0.0668, 0.0590, 0.0583,
        0.0698, 0.0564, 0.0620, 0.0686, 0.0576, 0.0669, 0.0604, 0.0607],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,236][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([7.2894e-04, 2.6853e-02, 4.4782e-05, 8.6143e-02, 3.2359e-02, 1.1402e-03,
        1.1685e-02, 3.6734e-05, 1.1598e-02, 9.0506e-02, 3.1213e-04, 8.4769e-02,
        2.5531e-02, 1.2126e-05, 7.1006e-03, 6.2117e-01, 1.1713e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,237][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1109, 0.0159, 0.0556, 0.0419, 0.0670, 0.2100, 0.0702, 0.0759, 0.0416,
        0.0838, 0.0571, 0.0400, 0.0437, 0.0226, 0.0310, 0.0140, 0.0186],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,239][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.1401, 0.0208, 0.0304, 0.0439, 0.0396, 0.0365, 0.0577, 0.0439, 0.0456,
        0.0587, 0.0598, 0.0734, 0.0497, 0.0700, 0.0674, 0.0803, 0.0824],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,240][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.2422, 0.1420, 0.1709, 0.1086, 0.0058, 0.0042, 0.0862, 0.0107, 0.0071,
        0.0259, 0.0088, 0.1195, 0.0298, 0.0055, 0.0130, 0.0129, 0.0070],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,242][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0066, 0.0709, 0.0670, 0.0637, 0.0557, 0.0600, 0.0536, 0.0645, 0.0647,
        0.0598, 0.0644, 0.0599, 0.0586, 0.0661, 0.0552, 0.0625, 0.0669],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,244][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0047, 0.0652, 0.0660, 0.0561, 0.0642, 0.0595, 0.0659, 0.0663, 0.0656,
        0.0699, 0.0539, 0.0582, 0.0653, 0.0590, 0.0582, 0.0610, 0.0610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,313][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:31,325][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,326][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,327][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,328][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,329][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,331][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,332][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,333][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,334][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,335][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,336][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,337][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,339][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.4176, 0.5824], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,340][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.9528, 0.0472], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,341][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.6617, 0.3383], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,343][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.5472, 0.4528], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,344][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.7500, 0.2500], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,346][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.9727, 0.0273], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,347][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.8360, 0.1640], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,348][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.9080, 0.0920], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,350][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.8869, 0.1131], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,351][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.9843, 0.0157], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,352][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.9759, 0.0241], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,354][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.9669, 0.0331], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,355][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.4197, 0.2796, 0.3007], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,357][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.9889, 0.0053, 0.0058], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,358][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.4709, 0.3695, 0.1596], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,359][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.3492, 0.2913, 0.3595], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,361][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.6583, 0.0839, 0.2578], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,362][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.9397, 0.0302, 0.0301], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,364][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.8246, 0.0708, 0.1046], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,364][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.8229, 0.0445, 0.1326], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,364][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.8926, 0.0615, 0.0459], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,365][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.9715, 0.0150, 0.0135], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,365][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9421, 0.0339, 0.0240], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,365][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.9381, 0.0166, 0.0454], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,366][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2305, 0.1314, 0.2817, 0.3565], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,366][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([9.9622e-01, 1.4832e-03, 8.3149e-04, 1.4690e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,366][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.4993, 0.1813, 0.1812, 0.1382], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,367][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.3525, 0.1929, 0.2919, 0.1627], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,368][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.4927, 0.0976, 0.2691, 0.1406], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,369][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.9877, 0.0051, 0.0041, 0.0031], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,370][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.6183, 0.1998, 0.1577, 0.0242], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,371][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.7897, 0.0439, 0.0921, 0.0743], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,373][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.8729, 0.0339, 0.0619, 0.0313], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,374][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.9347, 0.0400, 0.0179, 0.0074], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,375][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.9509, 0.0160, 0.0185, 0.0146], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,377][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.9055, 0.0285, 0.0403, 0.0258], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,378][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.1321, 0.1644, 0.2043, 0.2405, 0.2588], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,380][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.9578, 0.0145, 0.0054, 0.0154, 0.0070], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,381][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.1283, 0.2119, 0.1493, 0.2982, 0.2123], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,383][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0763, 0.1366, 0.2388, 0.1923, 0.3560], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,384][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.1799, 0.0590, 0.2359, 0.1599, 0.3653], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,386][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.8071, 0.0663, 0.0343, 0.0573, 0.0351], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,387][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.4664, 0.0805, 0.2533, 0.0570, 0.1428], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,388][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.5866, 0.0315, 0.1384, 0.0724, 0.1711], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,389][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.6562, 0.1050, 0.0536, 0.1107, 0.0745], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,391][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.8950, 0.0272, 0.0261, 0.0219, 0.0298], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,392][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.8511, 0.0460, 0.0406, 0.0332, 0.0291], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,394][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.7495, 0.0396, 0.0684, 0.0281, 0.1144], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,395][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0877, 0.1380, 0.1848, 0.1772, 0.2124, 0.1999], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,396][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.9800, 0.0074, 0.0011, 0.0062, 0.0037, 0.0016], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,398][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1849, 0.1096, 0.1079, 0.1389, 0.2917, 0.1670], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,399][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0877, 0.0799, 0.1531, 0.0991, 0.3880, 0.1921], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,400][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1769, 0.0394, 0.1009, 0.0926, 0.4131, 0.1770], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,402][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.8625, 0.0357, 0.0196, 0.0348, 0.0369, 0.0105], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,403][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.6099, 0.0347, 0.0873, 0.0197, 0.1984, 0.0500], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,405][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.4612, 0.0303, 0.0937, 0.0734, 0.2108, 0.1306], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,406][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.5614, 0.0694, 0.0458, 0.0978, 0.1588, 0.0668], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,407][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.9387, 0.0168, 0.0104, 0.0113, 0.0150, 0.0079], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,409][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.8996, 0.0224, 0.0248, 0.0201, 0.0151, 0.0179], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,410][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.8123, 0.0378, 0.0408, 0.0267, 0.0388, 0.0437], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,412][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0264, 0.0914, 0.1153, 0.1500, 0.1670, 0.3107, 0.1393],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,413][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.5674, 0.0403, 0.0458, 0.0597, 0.0751, 0.0908, 0.1207],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,415][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0193, 0.1449, 0.0937, 0.1555, 0.2068, 0.3249, 0.0548],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,416][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0133, 0.0912, 0.1308, 0.1256, 0.2464, 0.3342, 0.0585],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,417][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0101, 0.0369, 0.1163, 0.0947, 0.3706, 0.3168, 0.0546],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,419][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.4901, 0.0602, 0.0784, 0.0756, 0.0808, 0.1054, 0.1095],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,420][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0742, 0.0378, 0.1655, 0.0479, 0.3799, 0.2246, 0.0701],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,421][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0928, 0.0357, 0.1378, 0.1018, 0.3146, 0.1752, 0.1420],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,422][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.1073, 0.1153, 0.1000, 0.1393, 0.1685, 0.2889, 0.0807],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,422][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.7462, 0.0578, 0.0412, 0.0379, 0.0427, 0.0417, 0.0324],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,423][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.7178, 0.0330, 0.0339, 0.0311, 0.0399, 0.0366, 0.1076],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,423][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.4946, 0.0720, 0.0909, 0.0695, 0.1024, 0.0677, 0.1030],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,423][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.1951, 0.0825, 0.1220, 0.1471, 0.1678, 0.0982, 0.1556, 0.0317],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,424][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.9560, 0.0049, 0.0016, 0.0066, 0.0076, 0.0024, 0.0199, 0.0010],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,424][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0223, 0.1113, 0.0986, 0.1625, 0.2057, 0.2767, 0.0590, 0.0640],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,424][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0255, 0.0683, 0.1213, 0.1062, 0.2720, 0.2326, 0.0628, 0.1113],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,425][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0303, 0.0211, 0.0842, 0.0748, 0.4452, 0.1891, 0.0700, 0.0853],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,427][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.7987, 0.0308, 0.0152, 0.0364, 0.0419, 0.0126, 0.0561, 0.0083],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,428][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.4145, 0.0176, 0.1038, 0.0267, 0.2649, 0.0746, 0.0640, 0.0338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,429][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.2450, 0.0315, 0.0741, 0.0991, 0.2990, 0.1005, 0.0968, 0.0542],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,431][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.3840, 0.0641, 0.0473, 0.0947, 0.1720, 0.1120, 0.0860, 0.0398],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,432][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.8708, 0.0251, 0.0162, 0.0133, 0.0183, 0.0168, 0.0297, 0.0097],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,434][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.7904, 0.0380, 0.0293, 0.0324, 0.0228, 0.0268, 0.0344, 0.0259],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,435][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.7117, 0.0286, 0.0535, 0.0299, 0.0522, 0.0392, 0.0365, 0.0484],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,436][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0476, 0.1232, 0.1362, 0.1498, 0.1867, 0.1276, 0.1218, 0.0287, 0.0784],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,438][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([9.6189e-01, 6.3293e-03, 8.7580e-04, 6.5252e-03, 3.9219e-03, 2.0986e-03,
        1.6505e-02, 6.5213e-04, 1.1996e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,439][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0539, 0.0619, 0.0758, 0.1044, 0.2198, 0.1979, 0.0623, 0.0902, 0.1338],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,440][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0374, 0.0481, 0.0864, 0.0755, 0.2836, 0.1564, 0.0616, 0.0956, 0.1554],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,442][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0573, 0.0184, 0.0640, 0.0586, 0.3210, 0.2159, 0.0634, 0.0826, 0.1188],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,443][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.8010, 0.0281, 0.0107, 0.0334, 0.0282, 0.0140, 0.0588, 0.0128, 0.0129],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,445][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.5898, 0.0212, 0.0520, 0.0136, 0.1483, 0.0417, 0.0591, 0.0293, 0.0449],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,446][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2265, 0.0235, 0.0775, 0.0640, 0.2190, 0.1109, 0.1070, 0.0735, 0.0981],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,447][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.3394, 0.0564, 0.0385, 0.1078, 0.1377, 0.1346, 0.0752, 0.0503, 0.0602],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,449][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.9157, 0.0138, 0.0080, 0.0092, 0.0100, 0.0077, 0.0202, 0.0063, 0.0091],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,450][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.8730, 0.0174, 0.0197, 0.0138, 0.0135, 0.0160, 0.0137, 0.0131, 0.0197],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,452][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.7324, 0.0344, 0.0375, 0.0263, 0.0467, 0.0357, 0.0277, 0.0217, 0.0377],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,453][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0175, 0.0628, 0.0834, 0.1134, 0.1270, 0.2424, 0.0656, 0.0680, 0.1437,
        0.0762], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,455][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.9407, 0.0049, 0.0029, 0.0088, 0.0098, 0.0056, 0.0148, 0.0026, 0.0048,
        0.0051], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,456][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0254, 0.0917, 0.0736, 0.0948, 0.1809, 0.2417, 0.0532, 0.0573, 0.1309,
        0.0505], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,458][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0162, 0.0453, 0.0747, 0.0640, 0.1961, 0.1902, 0.0493, 0.0926, 0.1924,
        0.0791], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,459][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0279, 0.0173, 0.0700, 0.0555, 0.2537, 0.2431, 0.0477, 0.1010, 0.1416,
        0.0422], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,460][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.7144, 0.0345, 0.0186, 0.0344, 0.0524, 0.0226, 0.0514, 0.0170, 0.0311,
        0.0235], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,462][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.2572, 0.0324, 0.0850, 0.0263, 0.1918, 0.1052, 0.0940, 0.0447, 0.1090,
        0.0545], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,463][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.1772, 0.0225, 0.0794, 0.0870, 0.1391, 0.1498, 0.0853, 0.0937, 0.1013,
        0.0646], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,465][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.2153, 0.0554, 0.0496, 0.1165, 0.1728, 0.1529, 0.0582, 0.0456, 0.0944,
        0.0392], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,466][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.8295, 0.0247, 0.0153, 0.0199, 0.0194, 0.0140, 0.0327, 0.0104, 0.0162,
        0.0178], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,468][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.8105, 0.0193, 0.0199, 0.0172, 0.0126, 0.0160, 0.0210, 0.0205, 0.0205,
        0.0426], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,469][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.6129, 0.0373, 0.0486, 0.0349, 0.0522, 0.0356, 0.0416, 0.0376, 0.0401,
        0.0592], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,470][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0638, 0.1377, 0.1099, 0.1383, 0.1335, 0.1070, 0.0991, 0.0263, 0.0636,
        0.0657, 0.0552], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,472][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.8969, 0.0151, 0.0043, 0.0177, 0.0114, 0.0074, 0.0295, 0.0027, 0.0048,
        0.0054, 0.0047], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,473][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0316, 0.0480, 0.0633, 0.0832, 0.1759, 0.1648, 0.0480, 0.0593, 0.1299,
        0.0722, 0.1238], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,475][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0235, 0.0266, 0.0558, 0.0421, 0.1815, 0.1029, 0.0450, 0.0838, 0.1236,
        0.0835, 0.2318], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,476][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0356, 0.0138, 0.0516, 0.0457, 0.2058, 0.1751, 0.0559, 0.0721, 0.1299,
        0.0610, 0.1535], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,478][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.5480, 0.0436, 0.0347, 0.0433, 0.0681, 0.0347, 0.0977, 0.0290, 0.0336,
        0.0298, 0.0375], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,479][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.5068, 0.0274, 0.0541, 0.0216, 0.0976, 0.0489, 0.0522, 0.0203, 0.0459,
        0.0713, 0.0537], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,480][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1576, 0.0189, 0.0755, 0.0569, 0.1169, 0.0915, 0.0742, 0.0617, 0.0770,
        0.0582, 0.2116], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,480][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.2345, 0.0564, 0.0452, 0.0827, 0.1343, 0.1279, 0.0749, 0.0560, 0.0799,
        0.0462, 0.0619], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,481][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.7917, 0.0223, 0.0183, 0.0178, 0.0203, 0.0160, 0.0309, 0.0137, 0.0174,
        0.0204, 0.0313], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,481][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.6747, 0.0331, 0.0359, 0.0255, 0.0164, 0.0248, 0.0167, 0.0236, 0.0262,
        0.0319, 0.0912], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,481][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.6523, 0.0240, 0.0456, 0.0238, 0.0475, 0.0310, 0.0269, 0.0293, 0.0244,
        0.0344, 0.0608], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,482][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0443, 0.0382, 0.0760, 0.0954, 0.0961, 0.1605, 0.0873, 0.0347, 0.0708,
        0.0785, 0.0662, 0.1520], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,482][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([9.7062e-01, 3.4881e-03, 1.2133e-03, 3.1479e-03, 3.4394e-03, 1.3967e-03,
        8.7390e-03, 8.0488e-04, 7.0993e-04, 1.4425e-03, 1.8771e-03, 3.1177e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,483][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0251, 0.0609, 0.0670, 0.0623, 0.1355, 0.1891, 0.0388, 0.0543, 0.1159,
        0.0550, 0.1496, 0.0464], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,484][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0132, 0.0286, 0.0516, 0.0351, 0.1769, 0.1265, 0.0387, 0.0794, 0.1477,
        0.0612, 0.2008, 0.0402], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,485][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0124, 0.0148, 0.0485, 0.0314, 0.2078, 0.2052, 0.0353, 0.0729, 0.1237,
        0.0603, 0.1608, 0.0268], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,487][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.9148, 0.0111, 0.0048, 0.0069, 0.0093, 0.0048, 0.0249, 0.0038, 0.0033,
        0.0045, 0.0041, 0.0076], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,488][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.2485, 0.0381, 0.0668, 0.0113, 0.1302, 0.1123, 0.0470, 0.0344, 0.1184,
        0.0492, 0.1337, 0.0101], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,490][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.1469, 0.0309, 0.0735, 0.0682, 0.0944, 0.1057, 0.0732, 0.0719, 0.0741,
        0.0485, 0.1506, 0.0619], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,491][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.2653, 0.0402, 0.0529, 0.0509, 0.1607, 0.1100, 0.0653, 0.0445, 0.0602,
        0.0391, 0.0765, 0.0343], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,493][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.7587, 0.0432, 0.0183, 0.0087, 0.0207, 0.0146, 0.0313, 0.0107, 0.0205,
        0.0255, 0.0363, 0.0114], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,494][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.7458, 0.0148, 0.0205, 0.0177, 0.0147, 0.0159, 0.0153, 0.0197, 0.0172,
        0.0299, 0.0584, 0.0301], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,496][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.5220, 0.0429, 0.0521, 0.0369, 0.0507, 0.0479, 0.0295, 0.0274, 0.0330,
        0.0536, 0.0622, 0.0419], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,497][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0167, 0.0844, 0.0900, 0.1039, 0.1055, 0.1359, 0.0801, 0.0256, 0.0667,
        0.0599, 0.0477, 0.1236, 0.0599], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,499][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.6537, 0.0467, 0.0161, 0.0471, 0.0350, 0.0214, 0.0660, 0.0055, 0.0120,
        0.0162, 0.0118, 0.0374, 0.0310], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,500][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0072, 0.0500, 0.0605, 0.0858, 0.1117, 0.1843, 0.0276, 0.0584, 0.1506,
        0.0511, 0.1123, 0.0535, 0.0472], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,501][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0086, 0.0275, 0.0494, 0.0410, 0.1369, 0.1344, 0.0342, 0.0572, 0.1401,
        0.0688, 0.2067, 0.0438, 0.0513], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,503][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0122, 0.0163, 0.0461, 0.0529, 0.1634, 0.1995, 0.0431, 0.0692, 0.1288,
        0.0548, 0.1085, 0.0398, 0.0656], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,504][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.3410, 0.0824, 0.0498, 0.0672, 0.0930, 0.0490, 0.0968, 0.0212, 0.0313,
        0.0317, 0.0393, 0.0581, 0.0394], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,506][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.1439, 0.0166, 0.0904, 0.0174, 0.2048, 0.0782, 0.0497, 0.0446, 0.0867,
        0.0527, 0.1268, 0.0142, 0.0740], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,507][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0941, 0.0166, 0.0626, 0.0458, 0.0846, 0.1108, 0.0611, 0.0662, 0.0783,
        0.0438, 0.1407, 0.0431, 0.1523], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,509][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.1173, 0.0683, 0.0414, 0.0864, 0.1182, 0.1483, 0.0607, 0.0340, 0.0894,
        0.0537, 0.0634, 0.0710, 0.0477], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,510][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.6799, 0.0228, 0.0223, 0.0239, 0.0252, 0.0244, 0.0299, 0.0180, 0.0245,
        0.0260, 0.0456, 0.0312, 0.0264], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,512][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.5032, 0.0468, 0.0462, 0.0271, 0.0293, 0.0437, 0.0196, 0.0262, 0.0413,
        0.0266, 0.1140, 0.0422, 0.0338], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,513][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.4849, 0.0290, 0.0572, 0.0374, 0.0712, 0.0408, 0.0292, 0.0375, 0.0364,
        0.0334, 0.0537, 0.0415, 0.0479], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,515][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1490, 0.0730, 0.0612, 0.1055, 0.0888, 0.0678, 0.0998, 0.0152, 0.0422,
        0.0574, 0.0376, 0.0996, 0.0525, 0.0505], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,516][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.9251, 0.0055, 0.0018, 0.0069, 0.0073, 0.0032, 0.0187, 0.0015, 0.0023,
        0.0035, 0.0032, 0.0065, 0.0111, 0.0034], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,518][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0451, 0.0358, 0.0447, 0.0534, 0.1326, 0.1060, 0.0387, 0.0475, 0.0977,
        0.0550, 0.0907, 0.0408, 0.0725, 0.1396], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,519][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0341, 0.0210, 0.0432, 0.0374, 0.1424, 0.0762, 0.0317, 0.0507, 0.0815,
        0.0541, 0.1677, 0.0348, 0.0614, 0.1638], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,521][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0544, 0.0088, 0.0374, 0.0346, 0.1761, 0.1030, 0.0322, 0.0378, 0.0635,
        0.0391, 0.1075, 0.0271, 0.1046, 0.1741], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,522][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.7642, 0.0196, 0.0113, 0.0157, 0.0264, 0.0107, 0.0416, 0.0073, 0.0111,
        0.0124, 0.0149, 0.0203, 0.0315, 0.0129], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,524][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.3624, 0.0180, 0.0484, 0.0147, 0.1307, 0.0354, 0.0411, 0.0198, 0.0381,
        0.0618, 0.0535, 0.0109, 0.1058, 0.0596], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,525][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1100, 0.0096, 0.0402, 0.0378, 0.0859, 0.0497, 0.0437, 0.0301, 0.0491,
        0.0315, 0.1529, 0.0346, 0.1426, 0.1824], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,527][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.4015, 0.0268, 0.0210, 0.0421, 0.0754, 0.0603, 0.0494, 0.0287, 0.0512,
        0.0232, 0.0472, 0.0429, 0.0683, 0.0620], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,528][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.8486, 0.0120, 0.0093, 0.0103, 0.0097, 0.0081, 0.0175, 0.0063, 0.0086,
        0.0117, 0.0168, 0.0140, 0.0151, 0.0121], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,530][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7482, 0.0155, 0.0206, 0.0132, 0.0116, 0.0170, 0.0090, 0.0135, 0.0154,
        0.0134, 0.0561, 0.0225, 0.0139, 0.0301], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,531][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.6488, 0.0160, 0.0374, 0.0172, 0.0399, 0.0246, 0.0191, 0.0235, 0.0174,
        0.0227, 0.0439, 0.0222, 0.0208, 0.0465], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,533][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0603, 0.0852, 0.0643, 0.0974, 0.0817, 0.0951, 0.0670, 0.0191, 0.0518,
        0.0418, 0.0397, 0.1072, 0.0459, 0.0684, 0.0749], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,534][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.8510, 0.0130, 0.0058, 0.0145, 0.0096, 0.0122, 0.0288, 0.0055, 0.0055,
        0.0075, 0.0066, 0.0108, 0.0132, 0.0098, 0.0061], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,536][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0117, 0.0369, 0.0410, 0.0585, 0.0753, 0.1331, 0.0267, 0.0578, 0.1063,
        0.0559, 0.0908, 0.0433, 0.0555, 0.1436, 0.0636], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,537][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0157, 0.0181, 0.0378, 0.0283, 0.1065, 0.0833, 0.0265, 0.0449, 0.0986,
        0.0518, 0.1528, 0.0287, 0.0598, 0.1685, 0.0787], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,538][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0201, 0.0131, 0.0437, 0.0467, 0.1274, 0.1059, 0.0320, 0.0436, 0.0785,
        0.0416, 0.0962, 0.0311, 0.0772, 0.1652, 0.0778], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,538][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.5050, 0.0504, 0.0316, 0.0451, 0.0319, 0.0412, 0.0577, 0.0242, 0.0223,
        0.0254, 0.0317, 0.0347, 0.0390, 0.0415, 0.0182], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,539][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.2505, 0.0187, 0.0723, 0.0191, 0.0959, 0.0336, 0.0339, 0.0293, 0.0408,
        0.0652, 0.0803, 0.0119, 0.0839, 0.0886, 0.0760], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,539][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0772, 0.0112, 0.0406, 0.0304, 0.0625, 0.0556, 0.0377, 0.0391, 0.0454,
        0.0276, 0.1254, 0.0272, 0.1366, 0.1773, 0.1061], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,539][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.2171, 0.0446, 0.0280, 0.0555, 0.0450, 0.1103, 0.0505, 0.0462, 0.0634,
        0.0427, 0.0609, 0.0483, 0.0509, 0.0971, 0.0396], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,540][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.6746, 0.0218, 0.0192, 0.0211, 0.0222, 0.0178, 0.0292, 0.0130, 0.0175,
        0.0232, 0.0327, 0.0219, 0.0292, 0.0289, 0.0279], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,540][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.4817, 0.0398, 0.0471, 0.0354, 0.0240, 0.0314, 0.0166, 0.0262, 0.0238,
        0.0257, 0.0767, 0.0409, 0.0334, 0.0503, 0.0469], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,542][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.4874, 0.0245, 0.0496, 0.0220, 0.0595, 0.0355, 0.0233, 0.0249, 0.0228,
        0.0255, 0.0503, 0.0265, 0.0226, 0.0446, 0.0811], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,543][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0331, 0.0456, 0.0682, 0.0790, 0.0919, 0.0881, 0.0599, 0.0307, 0.0524,
        0.0469, 0.0591, 0.0778, 0.0559, 0.0733, 0.1065, 0.0318],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,544][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.9207, 0.0050, 0.0021, 0.0070, 0.0052, 0.0033, 0.0185, 0.0024, 0.0016,
        0.0036, 0.0034, 0.0069, 0.0089, 0.0041, 0.0063, 0.0011],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,546][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0280, 0.0280, 0.0364, 0.0469, 0.1102, 0.1022, 0.0282, 0.0422, 0.0792,
        0.0387, 0.0731, 0.0325, 0.0464, 0.1478, 0.0922, 0.0680],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,547][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0240, 0.0202, 0.0379, 0.0372, 0.1290, 0.0621, 0.0280, 0.0348, 0.0720,
        0.0441, 0.1170, 0.0318, 0.0501, 0.1341, 0.1045, 0.0731],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,548][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0252, 0.0083, 0.0291, 0.0371, 0.1364, 0.0892, 0.0250, 0.0321, 0.0499,
        0.0257, 0.0787, 0.0213, 0.0579, 0.1808, 0.1026, 0.1008],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,550][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.6247, 0.0363, 0.0132, 0.0307, 0.0238, 0.0159, 0.0500, 0.0131, 0.0146,
        0.0150, 0.0265, 0.0413, 0.0282, 0.0272, 0.0298, 0.0097],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,551][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.3067, 0.0107, 0.0410, 0.0073, 0.1305, 0.0270, 0.0353, 0.0259, 0.0287,
        0.0433, 0.0622, 0.0054, 0.0686, 0.0717, 0.1081, 0.0277],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,553][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0733, 0.0069, 0.0370, 0.0309, 0.0899, 0.0356, 0.0457, 0.0235, 0.0412,
        0.0254, 0.1131, 0.0242, 0.0852, 0.1741, 0.1548, 0.0391],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,554][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.1990, 0.0287, 0.0198, 0.0615, 0.0632, 0.0781, 0.0423, 0.0333, 0.0552,
        0.0257, 0.0613, 0.0612, 0.0676, 0.1014, 0.0731, 0.0285],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,556][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.8392, 0.0072, 0.0074, 0.0099, 0.0091, 0.0064, 0.0172, 0.0069, 0.0083,
        0.0123, 0.0142, 0.0139, 0.0146, 0.0110, 0.0196, 0.0029],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,557][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.7833, 0.0079, 0.0123, 0.0080, 0.0089, 0.0125, 0.0099, 0.0092, 0.0138,
        0.0125, 0.0413, 0.0143, 0.0113, 0.0251, 0.0167, 0.0129],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,559][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.6286, 0.0313, 0.0312, 0.0203, 0.0308, 0.0231, 0.0176, 0.0171, 0.0202,
        0.0208, 0.0313, 0.0256, 0.0149, 0.0306, 0.0289, 0.0279],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,560][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0796, 0.0578, 0.0611, 0.0878, 0.0830, 0.0672, 0.0791, 0.0149, 0.0392,
        0.0503, 0.0361, 0.0791, 0.0472, 0.0486, 0.0882, 0.0366, 0.0443],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,562][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.9413, 0.0034, 0.0010, 0.0047, 0.0037, 0.0024, 0.0142, 0.0011, 0.0015,
        0.0024, 0.0020, 0.0043, 0.0078, 0.0018, 0.0035, 0.0025, 0.0024],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,563][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0273, 0.0219, 0.0300, 0.0370, 0.0941, 0.0689, 0.0290, 0.0345, 0.0639,
        0.0386, 0.0580, 0.0282, 0.0391, 0.0996, 0.0955, 0.1150, 0.1193],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,565][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0290, 0.0156, 0.0297, 0.0286, 0.1074, 0.0570, 0.0230, 0.0358, 0.0600,
        0.0406, 0.1160, 0.0265, 0.0414, 0.1178, 0.0852, 0.0773, 0.1091],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,566][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0306, 0.0054, 0.0204, 0.0216, 0.1289, 0.0611, 0.0210, 0.0221, 0.0382,
        0.0272, 0.0622, 0.0169, 0.0575, 0.1077, 0.1037, 0.1276, 0.1479],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,568][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.7570, 0.0137, 0.0089, 0.0129, 0.0208, 0.0088, 0.0361, 0.0061, 0.0090,
        0.0105, 0.0119, 0.0164, 0.0273, 0.0100, 0.0191, 0.0181, 0.0134],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,569][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.3179, 0.0113, 0.0326, 0.0091, 0.1289, 0.0279, 0.0316, 0.0140, 0.0272,
        0.0400, 0.0437, 0.0064, 0.0669, 0.0419, 0.0890, 0.0658, 0.0457],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,570][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0711, 0.0055, 0.0249, 0.0200, 0.0714, 0.0313, 0.0322, 0.0193, 0.0318,
        0.0225, 0.1063, 0.0195, 0.0965, 0.1261, 0.1776, 0.0401, 0.1040],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,572][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.3953, 0.0166, 0.0142, 0.0298, 0.0481, 0.0407, 0.0390, 0.0208, 0.0362,
        0.0177, 0.0323, 0.0308, 0.0517, 0.0426, 0.0595, 0.0704, 0.0545],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,574][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.8361, 0.0103, 0.0078, 0.0079, 0.0082, 0.0068, 0.0156, 0.0054, 0.0075,
        0.0099, 0.0144, 0.0109, 0.0125, 0.0100, 0.0193, 0.0064, 0.0109],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,575][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7675, 0.0115, 0.0159, 0.0115, 0.0095, 0.0121, 0.0070, 0.0103, 0.0108,
        0.0086, 0.0385, 0.0176, 0.0083, 0.0186, 0.0164, 0.0099, 0.0261],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,576][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.6238, 0.0128, 0.0309, 0.0127, 0.0360, 0.0216, 0.0162, 0.0185, 0.0146,
        0.0165, 0.0372, 0.0166, 0.0142, 0.0380, 0.0372, 0.0140, 0.0391],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,578][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:31,580][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 8305],
        [ 4486],
        [ 3083],
        [ 2841],
        [14584],
        [ 6356],
        [ 5896],
        [ 3224],
        [ 2501],
        [  949],
        [ 2260],
        [ 3673],
        [ 3210],
        [ 2591],
        [ 2403],
        [ 4218],
        [ 1092]], device='cuda:0')
[2024-07-24 10:21:31,581][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8121],
        [ 5544],
        [ 6523],
        [ 5849],
        [21379],
        [11196],
        [17907],
        [ 5940],
        [ 5432],
        [ 2233],
        [ 5849],
        [ 6788],
        [ 6532],
        [ 6113],
        [ 3822],
        [11780],
        [ 3998]], device='cuda:0')
[2024-07-24 10:21:31,583][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[47921],
        [46472],
        [44270],
        [43903],
        [42354],
        [41258],
        [40523],
        [39568],
        [39015],
        [38564],
        [38264],
        [38564],
        [38194],
        [38080],
        [37993],
        [37799],
        [37885]], device='cuda:0')
[2024-07-24 10:21:31,585][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[24953],
        [17617],
        [10744],
        [10248],
        [ 9780],
        [ 9821],
        [ 9830],
        [ 9827],
        [ 9798],
        [ 9800],
        [ 9992],
        [10184],
        [10431],
        [10395],
        [10571],
        [10506],
        [10556]], device='cuda:0')
[2024-07-24 10:21:31,586][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[16982],
        [28218],
        [29211],
        [28883],
        [29315],
        [28603],
        [28155],
        [28303],
        [28440],
        [28412],
        [28784],
        [28934],
        [29095],
        [29108],
        [29223],
        [28922],
        [28968]], device='cuda:0')
[2024-07-24 10:21:31,587][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[27567],
        [27991],
        [28794],
        [29721],
        [30270],
        [29907],
        [29691],
        [29955],
        [29952],
        [30035],
        [30277],
        [30197],
        [30484],
        [30673],
        [30616],
        [30632],
        [30718]], device='cuda:0')
[2024-07-24 10:21:31,589][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 9515],
        [ 9785],
        [ 9935],
        [10259],
        [10313],
        [10148],
        [10596],
        [10372],
        [10409],
        [10680],
        [10746],
        [11019],
        [11061],
        [10926],
        [11161],
        [11055],
        [11144]], device='cuda:0')
[2024-07-24 10:21:31,590][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[31124],
        [26352],
        [27211],
        [26967],
        [27299],
        [27488],
        [28572],
        [28699],
        [28435],
        [29118],
        [29372],
        [29281],
        [29298],
        [29252],
        [29323],
        [29315],
        [29325]], device='cuda:0')
[2024-07-24 10:21:31,592][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[28235],
        [33155],
        [33280],
        [32458],
        [38275],
        [40084],
        [ 7798],
        [39657],
        [40317],
        [14807],
        [44059],
        [24031],
        [17972],
        [42083],
        [38506],
        [35903],
        [23330]], device='cuda:0')
[2024-07-24 10:21:31,593][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[ 4547],
        [18690],
        [28087],
        [28067],
        [29107],
        [27270],
        [26844],
        [25826],
        [25471],
        [25230],
        [24866],
        [24922],
        [25286],
        [24852],
        [25063],
        [24358],
        [24205]], device='cuda:0')
[2024-07-24 10:21:31,595][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[39869],
        [29864],
        [24560],
        [22724],
        [18902],
        [20506],
        [19308],
        [21839],
        [21433],
        [19686],
        [19435],
        [20510],
        [20055],
        [20817],
        [20845],
        [21322],
        [22662]], device='cuda:0')
[2024-07-24 10:21:31,596][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[48631],
        [37082],
        [30113],
        [16153],
        [37989],
        [44684],
        [12750],
        [26058],
        [31260],
        [13688],
        [10762],
        [14753],
        [ 9649],
        [10635],
        [ 9417],
        [ 7403],
        [ 8992]], device='cuda:0')
[2024-07-24 10:21:31,597][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[2528],
        [4439],
        [5123],
        [5180],
        [4784],
        [4653],
        [4789],
        [4867],
        [4722],
        [4790],
        [4535],
        [4723],
        [4642],
        [4379],
        [4354],
        [4082],
        [3876]], device='cuda:0')
[2024-07-24 10:21:31,598][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[24719],
        [19181],
        [17398],
        [17482],
        [15500],
        [13783],
        [13005],
        [11821],
        [10749],
        [10014],
        [ 9309],
        [ 8915],
        [ 8499],
        [ 8232],
        [ 7872],
        [ 7761],
        [ 7727]], device='cuda:0')
[2024-07-24 10:21:31,599][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[26334],
        [34212],
        [14203],
        [27008],
        [18434],
        [14075],
        [18088],
        [17687],
        [14642],
        [23100],
        [17674],
        [25763],
        [26498],
        [16787],
        [30794],
        [16910],
        [14158]], device='cuda:0')
[2024-07-24 10:21:31,601][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[15535],
        [15222],
        [11248],
        [13130],
        [12918],
        [12359],
        [12577],
        [12642],
        [12803],
        [12393],
        [12818],
        [13220],
        [13193],
        [13251],
        [13431],
        [13082],
        [13321]], device='cuda:0')
[2024-07-24 10:21:31,602][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[27237],
        [30213],
        [27816],
        [27483],
        [29816],
        [28393],
        [38854],
        [29388],
        [29197],
        [30276],
        [32971],
        [28747],
        [38858],
        [30976],
        [34538],
        [31204],
        [30029]], device='cuda:0')
[2024-07-24 10:21:31,603][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[ 2077],
        [ 7420],
        [10067],
        [ 9319],
        [10949],
        [11820],
        [11569],
        [11851],
        [13350],
        [12352],
        [12053],
        [11934],
        [12463],
        [11192],
        [10719],
        [ 9675],
        [ 8321]], device='cuda:0')
[2024-07-24 10:21:31,605][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[18721],
        [32242],
        [33372],
        [38910],
        [32709],
        [30530],
        [41243],
        [37268],
        [36104],
        [42514],
        [41835],
        [43123],
        [43985],
        [41135],
        [40817],
        [39713],
        [39966]], device='cuda:0')
[2024-07-24 10:21:31,606][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[23992],
        [23811],
        [28873],
        [30554],
        [35240],
        [36107],
        [38584],
        [36146],
        [35974],
        [36428],
        [33346],
        [33211],
        [32807],
        [28712],
        [29162],
        [29199],
        [28642]], device='cuda:0')
[2024-07-24 10:21:31,608][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[33888],
        [33260],
        [33044],
        [33716],
        [29370],
        [31516],
        [29628],
        [31688],
        [31952],
        [31438],
        [31799],
        [33538],
        [29215],
        [32671],
        [31177],
        [31988],
        [32825]], device='cuda:0')
[2024-07-24 10:21:31,609][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[47424],
        [47681],
        [47810],
        [45070],
        [41452],
        [45314],
        [37886],
        [43280],
        [46546],
        [41613],
        [45326],
        [41654],
        [39024],
        [41769],
        [41470],
        [43418],
        [43701]], device='cuda:0')
[2024-07-24 10:21:31,611][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[30174],
        [31347],
        [21899],
        [25721],
        [18566],
        [15569],
        [17433],
        [18976],
        [17521],
        [15123],
        [13310],
        [14130],
        [13270],
        [12670],
        [13169],
        [14188],
        [13701]], device='cuda:0')
[2024-07-24 10:21:31,612][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[37739],
        [39409],
        [39056],
        [39209],
        [45685],
        [46211],
        [48681],
        [46787],
        [47344],
        [48188],
        [48056],
        [47788],
        [48778],
        [47036],
        [48012],
        [47831],
        [46827]], device='cuda:0')
[2024-07-24 10:21:31,614][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[41223],
        [41302],
        [41334],
        [41463],
        [41433],
        [41308],
        [41492],
        [41143],
        [41164],
        [41331],
        [41380],
        [41619],
        [41838],
        [41454],
        [41462],
        [41263],
        [41249]], device='cuda:0')
[2024-07-24 10:21:31,615][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[21550],
        [22492],
        [22289],
        [22060],
        [25077],
        [23814],
        [34030],
        [29410],
        [25704],
        [29945],
        [36712],
        [34670],
        [41964],
        [35242],
        [41230],
        [33408],
        [33752]], device='cuda:0')
[2024-07-24 10:21:31,617][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[22475],
        [25544],
        [26649],
        [27087],
        [27170],
        [27411],
        [22093],
        [21824],
        [22767],
        [21730],
        [23721],
        [24770],
        [26619],
        [24832],
        [25879],
        [24538],
        [25160]], device='cuda:0')
[2024-07-24 10:21:31,618][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[8321],
        [5704],
        [6462],
        [5525],
        [6720],
        [6790],
        [4789],
        [6346],
        [5776],
        [5688],
        [4897],
        [5087],
        [4167],
        [6231],
        [5153],
        [6310],
        [6458]], device='cuda:0')
[2024-07-24 10:21:31,620][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 6354],
        [ 6238],
        [17206],
        [12635],
        [14910],
        [17582],
        [12160],
        [14109],
        [15643],
        [12551],
        [13752],
        [12495],
        [11155],
        [15743],
        [11040],
        [14623],
        [17401]], device='cuda:0')
[2024-07-24 10:21:31,621][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483],
        [22483]], device='cuda:0')
[2024-07-24 10:21:31,665][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:31,666][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,667][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,668][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,669][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,671][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,672][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,673][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,674][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,675][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,676][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,677][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,678][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,680][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.5279, 0.4721], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,681][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.8365, 0.1635], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,682][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.3056, 0.6944], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,684][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.9888, 0.0112], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,685][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.9981, 0.0019], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,686][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.0818, 0.9182], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,688][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.4774, 0.5226], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,689][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.4895, 0.5105], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,690][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.8881, 0.1119], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,692][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.0143, 0.9857], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,693][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.8779, 0.1221], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,694][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.4750, 0.5250], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,696][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.1340, 0.5736, 0.2924], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,697][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.9483, 0.0275, 0.0243], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,699][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.1863, 0.4020, 0.4117], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,700][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.9566, 0.0246, 0.0188], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,701][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ and] are: tensor([9.9813e-01, 7.4838e-04, 1.1166e-03], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,702][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0413, 0.4876, 0.4711], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,704][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2116, 0.2001, 0.5882], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,705][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3249, 0.3378, 0.3373], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,706][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.7106, 0.0763, 0.2131], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,708][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0052, 0.8310, 0.1638], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,709][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.8565, 0.0630, 0.0804], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,710][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.3240, 0.3087, 0.3673], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,712][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.1760, 0.1279, 0.4935, 0.2026], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,713][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.3985, 0.0988, 0.3912, 0.1115], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,715][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.1318, 0.2929, 0.2961, 0.2792], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,715][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.9602, 0.0145, 0.0121, 0.0133], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,715][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.9420, 0.0062, 0.0211, 0.0307], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,716][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0276, 0.3303, 0.3193, 0.3229], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,716][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.3010, 0.1958, 0.2465, 0.2567], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,717][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.2482, 0.2531, 0.2522, 0.2466], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,717][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.6841, 0.0515, 0.2043, 0.0601], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,717][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0104, 0.6980, 0.1055, 0.1860], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,718][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.3440, 0.1501, 0.2010, 0.3048], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,718][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.3445, 0.1332, 0.2818, 0.2405], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:31,719][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0447, 0.1261, 0.2639, 0.3690, 0.1963], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,720][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.3439, 0.1031, 0.3118, 0.1275, 0.1137], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,721][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.1043, 0.2286, 0.2336, 0.2202, 0.2134], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,723][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.8943, 0.0262, 0.0197, 0.0243, 0.0356], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,724][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.9258, 0.0044, 0.0108, 0.0130, 0.0459], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,725][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0174, 0.2575, 0.2500, 0.2518, 0.2233], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,726][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1964, 0.1825, 0.1203, 0.0549, 0.4458], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,727][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.1927, 0.2012, 0.2010, 0.1974, 0.2076], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,729][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.3080, 0.0530, 0.2690, 0.1213, 0.2487], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,730][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0063, 0.6726, 0.0972, 0.1456, 0.0783], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,732][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.2093, 0.0606, 0.1601, 0.2427, 0.3273], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,733][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0849, 0.0855, 0.2189, 0.2052, 0.4055], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:31,734][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0294, 0.1233, 0.1911, 0.2753, 0.2248, 0.1560], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,736][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.6405, 0.0453, 0.1128, 0.0674, 0.0613, 0.0727], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,737][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0867, 0.1909, 0.1948, 0.1838, 0.1778, 0.1659], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,738][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.9011, 0.0157, 0.0143, 0.0159, 0.0265, 0.0265], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,739][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([9.9602e-01, 2.2745e-04, 2.7917e-04, 5.9970e-04, 2.2837e-03, 5.9271e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,741][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0141, 0.2098, 0.2060, 0.2061, 0.1824, 0.1815], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,742][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1217, 0.1079, 0.0967, 0.0500, 0.1429, 0.4808], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,744][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1631, 0.1661, 0.1653, 0.1626, 0.1721, 0.1708], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,745][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2974, 0.0306, 0.1544, 0.0751, 0.2334, 0.2092], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,746][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0014, 0.5898, 0.0960, 0.1369, 0.1216, 0.0543], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,748][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.3375, 0.0381, 0.0489, 0.1336, 0.4191, 0.0227], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,749][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0748, 0.0367, 0.1258, 0.1157, 0.3601, 0.2870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:31,751][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0243, 0.0485, 0.1599, 0.1096, 0.2330, 0.3970, 0.0277],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,752][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.1521, 0.0541, 0.2170, 0.0791, 0.1388, 0.2316, 0.1274],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,753][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0715, 0.1671, 0.1680, 0.1596, 0.1531, 0.1416, 0.1391],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,755][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.7497, 0.0322, 0.0278, 0.0292, 0.0537, 0.0579, 0.0496],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,756][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.8695, 0.0020, 0.0065, 0.0039, 0.0329, 0.0148, 0.0704],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,758][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0097, 0.1860, 0.1809, 0.1806, 0.1578, 0.1570, 0.1280],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,759][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.2161, 0.0468, 0.0788, 0.0380, 0.0977, 0.1136, 0.4090],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,760][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1354, 0.1428, 0.1427, 0.1397, 0.1483, 0.1477, 0.1435],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,762][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0175, 0.0409, 0.1620, 0.1095, 0.2134, 0.4011, 0.0555],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,763][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0007, 0.4587, 0.2019, 0.0817, 0.1342, 0.1072, 0.0156],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,765][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.4028, 0.0274, 0.0542, 0.0774, 0.2279, 0.0223, 0.1879],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,766][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0031, 0.0425, 0.1124, 0.1254, 0.2611, 0.4130, 0.0425],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:31,768][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0316, 0.0866, 0.0919, 0.2366, 0.1584, 0.2114, 0.0759, 0.1077],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,769][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.5133, 0.0379, 0.1005, 0.0570, 0.0764, 0.0854, 0.1188, 0.0106],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,771][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0656, 0.1434, 0.1455, 0.1374, 0.1330, 0.1238, 0.1212, 0.1301],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,772][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.7421, 0.0266, 0.0259, 0.0262, 0.0457, 0.0523, 0.0528, 0.0283],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,773][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ at] are: tensor([9.7020e-01, 7.2646e-04, 8.3279e-04, 2.8784e-03, 1.4366e-03, 1.4750e-03,
        1.2565e-02, 9.8832e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,773][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0099, 0.1579, 0.1573, 0.1560, 0.1375, 0.1365, 0.1119, 0.1330],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,774][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0189, 0.0736, 0.1284, 0.0348, 0.0703, 0.1060, 0.1087, 0.4592],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,774][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1191, 0.1239, 0.1240, 0.1223, 0.1288, 0.1282, 0.1255, 0.1281],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,774][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0494, 0.0231, 0.1540, 0.1045, 0.2499, 0.2841, 0.0527, 0.0824],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,775][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0017, 0.5410, 0.0943, 0.1227, 0.1202, 0.0560, 0.0370, 0.0272],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,775][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0912, 0.0111, 0.0294, 0.0490, 0.2508, 0.0336, 0.2880, 0.2470],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,775][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0124, 0.0290, 0.0857, 0.1109, 0.3369, 0.3105, 0.0554, 0.0592],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:31,776][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0202, 0.0724, 0.1180, 0.1575, 0.1130, 0.1466, 0.0711, 0.2121, 0.0891],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,777][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.6581, 0.0190, 0.0608, 0.0357, 0.0358, 0.0612, 0.0666, 0.0122, 0.0506],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,778][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0564, 0.1271, 0.1289, 0.1222, 0.1178, 0.1098, 0.1067, 0.1158, 0.1153],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,780][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.7743, 0.0182, 0.0178, 0.0213, 0.0344, 0.0405, 0.0391, 0.0239, 0.0305],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,781][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([9.8006e-01, 9.0897e-05, 1.7382e-04, 4.0239e-04, 2.6505e-03, 9.5864e-04,
        5.0386e-03, 9.7346e-03, 8.9317e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,782][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0088, 0.1394, 0.1384, 0.1377, 0.1212, 0.1204, 0.0990, 0.1172, 0.1179],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,784][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0569, 0.0681, 0.0810, 0.0288, 0.0969, 0.1646, 0.1603, 0.0897, 0.2537],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,785][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1040, 0.1099, 0.1096, 0.1083, 0.1142, 0.1137, 0.1112, 0.1142, 0.1149],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,786][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1205, 0.0167, 0.0990, 0.0486, 0.2000, 0.1855, 0.0598, 0.0998, 0.1702],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,788][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0010, 0.5602, 0.0726, 0.1105, 0.0839, 0.0550, 0.0272, 0.0310, 0.0587],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,789][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1478, 0.0109, 0.0110, 0.0415, 0.2328, 0.0080, 0.3980, 0.1247, 0.0254],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,791][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0270, 0.0199, 0.0698, 0.0804, 0.2826, 0.2026, 0.0603, 0.0694, 0.1879],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:31,792][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0159, 0.0401, 0.0671, 0.0646, 0.1338, 0.2541, 0.0265, 0.1792, 0.1963,
        0.0225], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,794][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.4581, 0.0308, 0.1001, 0.0410, 0.0565, 0.1013, 0.0744, 0.0362, 0.0827,
        0.0189], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,795][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0511, 0.1149, 0.1178, 0.1108, 0.1065, 0.0984, 0.0972, 0.1045, 0.1036,
        0.0952], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,797][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.6790, 0.0304, 0.0247, 0.0277, 0.0454, 0.0491, 0.0437, 0.0289, 0.0415,
        0.0296], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,798][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.8394, 0.0034, 0.0026, 0.0064, 0.0215, 0.0052, 0.0634, 0.0191, 0.0031,
        0.0359], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,800][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0073, 0.1255, 0.1241, 0.1234, 0.1086, 0.1081, 0.0884, 0.1049, 0.1059,
        0.1038], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,801][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.2072, 0.0389, 0.0521, 0.0213, 0.0649, 0.0740, 0.0885, 0.1017, 0.0919,
        0.2595], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,802][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0940, 0.0984, 0.0981, 0.0970, 0.1023, 0.1019, 0.0997, 0.1024, 0.1030,
        0.1031], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,804][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0444, 0.0211, 0.0752, 0.0548, 0.1515, 0.2662, 0.0578, 0.0779, 0.1956,
        0.0554], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,805][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0022, 0.4988, 0.0691, 0.0873, 0.0980, 0.0685, 0.0249, 0.0402, 0.0853,
        0.0257], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,807][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.1660, 0.0159, 0.0330, 0.0238, 0.1282, 0.0288, 0.1707, 0.2544, 0.0568,
        0.1225], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,808][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0082, 0.0169, 0.0536, 0.0524, 0.1955, 0.2608, 0.0370, 0.0684, 0.2352,
        0.0720], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:31,809][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0100, 0.0411, 0.0525, 0.1079, 0.1400, 0.1931, 0.0574, 0.1435, 0.1404,
        0.0620, 0.0520], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,811][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.7428, 0.0162, 0.0255, 0.0274, 0.0329, 0.0390, 0.0521, 0.0090, 0.0358,
        0.0129, 0.0065], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,812][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0464, 0.1049, 0.1060, 0.1004, 0.0967, 0.0901, 0.0875, 0.0949, 0.0946,
        0.0869, 0.0917], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,814][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.6415, 0.0261, 0.0236, 0.0262, 0.0427, 0.0472, 0.0425, 0.0310, 0.0406,
        0.0332, 0.0454], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,815][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [,] are: tensor([9.3375e-01, 5.3657e-04, 1.4471e-03, 9.4887e-04, 3.5516e-03, 4.3829e-03,
        9.7330e-03, 2.7085e-02, 5.0525e-03, 5.0744e-03, 8.4377e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,816][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0068, 0.1135, 0.1130, 0.1123, 0.0984, 0.0976, 0.0798, 0.0944, 0.0954,
        0.0937, 0.0949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,818][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0172, 0.0488, 0.1096, 0.0316, 0.0659, 0.1159, 0.1044, 0.1717, 0.0797,
        0.1437, 0.1117], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,819][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0847, 0.0893, 0.0891, 0.0881, 0.0929, 0.0924, 0.0906, 0.0928, 0.0933,
        0.0934, 0.0934], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,821][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0588, 0.0168, 0.0719, 0.0520, 0.1236, 0.1720, 0.0527, 0.0778, 0.1685,
        0.0634, 0.1424], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,822][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0007, 0.5503, 0.0860, 0.0934, 0.0822, 0.0518, 0.0191, 0.0267, 0.0564,
        0.0180, 0.0153], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,824][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0695, 0.0053, 0.0211, 0.0248, 0.1803, 0.0314, 0.1957, 0.1987, 0.0621,
        0.1545, 0.0566], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,825][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0222, 0.0202, 0.0479, 0.0766, 0.1543, 0.1677, 0.0567, 0.0412, 0.1464,
        0.1389, 0.1279], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:31,826][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0378, 0.0295, 0.1304, 0.0550, 0.0495, 0.1667, 0.0469, 0.1515, 0.1310,
        0.0376, 0.1072, 0.0568], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,828][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.1306, 0.0403, 0.1267, 0.0530, 0.0563, 0.1629, 0.1145, 0.0340, 0.1344,
        0.0373, 0.0426, 0.0676], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,829][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0430, 0.0958, 0.0967, 0.0915, 0.0888, 0.0824, 0.0809, 0.0863, 0.0861,
        0.0802, 0.0838, 0.0843], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,831][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.6311, 0.0255, 0.0219, 0.0225, 0.0453, 0.0476, 0.0476, 0.0301, 0.0396,
        0.0308, 0.0393, 0.0187], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,831][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.5930, 0.0061, 0.0096, 0.0234, 0.0625, 0.0244, 0.0756, 0.0415, 0.0113,
        0.0806, 0.0299, 0.0421], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,832][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0064, 0.1040, 0.1023, 0.1021, 0.0900, 0.0892, 0.0732, 0.0867, 0.0872,
        0.0856, 0.0867, 0.0866], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,832][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0944, 0.0601, 0.0602, 0.0798, 0.0667, 0.1207, 0.0486, 0.0489, 0.1151,
        0.1507, 0.0439, 0.1110], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,832][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0788, 0.0815, 0.0815, 0.0800, 0.0845, 0.0847, 0.0826, 0.0850, 0.0858,
        0.0857, 0.0859, 0.0840], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,833][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0270, 0.0152, 0.0698, 0.0309, 0.1267, 0.2036, 0.0384, 0.0792, 0.1703,
        0.0756, 0.1480, 0.0155], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,833][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0019, 0.5345, 0.0637, 0.1014, 0.0582, 0.0480, 0.0250, 0.0359, 0.0494,
        0.0230, 0.0163, 0.0426], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,834][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.1307, 0.0295, 0.0783, 0.1170, 0.0885, 0.0320, 0.0387, 0.2064, 0.0598,
        0.0405, 0.0603, 0.1182], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,834][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0075, 0.0169, 0.0484, 0.0501, 0.1450, 0.2398, 0.0423, 0.0547, 0.1935,
        0.0666, 0.1111, 0.0241], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:31,836][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0061, 0.0302, 0.0751, 0.0686, 0.0876, 0.1910, 0.0313, 0.1770, 0.1383,
        0.0326, 0.0573, 0.0717, 0.0332], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,837][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.2358, 0.0528, 0.1112, 0.0618, 0.0479, 0.0755, 0.1143, 0.0324, 0.0840,
        0.0415, 0.0318, 0.0673, 0.0438], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,839][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0409, 0.0884, 0.0895, 0.0848, 0.0815, 0.0756, 0.0741, 0.0796, 0.0794,
        0.0736, 0.0772, 0.0774, 0.0781], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,840][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.5129, 0.0337, 0.0272, 0.0308, 0.0516, 0.0631, 0.0552, 0.0362, 0.0519,
        0.0352, 0.0494, 0.0235, 0.0294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,841][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.5420, 0.0056, 0.0075, 0.0026, 0.0382, 0.0079, 0.0491, 0.0425, 0.0040,
        0.0367, 0.0173, 0.0062, 0.2404], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,843][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0056, 0.0956, 0.0944, 0.0941, 0.0828, 0.0821, 0.0672, 0.0799, 0.0804,
        0.0789, 0.0799, 0.0802, 0.0788], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,844][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.1482, 0.0270, 0.0271, 0.0078, 0.0909, 0.0508, 0.0999, 0.0794, 0.0737,
        0.0480, 0.0255, 0.0106, 0.3110], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,846][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0730, 0.0756, 0.0754, 0.0743, 0.0781, 0.0778, 0.0761, 0.0780, 0.0786,
        0.0785, 0.0785, 0.0775, 0.0786], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,847][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0260, 0.0175, 0.0962, 0.0578, 0.1289, 0.1749, 0.0396, 0.0637, 0.1507,
        0.0520, 0.1148, 0.0196, 0.0584], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,849][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0018, 0.5249, 0.1041, 0.0736, 0.0964, 0.0473, 0.0182, 0.0233, 0.0429,
        0.0178, 0.0137, 0.0283, 0.0077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,850][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.2373, 0.0093, 0.0281, 0.0268, 0.0634, 0.0083, 0.2169, 0.0596, 0.0247,
        0.1183, 0.0548, 0.0459, 0.1068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,852][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0065, 0.0128, 0.0391, 0.0517, 0.0968, 0.1939, 0.0352, 0.0446, 0.1751,
        0.1070, 0.1189, 0.0313, 0.0870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:31,853][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0130, 0.0338, 0.0517, 0.1006, 0.0810, 0.1564, 0.0480, 0.0863, 0.0910,
        0.0628, 0.0614, 0.1102, 0.0517, 0.0520], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,855][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.5751, 0.0227, 0.0352, 0.0319, 0.0423, 0.0521, 0.0833, 0.0087, 0.0527,
        0.0172, 0.0122, 0.0327, 0.0233, 0.0106], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,856][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0382, 0.0826, 0.0831, 0.0789, 0.0755, 0.0704, 0.0684, 0.0742, 0.0738,
        0.0677, 0.0717, 0.0722, 0.0728, 0.0705], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,858][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.6307, 0.0177, 0.0187, 0.0200, 0.0365, 0.0404, 0.0370, 0.0263, 0.0338,
        0.0269, 0.0343, 0.0177, 0.0254, 0.0348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,859][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([8.9936e-01, 5.3954e-04, 8.8451e-04, 1.3042e-03, 5.8594e-03, 1.7942e-03,
        2.9698e-02, 1.7176e-02, 1.6895e-03, 9.4429e-03, 7.2309e-03, 5.1015e-03,
        1.5574e-02, 4.3445e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,860][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0054, 0.0887, 0.0879, 0.0877, 0.0768, 0.0763, 0.0629, 0.0742, 0.0747,
        0.0736, 0.0745, 0.0747, 0.0734, 0.0691], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,862][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0199, 0.0378, 0.0661, 0.0271, 0.0471, 0.0748, 0.0951, 0.1314, 0.0606,
        0.1108, 0.0409, 0.0316, 0.0373, 0.2194], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,863][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0684, 0.0697, 0.0700, 0.0687, 0.0726, 0.0720, 0.0707, 0.0723, 0.0727,
        0.0725, 0.0726, 0.0715, 0.0731, 0.0731], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,864][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0917, 0.0089, 0.0490, 0.0286, 0.0917, 0.1006, 0.0341, 0.0478, 0.0985,
        0.0459, 0.0949, 0.0135, 0.1167, 0.1782], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,866][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0009, 0.4968, 0.0691, 0.0848, 0.0774, 0.0471, 0.0225, 0.0290, 0.0616,
        0.0231, 0.0197, 0.0373, 0.0105, 0.0199], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,867][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0761, 0.0037, 0.0109, 0.0234, 0.1726, 0.0096, 0.1267, 0.0735, 0.0143,
        0.0575, 0.0249, 0.0656, 0.2128, 0.1283], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,869][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0211, 0.0108, 0.0320, 0.0515, 0.1294, 0.1031, 0.0372, 0.0225, 0.0799,
        0.0893, 0.0934, 0.0206, 0.1346, 0.1747], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:31,870][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0180, 0.0234, 0.0620, 0.0889, 0.0827, 0.0967, 0.0368, 0.0713, 0.0755,
        0.0601, 0.0652, 0.1000, 0.0852, 0.0979, 0.0361], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,872][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.1790, 0.0433, 0.0726, 0.0584, 0.0484, 0.0852, 0.1374, 0.0233, 0.0886,
        0.0477, 0.0224, 0.0731, 0.0419, 0.0293, 0.0493], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,873][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0362, 0.0774, 0.0775, 0.0737, 0.0705, 0.0656, 0.0645, 0.0691, 0.0688,
        0.0634, 0.0666, 0.0677, 0.0678, 0.0657, 0.0654], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,875][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.6093, 0.0214, 0.0203, 0.0225, 0.0382, 0.0413, 0.0336, 0.0238, 0.0332,
        0.0243, 0.0315, 0.0190, 0.0236, 0.0345, 0.0237], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,876][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.7084, 0.0028, 0.0018, 0.0066, 0.0207, 0.0070, 0.0298, 0.0199, 0.0043,
        0.0373, 0.0084, 0.0163, 0.1035, 0.0095, 0.0236], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,878][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0046, 0.0841, 0.0827, 0.0828, 0.0721, 0.0716, 0.0587, 0.0697, 0.0702,
        0.0692, 0.0698, 0.0701, 0.0689, 0.0649, 0.0606], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,879][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0736, 0.0350, 0.0447, 0.0092, 0.0579, 0.0621, 0.0951, 0.0783, 0.0624,
        0.0922, 0.0457, 0.0117, 0.0790, 0.0570, 0.1962], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,881][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0632, 0.0650, 0.0654, 0.0641, 0.0676, 0.0675, 0.0657, 0.0674, 0.0679,
        0.0677, 0.0676, 0.0666, 0.0681, 0.0682, 0.0681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,882][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0425, 0.0078, 0.0533, 0.0296, 0.0697, 0.1134, 0.0239, 0.0567, 0.1044,
        0.0458, 0.1033, 0.0140, 0.0898, 0.1808, 0.0648], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,884][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0011, 0.4556, 0.1043, 0.0847, 0.0738, 0.0513, 0.0145, 0.0318, 0.0534,
        0.0231, 0.0244, 0.0282, 0.0110, 0.0256, 0.0173], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,885][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0545, 0.0092, 0.0261, 0.0223, 0.0855, 0.0115, 0.0868, 0.0641, 0.0191,
        0.0370, 0.0184, 0.0315, 0.1723, 0.0669, 0.2949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,887][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0089, 0.0088, 0.0307, 0.0373, 0.1261, 0.0997, 0.0321, 0.0275, 0.0892,
        0.0720, 0.0791, 0.0164, 0.1019, 0.1920, 0.0784], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:31,888][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0128, 0.0298, 0.0622, 0.0754, 0.0999, 0.1033, 0.0360, 0.1044, 0.0733,
        0.0343, 0.0490, 0.0804, 0.0414, 0.0888, 0.0481, 0.0610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,889][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.5222, 0.0146, 0.0330, 0.0217, 0.0308, 0.0570, 0.0959, 0.0111, 0.0469,
        0.0245, 0.0143, 0.0314, 0.0254, 0.0207, 0.0225, 0.0280],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,889][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0357, 0.0722, 0.0724, 0.0690, 0.0658, 0.0615, 0.0602, 0.0647, 0.0645,
        0.0596, 0.0625, 0.0634, 0.0632, 0.0615, 0.0615, 0.0623],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,890][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.6612, 0.0135, 0.0146, 0.0164, 0.0289, 0.0333, 0.0326, 0.0222, 0.0270,
        0.0217, 0.0285, 0.0143, 0.0189, 0.0292, 0.0231, 0.0145],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,890][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ it] are: tensor([9.2153e-01, 2.7491e-04, 9.7228e-04, 5.2666e-04, 3.4391e-03, 2.1561e-03,
        8.8713e-03, 1.3958e-02, 1.6223e-03, 3.5184e-03, 4.3570e-03, 1.9053e-03,
        2.0909e-02, 4.4787e-03, 6.5510e-03, 4.9290e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,891][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0046, 0.0782, 0.0770, 0.0771, 0.0677, 0.0674, 0.0552, 0.0653, 0.0659,
        0.0649, 0.0658, 0.0659, 0.0646, 0.0614, 0.0574, 0.0617],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,891][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0403, 0.0186, 0.0190, 0.0063, 0.0756, 0.0370, 0.0617, 0.0356, 0.0767,
        0.0409, 0.0234, 0.0078, 0.0764, 0.0374, 0.1070, 0.3363],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,892][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0599, 0.0605, 0.0609, 0.0596, 0.0629, 0.0626, 0.0612, 0.0627, 0.0632,
        0.0631, 0.0630, 0.0619, 0.0633, 0.0635, 0.0636, 0.0680],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,893][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0498, 0.0065, 0.0431, 0.0223, 0.0815, 0.0922, 0.0239, 0.0518, 0.0838,
        0.0344, 0.0859, 0.0095, 0.0734, 0.1720, 0.0925, 0.0775],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,894][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0006, 0.4417, 0.0942, 0.0938, 0.0736, 0.0554, 0.0162, 0.0369, 0.0548,
        0.0181, 0.0211, 0.0365, 0.0089, 0.0190, 0.0174, 0.0119],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,896][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0396, 0.0030, 0.0037, 0.0226, 0.0937, 0.0028, 0.1471, 0.0333, 0.0047,
        0.1137, 0.0173, 0.0498, 0.0835, 0.0902, 0.2176, 0.0772],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,897][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0098, 0.0096, 0.0270, 0.0469, 0.1428, 0.1026, 0.0327, 0.0284, 0.0811,
        0.0634, 0.0646, 0.0143, 0.0635, 0.1538, 0.0845, 0.0749],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:31,899][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0092, 0.0307, 0.0431, 0.0890, 0.0567, 0.1135, 0.0422, 0.0704, 0.0743,
        0.0541, 0.0506, 0.1009, 0.0404, 0.0426, 0.0345, 0.0968, 0.0510],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,900][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.5355, 0.0215, 0.0287, 0.0304, 0.0381, 0.0469, 0.0849, 0.0068, 0.0498,
        0.0158, 0.0107, 0.0323, 0.0219, 0.0090, 0.0242, 0.0357, 0.0079],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,902][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0330, 0.0680, 0.0682, 0.0648, 0.0622, 0.0580, 0.0565, 0.0609, 0.0607,
        0.0561, 0.0590, 0.0594, 0.0596, 0.0581, 0.0581, 0.0586, 0.0588],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,903][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.6153, 0.0137, 0.0157, 0.0161, 0.0300, 0.0347, 0.0341, 0.0220, 0.0285,
        0.0235, 0.0287, 0.0145, 0.0204, 0.0298, 0.0265, 0.0186, 0.0281],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,904][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([8.9765e-01, 3.7367e-04, 6.2237e-04, 8.1741e-04, 5.4348e-03, 1.2518e-03,
        2.4930e-02, 1.1663e-02, 1.2128e-03, 8.7838e-03, 5.4382e-03, 3.5057e-03,
        1.6775e-02, 2.6424e-03, 1.1472e-02, 2.9673e-03, 4.4630e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,906][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0041, 0.0730, 0.0727, 0.0725, 0.0636, 0.0634, 0.0521, 0.0616, 0.0622,
        0.0614, 0.0622, 0.0624, 0.0613, 0.0577, 0.0542, 0.0583, 0.0573],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,907][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0225, 0.0278, 0.0473, 0.0201, 0.0307, 0.0486, 0.0798, 0.1107, 0.0436,
        0.1011, 0.0274, 0.0234, 0.0256, 0.1805, 0.0418, 0.0349, 0.1343],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,909][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0554, 0.0571, 0.0573, 0.0561, 0.0594, 0.0589, 0.0578, 0.0591, 0.0593,
        0.0592, 0.0592, 0.0584, 0.0595, 0.0596, 0.0599, 0.0641, 0.0598],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,910][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0400, 0.0055, 0.0335, 0.0206, 0.0705, 0.0702, 0.0208, 0.0294, 0.0657,
        0.0319, 0.0774, 0.0100, 0.0726, 0.1387, 0.0979, 0.0982, 0.1170],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,912][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0013, 0.4354, 0.0802, 0.0892, 0.0679, 0.0523, 0.0195, 0.0291, 0.0610,
        0.0209, 0.0185, 0.0321, 0.0089, 0.0181, 0.0192, 0.0255, 0.0208],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,913][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0355, 0.0017, 0.0041, 0.0102, 0.0938, 0.0036, 0.0941, 0.0390, 0.0054,
        0.0633, 0.0173, 0.0298, 0.1280, 0.0719, 0.2621, 0.0933, 0.0468],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,915][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0110, 0.0065, 0.0203, 0.0340, 0.1041, 0.0808, 0.0254, 0.0171, 0.0606,
        0.0673, 0.0740, 0.0135, 0.0955, 0.1332, 0.0907, 0.0801, 0.0858],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:31,960][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:31,961][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,963][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,964][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,965][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,966][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,967][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,968][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,969][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,970][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,972][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,973][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,974][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:31,975][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.7998, 0.2002], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,977][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.3904, 0.6096], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,978][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.5806, 0.4194], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,979][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.9295, 0.0705], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,981][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.9933, 0.0067], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,982][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.4909, 0.5091], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,984][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.9924, 0.0076], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,985][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.5162, 0.4838], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,987][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.8881, 0.1119], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,988][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.2456, 0.7544], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,989][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.9855, 0.0145], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,991][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.4750, 0.5250], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:31,992][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.6435, 0.1457, 0.2108], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,993][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.1577, 0.2810, 0.5613], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,995][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.4123, 0.2943, 0.2934], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,996][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.8350, 0.0717, 0.0933], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,998][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.9816, 0.0113, 0.0071], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:31,999][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3268, 0.1811, 0.4921], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,000][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.9818, 0.0073, 0.0109], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,002][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.2997, 0.2606, 0.4398], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,003][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7106, 0.0763, 0.2131], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,004][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.1838, 0.3859, 0.4304], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,004][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9685, 0.0145, 0.0170], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,005][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.3240, 0.3087, 0.3673], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,005][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.7735, 0.0617, 0.1091, 0.0557], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,005][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.2011, 0.1578, 0.3606, 0.2804], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,006][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.4137, 0.2268, 0.1365, 0.2231], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,006][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.8415, 0.0457, 0.0717, 0.0412], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,006][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.9754, 0.0091, 0.0105, 0.0050], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,007][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.2457, 0.1579, 0.3886, 0.2078], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,007][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.9732, 0.0101, 0.0108, 0.0059], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,008][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.2609, 0.1762, 0.3326, 0.2303], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,010][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.6841, 0.0515, 0.2043, 0.0601], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,011][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.2504, 0.2347, 0.2362, 0.2788], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,012][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.9525, 0.0190, 0.0159, 0.0126], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,014][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.3445, 0.1332, 0.2818, 0.2405], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,015][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.2218, 0.1107, 0.1956, 0.1983, 0.2735], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,016][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0506, 0.0762, 0.4114, 0.1917, 0.2701], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,018][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.1710, 0.1790, 0.1971, 0.2633, 0.1896], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,019][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.3789, 0.0990, 0.1349, 0.1257, 0.2615], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,021][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.9280, 0.0202, 0.0184, 0.0112, 0.0222], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,022][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.1245, 0.1158, 0.2898, 0.1800, 0.2899], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,023][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.9013, 0.0156, 0.0259, 0.0095, 0.0477], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,025][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.1842, 0.1106, 0.3022, 0.1757, 0.2272], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,026][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.3080, 0.0530, 0.2690, 0.1213, 0.2487], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,027][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0930, 0.2697, 0.2227, 0.2809, 0.1337], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,029][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.8119, 0.0428, 0.0425, 0.0319, 0.0708], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,030][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0849, 0.0855, 0.2189, 0.2052, 0.4055], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,031][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.3384, 0.0522, 0.1214, 0.0651, 0.2966, 0.1263], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,033][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0752, 0.0449, 0.1891, 0.1184, 0.1705, 0.4020], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,034][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2346, 0.1276, 0.1339, 0.1814, 0.1323, 0.1902], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,036][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.4709, 0.0527, 0.0912, 0.0567, 0.2052, 0.1232], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,037][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.9745, 0.0038, 0.0052, 0.0029, 0.0079, 0.0057], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,039][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1462, 0.0587, 0.1640, 0.0908, 0.2224, 0.3178], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,040][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.9562, 0.0071, 0.0084, 0.0039, 0.0104, 0.0140], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,042][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1309, 0.0688, 0.1933, 0.1021, 0.2218, 0.2831], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,043][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2974, 0.0306, 0.1544, 0.0751, 0.2334, 0.2092], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,044][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0762, 0.1853, 0.1521, 0.2667, 0.2308, 0.0889], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,046][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.9481, 0.0109, 0.0084, 0.0057, 0.0173, 0.0096], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,047][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0748, 0.0367, 0.1258, 0.1157, 0.3601, 0.2870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,049][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0247, 0.0745, 0.1457, 0.1155, 0.2574, 0.2992, 0.0828],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,050][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0021, 0.0385, 0.1349, 0.0831, 0.1751, 0.5244, 0.0419],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,051][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0358, 0.1307, 0.1159, 0.2056, 0.1398, 0.2816, 0.0906],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,053][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0696, 0.0849, 0.1383, 0.0899, 0.2742, 0.2455, 0.0976],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,054][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.8415, 0.0159, 0.0168, 0.0142, 0.0302, 0.0203, 0.0612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,056][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0106, 0.0734, 0.1384, 0.1306, 0.2196, 0.3699, 0.0575],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,057][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.6746, 0.0424, 0.0387, 0.0227, 0.0595, 0.0524, 0.1096],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,059][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0178, 0.0659, 0.1662, 0.1048, 0.2085, 0.3739, 0.0630],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,060][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0175, 0.0409, 0.1620, 0.1095, 0.2134, 0.4011, 0.0555],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,062][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0006, 0.0850, 0.3100, 0.1054, 0.1446, 0.3336, 0.0209],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,062][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.4012, 0.0910, 0.0926, 0.0683, 0.1095, 0.0939, 0.1434],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,062][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0031, 0.0425, 0.1124, 0.1254, 0.2611, 0.4130, 0.0425],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,063][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0569, 0.0606, 0.1117, 0.1065, 0.3082, 0.2136, 0.0855, 0.0569],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,063][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0075, 0.0345, 0.1099, 0.0749, 0.1571, 0.4828, 0.0541, 0.0792],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,064][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.1222, 0.1038, 0.1144, 0.1493, 0.1150, 0.1707, 0.0956, 0.1291],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,064][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1796, 0.0530, 0.0977, 0.0616, 0.2082, 0.1774, 0.1234, 0.0991],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,064][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.9132, 0.0090, 0.0086, 0.0048, 0.0124, 0.0087, 0.0345, 0.0090],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,065][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0623, 0.0458, 0.1378, 0.0987, 0.1943, 0.2727, 0.0753, 0.1131],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,066][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.8352, 0.0170, 0.0143, 0.0058, 0.0194, 0.0228, 0.0528, 0.0327],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,067][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0549, 0.0654, 0.1491, 0.1277, 0.1731, 0.2425, 0.0803, 0.1069],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,069][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0494, 0.0231, 0.1540, 0.1045, 0.2499, 0.2841, 0.0527, 0.0824],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,070][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0614, 0.1490, 0.1065, 0.1888, 0.2051, 0.0925, 0.1471, 0.0495],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,071][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.8507, 0.0197, 0.0155, 0.0099, 0.0234, 0.0124, 0.0600, 0.0084],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,073][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0124, 0.0290, 0.0857, 0.1109, 0.3369, 0.3105, 0.0554, 0.0592],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,074][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1836, 0.0341, 0.0963, 0.0579, 0.2004, 0.1391, 0.0984, 0.0737, 0.1164],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,076][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0188, 0.0224, 0.0938, 0.0752, 0.1111, 0.3906, 0.0488, 0.0928, 0.1465],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,077][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1753, 0.0755, 0.0964, 0.1193, 0.0860, 0.1202, 0.0772, 0.1162, 0.1339],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,078][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2347, 0.0332, 0.0645, 0.0443, 0.1581, 0.1235, 0.0904, 0.0974, 0.1538],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,080][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.9636, 0.0025, 0.0034, 0.0014, 0.0049, 0.0031, 0.0131, 0.0036, 0.0044],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,081][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0823, 0.0381, 0.1000, 0.0710, 0.1557, 0.2275, 0.0604, 0.0816, 0.1833],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,083][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.8976, 0.0078, 0.0079, 0.0033, 0.0118, 0.0108, 0.0294, 0.0114, 0.0199],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,084][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0465, 0.0437, 0.1147, 0.0719, 0.1470, 0.1985, 0.0547, 0.1137, 0.2093],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,086][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1205, 0.0167, 0.0990, 0.0486, 0.2000, 0.1855, 0.0598, 0.0998, 0.1702],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,087][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0446, 0.1448, 0.0809, 0.1879, 0.1249, 0.0850, 0.1230, 0.0633, 0.1457],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,088][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.9079, 0.0109, 0.0074, 0.0048, 0.0141, 0.0076, 0.0336, 0.0041, 0.0096],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,090][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0270, 0.0199, 0.0698, 0.0804, 0.2826, 0.2026, 0.0603, 0.0694, 0.1879],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,091][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0499, 0.0418, 0.0936, 0.0641, 0.1576, 0.2328, 0.0601, 0.0772, 0.1566,
        0.0662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,093][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0046, 0.0191, 0.0662, 0.0622, 0.1034, 0.3547, 0.0359, 0.0700, 0.2088,
        0.0750], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,094][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0987, 0.0766, 0.0973, 0.1036, 0.1037, 0.1393, 0.0688, 0.1003, 0.1397,
        0.0720], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,095][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.1275, 0.0480, 0.0692, 0.0527, 0.1555, 0.1234, 0.0709, 0.0984, 0.1702,
        0.0842], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,097][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.9161, 0.0052, 0.0059, 0.0038, 0.0084, 0.0075, 0.0250, 0.0083, 0.0106,
        0.0092], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,098][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0292, 0.0319, 0.0922, 0.0672, 0.1641, 0.2336, 0.0444, 0.0781, 0.1911,
        0.0682], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,100][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.8522, 0.0109, 0.0095, 0.0050, 0.0139, 0.0134, 0.0383, 0.0137, 0.0190,
        0.0241], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,101][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0353, 0.0409, 0.0930, 0.0791, 0.1400, 0.2059, 0.0596, 0.0990, 0.1864,
        0.0607], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,103][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0444, 0.0211, 0.0752, 0.0548, 0.1515, 0.2662, 0.0578, 0.0779, 0.1956,
        0.0554], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,104][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0450, 0.0979, 0.0697, 0.0991, 0.1331, 0.1063, 0.0674, 0.0775, 0.2180,
        0.0858], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,105][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.7701, 0.0186, 0.0133, 0.0100, 0.0249, 0.0171, 0.0586, 0.0091, 0.0233,
        0.0551], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,107][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0082, 0.0169, 0.0536, 0.0524, 0.1955, 0.2608, 0.0370, 0.0684, 0.2352,
        0.0720], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,108][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0863, 0.0317, 0.0744, 0.0482, 0.1669, 0.1143, 0.0692, 0.0477, 0.1001,
        0.0873, 0.1739], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,110][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0074, 0.0205, 0.0591, 0.0586, 0.0876, 0.2878, 0.0331, 0.0627, 0.1688,
        0.0832, 0.1313], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,111][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0993, 0.0715, 0.0974, 0.1049, 0.0770, 0.1201, 0.0773, 0.0860, 0.1063,
        0.0658, 0.0944], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,113][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0999, 0.0341, 0.0585, 0.0374, 0.1354, 0.1091, 0.0536, 0.0871, 0.1490,
        0.0783, 0.1575], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,114][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.8845, 0.0053, 0.0082, 0.0044, 0.0105, 0.0090, 0.0175, 0.0084, 0.0093,
        0.0074, 0.0357], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,116][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0375, 0.0281, 0.0866, 0.0644, 0.1100, 0.1957, 0.0408, 0.0695, 0.1441,
        0.0711, 0.1519], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,117][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.7212, 0.0171, 0.0206, 0.0082, 0.0215, 0.0228, 0.0446, 0.0228, 0.0297,
        0.0308, 0.0607], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,119][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0220, 0.0421, 0.0937, 0.0743, 0.1053, 0.1801, 0.0431, 0.0873, 0.1683,
        0.0550, 0.1289], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,120][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0588, 0.0168, 0.0719, 0.0520, 0.1236, 0.1720, 0.0527, 0.0778, 0.1685,
        0.0634, 0.1424], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,120][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0191, 0.1463, 0.0737, 0.1378, 0.1114, 0.0830, 0.0745, 0.0437, 0.1494,
        0.0740, 0.0872], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,120][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.6264, 0.0279, 0.0273, 0.0138, 0.0387, 0.0211, 0.0553, 0.0129, 0.0279,
        0.0680, 0.0806], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,121][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0222, 0.0202, 0.0479, 0.0766, 0.1543, 0.1677, 0.0567, 0.0412, 0.1464,
        0.1389, 0.1279], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,121][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0532, 0.0240, 0.0647, 0.0359, 0.0992, 0.1438, 0.0762, 0.0575, 0.1686,
        0.0882, 0.1547, 0.0339], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,122][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0052, 0.0161, 0.0683, 0.0477, 0.0656, 0.3508, 0.0295, 0.0583, 0.1295,
        0.0703, 0.1245, 0.0343], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,122][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.1071, 0.0625, 0.0574, 0.0785, 0.0757, 0.1205, 0.0710, 0.0643, 0.1184,
        0.0827, 0.0893, 0.0725], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,122][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.1022, 0.0233, 0.0483, 0.0252, 0.1321, 0.0988, 0.0646, 0.0889, 0.1424,
        0.0794, 0.1585, 0.0360], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,123][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.8896, 0.0042, 0.0072, 0.0029, 0.0080, 0.0073, 0.0194, 0.0074, 0.0088,
        0.0076, 0.0293, 0.0083], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,125][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0175, 0.0249, 0.0627, 0.0447, 0.1255, 0.1906, 0.0345, 0.0633, 0.1663,
        0.0823, 0.1557, 0.0322], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,126][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.7787, 0.0125, 0.0140, 0.0066, 0.0174, 0.0157, 0.0355, 0.0188, 0.0208,
        0.0212, 0.0363, 0.0226], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,127][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0174, 0.0244, 0.0713, 0.0455, 0.0847, 0.2049, 0.0395, 0.0907, 0.1874,
        0.0597, 0.1392, 0.0354], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,129][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0270, 0.0152, 0.0698, 0.0309, 0.1267, 0.2036, 0.0384, 0.0792, 0.1703,
        0.0756, 0.1480, 0.0155], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,130][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0531, 0.0947, 0.0836, 0.1110, 0.1039, 0.0645, 0.0848, 0.0615, 0.0866,
        0.0621, 0.0898, 0.1043], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,132][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.7048, 0.0211, 0.0141, 0.0111, 0.0226, 0.0157, 0.0511, 0.0076, 0.0186,
        0.0439, 0.0546, 0.0348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,133][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0075, 0.0169, 0.0484, 0.0501, 0.1450, 0.2398, 0.0423, 0.0547, 0.1935,
        0.0666, 0.1111, 0.0241], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,135][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0259, 0.0315, 0.0705, 0.0541, 0.1449, 0.1636, 0.0435, 0.0621, 0.1273,
        0.0691, 0.1124, 0.0260, 0.0692], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,136][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0065, 0.0136, 0.0791, 0.0492, 0.0788, 0.2408, 0.0376, 0.0769, 0.1127,
        0.0865, 0.1317, 0.0342, 0.0525], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,138][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0571, 0.0606, 0.0906, 0.0977, 0.0768, 0.1235, 0.0521, 0.0835, 0.1066,
        0.0492, 0.0987, 0.0632, 0.0403], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,139][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0464, 0.0296, 0.0592, 0.0427, 0.1143, 0.1255, 0.0559, 0.0789, 0.1570,
        0.0662, 0.1292, 0.0351, 0.0600], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,140][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.8030, 0.0057, 0.0135, 0.0045, 0.0134, 0.0141, 0.0218, 0.0124, 0.0143,
        0.0090, 0.0512, 0.0118, 0.0254], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,142][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0213, 0.0276, 0.0825, 0.0531, 0.1004, 0.1956, 0.0321, 0.0665, 0.1387,
        0.0582, 0.1232, 0.0311, 0.0695], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,143][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.6927, 0.0125, 0.0153, 0.0063, 0.0184, 0.0201, 0.0384, 0.0206, 0.0258,
        0.0211, 0.0413, 0.0172, 0.0703], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,145][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0180, 0.0319, 0.0807, 0.0582, 0.0885, 0.1769, 0.0387, 0.0898, 0.1725,
        0.0505, 0.1047, 0.0385, 0.0513], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,146][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0260, 0.0175, 0.0962, 0.0578, 0.1289, 0.1749, 0.0396, 0.0637, 0.1507,
        0.0520, 0.1148, 0.0196, 0.0584], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,148][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0078, 0.1315, 0.1338, 0.0887, 0.1168, 0.0801, 0.0477, 0.0389, 0.0991,
        0.0602, 0.0642, 0.0751, 0.0561], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,149][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.4576, 0.0254, 0.0223, 0.0257, 0.0339, 0.0251, 0.0666, 0.0158, 0.0305,
        0.0710, 0.0857, 0.0600, 0.0804], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,151][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0065, 0.0128, 0.0391, 0.0517, 0.0968, 0.1939, 0.0352, 0.0446, 0.1751,
        0.1070, 0.1189, 0.0313, 0.0870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,152][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1138, 0.0176, 0.0472, 0.0293, 0.1159, 0.0689, 0.0614, 0.0322, 0.0665,
        0.0662, 0.1147, 0.0210, 0.1061, 0.1392], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,154][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0182, 0.0124, 0.0456, 0.0424, 0.0788, 0.1616, 0.0364, 0.0515, 0.0998,
        0.0731, 0.1067, 0.0346, 0.0538, 0.1851], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,155][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1481, 0.0594, 0.0645, 0.0798, 0.0523, 0.0893, 0.0620, 0.0559, 0.0771,
        0.0518, 0.0758, 0.0570, 0.0413, 0.0859], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,157][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1517, 0.0170, 0.0360, 0.0221, 0.0868, 0.0673, 0.0440, 0.0593, 0.0939,
        0.0503, 0.1039, 0.0275, 0.0833, 0.1569], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,158][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.9505, 0.0016, 0.0023, 0.0012, 0.0032, 0.0021, 0.0082, 0.0020, 0.0024,
        0.0020, 0.0109, 0.0029, 0.0054, 0.0053], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,160][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0691, 0.0198, 0.0594, 0.0378, 0.0793, 0.1199, 0.0336, 0.0427, 0.0939,
        0.0495, 0.1129, 0.0223, 0.0771, 0.1828], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,161][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.7891, 0.0109, 0.0089, 0.0044, 0.0095, 0.0100, 0.0292, 0.0130, 0.0133,
        0.0157, 0.0267, 0.0146, 0.0300, 0.0247], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,163][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0423, 0.0224, 0.0770, 0.0430, 0.0800, 0.1171, 0.0384, 0.0678, 0.1203,
        0.0331, 0.1057, 0.0266, 0.0813, 0.1450], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,164][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0917, 0.0089, 0.0490, 0.0286, 0.0917, 0.1006, 0.0341, 0.0478, 0.0985,
        0.0459, 0.0949, 0.0135, 0.1167, 0.1782], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,166][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0400, 0.0710, 0.0512, 0.0595, 0.0821, 0.0526, 0.0708, 0.0339, 0.1054,
        0.0661, 0.0928, 0.0819, 0.1019, 0.0909], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,167][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7630, 0.0121, 0.0087, 0.0051, 0.0135, 0.0077, 0.0296, 0.0051, 0.0104,
        0.0254, 0.0366, 0.0157, 0.0299, 0.0373], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,169][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0211, 0.0108, 0.0320, 0.0515, 0.1294, 0.1031, 0.0372, 0.0225, 0.0799,
        0.0893, 0.0934, 0.0206, 0.1346, 0.1747], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,170][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0368, 0.0182, 0.0529, 0.0367, 0.0981, 0.0778, 0.0384, 0.0328, 0.0738,
        0.0550, 0.1175, 0.0230, 0.1087, 0.1518, 0.0785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,172][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0055, 0.0099, 0.0410, 0.0269, 0.0589, 0.1684, 0.0288, 0.0671, 0.1408,
        0.0669, 0.0936, 0.0225, 0.0503, 0.1523, 0.0668], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,173][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0823, 0.0619, 0.0543, 0.0786, 0.0513, 0.0938, 0.0565, 0.0593, 0.0851,
        0.0559, 0.0716, 0.0687, 0.0480, 0.0847, 0.0480], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,175][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0946, 0.0235, 0.0445, 0.0292, 0.0858, 0.0775, 0.0393, 0.0561, 0.1021,
        0.0487, 0.0879, 0.0309, 0.0671, 0.1463, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,176][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.8075, 0.0077, 0.0095, 0.0063, 0.0113, 0.0090, 0.0187, 0.0078, 0.0107,
        0.0067, 0.0290, 0.0128, 0.0188, 0.0185, 0.0257], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,178][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0256, 0.0201, 0.0565, 0.0393, 0.0726, 0.1236, 0.0262, 0.0473, 0.1008,
        0.0483, 0.0941, 0.0236, 0.0674, 0.1701, 0.0847], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,178][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.6373, 0.0154, 0.0145, 0.0071, 0.0203, 0.0173, 0.0354, 0.0163, 0.0212,
        0.0223, 0.0373, 0.0191, 0.0447, 0.0307, 0.0612], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,179][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0207, 0.0229, 0.0714, 0.0415, 0.0682, 0.1357, 0.0303, 0.0692, 0.1227,
        0.0371, 0.0923, 0.0273, 0.0701, 0.1379, 0.0527], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,179][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0425, 0.0078, 0.0533, 0.0296, 0.0697, 0.1134, 0.0239, 0.0567, 0.1044,
        0.0458, 0.1033, 0.0140, 0.0898, 0.1808, 0.0648], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,179][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0115, 0.0648, 0.0868, 0.0727, 0.0673, 0.0691, 0.0307, 0.0447, 0.1059,
        0.0644, 0.1059, 0.0581, 0.0704, 0.1120, 0.0356], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,180][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.6517, 0.0181, 0.0126, 0.0095, 0.0178, 0.0098, 0.0400, 0.0072, 0.0127,
        0.0299, 0.0347, 0.0241, 0.0342, 0.0455, 0.0521], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,180][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0089, 0.0088, 0.0307, 0.0373, 0.1261, 0.0997, 0.0321, 0.0275, 0.0892,
        0.0720, 0.0791, 0.0164, 0.1019, 0.1920, 0.0784], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,181][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0673, 0.0138, 0.0366, 0.0264, 0.1166, 0.0642, 0.0444, 0.0380, 0.0599,
        0.0436, 0.0812, 0.0130, 0.0575, 0.1413, 0.0933, 0.1030],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,182][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0095, 0.0056, 0.0353, 0.0230, 0.0486, 0.1663, 0.0252, 0.0532, 0.0823,
        0.0542, 0.0866, 0.0167, 0.0432, 0.1696, 0.0710, 0.1096],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,184][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.1373, 0.0495, 0.0517, 0.0734, 0.0439, 0.0806, 0.0522, 0.0555, 0.0761,
        0.0421, 0.0653, 0.0592, 0.0339, 0.0675, 0.0520, 0.0599],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,185][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.1188, 0.0111, 0.0299, 0.0192, 0.0694, 0.0616, 0.0360, 0.0581, 0.0818,
        0.0432, 0.0964, 0.0220, 0.0569, 0.1492, 0.0821, 0.0642],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,187][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.9322, 0.0019, 0.0027, 0.0014, 0.0035, 0.0025, 0.0092, 0.0023, 0.0031,
        0.0024, 0.0120, 0.0036, 0.0053, 0.0047, 0.0084, 0.0049],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,187][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0355, 0.0153, 0.0413, 0.0306, 0.0722, 0.0959, 0.0251, 0.0351, 0.0814,
        0.0413, 0.0876, 0.0175, 0.0645, 0.1594, 0.0821, 0.1154],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,189][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.8177, 0.0079, 0.0045, 0.0033, 0.0070, 0.0062, 0.0243, 0.0078, 0.0098,
        0.0105, 0.0151, 0.0104, 0.0209, 0.0136, 0.0233, 0.0177],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,190][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0359, 0.0170, 0.0671, 0.0330, 0.0746, 0.1211, 0.0302, 0.0538, 0.1036,
        0.0342, 0.0876, 0.0189, 0.0582, 0.1142, 0.0571, 0.0935],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,192][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0498, 0.0065, 0.0431, 0.0223, 0.0815, 0.0922, 0.0239, 0.0518, 0.0838,
        0.0344, 0.0859, 0.0095, 0.0734, 0.1720, 0.0925, 0.0775],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,193][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0105, 0.0532, 0.0531, 0.0666, 0.0587, 0.0558, 0.0382, 0.0488, 0.1050,
        0.0518, 0.1015, 0.0804, 0.0784, 0.1011, 0.0579, 0.0390],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,195][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.7452, 0.0119, 0.0062, 0.0053, 0.0104, 0.0072, 0.0277, 0.0039, 0.0088,
        0.0255, 0.0299, 0.0156, 0.0287, 0.0309, 0.0284, 0.0145],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,196][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0098, 0.0096, 0.0270, 0.0469, 0.1428, 0.1026, 0.0327, 0.0284, 0.0811,
        0.0634, 0.0646, 0.0143, 0.0635, 0.1538, 0.0845, 0.0749],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,198][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0679, 0.0108, 0.0294, 0.0200, 0.0848, 0.0503, 0.0434, 0.0236, 0.0454,
        0.0509, 0.0794, 0.0144, 0.0628, 0.1037, 0.0977, 0.1055, 0.1100],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,199][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0103, 0.0067, 0.0268, 0.0233, 0.0470, 0.1054, 0.0223, 0.0370, 0.0674,
        0.0509, 0.0730, 0.0227, 0.0315, 0.1370, 0.0676, 0.1591, 0.1119],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,201][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1569, 0.0455, 0.0519, 0.0622, 0.0421, 0.0652, 0.0523, 0.0452, 0.0573,
        0.0433, 0.0589, 0.0460, 0.0334, 0.0653, 0.0488, 0.0577, 0.0679],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,202][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1321, 0.0111, 0.0253, 0.0149, 0.0600, 0.0459, 0.0375, 0.0396, 0.0631,
        0.0384, 0.0701, 0.0179, 0.0518, 0.1108, 0.0814, 0.0743, 0.1258],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,203][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([9.4587e-01, 1.4134e-03, 1.8898e-03, 9.4144e-04, 2.6336e-03, 1.6887e-03,
        6.6448e-03, 1.6770e-03, 1.9469e-03, 1.4978e-03, 8.9719e-03, 2.3277e-03,
        4.1601e-03, 3.4893e-03, 7.1619e-03, 3.2673e-03, 4.4199e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,205][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0416, 0.0118, 0.0363, 0.0260, 0.0550, 0.0845, 0.0226, 0.0302, 0.0686,
        0.0350, 0.0798, 0.0163, 0.0554, 0.1283, 0.0809, 0.1130, 0.1149],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,206][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.7404, 0.0093, 0.0081, 0.0038, 0.0092, 0.0090, 0.0274, 0.0125, 0.0116,
        0.0139, 0.0234, 0.0122, 0.0275, 0.0202, 0.0349, 0.0170, 0.0198],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,208][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0323, 0.0154, 0.0555, 0.0301, 0.0599, 0.0856, 0.0268, 0.0486, 0.0888,
        0.0254, 0.0802, 0.0191, 0.0545, 0.1058, 0.0572, 0.1096, 0.1050],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,209][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0400, 0.0055, 0.0335, 0.0206, 0.0705, 0.0702, 0.0208, 0.0294, 0.0657,
        0.0319, 0.0774, 0.0100, 0.0726, 0.1387, 0.0979, 0.0982, 0.1170],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,210][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0435, 0.0391, 0.0355, 0.0408, 0.0517, 0.0386, 0.0540, 0.0242, 0.0759,
        0.0493, 0.0636, 0.0535, 0.0687, 0.0693, 0.0608, 0.1126, 0.1189],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,212][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7502, 0.0084, 0.0064, 0.0035, 0.0096, 0.0059, 0.0224, 0.0038, 0.0079,
        0.0203, 0.0267, 0.0118, 0.0234, 0.0271, 0.0306, 0.0146, 0.0275],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,214][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0110, 0.0065, 0.0203, 0.0340, 0.1041, 0.0808, 0.0254, 0.0171, 0.0606,
        0.0673, 0.0740, 0.0135, 0.0955, 0.1332, 0.0907, 0.0801, 0.0858],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,215][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:32,217][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 7646],
        [20651],
        [ 6232],
        [10496],
        [19091],
        [ 7839],
        [ 7858],
        [ 2893],
        [ 2083],
        [  700],
        [ 1806],
        [ 5347],
        [ 3061],
        [ 2310],
        [ 1726],
        [ 2825],
        [  879]], device='cuda:0')
[2024-07-24 10:21:32,218][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8382],
        [18356],
        [ 8194],
        [ 8627],
        [21898],
        [11861],
        [10607],
        [ 5216],
        [ 4285],
        [ 1514],
        [ 4720],
        [ 8791],
        [ 5310],
        [ 5046],
        [ 3866],
        [ 7337],
        [ 2319]], device='cuda:0')
[2024-07-24 10:21:32,220][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[39906],
        [43091],
        [40581],
        [38671],
        [40898],
        [40628],
        [38410],
        [39345],
        [38574],
        [39159],
        [39649],
        [40165],
        [40349],
        [41304],
        [40717],
        [40075],
        [41202]], device='cuda:0')
[2024-07-24 10:21:32,221][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 5432],
        [ 9094],
        [ 7351],
        [25156],
        [26121],
        [19944],
        [26222],
        [22316],
        [16423],
        [18558],
        [14011],
        [20389],
        [20128],
        [17032],
        [20682],
        [17188],
        [16841]], device='cuda:0')
[2024-07-24 10:21:32,223][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[464],
        [455],
        [551],
        [543],
        [556],
        [590],
        [648],
        [666],
        [693],
        [728],
        [767],
        [783],
        [811],
        [836],
        [841],
        [858],
        [873]], device='cuda:0')
[2024-07-24 10:21:32,224][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 4807],
        [ 5563],
        [ 6895],
        [ 6221],
        [ 8927],
        [ 7397],
        [13607],
        [14229],
        [12663],
        [15587],
        [16165],
        [16037],
        [17322],
        [16059],
        [16728],
        [15497],
        [16578]], device='cuda:0')
[2024-07-24 10:21:32,226][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 8726],
        [ 8306],
        [ 8640],
        [ 8488],
        [ 6339],
        [ 8531],
        [ 9503],
        [ 8837],
        [ 8760],
        [16458],
        [10931],
        [28131],
        [45466],
        [15469],
        [38358],
        [13761],
        [14285]], device='cuda:0')
[2024-07-24 10:21:32,227][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 4757],
        [11188],
        [11058],
        [10888],
        [10903],
        [10440],
        [10392],
        [10214],
        [10012],
        [ 9840],
        [ 9650],
        [ 9594],
        [ 9592],
        [ 9594],
        [ 9621],
        [ 9760],
        [ 9943]], device='cuda:0')
[2024-07-24 10:21:32,229][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[  592],
        [16519],
        [ 1377],
        [ 3785],
        [  342],
        [  408],
        [  326],
        [  532],
        [  330],
        [   43],
        [  155],
        [  273],
        [   37],
        [  146],
        [   89],
        [  112],
        [  119]], device='cuda:0')
[2024-07-24 10:21:32,230][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[32841],
        [38650],
        [38653],
        [39579],
        [39587],
        [38941],
        [38081],
        [37631],
        [36902],
        [36232],
        [35945],
        [35884],
        [35971],
        [35730],
        [35548],
        [35502],
        [35396]], device='cuda:0')
[2024-07-24 10:21:32,232][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[24897],
        [29566],
        [31456],
        [33471],
        [38783],
        [36855],
        [36782],
        [36047],
        [34440],
        [35506],
        [36026],
        [36031],
        [36760],
        [36620],
        [36642],
        [36020],
        [34812]], device='cuda:0')
[2024-07-24 10:21:32,233][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[36235],
        [47997],
        [47114],
        [46577],
        [46071],
        [45084],
        [43185],
        [44420],
        [44676],
        [43746],
        [44495],
        [44570],
        [44245],
        [44044],
        [43549],
        [43417],
        [43340]], device='cuda:0')
[2024-07-24 10:21:32,234][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[32382],
        [25363],
        [33924],
        [31321],
        [38658],
        [40105],
        [34121],
        [32781],
        [30291],
        [31712],
        [32304],
        [32073],
        [29622],
        [33623],
        [31769],
        [29795],
        [31754]], device='cuda:0')
[2024-07-24 10:21:32,236][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[17800],
        [34273],
        [15081],
        [15504],
        [11627],
        [ 8418],
        [ 8719],
        [ 9129],
        [ 8860],
        [ 9407],
        [11508],
        [ 9410],
        [10925],
        [10755],
        [ 9731],
        [ 9449],
        [ 9405]], device='cuda:0')
[2024-07-24 10:21:32,238][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[32455],
        [ 3871],
        [ 7223],
        [11221],
        [15505],
        [18687],
        [21746],
        [11575],
        [15304],
        [13334],
        [ 9357],
        [10743],
        [12028],
        [17940],
        [12724],
        [16659],
        [11047]], device='cuda:0')
[2024-07-24 10:21:32,239][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[13745],
        [12099],
        [14863],
        [13371],
        [18466],
        [23985],
        [25906],
        [26463],
        [30281],
        [30024],
        [31366],
        [33303],
        [29711],
        [24971],
        [25620],
        [27107],
        [25090]], device='cuda:0')
[2024-07-24 10:21:32,240][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[49253],
        [39378],
        [21390],
        [23661],
        [22779],
        [14671],
        [13758],
        [14828],
        [14888],
        [12788],
        [12874],
        [12600],
        [14145],
        [14722],
        [14990],
        [15300],
        [15595]], device='cuda:0')
[2024-07-24 10:21:32,241][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[17075],
        [12376],
        [10464],
        [ 9724],
        [11688],
        [11397],
        [12807],
        [13463],
        [13040],
        [13248],
        [11801],
        [11808],
        [11592],
        [10398],
        [ 9794],
        [ 9755],
        [ 8974]], device='cuda:0')
[2024-07-24 10:21:32,242][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[20995],
        [23724],
        [20896],
        [24238],
        [23839],
        [18800],
        [20769],
        [19220],
        [16224],
        [15955],
        [13394],
        [13748],
        [15836],
        [12832],
        [13541],
        [12441],
        [10658]], device='cuda:0')
[2024-07-24 10:21:32,243][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[15986],
        [15587],
        [15418],
        [15252],
        [14118],
        [15917],
        [17809],
        [16688],
        [16230],
        [16976],
        [18056],
        [17456],
        [19901],
        [16601],
        [20415],
        [16845],
        [16771]], device='cuda:0')
[2024-07-24 10:21:32,245][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[15413],
        [31499],
        [48474],
        [47665],
        [48703],
        [47273],
        [47182],
        [47974],
        [47326],
        [47524],
        [47503],
        [47491],
        [46844],
        [47230],
        [47265],
        [47500],
        [47328]], device='cuda:0')
[2024-07-24 10:21:32,246][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[43983],
        [44086],
        [43507],
        [43433],
        [37102],
        [42220],
        [15331],
        [28279],
        [35623],
        [28507],
        [15457],
        [21852],
        [18303],
        [26820],
        [16075],
        [32039],
        [24622]], device='cuda:0')
[2024-07-24 10:21:32,248][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[46895],
        [32269],
        [32178],
        [31404],
        [34447],
        [35932],
        [34022],
        [33828],
        [37847],
        [37531],
        [38256],
        [38925],
        [39363],
        [40366],
        [39985],
        [39919],
        [40415]], device='cuda:0')
[2024-07-24 10:21:32,249][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[33263],
        [32953],
        [32388],
        [32509],
        [29402],
        [27754],
        [30473],
        [28454],
        [28007],
        [29928],
        [29743],
        [29608],
        [30384],
        [29863],
        [29895],
        [29328],
        [29760]], device='cuda:0')
[2024-07-24 10:21:32,251][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[35710],
        [15749],
        [14693],
        [12170],
        [12129],
        [12164],
        [ 8470],
        [10162],
        [10417],
        [ 9711],
        [11080],
        [11225],
        [10944],
        [10779],
        [10325],
        [10407],
        [10705]], device='cuda:0')
[2024-07-24 10:21:32,252][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[27462],
        [27579],
        [28566],
        [28458],
        [33652],
        [29544],
        [38130],
        [32600],
        [30809],
        [36167],
        [37642],
        [35995],
        [40232],
        [35877],
        [38214],
        [36159],
        [34936]], device='cuda:0')
[2024-07-24 10:21:32,254][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[20250],
        [33622],
        [42295],
        [40608],
        [42517],
        [46387],
        [47609],
        [47019],
        [45990],
        [46548],
        [45647],
        [46372],
        [45061],
        [42217],
        [42684],
        [43165],
        [41823]], device='cuda:0')
[2024-07-24 10:21:32,255][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[3622],
        [4766],
        [5781],
        [7313],
        [7904],
        [7609],
        [8363],
        [7759],
        [7358],
        [7076],
        [7839],
        [7196],
        [7257],
        [8494],
        [8257],
        [7600],
        [8833]], device='cuda:0')
[2024-07-24 10:21:32,256][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[13749],
        [47541],
        [39969],
        [35502],
        [20956],
        [24005],
        [20594],
        [30696],
        [29674],
        [30881],
        [32436],
        [31229],
        [26723],
        [28140],
        [31261],
        [27958],
        [34499]], device='cuda:0')
[2024-07-24 10:21:32,258][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616],
        [15616]], device='cuda:0')
[2024-07-24 10:21:32,303][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:32,304][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,305][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,306][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,306][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,307][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,308][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,309][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,310][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,311][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,313][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,314][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,315][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,317][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.9745, 0.0255], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,318][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.7678, 0.2322], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,320][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.3938, 0.6062], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,322][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.2016, 0.7984], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,323][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.9575, 0.0425], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,325][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.9471, 0.0529], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,326][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.6038, 0.3962], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,328][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.1161, 0.8839], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,330][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.9867, 0.0133], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,331][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.1926, 0.8074], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,333][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.0060, 0.9940], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,334][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.8506, 0.1494], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,336][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.8595, 0.0976, 0.0430], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,337][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.1624, 0.3998, 0.4378], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,339][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.1712, 0.3202, 0.5086], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,340][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.1205, 0.3861, 0.4933], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,342][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9462, 0.0074, 0.0464], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,344][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.9506, 0.0246, 0.0248], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,345][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.3100, 0.2754, 0.4146], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,347][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0425, 0.4632, 0.4943], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,348][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.9845, 0.0098, 0.0057], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,350][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.1137, 0.3194, 0.5670], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,352][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0013, 0.3658, 0.6329], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,353][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.7247, 0.1039, 0.1713], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,355][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.6321, 0.0980, 0.2442, 0.0257], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,356][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.1455, 0.2231, 0.2652, 0.3662], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,358][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.3578, 0.1810, 0.3126, 0.1486], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,360][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0847, 0.2814, 0.3503, 0.2836], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,361][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.9314, 0.0111, 0.0423, 0.0152], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,363][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.8677, 0.0407, 0.0563, 0.0353], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,364][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.1838, 0.2120, 0.3554, 0.2488], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,366][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0326, 0.3263, 0.3326, 0.3085], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,367][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.9922, 0.0034, 0.0026, 0.0018], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,367][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0998, 0.1405, 0.4743, 0.2854], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,368][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0030, 0.3469, 0.3588, 0.2913], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,369][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.1672, 0.1031, 0.3908, 0.3389], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,370][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.8905, 0.0155, 0.0768, 0.0062, 0.0110], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,372][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0837, 0.1002, 0.1605, 0.0896, 0.5660], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,374][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.1629, 0.2306, 0.2829, 0.1290, 0.1947], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,375][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0577, 0.1847, 0.2470, 0.2115, 0.2992], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,377][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.9369, 0.0103, 0.0322, 0.0120, 0.0086], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,378][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.6650, 0.0516, 0.0843, 0.0376, 0.1615], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,380][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.1075, 0.1799, 0.2439, 0.2121, 0.2566], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,381][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0078, 0.3137, 0.3289, 0.2419, 0.1077], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,383][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.9611, 0.0103, 0.0075, 0.0057, 0.0153], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,385][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0287, 0.0974, 0.2025, 0.2416, 0.4299], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,386][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ got] are: tensor([2.9449e-04, 1.9342e-01, 3.6045e-01, 2.5787e-01, 1.8796e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,387][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.2968, 0.0457, 0.4133, 0.1304, 0.1137], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,389][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.7751, 0.0356, 0.0940, 0.0146, 0.0489, 0.0317], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,391][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0458, 0.0685, 0.0767, 0.0991, 0.4507, 0.2592], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,392][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.3768, 0.1420, 0.1475, 0.0831, 0.1052, 0.1453], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,394][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0545, 0.1321, 0.1877, 0.1504, 0.2267, 0.2486], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,396][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.9178, 0.0084, 0.0399, 0.0120, 0.0122, 0.0096], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,397][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.6781, 0.0295, 0.0486, 0.0183, 0.1775, 0.0479], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,399][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0876, 0.1265, 0.1756, 0.1802, 0.2030, 0.2271], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,400][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0131, 0.2699, 0.2614, 0.2309, 0.1135, 0.1113], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,402][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.9729, 0.0055, 0.0037, 0.0026, 0.0095, 0.0058], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,404][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0169, 0.0296, 0.0869, 0.1043, 0.2965, 0.4659], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,405][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([2.6118e-04, 8.1168e-02, 1.7899e-01, 1.4619e-01, 3.8105e-01, 2.1234e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,406][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1832, 0.1013, 0.4197, 0.1280, 0.1174, 0.0504], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,408][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.3476, 0.0568, 0.1886, 0.0224, 0.0958, 0.2668, 0.0220],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,410][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0726, 0.0533, 0.0778, 0.0561, 0.3229, 0.3335, 0.0839],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,411][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.3311, 0.0908, 0.1779, 0.0637, 0.0952, 0.1618, 0.0795],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,413][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0116, 0.1156, 0.1705, 0.1055, 0.2449, 0.2416, 0.1102],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,414][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([9.5264e-01, 1.0532e-02, 2.0283e-02, 7.0545e-03, 4.3678e-03, 4.1972e-03,
        9.2303e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,416][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.1734, 0.0729, 0.0872, 0.0540, 0.2781, 0.1634, 0.1710],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,417][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0493, 0.1248, 0.1639, 0.1609, 0.1749, 0.1985, 0.1277],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,419][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0076, 0.2644, 0.2609, 0.2065, 0.0935, 0.1061, 0.0610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,421][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.7747, 0.0368, 0.0262, 0.0197, 0.0487, 0.0375, 0.0563],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,422][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0016, 0.0584, 0.0858, 0.1289, 0.2090, 0.4953, 0.0211],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,423][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([2.3062e-05, 6.7389e-02, 2.6043e-01, 1.2469e-01, 1.8390e-01, 3.6146e-01,
        2.0955e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,425][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0429, 0.1251, 0.0945, 0.3643, 0.1468, 0.1343, 0.0921],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,427][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.4512, 0.0940, 0.0927, 0.0489, 0.0791, 0.0789, 0.1218, 0.0334],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,428][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0333, 0.0451, 0.0709, 0.0558, 0.3423, 0.2328, 0.0750, 0.1447],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,429][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.2480, 0.0989, 0.1390, 0.0639, 0.1150, 0.1417, 0.0937, 0.0997],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,429][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0294, 0.1055, 0.1438, 0.1059, 0.1838, 0.2012, 0.0942, 0.1362],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,430][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.9477, 0.0076, 0.0218, 0.0074, 0.0059, 0.0061, 0.0013, 0.0024],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,431][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.1917, 0.0509, 0.0695, 0.0510, 0.3060, 0.1098, 0.1323, 0.0887],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,433][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0441, 0.1035, 0.1295, 0.1541, 0.1465, 0.1685, 0.1236, 0.1301],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,434][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0076, 0.2481, 0.2348, 0.2010, 0.0912, 0.0930, 0.0580, 0.0663],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,436][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.9150, 0.0097, 0.0071, 0.0053, 0.0200, 0.0114, 0.0254, 0.0062],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,438][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0034, 0.0359, 0.0461, 0.1109, 0.1658, 0.5639, 0.0251, 0.0489],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,439][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ at] are: tensor([2.2652e-04, 8.7955e-02, 1.7940e-01, 1.1957e-01, 2.5160e-01, 2.7082e-01,
        9.5662e-03, 8.0866e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,440][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0936, 0.0395, 0.2094, 0.1418, 0.1743, 0.1774, 0.0782, 0.0856],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,442][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.4988, 0.0614, 0.0475, 0.0274, 0.0611, 0.0436, 0.0234, 0.1589, 0.0780],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,444][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0247, 0.0384, 0.0510, 0.0466, 0.3079, 0.2113, 0.0576, 0.1140, 0.1484],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,445][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3160, 0.1001, 0.1051, 0.0542, 0.0831, 0.1128, 0.0678, 0.0743, 0.0866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,447][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0389, 0.0847, 0.1215, 0.0939, 0.1538, 0.1624, 0.0743, 0.1182, 0.1524],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,449][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.9381, 0.0054, 0.0241, 0.0076, 0.0077, 0.0073, 0.0016, 0.0042, 0.0041],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,450][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.4667, 0.0223, 0.0337, 0.0136, 0.1533, 0.0505, 0.0731, 0.0792, 0.1076],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,452][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0408, 0.0851, 0.1178, 0.1289, 0.1283, 0.1547, 0.1076, 0.1277, 0.1090],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,454][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0096, 0.2155, 0.2063, 0.1860, 0.0903, 0.0894, 0.0570, 0.0688, 0.0769],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,455][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.9493, 0.0052, 0.0038, 0.0026, 0.0097, 0.0061, 0.0153, 0.0034, 0.0046],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,457][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0064, 0.0194, 0.0484, 0.0706, 0.1893, 0.4166, 0.0269, 0.0479, 0.1746],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,458][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([2.1984e-04, 6.6549e-02, 1.0315e-01, 1.0393e-01, 1.8169e-01, 1.7690e-01,
        8.2460e-03, 9.6129e-02, 2.6318e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,460][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1485, 0.0334, 0.2671, 0.0597, 0.0881, 0.0402, 0.0774, 0.2390, 0.0466],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,461][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.1030, 0.0448, 0.1084, 0.0121, 0.0687, 0.1855, 0.0102, 0.2211, 0.2410,
        0.0053], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,463][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.1865, 0.0197, 0.0445, 0.0253, 0.1631, 0.1972, 0.0343, 0.0961, 0.1706,
        0.0626], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,465][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.3108, 0.0665, 0.1212, 0.0484, 0.0694, 0.1020, 0.0467, 0.0702, 0.1043,
        0.0607], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,466][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0224, 0.0781, 0.1125, 0.0867, 0.1429, 0.1478, 0.0680, 0.1000, 0.1326,
        0.1091], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,468][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ station] are: tensor([9.5193e-01, 5.5087e-03, 1.8971e-02, 6.2992e-03, 4.6371e-03, 4.0490e-03,
        9.3972e-04, 2.2119e-03, 2.1853e-03, 3.2665e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,469][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.2887, 0.0253, 0.0478, 0.0183, 0.1998, 0.0920, 0.0799, 0.0815, 0.1118,
        0.0550], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,471][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0209, 0.0623, 0.0940, 0.0978, 0.1165, 0.1450, 0.0871, 0.1196, 0.0910,
        0.1657], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,473][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0094, 0.1915, 0.1914, 0.1782, 0.0806, 0.0780, 0.0496, 0.0597, 0.0696,
        0.0920], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,474][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.9267, 0.0074, 0.0051, 0.0034, 0.0125, 0.0083, 0.0185, 0.0044, 0.0060,
        0.0077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,476][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0023, 0.0172, 0.0448, 0.0574, 0.1579, 0.3982, 0.0174, 0.0350, 0.1763,
        0.0935], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,478][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0005, 0.0373, 0.0789, 0.0589, 0.2531, 0.1459, 0.0077, 0.0877, 0.2427,
        0.0873], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,479][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.1416, 0.0503, 0.1517, 0.1564, 0.0673, 0.0609, 0.0894, 0.1734, 0.0585,
        0.0505], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,481][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.5432, 0.0447, 0.0213, 0.0147, 0.0442, 0.0396, 0.0194, 0.0456, 0.1831,
        0.0147, 0.0295], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,483][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0184, 0.0306, 0.0420, 0.0485, 0.2086, 0.1845, 0.0416, 0.0993, 0.1509,
        0.1284, 0.0474], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,484][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.2099, 0.0814, 0.1072, 0.0513, 0.0776, 0.1066, 0.0588, 0.0727, 0.0864,
        0.0603, 0.0878], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,486][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0195, 0.0728, 0.0993, 0.0852, 0.1225, 0.1270, 0.0540, 0.0907, 0.1193,
        0.1013, 0.1084], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,487][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [,] are: tensor([9.6332e-01, 2.6651e-03, 1.3424e-02, 3.5328e-03, 4.5333e-03, 3.2403e-03,
        8.3214e-04, 2.2678e-03, 1.7560e-03, 2.9585e-03, 1.4708e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,489][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.2601, 0.0199, 0.0371, 0.0143, 0.1612, 0.0502, 0.0625, 0.0683, 0.1004,
        0.0456, 0.1805], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,490][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0288, 0.0621, 0.0853, 0.0985, 0.0994, 0.1102, 0.0795, 0.0906, 0.0793,
        0.1493, 0.1170], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,491][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0059, 0.2089, 0.1845, 0.1632, 0.0681, 0.0712, 0.0416, 0.0528, 0.0626,
        0.0841, 0.0570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,492][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.8761, 0.0105, 0.0069, 0.0052, 0.0184, 0.0125, 0.0299, 0.0077, 0.0107,
        0.0136, 0.0084], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,492][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0026, 0.0168, 0.0359, 0.0749, 0.1070, 0.3390, 0.0185, 0.0362, 0.1693,
        0.1186, 0.0813], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,494][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [,] are: tensor([8.0609e-05, 5.5509e-02, 8.8142e-02, 6.5993e-02, 1.3699e-01, 1.6038e-01,
        4.0037e-03, 6.2603e-02, 2.6127e-01, 5.8523e-02, 1.0650e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,495][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0421, 0.0333, 0.0330, 0.1656, 0.1990, 0.1221, 0.1041, 0.1218, 0.0500,
        0.1101, 0.0190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,497][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.2416, 0.0333, 0.0940, 0.0087, 0.0360, 0.0768, 0.0276, 0.0546, 0.2146,
        0.0142, 0.1907, 0.0077], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,499][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0279, 0.0298, 0.0358, 0.0485, 0.1882, 0.1821, 0.0381, 0.1020, 0.1605,
        0.0866, 0.0495, 0.0511], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,500][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.1825, 0.0814, 0.1126, 0.0551, 0.0656, 0.0948, 0.0440, 0.0665, 0.0827,
        0.0581, 0.1023, 0.0544], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,502][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0156, 0.0684, 0.0945, 0.0712, 0.1163, 0.1208, 0.0599, 0.0820, 0.1103,
        0.1000, 0.0987, 0.0623], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,504][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.9105, 0.0115, 0.0292, 0.0131, 0.0086, 0.0078, 0.0016, 0.0038, 0.0044,
        0.0061, 0.0024, 0.0011], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,506][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.2498, 0.0218, 0.0352, 0.0215, 0.1796, 0.0520, 0.0814, 0.0643, 0.0902,
        0.0561, 0.1177, 0.0305], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,507][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0153, 0.0479, 0.0698, 0.0706, 0.0908, 0.1145, 0.0697, 0.1028, 0.0800,
        0.1302, 0.0991, 0.1093], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,509][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0111, 0.1594, 0.1551, 0.1478, 0.0762, 0.0652, 0.0476, 0.0539, 0.0608,
        0.0816, 0.0602, 0.0812], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,511][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.9079, 0.0074, 0.0055, 0.0036, 0.0122, 0.0098, 0.0214, 0.0052, 0.0077,
        0.0100, 0.0055, 0.0038], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,512][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0010, 0.0121, 0.0381, 0.0296, 0.1145, 0.3631, 0.0118, 0.0323, 0.1676,
        0.1246, 0.0849, 0.0204], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,514][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0005, 0.0722, 0.0919, 0.0767, 0.1662, 0.0886, 0.0082, 0.0628, 0.2098,
        0.0920, 0.0845, 0.0465], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,516][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0270, 0.0235, 0.1068, 0.1187, 0.0578, 0.1072, 0.0380, 0.1158, 0.1034,
        0.0708, 0.1304, 0.1006], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,518][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.3919, 0.0165, 0.0818, 0.0104, 0.0170, 0.0464, 0.0096, 0.0994, 0.1268,
        0.0094, 0.1809, 0.0075, 0.0024], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,519][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0466, 0.0259, 0.0402, 0.0293, 0.1404, 0.1730, 0.0357, 0.0838, 0.1418,
        0.0924, 0.0476, 0.0368, 0.1065], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,521][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.2308, 0.0687, 0.0890, 0.0362, 0.0713, 0.0883, 0.0425, 0.0634, 0.0676,
        0.0476, 0.0775, 0.0392, 0.0780], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,523][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0146, 0.0611, 0.0924, 0.0700, 0.1080, 0.1131, 0.0496, 0.0841, 0.1008,
        0.0795, 0.0922, 0.0543, 0.0803], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,524][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([9.5793e-01, 3.2540e-03, 1.5270e-02, 4.2278e-03, 3.8408e-03, 3.2509e-03,
        8.4557e-04, 1.7358e-03, 1.7969e-03, 2.5664e-03, 1.3113e-03, 4.6034e-04,
        3.5121e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,526][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.1226, 0.0203, 0.0361, 0.0172, 0.1284, 0.0684, 0.0443, 0.0720, 0.1674,
        0.0360, 0.1387, 0.0238, 0.1248], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,528][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0145, 0.0449, 0.0589, 0.0624, 0.0729, 0.0939, 0.0609, 0.0769, 0.0714,
        0.1294, 0.0952, 0.1147, 0.1040], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,529][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0082, 0.1599, 0.1633, 0.1438, 0.0651, 0.0626, 0.0412, 0.0453, 0.0535,
        0.0768, 0.0528, 0.0756, 0.0519], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,531][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.8340, 0.0131, 0.0096, 0.0070, 0.0208, 0.0165, 0.0320, 0.0081, 0.0119,
        0.0138, 0.0091, 0.0065, 0.0176], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,533][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0021, 0.0180, 0.0369, 0.0602, 0.0919, 0.3423, 0.0132, 0.0301, 0.1454,
        0.1015, 0.0531, 0.0293, 0.0761], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,534][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0002, 0.0683, 0.1409, 0.0497, 0.1263, 0.1595, 0.0044, 0.0616, 0.1507,
        0.0524, 0.0982, 0.0471, 0.0405], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,536][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.2012, 0.0262, 0.0444, 0.0989, 0.0652, 0.0439, 0.0873, 0.1291, 0.0593,
        0.0280, 0.1423, 0.0629, 0.0112], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,538][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1995, 0.0354, 0.0350, 0.0162, 0.0397, 0.0488, 0.0158, 0.0741, 0.1753,
        0.0179, 0.1931, 0.0133, 0.0482, 0.0878], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,540][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0108, 0.0276, 0.0355, 0.0345, 0.1826, 0.1285, 0.0521, 0.0859, 0.1107,
        0.0991, 0.0381, 0.0443, 0.1091, 0.0411], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,541][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1689, 0.0698, 0.0895, 0.0410, 0.0640, 0.0805, 0.0464, 0.0600, 0.0713,
        0.0471, 0.0765, 0.0431, 0.0703, 0.0716], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,543][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0188, 0.0492, 0.0753, 0.0634, 0.0893, 0.0985, 0.0393, 0.0757, 0.0956,
        0.0750, 0.0882, 0.0534, 0.0734, 0.1050], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,545][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.4138e-01, 3.3553e-03, 2.0084e-02, 4.3764e-03, 5.7942e-03, 4.5553e-03,
        9.5237e-04, 3.0893e-03, 2.6221e-03, 3.6908e-03, 1.9813e-03, 5.9755e-04,
        5.3232e-03, 2.1950e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,546][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.3297, 0.0114, 0.0177, 0.0100, 0.0779, 0.0294, 0.0435, 0.0305, 0.0535,
        0.0300, 0.0804, 0.0158, 0.0920, 0.1781], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,548][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0203, 0.0363, 0.0586, 0.0630, 0.0654, 0.0755, 0.0606, 0.0683, 0.0564,
        0.1109, 0.0872, 0.1155, 0.1056, 0.0763], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,550][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0059, 0.1589, 0.1520, 0.1267, 0.0569, 0.0617, 0.0386, 0.0453, 0.0544,
        0.0739, 0.0498, 0.0739, 0.0534, 0.0487], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,551][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.8678, 0.0078, 0.0057, 0.0043, 0.0145, 0.0099, 0.0235, 0.0062, 0.0087,
        0.0104, 0.0070, 0.0046, 0.0176, 0.0120], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,552][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0051, 0.0125, 0.0276, 0.0476, 0.0897, 0.2174, 0.0188, 0.0273, 0.1102,
        0.0892, 0.0774, 0.0257, 0.1095, 0.1421], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,552][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0003, 0.0366, 0.0694, 0.0506, 0.1456, 0.1033, 0.0066, 0.0522, 0.1927,
        0.0654, 0.0911, 0.0486, 0.0427, 0.0949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,553][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0487, 0.0216, 0.1133, 0.0450, 0.1366, 0.1129, 0.0467, 0.0911, 0.0986,
        0.0723, 0.0873, 0.0613, 0.0224, 0.0422], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,555][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.1352, 0.0120, 0.0857, 0.0046, 0.0130, 0.0270, 0.0045, 0.1382, 0.1011,
        0.0075, 0.1500, 0.0030, 0.0071, 0.2972, 0.0138], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,557][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0143, 0.0252, 0.0360, 0.0193, 0.1998, 0.1310, 0.0416, 0.0974, 0.0963,
        0.0625, 0.0369, 0.0239, 0.0966, 0.0489, 0.0702], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,558][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.1649, 0.0607, 0.0800, 0.0394, 0.0543, 0.0808, 0.0471, 0.0587, 0.0621,
        0.0431, 0.0687, 0.0413, 0.0693, 0.0740, 0.0556], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,560][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0144, 0.0471, 0.0695, 0.0558, 0.0779, 0.0912, 0.0356, 0.0700, 0.0874,
        0.0633, 0.0780, 0.0436, 0.0608, 0.1015, 0.1040], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,561][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ give] are: tensor([9.5869e-01, 2.8218e-03, 1.2746e-02, 3.6765e-03, 3.1199e-03, 3.1827e-03,
        8.3347e-04, 1.7729e-03, 1.7178e-03, 2.3842e-03, 1.3195e-03, 4.7684e-04,
        3.3502e-03, 1.3161e-03, 2.5911e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,563][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.1436, 0.0163, 0.0303, 0.0147, 0.0996, 0.0483, 0.0383, 0.0412, 0.0719,
        0.0281, 0.0893, 0.0193, 0.0928, 0.2000, 0.0662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,565][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0189, 0.0367, 0.0565, 0.0549, 0.0669, 0.0752, 0.0585, 0.0683, 0.0579,
        0.1062, 0.0803, 0.0952, 0.0908, 0.0748, 0.0589], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,567][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0093, 0.1369, 0.1399, 0.1141, 0.0557, 0.0572, 0.0406, 0.0451, 0.0522,
        0.0716, 0.0510, 0.0634, 0.0482, 0.0510, 0.0638], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,568][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.7696, 0.0134, 0.0107, 0.0086, 0.0207, 0.0178, 0.0340, 0.0098, 0.0137,
        0.0161, 0.0102, 0.0072, 0.0233, 0.0172, 0.0277], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,570][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0022, 0.0158, 0.0262, 0.0614, 0.0744, 0.2487, 0.0143, 0.0267, 0.1149,
        0.0724, 0.0584, 0.0280, 0.0719, 0.1415, 0.0432], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,571][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ give] are: tensor([1.8505e-04, 3.9676e-02, 1.1049e-01, 4.7378e-02, 8.0777e-02, 1.1770e-01,
        4.0046e-03, 6.0942e-02, 1.9401e-01, 5.8375e-02, 9.2862e-02, 3.1467e-02,
        3.4788e-02, 1.0100e-01, 2.6345e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,573][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0185, 0.0193, 0.0816, 0.0796, 0.0849, 0.0549, 0.0423, 0.1545, 0.0926,
        0.0526, 0.0964, 0.0755, 0.0146, 0.0851, 0.0476], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,575][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0595, 0.0241, 0.0449, 0.0072, 0.0215, 0.0337, 0.0036, 0.1893, 0.0637,
        0.0081, 0.2460, 0.0049, 0.0109, 0.1970, 0.0710, 0.0145],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,577][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0196, 0.0216, 0.0322, 0.0188, 0.1506, 0.1244, 0.0307, 0.0760, 0.0928,
        0.0658, 0.0366, 0.0249, 0.0701, 0.0484, 0.0684, 0.1192],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,579][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.3108, 0.0511, 0.0761, 0.0303, 0.0499, 0.0602, 0.0319, 0.0459, 0.0434,
        0.0323, 0.0561, 0.0278, 0.0486, 0.0570, 0.0457, 0.0328],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,580][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0172, 0.0379, 0.0600, 0.0490, 0.0769, 0.0834, 0.0376, 0.0632, 0.0818,
        0.0625, 0.0673, 0.0398, 0.0574, 0.0887, 0.0925, 0.0849],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,582][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ it] are: tensor([9.5886e-01, 1.7375e-03, 1.0975e-02, 2.9957e-03, 2.9973e-03, 2.8865e-03,
        6.6526e-04, 1.8110e-03, 1.5983e-03, 2.3950e-03, 1.3270e-03, 4.1570e-04,
        3.2415e-03, 1.5499e-03, 3.0778e-03, 3.4634e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,584][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.2709, 0.0134, 0.0183, 0.0105, 0.0895, 0.0314, 0.0403, 0.0250, 0.0448,
        0.0227, 0.0658, 0.0116, 0.0469, 0.1267, 0.0931, 0.0890],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,585][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0284, 0.0362, 0.0545, 0.0541, 0.0621, 0.0689, 0.0580, 0.0631, 0.0528,
        0.0894, 0.0732, 0.0911, 0.0849, 0.0691, 0.0582, 0.0558],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,587][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0070, 0.1349, 0.1320, 0.1111, 0.0529, 0.0534, 0.0373, 0.0404, 0.0477,
        0.0699, 0.0471, 0.0644, 0.0477, 0.0461, 0.0634, 0.0447],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,589][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.8693, 0.0055, 0.0048, 0.0034, 0.0118, 0.0091, 0.0192, 0.0049, 0.0068,
        0.0084, 0.0053, 0.0032, 0.0116, 0.0092, 0.0174, 0.0102],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,590][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0039, 0.0094, 0.0194, 0.0480, 0.0856, 0.1943, 0.0134, 0.0249, 0.0994,
        0.0670, 0.0514, 0.0209, 0.0780, 0.1473, 0.0410, 0.0961],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,592][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0003, 0.0326, 0.0614, 0.0624, 0.1046, 0.1023, 0.0061, 0.0588, 0.1510,
        0.0656, 0.0760, 0.0461, 0.0285, 0.0762, 0.0504, 0.0776],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,594][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0611, 0.0184, 0.0668, 0.0558, 0.0621, 0.0186, 0.0585, 0.2537, 0.0301,
        0.0344, 0.1734, 0.0597, 0.0161, 0.0444, 0.0263, 0.0207],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,596][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1884, 0.0168, 0.0416, 0.0095, 0.0227, 0.0297, 0.0151, 0.0409, 0.1235,
        0.0188, 0.1199, 0.0075, 0.0236, 0.0647, 0.0432, 0.1585, 0.0756],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,598][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0078, 0.0230, 0.0291, 0.0261, 0.1446, 0.1002, 0.0439, 0.0668, 0.0852,
        0.0805, 0.0289, 0.0337, 0.0907, 0.0306, 0.0548, 0.1210, 0.0332],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,599][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1663, 0.0534, 0.0730, 0.0341, 0.0511, 0.0663, 0.0393, 0.0492, 0.0593,
        0.0377, 0.0620, 0.0351, 0.0565, 0.0563, 0.0524, 0.0511, 0.0570],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,601][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0160, 0.0343, 0.0548, 0.0451, 0.0703, 0.0753, 0.0315, 0.0570, 0.0740,
        0.0586, 0.0658, 0.0405, 0.0564, 0.0812, 0.0824, 0.0839, 0.0728],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,602][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([9.3219e-01, 4.0786e-03, 2.0129e-02, 4.6287e-03, 5.1341e-03, 4.7745e-03,
        8.2019e-04, 2.9533e-03, 2.7194e-03, 3.1969e-03, 1.8570e-03, 6.2922e-04,
        4.3783e-03, 2.1444e-03, 4.1833e-03, 4.5041e-03, 1.6831e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,604][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.2128, 0.0075, 0.0114, 0.0070, 0.0602, 0.0188, 0.0303, 0.0207, 0.0316,
        0.0211, 0.0475, 0.0109, 0.0632, 0.1193, 0.1118, 0.1017, 0.1243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,606][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0277, 0.0322, 0.0485, 0.0501, 0.0550, 0.0608, 0.0554, 0.0596, 0.0478,
        0.0822, 0.0676, 0.0956, 0.0912, 0.0656, 0.0596, 0.0539, 0.0473],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,608][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0054, 0.1357, 0.1303, 0.1026, 0.0475, 0.0524, 0.0347, 0.0393, 0.0477,
        0.0647, 0.0440, 0.0624, 0.0466, 0.0430, 0.0605, 0.0451, 0.0382],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,610][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.8310, 0.0072, 0.0055, 0.0041, 0.0130, 0.0091, 0.0224, 0.0056, 0.0079,
        0.0100, 0.0064, 0.0043, 0.0154, 0.0111, 0.0239, 0.0130, 0.0101],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,611][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0026, 0.0073, 0.0188, 0.0318, 0.0622, 0.1499, 0.0114, 0.0188, 0.0895,
        0.0692, 0.0669, 0.0179, 0.0924, 0.1158, 0.0458, 0.1191, 0.0805],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,612][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0003, 0.0219, 0.0392, 0.0299, 0.0846, 0.0702, 0.0055, 0.0344, 0.1312,
        0.0466, 0.0624, 0.0306, 0.0273, 0.0682, 0.0406, 0.2031, 0.1040],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,613][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0154, 0.0156, 0.0856, 0.0288, 0.1203, 0.0950, 0.0270, 0.0725, 0.0990,
        0.0681, 0.0687, 0.0402, 0.0132, 0.0398, 0.0474, 0.1230, 0.0404],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,692][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:32,693][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,695][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,696][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,697][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,698][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,699][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,700][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,701][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,702][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,704][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,705][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,706][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:32,707][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.5783, 0.4217], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,709][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.5159, 0.4841], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,710][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.0036, 0.9964], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,711][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.4214, 0.5786], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,713][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.1779, 0.8221], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,714][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.9014, 0.0986], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,716][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.8134, 0.1866], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,717][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.3297, 0.6703], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,718][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.3202, 0.6798], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,720][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.0907, 0.9093], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,721][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.0061, 0.9939], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,723][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.7231, 0.2769], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:32,724][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.4356, 0.0854, 0.4790], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,725][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.3799, 0.4865, 0.1336], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,726][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([6.0145e-04, 2.1869e-01, 7.8071e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,728][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.1895, 0.1604, 0.6501], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,729][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0945, 0.1571, 0.7484], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,731][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.8165, 0.0851, 0.0984], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,732][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5904, 0.2072, 0.2024], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,732][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.1529, 0.4849, 0.3622], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,733][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.1601, 0.3722, 0.4677], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,733][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0595, 0.3498, 0.5907], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,733][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0014, 0.3697, 0.6289], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,734][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.6224, 0.2468, 0.1308], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:32,734][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.2377, 0.0459, 0.4383, 0.2781], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,734][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.5615, 0.2508, 0.0991, 0.0886], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,735][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0028, 0.2485, 0.4714, 0.2772], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,735][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.1688, 0.1039, 0.4277, 0.2996], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,736][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0765, 0.1184, 0.5528, 0.2522], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,737][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.7778, 0.0795, 0.0957, 0.0470], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,738][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.4787, 0.1642, 0.2175, 0.1396], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,740][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.2396, 0.3376, 0.1803, 0.2424], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,741][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1866, 0.1117, 0.3542, 0.3476], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,743][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0513, 0.1606, 0.5219, 0.2661], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,744][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0029, 0.3391, 0.3649, 0.2930], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,745][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.6441, 0.1619, 0.0863, 0.1077], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:32,747][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.1220, 0.0276, 0.2140, 0.1351, 0.5013], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,748][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.1501, 0.3523, 0.0934, 0.1505, 0.2537], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,748][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([3.0154e-04, 1.0743e-01, 5.1583e-01, 1.6487e-01, 2.1158e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,750][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0716, 0.0722, 0.2509, 0.2434, 0.3618], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,751][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0289, 0.0650, 0.2618, 0.1302, 0.5142], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,753][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.4749, 0.1007, 0.1501, 0.0675, 0.2069], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,754][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.2425, 0.2039, 0.1943, 0.1785, 0.1808], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,756][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0467, 0.3348, 0.2684, 0.2177, 0.1325], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,757][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0311, 0.0642, 0.2316, 0.2419, 0.4311], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,758][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0179, 0.1089, 0.2336, 0.2363, 0.4033], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,759][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([3.0250e-04, 1.9393e-01, 3.5852e-01, 2.5966e-01, 1.8759e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,760][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.1926, 0.1589, 0.1873, 0.1381, 0.3231], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:32,762][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1245, 0.0060, 0.0593, 0.0325, 0.3474, 0.4302], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,763][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.3019, 0.1832, 0.0731, 0.0819, 0.1853, 0.1746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,765][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0004, 0.0923, 0.2619, 0.1073, 0.2443, 0.2939], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,766][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0496, 0.0355, 0.1675, 0.1102, 0.2730, 0.3642], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,768][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0181, 0.0240, 0.1466, 0.0490, 0.3806, 0.3817], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,769][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.6071, 0.0484, 0.0660, 0.0294, 0.1716, 0.0775], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,770][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.3617, 0.1265, 0.1451, 0.1117, 0.1467, 0.1083], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,772][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1175, 0.2780, 0.1420, 0.2342, 0.1313, 0.0970], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,773][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0220, 0.0318, 0.1009, 0.1079, 0.4607, 0.2767], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,775][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0108, 0.0396, 0.1188, 0.1172, 0.2972, 0.4163], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,775][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([2.7048e-04, 8.1552e-02, 1.8076e-01, 1.4669e-01, 3.8194e-01, 2.0878e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,777][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.3234, 0.1391, 0.0889, 0.0820, 0.2490, 0.1176], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:32,778][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0019, 0.0149, 0.0936, 0.0649, 0.2386, 0.5671, 0.0189],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,780][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0396, 0.2594, 0.0970, 0.1494, 0.1706, 0.1992, 0.0848],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,781][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([8.0498e-06, 6.4138e-02, 3.9328e-01, 7.3137e-02, 1.2214e-01, 3.4584e-01,
        1.4552e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,782][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0052, 0.0570, 0.1681, 0.1256, 0.2446, 0.3645, 0.0351],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,784][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0015, 0.0466, 0.2055, 0.0921, 0.2253, 0.4129, 0.0161],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,785][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.1232, 0.1085, 0.1133, 0.0735, 0.2180, 0.1794, 0.1841],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,786][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0931, 0.1343, 0.1839, 0.1527, 0.1668, 0.1643, 0.1049],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,788][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0056, 0.2584, 0.2605, 0.2151, 0.1047, 0.1189, 0.0367],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,789][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0029, 0.0498, 0.1300, 0.1381, 0.3422, 0.3124, 0.0247],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,790][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0011, 0.0684, 0.1039, 0.1341, 0.2147, 0.4606, 0.0173],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,790][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([2.2440e-05, 6.6427e-02, 2.6445e-01, 1.2640e-01, 1.8187e-01, 3.5878e-01,
        2.0440e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,791][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0258, 0.1101, 0.1897, 0.1077, 0.2457, 0.2582, 0.0626],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:32,791][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0167, 0.0035, 0.0527, 0.0397, 0.2888, 0.4922, 0.0380, 0.0684],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,792][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.1690, 0.1327, 0.0511, 0.0683, 0.1373, 0.1245, 0.1952, 0.1218],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,792][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0003, 0.0955, 0.2467, 0.0695, 0.1569, 0.2795, 0.0159, 0.1356],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,792][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0127, 0.0332, 0.1401, 0.0907, 0.2160, 0.3494, 0.0359, 0.1220],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,793][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0039, 0.0216, 0.1309, 0.0519, 0.2571, 0.4196, 0.0183, 0.0967],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,793][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.2060, 0.0680, 0.0700, 0.0561, 0.1881, 0.1109, 0.1878, 0.1131],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,794][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1909, 0.1353, 0.1322, 0.1187, 0.1038, 0.0966, 0.1195, 0.1031],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,796][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0824, 0.2177, 0.1320, 0.2096, 0.1211, 0.0679, 0.1059, 0.0633],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,797][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0027, 0.0216, 0.0764, 0.1137, 0.4678, 0.2309, 0.0185, 0.0684],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,798][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0029, 0.0473, 0.0653, 0.1229, 0.1747, 0.4974, 0.0209, 0.0686],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,799][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([2.2723e-04, 8.8839e-02, 1.8093e-01, 1.2062e-01, 2.5306e-01, 2.6589e-01,
        9.5778e-03, 8.0857e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,801][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.1211, 0.1357, 0.1089, 0.0974, 0.1500, 0.1692, 0.0960, 0.1219],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:32,802][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0685, 0.0026, 0.0365, 0.0229, 0.2212, 0.3577, 0.0492, 0.0901, 0.1514],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,804][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2308, 0.0991, 0.0398, 0.0518, 0.0974, 0.1000, 0.1487, 0.1139, 0.1186],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,805][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0003, 0.0701, 0.1806, 0.0727, 0.1110, 0.2014, 0.0125, 0.0973, 0.2540],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,806][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0230, 0.0192, 0.0924, 0.0650, 0.1761, 0.2384, 0.0367, 0.0994, 0.2497],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,808][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0081, 0.0134, 0.0870, 0.0349, 0.2215, 0.2819, 0.0199, 0.1151, 0.2182],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,809][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.4084, 0.0293, 0.0344, 0.0174, 0.1150, 0.0518, 0.1547, 0.0976, 0.0915],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,811][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2710, 0.0814, 0.0894, 0.0693, 0.0871, 0.0731, 0.1141, 0.0994, 0.1152],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,812][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0870, 0.2115, 0.1076, 0.1819, 0.0955, 0.0731, 0.0929, 0.0813, 0.0693],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,814][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0101, 0.0186, 0.0791, 0.0864, 0.3289, 0.2134, 0.0275, 0.0699, 0.1661],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,815][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0053, 0.0268, 0.0660, 0.0771, 0.1830, 0.3469, 0.0209, 0.0624, 0.2116],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,816][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([2.2300e-04, 6.7387e-02, 1.0254e-01, 1.0530e-01, 1.8050e-01, 1.7449e-01,
        8.2939e-03, 9.6917e-02, 2.6435e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,817][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1830, 0.1228, 0.0745, 0.0770, 0.1508, 0.1085, 0.0857, 0.0840, 0.1137],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:32,819][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0052, 0.0025, 0.0300, 0.0192, 0.1302, 0.5086, 0.0129, 0.0634, 0.1864,
        0.0415], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,821][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.1455, 0.1018, 0.0352, 0.0366, 0.0969, 0.0958, 0.1554, 0.1082, 0.1185,
        0.1061], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,822][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0005, 0.0363, 0.1837, 0.0561, 0.1540, 0.2006, 0.0092, 0.0951, 0.1993,
        0.0652], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,823][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0112, 0.0218, 0.0946, 0.0757, 0.1661, 0.2445, 0.0264, 0.0772, 0.2059,
        0.0765], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,825][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0022, 0.0135, 0.0828, 0.0365, 0.2090, 0.2857, 0.0129, 0.0922, 0.2109,
        0.0543], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,826][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.1819, 0.0342, 0.0556, 0.0222, 0.1449, 0.0990, 0.1182, 0.1136, 0.1170,
        0.1135], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,828][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.2269, 0.0676, 0.0992, 0.0581, 0.0956, 0.0808, 0.0880, 0.1124, 0.1164,
        0.0551], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,829][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0406, 0.1915, 0.1140, 0.2099, 0.0834, 0.0618, 0.0531, 0.0623, 0.0725,
        0.1110], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,831][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0045, 0.0179, 0.0646, 0.0640, 0.3020, 0.2232, 0.0190, 0.0608, 0.1467,
        0.0974], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,832][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0015, 0.0247, 0.0600, 0.0679, 0.1508, 0.3376, 0.0130, 0.0501, 0.2145,
        0.0800], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,833][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0005, 0.0371, 0.0799, 0.0598, 0.2463, 0.1441, 0.0075, 0.0867, 0.2514,
        0.0866], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,835][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.1217, 0.0812, 0.0737, 0.0658, 0.1697, 0.1332, 0.0658, 0.0995, 0.1132,
        0.0762], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:32,836][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0267, 0.0035, 0.0296, 0.0249, 0.1237, 0.3381, 0.0367, 0.0665, 0.1560,
        0.0819, 0.1123], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,838][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.1145, 0.1083, 0.0504, 0.0449, 0.0995, 0.0860, 0.1036, 0.1130, 0.1152,
        0.0759, 0.0887], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,839][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([1.0722e-04, 7.3124e-02, 1.6528e-01, 4.7708e-02, 8.9781e-02, 1.6762e-01,
        4.7347e-03, 5.9178e-02, 2.3556e-01, 4.6831e-02, 1.1008e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,840][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0097, 0.0196, 0.0782, 0.0838, 0.1312, 0.1846, 0.0223, 0.0683, 0.1784,
        0.0710, 0.1530], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,842][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0054, 0.0107, 0.0683, 0.0257, 0.2150, 0.2178, 0.0163, 0.0859, 0.1726,
        0.0673, 0.1149], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,843][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1936, 0.0289, 0.0368, 0.0204, 0.1110, 0.0526, 0.1182, 0.0838, 0.0931,
        0.1045, 0.1571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,845][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.1145, 0.0774, 0.0907, 0.0606, 0.0818, 0.0803, 0.0753, 0.0990, 0.1008,
        0.0592, 0.1603], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,846][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0190, 0.2177, 0.0962, 0.1802, 0.0616, 0.0503, 0.0363, 0.0577, 0.0575,
        0.0954, 0.1281], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,847][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0028, 0.0158, 0.0431, 0.0662, 0.2248, 0.1792, 0.0143, 0.0671, 0.1794,
        0.0939, 0.1132], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,848][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0024, 0.0233, 0.0481, 0.0866, 0.1079, 0.2757, 0.0156, 0.0475, 0.1952,
        0.0985, 0.0992], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,848][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([8.2436e-05, 5.5156e-02, 8.7932e-02, 6.5639e-02, 1.3587e-01, 1.5821e-01,
        4.0755e-03, 6.4009e-02, 2.6507e-01, 5.8493e-02, 1.0546e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,849][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0609, 0.1231, 0.0677, 0.0827, 0.1201, 0.1028, 0.0524, 0.0713, 0.1388,
        0.0775, 0.1026], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:32,849][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0013, 0.0018, 0.0260, 0.0173, 0.1086, 0.4254, 0.0095, 0.0530, 0.1965,
        0.0557, 0.0992, 0.0058], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,850][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.1991, 0.0675, 0.0362, 0.0288, 0.0753, 0.0614, 0.1221, 0.0885, 0.1053,
        0.0808, 0.0943, 0.0408], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,850][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0005, 0.0593, 0.1386, 0.0846, 0.0923, 0.1444, 0.0095, 0.0673, 0.1493,
        0.0657, 0.1142, 0.0744], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,850][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0047, 0.0158, 0.0693, 0.0529, 0.1189, 0.2003, 0.0202, 0.0599, 0.1938,
        0.0899, 0.1423, 0.0321], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,851][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0014, 0.0125, 0.0719, 0.0366, 0.1778, 0.2448, 0.0099, 0.0825, 0.2008,
        0.0631, 0.0839, 0.0148], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,852][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.1394, 0.0266, 0.0358, 0.0204, 0.1101, 0.0676, 0.1097, 0.0863, 0.1122,
        0.1213, 0.1195, 0.0513], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,854][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.1148, 0.0505, 0.0752, 0.0436, 0.0859, 0.0736, 0.0735, 0.1000, 0.1098,
        0.0553, 0.1472, 0.0705], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,855][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0760, 0.1421, 0.0848, 0.1167, 0.0617, 0.0359, 0.0656, 0.0413, 0.0448,
        0.0950, 0.1112, 0.1249], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,857][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0015, 0.0106, 0.0536, 0.0536, 0.1482, 0.2243, 0.0130, 0.0532, 0.1896,
        0.1227, 0.1114, 0.0183], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,858][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0007, 0.0163, 0.0506, 0.0348, 0.1221, 0.3230, 0.0092, 0.0434, 0.1868,
        0.1024, 0.0960, 0.0147], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,859][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0005, 0.0710, 0.0922, 0.0774, 0.1602, 0.0875, 0.0082, 0.0641, 0.2168,
        0.0918, 0.0840, 0.0463], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,861][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0902, 0.0808, 0.0479, 0.0611, 0.1282, 0.0876, 0.0701, 0.0781, 0.0958,
        0.0734, 0.1183, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:32,863][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0094, 0.0043, 0.0319, 0.0232, 0.1100, 0.3627, 0.0169, 0.0583, 0.1520,
        0.0507, 0.0865, 0.0077, 0.0863], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,864][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0784, 0.1220, 0.0419, 0.0786, 0.0431, 0.1064, 0.0850, 0.0466, 0.0921,
        0.0741, 0.0610, 0.0865, 0.0842], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,865][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([2.0166e-04, 3.7093e-02, 2.4476e-01, 3.5894e-02, 9.5106e-02, 1.5482e-01,
        4.9461e-03, 7.2031e-02, 1.6035e-01, 3.8051e-02, 9.5915e-02, 2.9634e-02,
        3.1196e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,866][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0074, 0.0188, 0.0825, 0.0654, 0.1145, 0.1719, 0.0179, 0.0757, 0.1528,
        0.0480, 0.1296, 0.0297, 0.0859], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,868][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0032, 0.0123, 0.0706, 0.0297, 0.1577, 0.2247, 0.0130, 0.0722, 0.1973,
        0.0515, 0.0884, 0.0127, 0.0667], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,869][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.1196, 0.0247, 0.0325, 0.0180, 0.0734, 0.0551, 0.0794, 0.0781, 0.1047,
        0.0798, 0.1177, 0.0442, 0.1729], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,871][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.1073, 0.0491, 0.0613, 0.0527, 0.0680, 0.0583, 0.0662, 0.0673, 0.0862,
        0.0547, 0.1323, 0.0879, 0.1087], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,872][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0243, 0.1508, 0.1177, 0.1370, 0.0608, 0.0520, 0.0386, 0.0432, 0.0482,
        0.0942, 0.1062, 0.1003, 0.0268], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,874][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0018, 0.0141, 0.0612, 0.0629, 0.1647, 0.2220, 0.0107, 0.0430, 0.1434,
        0.0590, 0.0872, 0.0174, 0.1125], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,875][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0016, 0.0248, 0.0519, 0.0652, 0.0942, 0.2928, 0.0104, 0.0409, 0.1717,
        0.0873, 0.0688, 0.0206, 0.0697], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,877][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0002, 0.0686, 0.1366, 0.0504, 0.1262, 0.1583, 0.0045, 0.0626, 0.1534,
        0.0526, 0.0967, 0.0485, 0.0414], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,878][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0545, 0.0727, 0.1022, 0.0609, 0.1391, 0.0884, 0.0456, 0.0686, 0.0937,
        0.0575, 0.0797, 0.0680, 0.0691], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:32,880][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0462, 0.0024, 0.0190, 0.0167, 0.0851, 0.2045, 0.0280, 0.0363, 0.0790,
        0.0486, 0.0785, 0.0065, 0.1262, 0.2229], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,881][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.3034, 0.0596, 0.0221, 0.0250, 0.0480, 0.0457, 0.1170, 0.0511, 0.0533,
        0.0571, 0.0516, 0.0460, 0.0586, 0.0616], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,883][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0006, 0.0587, 0.1352, 0.0377, 0.0734, 0.1145, 0.0098, 0.0516, 0.1637,
        0.0445, 0.1001, 0.0413, 0.0370, 0.1320], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,884][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0196, 0.0118, 0.0552, 0.0410, 0.0938, 0.1195, 0.0202, 0.0539, 0.1193,
        0.0486, 0.1216, 0.0236, 0.0835, 0.1884], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,886][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0065, 0.0066, 0.0510, 0.0169, 0.1459, 0.1369, 0.0116, 0.0544, 0.1135,
        0.0438, 0.0830, 0.0079, 0.0720, 0.2500], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,887][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.2202, 0.0152, 0.0195, 0.0135, 0.0623, 0.0282, 0.0909, 0.0407, 0.0499,
        0.0633, 0.0757, 0.0341, 0.1386, 0.1477], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,889][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.1953, 0.0469, 0.0555, 0.0399, 0.0477, 0.0439, 0.0656, 0.0590, 0.0607,
        0.0375, 0.1285, 0.0677, 0.0837, 0.0681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,890][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0690, 0.1144, 0.0703, 0.0969, 0.0425, 0.0361, 0.0576, 0.0348, 0.0412,
        0.0828, 0.1060, 0.0962, 0.0312, 0.1209], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,892][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0042, 0.0081, 0.0252, 0.0376, 0.1302, 0.0952, 0.0110, 0.0370, 0.1000,
        0.0528, 0.0763, 0.0121, 0.2732, 0.1373], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,893][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0042, 0.0155, 0.0351, 0.0516, 0.0866, 0.1807, 0.0143, 0.0342, 0.1259,
        0.0724, 0.0896, 0.0185, 0.0950, 0.1765], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,895][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0003, 0.0365, 0.0695, 0.0508, 0.1431, 0.1018, 0.0066, 0.0526, 0.1968,
        0.0655, 0.0903, 0.0491, 0.0430, 0.0940], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,896][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1384, 0.0611, 0.0356, 0.0411, 0.0853, 0.0660, 0.0617, 0.0521, 0.0844,
        0.0504, 0.0824, 0.0505, 0.0806, 0.1105], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:32,898][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0130, 0.0019, 0.0248, 0.0138, 0.0851, 0.2036, 0.0179, 0.0441, 0.0881,
        0.0404, 0.0896, 0.0055, 0.0976, 0.2296, 0.0452], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,899][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0962, 0.0793, 0.0324, 0.0400, 0.0879, 0.0573, 0.0733, 0.0835, 0.0726,
        0.0596, 0.0573, 0.0435, 0.0712, 0.0773, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,901][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0005, 0.0620, 0.1744, 0.0594, 0.0553, 0.1019, 0.0069, 0.0612, 0.1326,
        0.0386, 0.0771, 0.0458, 0.0250, 0.1109, 0.0483], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,902][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0113, 0.0156, 0.0580, 0.0464, 0.0773, 0.1252, 0.0168, 0.0555, 0.1145,
        0.0410, 0.1050, 0.0238, 0.0596, 0.1811, 0.0689], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,904][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0026, 0.0081, 0.0517, 0.0231, 0.0995, 0.1608, 0.0100, 0.0519, 0.1290,
        0.0421, 0.0765, 0.0100, 0.0577, 0.1997, 0.0773], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,905][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0856, 0.0200, 0.0281, 0.0174, 0.0589, 0.0445, 0.0577, 0.0513, 0.0606,
        0.0552, 0.0769, 0.0351, 0.1301, 0.1788, 0.0998], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,906][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.1164, 0.0487, 0.0589, 0.0438, 0.0628, 0.0467, 0.0600, 0.0588, 0.0677,
        0.0452, 0.1106, 0.0644, 0.0850, 0.0670, 0.0642], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,906][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0754, 0.0907, 0.0788, 0.0629, 0.0388, 0.0382, 0.0572, 0.0401, 0.0445,
        0.0832, 0.1063, 0.0566, 0.0302, 0.1237, 0.0734], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,907][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0028, 0.0105, 0.0391, 0.0479, 0.0964, 0.1420, 0.0116, 0.0398, 0.1081,
        0.0572, 0.0713, 0.0143, 0.1866, 0.1207, 0.0516], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,907][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0019, 0.0188, 0.0325, 0.0598, 0.0743, 0.2069, 0.0111, 0.0336, 0.1298,
        0.0605, 0.0709, 0.0194, 0.0639, 0.1680, 0.0487], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,908][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([1.7927e-04, 3.8855e-02, 1.1034e-01, 4.7841e-02, 7.9863e-02, 1.1609e-01,
        3.9495e-03, 6.0771e-02, 1.9909e-01, 5.8816e-02, 9.2303e-02, 3.1902e-02,
        3.5034e-02, 9.9467e-02, 2.5498e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,908][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0842, 0.0714, 0.0449, 0.0381, 0.0767, 0.0667, 0.0440, 0.0463, 0.0893,
        0.0572, 0.0775, 0.0388, 0.0692, 0.1127, 0.0829], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:32,909][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0171, 0.0012, 0.0159, 0.0098, 0.1078, 0.1945, 0.0167, 0.0345, 0.0821,
        0.0305, 0.0694, 0.0031, 0.0666, 0.2172, 0.0642, 0.0694],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,911][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.1734, 0.0457, 0.0227, 0.0270, 0.0551, 0.0563, 0.0869, 0.0623, 0.0568,
        0.0490, 0.0520, 0.0349, 0.0537, 0.0673, 0.0692, 0.0877],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,912][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0005, 0.0374, 0.1329, 0.0402, 0.0659, 0.0800, 0.0076, 0.0654, 0.1033,
        0.0396, 0.1005, 0.0378, 0.0271, 0.1473, 0.0572, 0.0574],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,914][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0178, 0.0078, 0.0379, 0.0293, 0.0809, 0.1085, 0.0206, 0.0483, 0.1115,
        0.0410, 0.0901, 0.0163, 0.0582, 0.1639, 0.0651, 0.1026],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,915][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0031, 0.0047, 0.0396, 0.0136, 0.0955, 0.1277, 0.0076, 0.0477, 0.0911,
        0.0283, 0.0525, 0.0049, 0.0386, 0.2101, 0.0827, 0.1522],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,916][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.1695, 0.0136, 0.0181, 0.0105, 0.0530, 0.0304, 0.0675, 0.0372, 0.0428,
        0.0444, 0.0627, 0.0229, 0.0763, 0.1101, 0.1296, 0.1113],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,917][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.2125, 0.0391, 0.0425, 0.0359, 0.0453, 0.0402, 0.0603, 0.0472, 0.0578,
        0.0389, 0.0990, 0.0530, 0.0696, 0.0583, 0.0594, 0.0408],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,919][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0853, 0.0976, 0.0581, 0.0706, 0.0371, 0.0270, 0.0605, 0.0269, 0.0308,
        0.0941, 0.0957, 0.0766, 0.0276, 0.1013, 0.0863, 0.0245],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,920][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0048, 0.0042, 0.0276, 0.0282, 0.1222, 0.1261, 0.0099, 0.0312, 0.0875,
        0.0546, 0.0681, 0.0075, 0.1423, 0.1355, 0.0724, 0.0780],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,922][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0029, 0.0124, 0.0273, 0.0487, 0.0807, 0.1611, 0.0100, 0.0305, 0.1107,
        0.0549, 0.0629, 0.0141, 0.0651, 0.1752, 0.0444, 0.0992],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,923][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0003, 0.0327, 0.0623, 0.0635, 0.1041, 0.1016, 0.0060, 0.0585, 0.1556,
        0.0657, 0.0749, 0.0467, 0.0287, 0.0755, 0.0494, 0.0744],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,925][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.1027, 0.0532, 0.0405, 0.0364, 0.0753, 0.0584, 0.0496, 0.0531, 0.0739,
        0.0454, 0.0761, 0.0336, 0.0594, 0.0970, 0.0781, 0.0671],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:32,926][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0161, 0.0012, 0.0124, 0.0110, 0.0890, 0.1476, 0.0169, 0.0261, 0.0623,
        0.0361, 0.0679, 0.0048, 0.0991, 0.1778, 0.0675, 0.0945, 0.0696],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,928][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.2949, 0.0429, 0.0149, 0.0160, 0.0382, 0.0314, 0.1019, 0.0358, 0.0373,
        0.0431, 0.0374, 0.0283, 0.0413, 0.0387, 0.0619, 0.0958, 0.0402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,929][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0007, 0.0352, 0.0928, 0.0202, 0.0478, 0.0726, 0.0091, 0.0362, 0.1160,
        0.0330, 0.0697, 0.0268, 0.0288, 0.1001, 0.0566, 0.1240, 0.1302],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,931][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0101, 0.0074, 0.0361, 0.0281, 0.0687, 0.0907, 0.0134, 0.0397, 0.0932,
        0.0379, 0.0850, 0.0173, 0.0556, 0.1407, 0.0514, 0.1052, 0.1194],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,932][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0027, 0.0048, 0.0350, 0.0134, 0.0892, 0.1001, 0.0066, 0.0369, 0.0772,
        0.0308, 0.0529, 0.0056, 0.0374, 0.1691, 0.0816, 0.1402, 0.1165],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,934][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.1577, 0.0098, 0.0113, 0.0086, 0.0453, 0.0176, 0.0633, 0.0259, 0.0298,
        0.0409, 0.0447, 0.0216, 0.0894, 0.0958, 0.1388, 0.1128, 0.0866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,935][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.1858, 0.0354, 0.0391, 0.0302, 0.0412, 0.0340, 0.0571, 0.0461, 0.0510,
        0.0308, 0.1009, 0.0542, 0.0697, 0.0551, 0.0579, 0.0485, 0.0630],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,937][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0639, 0.0970, 0.0602, 0.0652, 0.0292, 0.0262, 0.0531, 0.0272, 0.0329,
        0.0687, 0.0868, 0.0690, 0.0254, 0.0982, 0.0767, 0.0298, 0.0905],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,938][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0028, 0.0050, 0.0198, 0.0282, 0.1041, 0.0709, 0.0089, 0.0266, 0.0744,
        0.0489, 0.0658, 0.0100, 0.1938, 0.1103, 0.0915, 0.0805, 0.0585],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,940][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0020, 0.0090, 0.0226, 0.0340, 0.0593, 0.1275, 0.0087, 0.0226, 0.0984,
        0.0555, 0.0731, 0.0132, 0.0772, 0.1400, 0.0455, 0.1180, 0.0934],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,941][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0003, 0.0221, 0.0399, 0.0303, 0.0844, 0.0697, 0.0056, 0.0352, 0.1348,
        0.0468, 0.0623, 0.0311, 0.0277, 0.0680, 0.0398, 0.1982, 0.1037],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,943][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1077, 0.0422, 0.0308, 0.0260, 0.0704, 0.0535, 0.0452, 0.0428, 0.0696,
        0.0322, 0.0608, 0.0306, 0.0573, 0.0856, 0.0760, 0.0754, 0.0941],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:32,944][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:32,946][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 4270],
        [15238],
        [ 2498],
        [ 2096],
        [ 1973],
        [  730],
        [  889],
        [  290],
        [  179],
        [   90],
        [  428],
        [ 1127],
        [  443],
        [  453],
        [  244],
        [  586],
        [  135]], device='cuda:0')
[2024-07-24 10:21:32,948][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 5983],
        [33428],
        [10134],
        [11912],
        [ 9989],
        [ 4761],
        [ 8443],
        [ 2202],
        [ 1387],
        [  523],
        [ 2365],
        [ 6480],
        [ 2821],
        [ 2067],
        [ 1470],
        [ 1959],
        [  568]], device='cuda:0')
[2024-07-24 10:21:32,949][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[22747],
        [19736],
        [12281],
        [ 9051],
        [17239],
        [15520],
        [14795],
        [11312],
        [16199],
        [18655],
        [18569],
        [17042],
        [17111],
        [16063],
        [15306],
        [15340],
        [14127]], device='cuda:0')
[2024-07-24 10:21:32,950][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[15773],
        [ 3244],
        [ 3160],
        [ 5996],
        [ 5832],
        [ 6523],
        [ 7661],
        [ 7962],
        [ 8395],
        [ 8945],
        [ 8596],
        [ 8916],
        [ 8881],
        [ 8887],
        [ 9007],
        [ 9541],
        [ 9688]], device='cuda:0')
[2024-07-24 10:21:32,952][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[15765],
        [49714],
        [39328],
        [36436],
        [35717],
        [31936],
        [25362],
        [24927],
        [24790],
        [20681],
        [22389],
        [23738],
        [22987],
        [22855],
        [22526],
        [22012],
        [22322]], device='cuda:0')
[2024-07-24 10:21:32,954][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 1822],
        [ 5788],
        [ 9704],
        [ 8425],
        [11393],
        [12015],
        [12476],
        [11506],
        [11482],
        [11100],
        [10242],
        [ 9827],
        [ 9153],
        [ 9230],
        [ 9067],
        [ 9753],
        [ 9588]], device='cuda:0')
[2024-07-24 10:21:32,955][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[10998],
        [10556],
        [10320],
        [10386],
        [10445],
        [10320],
        [10453],
        [10382],
        [10321],
        [10393],
        [10461],
        [10294],
        [10497],
        [10445],
        [10535],
        [10434],
        [10295]], device='cuda:0')
[2024-07-24 10:21:32,956][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[39769],
        [42445],
        [40748],
        [40643],
        [40846],
        [41276],
        [35374],
        [34383],
        [38151],
        [36935],
        [40041],
        [39426],
        [40786],
        [43603],
        [44170],
        [45060],
        [45986]], device='cuda:0')
[2024-07-24 10:21:32,958][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[35912],
        [32810],
        [27937],
        [25164],
        [20565],
        [23184],
        [22993],
        [22548],
        [22323],
        [21575],
        [21382],
        [20355],
        [20254],
        [19830],
        [19973],
        [20409],
        [20846]], device='cuda:0')
[2024-07-24 10:21:32,959][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[21031],
        [39200],
        [31662],
        [29281],
        [28201],
        [26662],
        [25538],
        [24416],
        [23155],
        [21020],
        [21662],
        [19022],
        [20489],
        [20500],
        [19530],
        [19841],
        [20356]], device='cuda:0')
[2024-07-24 10:21:32,961][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[23786],
        [23947],
        [23616],
        [23644],
        [22856],
        [22927],
        [18648],
        [21773],
        [22487],
        [21923],
        [20700],
        [21513],
        [20423],
        [21295],
        [19357],
        [21268],
        [20602]], device='cuda:0')
[2024-07-24 10:21:32,962][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[20154],
        [ 5165],
        [ 9669],
        [19585],
        [29064],
        [16109],
        [14268],
        [12603],
        [17147],
        [17554],
        [17859],
        [16910],
        [16752],
        [18571],
        [18716],
        [19939],
        [19423]], device='cuda:0')
[2024-07-24 10:21:32,964][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[33395],
        [50243],
        [49217],
        [49068],
        [46877],
        [43088],
        [42646],
        [44290],
        [43100],
        [43640],
        [44865],
        [46120],
        [45825],
        [45131],
        [45481],
        [46074],
        [46087]], device='cuda:0')
[2024-07-24 10:21:32,965][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[12203],
        [ 2393],
        [ 2919],
        [ 1956],
        [ 2977],
        [ 2581],
        [ 2003],
        [ 2673],
        [ 2638],
        [ 2091],
        [ 2376],
        [ 1890],
        [ 2283],
        [ 2311],
        [ 2242],
        [ 2669],
        [ 2800]], device='cuda:0')
[2024-07-24 10:21:32,966][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[6237],
        [1439],
        [2909],
        [2358],
        [4020],
        [4607],
        [4521],
        [6322],
        [4475],
        [2304],
        [4396],
        [2611],
        [2697],
        [8979],
        [4770],
        [7403],
        [7779]], device='cuda:0')
[2024-07-24 10:21:32,967][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[31933],
        [35229],
        [34077],
        [33863],
        [36218],
        [31741],
        [32871],
        [32235],
        [31955],
        [33115],
        [33291],
        [33716],
        [34130],
        [32758],
        [33046],
        [32065],
        [31167]], device='cuda:0')
[2024-07-24 10:21:32,968][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[29369],
        [23386],
        [20872],
        [19623],
        [19588],
        [25240],
        [20754],
        [26005],
        [30568],
        [29771],
        [30306],
        [32136],
        [26986],
        [31128],
        [28157],
        [29844],
        [31913]], device='cuda:0')
[2024-07-24 10:21:32,969][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[ 7491],
        [42539],
        [26909],
        [31785],
        [28361],
        [21423],
        [19375],
        [18076],
        [14322],
        [13886],
        [15515],
        [17384],
        [16886],
        [18149],
        [19932],
        [19147],
        [18390]], device='cuda:0')
[2024-07-24 10:21:32,970][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[13237],
        [16338],
        [15523],
        [19987],
        [10273],
        [11541],
        [12436],
        [13061],
        [11144],
        [10818],
        [10295],
        [ 9678],
        [12766],
        [ 8943],
        [ 8585],
        [ 6494],
        [ 5302]], device='cuda:0')
[2024-07-24 10:21:32,972][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[19920],
        [15180],
        [ 8646],
        [ 5496],
        [ 8696],
        [11463],
        [ 9594],
        [11972],
        [14847],
        [14709],
        [13468],
        [13134],
        [12109],
        [12392],
        [11443],
        [12516],
        [13468]], device='cuda:0')
[2024-07-24 10:21:32,973][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[17271],
        [17595],
        [20234],
        [19801],
        [22864],
        [23428],
        [21797],
        [27943],
        [33111],
        [23451],
        [20834],
        [20548],
        [18968],
        [24098],
        [23702],
        [25240],
        [26805]], device='cuda:0')
[2024-07-24 10:21:32,975][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[10868],
        [ 9765],
        [ 6734],
        [ 3816],
        [ 3962],
        [ 6522],
        [ 5950],
        [ 5051],
        [10081],
        [10972],
        [12272],
        [14365],
        [15715],
        [16749],
        [17440],
        [20936],
        [24022]], device='cuda:0')
[2024-07-24 10:21:32,976][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[35610],
        [ 7678],
        [ 9356],
        [ 5165],
        [ 5595],
        [ 6044],
        [ 7035],
        [ 6842],
        [ 9093],
        [ 9837],
        [10928],
        [13336],
        [11793],
        [13526],
        [14073],
        [13075],
        [11695]], device='cuda:0')
[2024-07-24 10:21:32,978][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[31381],
        [22816],
        [35268],
        [35993],
        [32619],
        [30200],
        [30220],
        [30717],
        [30885],
        [32120],
        [29509],
        [29926],
        [26032],
        [21233],
        [21322],
        [20272],
        [18613]], device='cuda:0')
[2024-07-24 10:21:32,979][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[38530],
        [ 5966],
        [10713],
        [ 9758],
        [ 8538],
        [10655],
        [10080],
        [10015],
        [ 9265],
        [10762],
        [10077],
        [11207],
        [11657],
        [11043],
        [ 8813],
        [11690],
        [13118]], device='cuda:0')
[2024-07-24 10:21:32,981][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[47305],
        [24542],
        [36561],
        [37772],
        [35235],
        [32636],
        [35607],
        [33479],
        [29704],
        [28165],
        [29617],
        [29808],
        [32402],
        [31981],
        [31825],
        [33523],
        [37300]], device='cuda:0')
[2024-07-24 10:21:32,982][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[30977],
        [42200],
        [40067],
        [40703],
        [12005],
        [11306],
        [ 4757],
        [ 5680],
        [ 4227],
        [ 2671],
        [ 3097],
        [ 2666],
        [ 2381],
        [ 2370],
        [ 2202],
        [ 2101],
        [ 1912]], device='cuda:0')
[2024-07-24 10:21:32,984][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 7090],
        [21103],
        [23080],
        [27012],
        [34137],
        [31893],
        [35530],
        [33960],
        [30411],
        [31978],
        [32033],
        [29497],
        [30711],
        [29746],
        [31174],
        [29542],
        [27758]], device='cuda:0')
[2024-07-24 10:21:32,985][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[40877],
        [42987],
        [38083],
        [36542],
        [31534],
        [31838],
        [27338],
        [23196],
        [25821],
        [32747],
        [26923],
        [27393],
        [30721],
        [26315],
        [28655],
        [28795],
        [28307]], device='cuda:0')
[2024-07-24 10:21:32,987][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893],
        [11893]], device='cuda:0')
[2024-07-24 10:21:33,040][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:33,041][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,042][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,043][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,045][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,046][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,047][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,048][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,049][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,050][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,051][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,052][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,053][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,055][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.9810, 0.0190], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,062][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.0040, 0.9960], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,064][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.9234, 0.0766], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,065][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.1430, 0.8570], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,066][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.0821, 0.9179], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,068][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.6241, 0.3759], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,069][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.5724, 0.4276], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,071][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.2426, 0.7574], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,072][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.4176, 0.5824], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,073][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.6365, 0.3635], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,075][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.2373, 0.7627], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,076][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.0771, 0.9229], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,077][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.5839, 0.0170, 0.3991], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,078][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ and] are: tensor([2.6238e-04, 5.4082e-01, 4.5891e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,080][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.6993, 0.1496, 0.1511], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,081][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0727, 0.4653, 0.4620], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,082][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0168, 0.0490, 0.9342], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,084][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.3291, 0.1362, 0.5348], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,085][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.2868, 0.4329, 0.2804], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,087][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0693, 0.0463, 0.8844], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,088][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.1052, 0.3135, 0.5814], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,090][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.3032, 0.2700, 0.4268], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,090][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0402, 0.1781, 0.7817], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,090][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0156, 0.1785, 0.8059], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,091][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.6185, 0.0125, 0.3326, 0.0364], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,091][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0008, 0.2627, 0.2296, 0.5069], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,091][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.8114, 0.0853, 0.0688, 0.0345], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,092][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0255, 0.3371, 0.3085, 0.3290], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,092][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0223, 0.0236, 0.6756, 0.2785], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,092][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.3670, 0.0421, 0.3853, 0.2056], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,094][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.3442, 0.3070, 0.2265, 0.1223], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,095][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0883, 0.0157, 0.4971, 0.3989], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,096][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.1497, 0.2752, 0.3839, 0.1912], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,098][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.2147, 0.2494, 0.3760, 0.1600], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,099][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0444, 0.1544, 0.6050, 0.1962], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,100][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0040, 0.1121, 0.7254, 0.1585], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,102][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.4131, 0.0196, 0.4929, 0.0461, 0.0283], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,103][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0005, 0.2179, 0.3421, 0.3488, 0.0907], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,104][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.3190, 0.1972, 0.2044, 0.0799, 0.1996], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,106][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0135, 0.2847, 0.2689, 0.2761, 0.1568], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,107][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0154, 0.0230, 0.4002, 0.1259, 0.4355], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,109][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.1374, 0.0357, 0.1704, 0.1083, 0.5482], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,110][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0890, 0.3085, 0.2285, 0.1301, 0.2440], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,111][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0368, 0.0111, 0.1826, 0.1104, 0.6592], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,113][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0590, 0.1448, 0.3328, 0.1386, 0.3248], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,114][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0960, 0.1745, 0.3150, 0.1299, 0.2846], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,115][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0214, 0.1241, 0.3784, 0.1171, 0.3591], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,117][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0061, 0.1362, 0.5149, 0.1527, 0.1902], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,118][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1075, 0.0010, 0.0256, 0.0022, 0.0014, 0.8622], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,119][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([2.7332e-04, 1.6599e-01, 1.7286e-01, 4.4629e-01, 8.5119e-02, 1.2947e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,120][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.4081, 0.1201, 0.1238, 0.0556, 0.1244, 0.1681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,122][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0160, 0.2080, 0.2084, 0.2319, 0.1838, 0.1519], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,123][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0101, 0.0067, 0.2266, 0.0374, 0.3413, 0.3779], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,125][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0874, 0.0131, 0.0941, 0.0335, 0.4057, 0.3662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,126][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.2078, 0.1596, 0.1525, 0.0769, 0.2353, 0.1679], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,127][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0166, 0.0018, 0.0338, 0.0196, 0.2953, 0.6328], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,129][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0620, 0.1011, 0.2378, 0.0946, 0.2299, 0.2746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,130][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1116, 0.1314, 0.1952, 0.0747, 0.2119, 0.2752], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,132][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0241, 0.0712, 0.2102, 0.0582, 0.2364, 0.3999], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,133][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0043, 0.0760, 0.3363, 0.1023, 0.1429, 0.3382], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,135][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0236, 0.0015, 0.0345, 0.0039, 0.0020, 0.9319, 0.0026],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,135][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([9.8301e-06, 1.4561e-01, 2.6269e-01, 2.4924e-01, 5.2875e-02, 2.8886e-01,
        7.1474e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,137][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.1838, 0.1207, 0.1162, 0.0893, 0.1318, 0.1603, 0.1978],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,138][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0031, 0.1848, 0.2327, 0.2290, 0.1312, 0.1883, 0.0309],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,140][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0004, 0.0239, 0.2794, 0.0796, 0.2845, 0.3246, 0.0076],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,141][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0139, 0.0300, 0.1249, 0.0678, 0.2810, 0.4362, 0.0462],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,143][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0109, 0.2704, 0.2207, 0.1427, 0.1257, 0.1969, 0.0328],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,144][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([5.2209e-04, 6.4240e-03, 6.8452e-02, 3.6657e-02, 2.0082e-01, 6.8059e-01,
        6.5366e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,145][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0147, 0.1187, 0.2137, 0.1289, 0.2216, 0.2445, 0.0579],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,146][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0268, 0.1657, 0.2057, 0.0956, 0.1773, 0.2622, 0.0667],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,147][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0025, 0.0931, 0.2467, 0.0894, 0.1993, 0.3501, 0.0188],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,148][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0007, 0.0536, 0.2907, 0.1257, 0.1314, 0.3915, 0.0065],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,148][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.1060, 0.0016, 0.0423, 0.0050, 0.0030, 0.8044, 0.0045, 0.0332],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,148][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ at] are: tensor([2.7573e-04, 1.8435e-01, 1.7764e-01, 3.0147e-01, 6.3170e-02, 1.8943e-01,
        6.4018e-03, 7.7256e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,149][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.2661, 0.0739, 0.0956, 0.0410, 0.1172, 0.0801, 0.2085, 0.1177],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,149][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0149, 0.1966, 0.1434, 0.1875, 0.1527, 0.1673, 0.0640, 0.0736],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,150][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0011, 0.0066, 0.2004, 0.0552, 0.3797, 0.2678, 0.0082, 0.0810],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,150][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0191, 0.0096, 0.0759, 0.0514, 0.3024, 0.3529, 0.0373, 0.1513],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,150][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0533, 0.2219, 0.1875, 0.0816, 0.1536, 0.1406, 0.0505, 0.1110],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,151][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0018, 0.0008, 0.0231, 0.0208, 0.2377, 0.6152, 0.0068, 0.0937],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,152][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0162, 0.1013, 0.1860, 0.0972, 0.2250, 0.2092, 0.0490, 0.1161],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,154][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0568, 0.1439, 0.1627, 0.0782, 0.1666, 0.2130, 0.0738, 0.1051],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,155][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0107, 0.0696, 0.1589, 0.0493, 0.1843, 0.3404, 0.0283, 0.1585],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,157][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0022, 0.0548, 0.2684, 0.0915, 0.1054, 0.2878, 0.0082, 0.1818],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,157][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1021, 0.0008, 0.0215, 0.0023, 0.0012, 0.6515, 0.0036, 0.0245, 0.1926],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,158][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.2247e-04, 1.1681e-01, 1.1748e-01, 3.0021e-01, 5.1687e-02, 1.2589e-01,
        3.7017e-03, 7.3635e-02, 2.1036e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,159][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3814, 0.0568, 0.0654, 0.0255, 0.0738, 0.0641, 0.1737, 0.0809, 0.0784],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,161][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0106, 0.1489, 0.1231, 0.1710, 0.1178, 0.1051, 0.0479, 0.0817, 0.1939],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,162][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0038, 0.0036, 0.1305, 0.0270, 0.2533, 0.2132, 0.0134, 0.1044, 0.2508],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,164][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0412, 0.0056, 0.0477, 0.0225, 0.2213, 0.2594, 0.0396, 0.1477, 0.2151],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,165][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1207, 0.1379, 0.1009, 0.0581, 0.1624, 0.1218, 0.0678, 0.0850, 0.1453],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,167][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0080, 0.0006, 0.0167, 0.0120, 0.1656, 0.4165, 0.0105, 0.0835, 0.2866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,168][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0458, 0.0761, 0.1444, 0.0776, 0.1473, 0.1524, 0.0681, 0.1159, 0.1725],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,169][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0841, 0.0897, 0.1328, 0.0585, 0.1421, 0.1679, 0.0730, 0.0759, 0.1760],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,171][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0124, 0.0384, 0.1189, 0.0360, 0.1709, 0.2394, 0.0246, 0.1166, 0.2429],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,172][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0029, 0.0507, 0.1975, 0.0705, 0.0815, 0.2419, 0.0079, 0.1503, 0.1967],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,174][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0568, 0.0011, 0.0271, 0.0023, 0.0013, 0.7186, 0.0045, 0.0236, 0.1539,
        0.0107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,175][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0003, 0.0764, 0.1067, 0.1919, 0.0562, 0.1840, 0.0043, 0.0652, 0.2532,
        0.0618], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,176][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.2862, 0.0541, 0.0546, 0.0210, 0.0866, 0.0895, 0.1422, 0.0840, 0.0825,
        0.0991], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,178][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0070, 0.1069, 0.1081, 0.1509, 0.1038, 0.1212, 0.0389, 0.0791, 0.1872,
        0.0969], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,179][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0008, 0.0018, 0.0962, 0.0181, 0.1322, 0.3446, 0.0064, 0.1016, 0.2526,
        0.0458], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,181][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0084, 0.0040, 0.0402, 0.0135, 0.2114, 0.3296, 0.0204, 0.1010, 0.2087,
        0.0628], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,182][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0784, 0.1606, 0.1093, 0.0611, 0.1379, 0.1236, 0.0675, 0.0666, 0.1204,
        0.0746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,183][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ station] are: tensor([9.9871e-04, 3.7353e-04, 1.5327e-02, 8.1672e-03, 8.7756e-02, 5.4770e-01,
        3.1813e-03, 6.3005e-02, 2.5381e-01, 1.9681e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,185][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0134, 0.0561, 0.1555, 0.0740, 0.1401, 0.1737, 0.0356, 0.1058, 0.1741,
        0.0717], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,186][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ station] are: tensor([0.0538, 0.0909, 0.1206, 0.0722, 0.1213, 0.1522, 0.0618, 0.0701, 0.1403,
        0.1169], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,188][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.0069, 0.0320, 0.1053, 0.0300, 0.1567, 0.2339, 0.0193, 0.0924, 0.1982,
        0.1252], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,189][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0014, 0.0335, 0.1558, 0.0405, 0.0881, 0.2754, 0.0056, 0.1251, 0.2220,
        0.0527], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,190][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0678, 0.0018, 0.0311, 0.0038, 0.0019, 0.5814, 0.0039, 0.0224, 0.1685,
        0.0146, 0.1029], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,191][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [,] are: tensor([5.0758e-05, 1.3007e-01, 8.7352e-02, 3.3118e-01, 2.1106e-02, 8.8882e-02,
        1.3640e-03, 3.4310e-02, 1.6566e-01, 3.8144e-02, 1.0189e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,193][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1713, 0.0796, 0.0698, 0.0401, 0.0827, 0.0828, 0.1097, 0.0798, 0.0693,
        0.1092, 0.1058], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,194][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0069, 0.1367, 0.1121, 0.1827, 0.0709, 0.0821, 0.0302, 0.0666, 0.1339,
        0.0700, 0.1079], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,196][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0023, 0.0030, 0.0896, 0.0257, 0.1230, 0.1741, 0.0099, 0.0823, 0.2091,
        0.0584, 0.2227], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,197][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0171, 0.0038, 0.0308, 0.0195, 0.1659, 0.2319, 0.0269, 0.1107, 0.1656,
        0.0840, 0.1439], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,199][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0404, 0.1832, 0.0918, 0.0594, 0.1101, 0.0887, 0.0348, 0.0590, 0.1097,
        0.0659, 0.1570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,200][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0037, 0.0004, 0.0109, 0.0103, 0.0953, 0.3386, 0.0075, 0.0587, 0.2252,
        0.0304, 0.2190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,201][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0115, 0.0554, 0.0999, 0.0741, 0.1438, 0.1336, 0.0300, 0.0841, 0.1458,
        0.0621, 0.1596], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,203][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0318, 0.0802, 0.1234, 0.0599, 0.0964, 0.1328, 0.0371, 0.0651, 0.1285,
        0.0881, 0.1569], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,205][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0061, 0.0445, 0.1063, 0.0417, 0.1256, 0.1941, 0.0167, 0.0942, 0.1622,
        0.0929, 0.1157], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,205][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0022, 0.0529, 0.1156, 0.0836, 0.0779, 0.1650, 0.0084, 0.0992, 0.1615,
        0.0534, 0.1804], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,206][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0532, 0.0012, 0.0292, 0.0041, 0.0014, 0.6036, 0.0041, 0.0275, 0.1506,
        0.0134, 0.0913, 0.0203], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,206][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([1.8044e-04, 7.3537e-02, 8.0554e-02, 1.5992e-01, 3.5161e-02, 1.1044e-01,
        3.0414e-03, 4.2419e-02, 2.3905e-01, 6.3132e-02, 9.9426e-02, 9.3139e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,207][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.2611, 0.0466, 0.0422, 0.0210, 0.0532, 0.0624, 0.1375, 0.0713, 0.0535,
        0.1025, 0.0824, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,207][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0024, 0.0972, 0.0904, 0.1121, 0.0617, 0.0975, 0.0193, 0.0545, 0.1476,
        0.0971, 0.0918, 0.1283], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,207][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([1.4087e-04, 1.3994e-03, 5.2205e-02, 1.8290e-02, 1.2050e-01, 2.5276e-01,
        3.7089e-03, 1.0403e-01, 2.2771e-01, 5.3359e-02, 1.6165e-01, 4.2583e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,208][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0019, 0.0020, 0.0276, 0.0157, 0.1176, 0.2961, 0.0118, 0.1129, 0.1885,
        0.0820, 0.1372, 0.0068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,209][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0608, 0.1166, 0.0818, 0.0439, 0.0742, 0.1136, 0.0503, 0.0476, 0.0999,
        0.0894, 0.1388, 0.0831], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,210][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([6.9755e-05, 1.7448e-04, 7.8964e-03, 8.3345e-03, 7.3340e-02, 4.4335e-01,
        1.3024e-03, 4.9728e-02, 2.6353e-01, 3.0757e-02, 1.1981e-01, 1.7003e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,211][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0077, 0.0291, 0.0801, 0.0371, 0.1260, 0.1318, 0.0263, 0.1006, 0.1671,
        0.0766, 0.1742, 0.0435], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,213][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0240, 0.0611, 0.1012, 0.0495, 0.0831, 0.1332, 0.0380, 0.0648, 0.1261,
        0.1207, 0.1539, 0.0443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,214][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0032, 0.0274, 0.1083, 0.0349, 0.1121, 0.2104, 0.0137, 0.0789, 0.1667,
        0.1156, 0.1081, 0.0208], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,216][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0007, 0.0196, 0.1602, 0.0272, 0.0667, 0.2359, 0.0040, 0.0939, 0.1680,
        0.0554, 0.1544, 0.0139], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,217][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0614, 0.0014, 0.0329, 0.0034, 0.0018, 0.5357, 0.0044, 0.0222, 0.1473,
        0.0153, 0.0787, 0.0171, 0.0785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,218][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([1.0812e-04, 1.0496e-01, 1.4901e-01, 1.8260e-01, 3.1198e-02, 1.0530e-01,
        2.4130e-03, 3.7404e-02, 1.6757e-01, 3.2526e-02, 9.9045e-02, 7.9015e-02,
        8.8497e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,220][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.1257, 0.0748, 0.0585, 0.0462, 0.0704, 0.0615, 0.0934, 0.0743, 0.0752,
        0.0883, 0.0843, 0.0895, 0.0578], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,221][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0065, 0.1189, 0.1056, 0.1161, 0.0552, 0.0826, 0.0305, 0.0580, 0.1039,
        0.0748, 0.0738, 0.1115, 0.0626], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,222][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0010, 0.0037, 0.0934, 0.0224, 0.0745, 0.2256, 0.0070, 0.0585, 0.1761,
        0.0449, 0.1328, 0.0044, 0.1556], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,224][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0126, 0.0059, 0.0325, 0.0201, 0.1350, 0.2588, 0.0218, 0.0994, 0.1684,
        0.0557, 0.1000, 0.0080, 0.0816], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,225][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0471, 0.1325, 0.1003, 0.0590, 0.0919, 0.1030, 0.0399, 0.0467, 0.0950,
        0.0449, 0.1120, 0.0549, 0.0726], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,227][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0027, 0.0009, 0.0160, 0.0107, 0.0707, 0.3792, 0.0052, 0.0435, 0.1994,
        0.0274, 0.1309, 0.0019, 0.1115], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,228][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0094, 0.0401, 0.1070, 0.0721, 0.1061, 0.1407, 0.0253, 0.0742, 0.1276,
        0.0559, 0.1138, 0.0611, 0.0668], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,230][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0297, 0.0660, 0.1251, 0.0574, 0.0805, 0.1288, 0.0357, 0.0537, 0.1208,
        0.0893, 0.1166, 0.0325, 0.0637], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,231][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.0062, 0.0344, 0.0960, 0.0298, 0.1056, 0.1712, 0.0163, 0.0747, 0.1581,
        0.0768, 0.0961, 0.0181, 0.1167], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,233][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0020, 0.0360, 0.1208, 0.0475, 0.0620, 0.2254, 0.0065, 0.0897, 0.1465,
        0.0457, 0.1373, 0.0207, 0.0599], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,234][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1634, 0.0011, 0.0164, 0.0024, 0.0015, 0.4109, 0.0040, 0.0174, 0.1405,
        0.0099, 0.0715, 0.0202, 0.0744, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,236][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0006, 0.0855, 0.0636, 0.1485, 0.0296, 0.0625, 0.0051, 0.0368, 0.1484,
        0.0429, 0.1164, 0.0960, 0.0238, 0.1402], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,237][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2335, 0.0462, 0.0396, 0.0230, 0.0456, 0.0429, 0.1003, 0.0605, 0.0420,
        0.0588, 0.0683, 0.0566, 0.0543, 0.1284], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,239][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0093, 0.0689, 0.0651, 0.1073, 0.0508, 0.0594, 0.0293, 0.0445, 0.1005,
        0.0592, 0.0867, 0.1288, 0.0354, 0.1548], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,240][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0020, 0.0012, 0.0441, 0.0138, 0.0510, 0.0883, 0.0053, 0.0414, 0.0894,
        0.0256, 0.1403, 0.0034, 0.1176, 0.3766], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,242][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0251, 0.0027, 0.0187, 0.0132, 0.1176, 0.1139, 0.0230, 0.0687, 0.1047,
        0.0480, 0.1079, 0.0060, 0.0904, 0.2601], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,243][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1197, 0.0721, 0.0496, 0.0263, 0.0757, 0.0575, 0.0490, 0.0399, 0.0615,
        0.0509, 0.1294, 0.0485, 0.0878, 0.1323], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,244][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([5.8370e-03, 2.7649e-04, 5.8029e-03, 5.1284e-03, 4.0931e-02, 1.9188e-01,
        5.0766e-03, 2.9685e-02, 1.0730e-01, 1.6569e-02, 1.2866e-01, 1.1059e-03,
        1.3795e-01, 3.2379e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,246][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0251, 0.0287, 0.0589, 0.0355, 0.0775, 0.0938, 0.0359, 0.0612, 0.0919,
        0.0453, 0.1233, 0.0429, 0.0674, 0.2125], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,247][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0557, 0.0506, 0.0809, 0.0399, 0.0664, 0.0956, 0.0390, 0.0518, 0.0840,
        0.0655, 0.1163, 0.0332, 0.0512, 0.1699], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,249][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0089, 0.0260, 0.0764, 0.0241, 0.0827, 0.1327, 0.0154, 0.0672, 0.1160,
        0.0719, 0.0935, 0.0155, 0.0827, 0.1871], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,250][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0043, 0.0278, 0.0814, 0.0479, 0.0474, 0.1080, 0.0059, 0.0642, 0.0972,
        0.0312, 0.1333, 0.0225, 0.0523, 0.2764], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,252][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0909, 0.0018, 0.0248, 0.0039, 0.0025, 0.4222, 0.0047, 0.0241, 0.1437,
        0.0119, 0.0742, 0.0201, 0.0843, 0.0664, 0.0246], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,253][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0008, 0.0850, 0.0907, 0.1217, 0.0329, 0.0817, 0.0054, 0.0524, 0.1596,
        0.0463, 0.0994, 0.0532, 0.0178, 0.1301, 0.0231], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,254][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.1267, 0.0476, 0.0410, 0.0296, 0.0449, 0.0411, 0.0704, 0.0774, 0.0464,
        0.0454, 0.0597, 0.0572, 0.0579, 0.1195, 0.1352], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,256][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0048, 0.0780, 0.0899, 0.1132, 0.0670, 0.0722, 0.0216, 0.0512, 0.0941,
        0.0475, 0.0536, 0.0955, 0.0383, 0.1107, 0.0624], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,258][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0009, 0.0022, 0.0574, 0.0190, 0.0615, 0.1045, 0.0059, 0.0412, 0.1036,
        0.0323, 0.1094, 0.0044, 0.0906, 0.3225, 0.0447], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,259][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0100, 0.0040, 0.0255, 0.0185, 0.1017, 0.1135, 0.0168, 0.0626, 0.1063,
        0.0498, 0.1066, 0.0076, 0.0805, 0.2257, 0.0709], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,261][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0930, 0.0780, 0.0537, 0.0295, 0.0565, 0.0653, 0.0520, 0.0404, 0.0745,
        0.0386, 0.1019, 0.0408, 0.0758, 0.1062, 0.0939], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,262][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0020, 0.0005, 0.0086, 0.0081, 0.0485, 0.2261, 0.0039, 0.0331, 0.1181,
        0.0213, 0.1114, 0.0019, 0.0896, 0.2876, 0.0393], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,263][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0140, 0.0340, 0.0678, 0.0549, 0.0897, 0.0853, 0.0283, 0.0412, 0.0811,
        0.0405, 0.0990, 0.0493, 0.0452, 0.1691, 0.1006], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,263][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ give] are: tensor([0.0526, 0.0525, 0.0712, 0.0362, 0.0678, 0.0862, 0.0365, 0.0416, 0.0759,
        0.0676, 0.0967, 0.0338, 0.0476, 0.1366, 0.0972], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,264][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.0054, 0.0295, 0.0711, 0.0313, 0.0925, 0.1180, 0.0130, 0.0679, 0.1076,
        0.0673, 0.0745, 0.0162, 0.0778, 0.1540, 0.0740], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,264][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0021, 0.0282, 0.0732, 0.0434, 0.0484, 0.1242, 0.0044, 0.0704, 0.0990,
        0.0288, 0.1009, 0.0203, 0.0415, 0.2359, 0.0793], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,265][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.1318, 0.0008, 0.0144, 0.0020, 0.0016, 0.4084, 0.0046, 0.0155, 0.1291,
        0.0082, 0.0623, 0.0155, 0.0613, 0.0477, 0.0172, 0.0797],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,265][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0007, 0.0580, 0.0625, 0.1977, 0.0290, 0.0713, 0.0043, 0.0440, 0.1456,
        0.0428, 0.1067, 0.0815, 0.0201, 0.0795, 0.0277, 0.0284],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,265][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.2084, 0.0334, 0.0305, 0.0186, 0.0299, 0.0376, 0.0848, 0.0470, 0.0418,
        0.0494, 0.0512, 0.0518, 0.0480, 0.0932, 0.0956, 0.0789],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,266][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0079, 0.0527, 0.0705, 0.0968, 0.0534, 0.0565, 0.0284, 0.0478, 0.0917,
        0.0398, 0.0528, 0.0871, 0.0371, 0.1216, 0.0624, 0.0933],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,268][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0013, 0.0014, 0.0400, 0.0106, 0.0599, 0.1005, 0.0054, 0.0324, 0.1020,
        0.0238, 0.0886, 0.0019, 0.0953, 0.2777, 0.0501, 0.1092],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,269][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0167, 0.0025, 0.0178, 0.0110, 0.0955, 0.1106, 0.0169, 0.0583, 0.0995,
        0.0399, 0.0899, 0.0036, 0.0721, 0.1955, 0.0624, 0.1078],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,271][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ it] are: tensor([0.0845, 0.0652, 0.0401, 0.0326, 0.0662, 0.0579, 0.0501, 0.0420, 0.0604,
        0.0348, 0.0891, 0.0416, 0.0547, 0.1160, 0.0846, 0.0801],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,272][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ it] are: tensor([2.5633e-03, 2.2297e-04, 5.6579e-03, 4.4903e-03, 6.4988e-02, 1.8923e-01,
        3.4191e-03, 2.9665e-02, 1.0762e-01, 1.2217e-02, 9.7885e-02, 7.2298e-04,
        8.8424e-02, 2.6618e-01, 3.2293e-02, 9.4421e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,272][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0277, 0.0144, 0.0407, 0.0227, 0.0583, 0.0806, 0.0315, 0.0511, 0.0837,
        0.0321, 0.0956, 0.0277, 0.0521, 0.1628, 0.0991, 0.1199],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,274][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ it] are: tensor([0.0662, 0.0322, 0.0517, 0.0283, 0.0698, 0.0799, 0.0394, 0.0451, 0.0804,
        0.0541, 0.0820, 0.0258, 0.0520, 0.1287, 0.0830, 0.0813],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,275][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.0070, 0.0195, 0.0602, 0.0190, 0.0727, 0.1067, 0.0127, 0.0517, 0.1010,
        0.0575, 0.0668, 0.0102, 0.0777, 0.1380, 0.0494, 0.1500],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,277][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0038, 0.0143, 0.0892, 0.0231, 0.0398, 0.1151, 0.0040, 0.0588, 0.0913,
        0.0207, 0.1110, 0.0088, 0.0406, 0.2488, 0.0713, 0.0591],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,278][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1414, 0.0008, 0.0128, 0.0022, 0.0013, 0.2944, 0.0035, 0.0160, 0.1117,
        0.0086, 0.0607, 0.0187, 0.0626, 0.0539, 0.0194, 0.0795, 0.1124],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,280][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0010, 0.0635, 0.0468, 0.0958, 0.0210, 0.0463, 0.0060, 0.0322, 0.1203,
        0.0324, 0.1042, 0.0683, 0.0191, 0.1152, 0.0227, 0.0625, 0.1426],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,281][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.2126, 0.0309, 0.0262, 0.0163, 0.0267, 0.0297, 0.0652, 0.0448, 0.0319,
        0.0452, 0.0453, 0.0439, 0.0378, 0.0890, 0.0895, 0.0751, 0.0899],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,283][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0077, 0.0478, 0.0503, 0.0759, 0.0361, 0.0443, 0.0246, 0.0320, 0.0742,
        0.0478, 0.0581, 0.0957, 0.0275, 0.1012, 0.0535, 0.1039, 0.1195],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,284][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0006, 0.0008, 0.0317, 0.0158, 0.0421, 0.0633, 0.0031, 0.0243, 0.0670,
        0.0275, 0.1106, 0.0036, 0.0964, 0.2946, 0.0414, 0.0908, 0.0865],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,286][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0090, 0.0017, 0.0120, 0.0109, 0.0825, 0.0798, 0.0128, 0.0463, 0.0788,
        0.0415, 0.0782, 0.0051, 0.0583, 0.1975, 0.0764, 0.1140, 0.0953],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,287][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0868, 0.0552, 0.0348, 0.0186, 0.0516, 0.0416, 0.0365, 0.0297, 0.0467,
        0.0375, 0.1019, 0.0350, 0.0617, 0.1002, 0.0890, 0.0874, 0.0857],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,288][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.4967e-03, 1.3232e-04, 3.7392e-03, 3.8631e-03, 3.5738e-02, 1.3217e-01,
        2.8217e-03, 2.0489e-02, 8.0998e-02, 1.3703e-02, 1.1835e-01, 1.0546e-03,
        1.0380e-01, 2.8528e-01, 3.1890e-02, 9.2173e-02, 7.2306e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,290][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0172, 0.0203, 0.0373, 0.0216, 0.0580, 0.0655, 0.0261, 0.0442, 0.0667,
        0.0316, 0.0803, 0.0273, 0.0433, 0.1302, 0.0824, 0.1173, 0.1307],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,291][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0519, 0.0395, 0.0529, 0.0282, 0.0515, 0.0696, 0.0333, 0.0372, 0.0640,
        0.0532, 0.0817, 0.0255, 0.0418, 0.1205, 0.0667, 0.0665, 0.1160],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,293][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0052, 0.0182, 0.0509, 0.0176, 0.0552, 0.0988, 0.0106, 0.0468, 0.0904,
        0.0556, 0.0661, 0.0111, 0.0628, 0.1280, 0.0444, 0.1308, 0.1075],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,294][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0032, 0.0196, 0.0587, 0.0342, 0.0331, 0.0793, 0.0044, 0.0440, 0.0697,
        0.0236, 0.0941, 0.0162, 0.0364, 0.1739, 0.0632, 0.0784, 0.1681],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,347][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:33,348][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,349][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,350][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,351][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,353][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,354][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,355][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,356][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,357][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,358][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,359][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,360][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,362][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.9810, 0.0190], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,363][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.0040, 0.9960], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,364][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.9234, 0.0766], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,366][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.1430, 0.8570], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,367][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.0821, 0.9179], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,369][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.6241, 0.3759], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,370][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.5724, 0.4276], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,372][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.2426, 0.7574], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,373][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.4176, 0.5824], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,374][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.6365, 0.3635], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,376][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.2373, 0.7627], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,377][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.0771, 0.9229], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,379][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.5839, 0.0170, 0.3991], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,379][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([2.6238e-04, 5.4082e-01, 4.5891e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,379][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.6993, 0.1496, 0.1511], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,380][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0727, 0.4653, 0.4620], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,380][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0168, 0.0490, 0.9342], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,380][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3291, 0.1362, 0.5348], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,381][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2868, 0.4329, 0.2804], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,381][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0693, 0.0463, 0.8844], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,381][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.1052, 0.3135, 0.5814], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,382][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.3032, 0.2700, 0.4268], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,382][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0402, 0.1781, 0.7817], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,384][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0156, 0.1785, 0.8059], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,385][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.6185, 0.0125, 0.3326, 0.0364], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,386][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0008, 0.2627, 0.2296, 0.5069], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,388][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.8114, 0.0853, 0.0688, 0.0345], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,389][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0255, 0.3371, 0.3085, 0.3290], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,390][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0223, 0.0236, 0.6756, 0.2785], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,392][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.3670, 0.0421, 0.3853, 0.2056], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,393][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.3442, 0.3070, 0.2265, 0.1223], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,394][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0883, 0.0157, 0.4971, 0.3989], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,396][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1497, 0.2752, 0.3839, 0.1912], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,397][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.2147, 0.2494, 0.3760, 0.1600], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,398][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0444, 0.1544, 0.6050, 0.1962], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,400][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0040, 0.1121, 0.7254, 0.1585], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,401][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.4131, 0.0196, 0.4929, 0.0461, 0.0283], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,402][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0005, 0.2179, 0.3421, 0.3488, 0.0907], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,404][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.3190, 0.1972, 0.2044, 0.0799, 0.1996], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,405][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0135, 0.2847, 0.2689, 0.2761, 0.1568], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,406][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0154, 0.0230, 0.4002, 0.1259, 0.4355], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,408][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.1374, 0.0357, 0.1704, 0.1083, 0.5482], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,409][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0890, 0.3085, 0.2285, 0.1301, 0.2440], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,411][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0368, 0.0111, 0.1826, 0.1104, 0.6592], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,412][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0590, 0.1448, 0.3328, 0.1386, 0.3248], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,414][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0960, 0.1745, 0.3150, 0.1299, 0.2846], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,415][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0214, 0.1241, 0.3784, 0.1171, 0.3591], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,417][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0061, 0.1362, 0.5149, 0.1527, 0.1902], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,418][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1075, 0.0010, 0.0256, 0.0022, 0.0014, 0.8622], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,419][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([2.7332e-04, 1.6599e-01, 1.7286e-01, 4.4629e-01, 8.5119e-02, 1.2947e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,420][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4081, 0.1201, 0.1238, 0.0556, 0.1244, 0.1681], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,422][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0160, 0.2080, 0.2084, 0.2319, 0.1838, 0.1519], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,423][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0101, 0.0067, 0.2266, 0.0374, 0.3413, 0.3779], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,425][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0874, 0.0131, 0.0941, 0.0335, 0.4057, 0.3662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,426][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2078, 0.1596, 0.1525, 0.0769, 0.2353, 0.1679], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,427][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0166, 0.0018, 0.0338, 0.0196, 0.2953, 0.6328], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,429][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0620, 0.1011, 0.2378, 0.0946, 0.2299, 0.2746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,430][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1116, 0.1314, 0.1952, 0.0747, 0.2119, 0.2752], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,432][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0241, 0.0712, 0.2102, 0.0582, 0.2364, 0.3999], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,433][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0043, 0.0760, 0.3363, 0.1023, 0.1429, 0.3382], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,434][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0236, 0.0015, 0.0345, 0.0039, 0.0020, 0.9319, 0.0026],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,435][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([9.8301e-06, 1.4561e-01, 2.6269e-01, 2.4924e-01, 5.2875e-02, 2.8886e-01,
        7.1474e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,437][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.1838, 0.1207, 0.1162, 0.0893, 0.1318, 0.1603, 0.1978],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,437][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0031, 0.1848, 0.2327, 0.2290, 0.1312, 0.1883, 0.0309],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,437][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0004, 0.0239, 0.2794, 0.0796, 0.2845, 0.3246, 0.0076],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,438][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0139, 0.0300, 0.1249, 0.0678, 0.2810, 0.4362, 0.0462],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,438][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0109, 0.2704, 0.2207, 0.1427, 0.1257, 0.1969, 0.0328],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,439][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([5.2209e-04, 6.4240e-03, 6.8452e-02, 3.6657e-02, 2.0082e-01, 6.8059e-01,
        6.5366e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,439][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0147, 0.1187, 0.2137, 0.1289, 0.2216, 0.2445, 0.0579],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,439][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0268, 0.1657, 0.2057, 0.0956, 0.1773, 0.2622, 0.0667],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,440][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0025, 0.0931, 0.2467, 0.0894, 0.1993, 0.3501, 0.0188],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,441][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0007, 0.0536, 0.2907, 0.1257, 0.1314, 0.3915, 0.0065],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,442][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.1060, 0.0016, 0.0423, 0.0050, 0.0030, 0.8044, 0.0045, 0.0332],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,443][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([2.7573e-04, 1.8435e-01, 1.7764e-01, 3.0147e-01, 6.3170e-02, 1.8943e-01,
        6.4018e-03, 7.7256e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,445][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.2661, 0.0739, 0.0956, 0.0410, 0.1172, 0.0801, 0.2085, 0.1177],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,446][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0149, 0.1966, 0.1434, 0.1875, 0.1527, 0.1673, 0.0640, 0.0736],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,447][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0011, 0.0066, 0.2004, 0.0552, 0.3797, 0.2678, 0.0082, 0.0810],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,449][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0191, 0.0096, 0.0759, 0.0514, 0.3024, 0.3529, 0.0373, 0.1513],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,450][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0533, 0.2219, 0.1875, 0.0816, 0.1536, 0.1406, 0.0505, 0.1110],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,452][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0018, 0.0008, 0.0231, 0.0208, 0.2377, 0.6152, 0.0068, 0.0937],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,453][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0162, 0.1013, 0.1860, 0.0972, 0.2250, 0.2092, 0.0490, 0.1161],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,454][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0568, 0.1439, 0.1627, 0.0782, 0.1666, 0.2130, 0.0738, 0.1051],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,456][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0107, 0.0696, 0.1589, 0.0493, 0.1843, 0.3404, 0.0283, 0.1585],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,457][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0022, 0.0548, 0.2684, 0.0915, 0.1054, 0.2878, 0.0082, 0.1818],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,459][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1021, 0.0008, 0.0215, 0.0023, 0.0012, 0.6515, 0.0036, 0.0245, 0.1926],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,460][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([2.2247e-04, 1.1681e-01, 1.1748e-01, 3.0021e-01, 5.1687e-02, 1.2589e-01,
        3.7017e-03, 7.3635e-02, 2.1036e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,461][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3814, 0.0568, 0.0654, 0.0255, 0.0738, 0.0641, 0.1737, 0.0809, 0.0784],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,463][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0106, 0.1489, 0.1231, 0.1710, 0.1178, 0.1051, 0.0479, 0.0817, 0.1939],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,464][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0038, 0.0036, 0.1305, 0.0270, 0.2533, 0.2132, 0.0134, 0.1044, 0.2508],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,465][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0412, 0.0056, 0.0477, 0.0225, 0.2213, 0.2594, 0.0396, 0.1477, 0.2151],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,467][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1207, 0.1379, 0.1009, 0.0581, 0.1624, 0.1218, 0.0678, 0.0850, 0.1453],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,468][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0080, 0.0006, 0.0167, 0.0120, 0.1656, 0.4165, 0.0105, 0.0835, 0.2866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,469][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0458, 0.0761, 0.1444, 0.0776, 0.1473, 0.1524, 0.0681, 0.1159, 0.1725],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,471][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0841, 0.0897, 0.1328, 0.0585, 0.1421, 0.1679, 0.0730, 0.0759, 0.1760],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,472][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0124, 0.0384, 0.1189, 0.0360, 0.1709, 0.2394, 0.0246, 0.1166, 0.2429],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,474][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0029, 0.0507, 0.1975, 0.0705, 0.0815, 0.2419, 0.0079, 0.1503, 0.1967],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,475][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0568, 0.0011, 0.0271, 0.0023, 0.0013, 0.7186, 0.0045, 0.0236, 0.1539,
        0.0107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,477][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0003, 0.0764, 0.1067, 0.1919, 0.0562, 0.1840, 0.0043, 0.0652, 0.2532,
        0.0618], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,478][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.2862, 0.0541, 0.0546, 0.0210, 0.0866, 0.0895, 0.1422, 0.0840, 0.0825,
        0.0991], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,480][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0070, 0.1069, 0.1081, 0.1509, 0.1038, 0.1212, 0.0389, 0.0791, 0.1872,
        0.0969], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,481][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0008, 0.0018, 0.0962, 0.0181, 0.1322, 0.3446, 0.0064, 0.1016, 0.2526,
        0.0458], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,482][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0084, 0.0040, 0.0402, 0.0135, 0.2114, 0.3296, 0.0204, 0.1010, 0.2087,
        0.0628], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,484][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0784, 0.1606, 0.1093, 0.0611, 0.1379, 0.1236, 0.0675, 0.0666, 0.1204,
        0.0746], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,485][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([9.9871e-04, 3.7353e-04, 1.5327e-02, 8.1672e-03, 8.7756e-02, 5.4770e-01,
        3.1813e-03, 6.3005e-02, 2.5381e-01, 1.9681e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,486][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0134, 0.0561, 0.1555, 0.0740, 0.1401, 0.1737, 0.0356, 0.1058, 0.1741,
        0.0717], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,488][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0538, 0.0909, 0.1206, 0.0722, 0.1213, 0.1522, 0.0618, 0.0701, 0.1403,
        0.1169], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,489][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0069, 0.0320, 0.1053, 0.0300, 0.1567, 0.2339, 0.0193, 0.0924, 0.1982,
        0.1252], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,490][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0014, 0.0335, 0.1558, 0.0405, 0.0881, 0.2754, 0.0056, 0.1251, 0.2220,
        0.0527], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,492][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0678, 0.0018, 0.0311, 0.0038, 0.0019, 0.5814, 0.0039, 0.0224, 0.1685,
        0.0146, 0.1029], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,493][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([5.0758e-05, 1.3007e-01, 8.7352e-02, 3.3118e-01, 2.1106e-02, 8.8882e-02,
        1.3640e-03, 3.4310e-02, 1.6566e-01, 3.8144e-02, 1.0189e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,494][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1713, 0.0796, 0.0698, 0.0401, 0.0827, 0.0828, 0.1097, 0.0798, 0.0693,
        0.1092, 0.1058], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,495][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0069, 0.1367, 0.1121, 0.1827, 0.0709, 0.0821, 0.0302, 0.0666, 0.1339,
        0.0700, 0.1079], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,495][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0023, 0.0030, 0.0896, 0.0257, 0.1230, 0.1741, 0.0099, 0.0823, 0.2091,
        0.0584, 0.2227], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,496][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0171, 0.0038, 0.0308, 0.0195, 0.1659, 0.2319, 0.0269, 0.1107, 0.1656,
        0.0840, 0.1439], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,496][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0404, 0.1832, 0.0918, 0.0594, 0.1101, 0.0887, 0.0348, 0.0590, 0.1097,
        0.0659, 0.1570], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,496][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0037, 0.0004, 0.0109, 0.0103, 0.0953, 0.3386, 0.0075, 0.0587, 0.2252,
        0.0304, 0.2190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,497][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0115, 0.0554, 0.0999, 0.0741, 0.1438, 0.1336, 0.0300, 0.0841, 0.1458,
        0.0621, 0.1596], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,497][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0318, 0.0802, 0.1234, 0.0599, 0.0964, 0.1328, 0.0371, 0.0651, 0.1285,
        0.0881, 0.1569], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,498][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0061, 0.0445, 0.1063, 0.0417, 0.1256, 0.1941, 0.0167, 0.0942, 0.1622,
        0.0929, 0.1157], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,499][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0022, 0.0529, 0.1156, 0.0836, 0.0779, 0.1650, 0.0084, 0.0992, 0.1615,
        0.0534, 0.1804], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,500][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0532, 0.0012, 0.0292, 0.0041, 0.0014, 0.6036, 0.0041, 0.0275, 0.1506,
        0.0134, 0.0913, 0.0203], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,501][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([1.8044e-04, 7.3537e-02, 8.0554e-02, 1.5992e-01, 3.5161e-02, 1.1044e-01,
        3.0414e-03, 4.2419e-02, 2.3905e-01, 6.3132e-02, 9.9426e-02, 9.3139e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,502][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.2611, 0.0466, 0.0422, 0.0210, 0.0532, 0.0624, 0.1375, 0.0713, 0.0535,
        0.1025, 0.0824, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,504][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0024, 0.0972, 0.0904, 0.1121, 0.0617, 0.0975, 0.0193, 0.0545, 0.1476,
        0.0971, 0.0918, 0.1283], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,505][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([1.4087e-04, 1.3994e-03, 5.2205e-02, 1.8290e-02, 1.2050e-01, 2.5276e-01,
        3.7089e-03, 1.0403e-01, 2.2771e-01, 5.3359e-02, 1.6165e-01, 4.2583e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,506][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0019, 0.0020, 0.0276, 0.0157, 0.1176, 0.2961, 0.0118, 0.1129, 0.1885,
        0.0820, 0.1372, 0.0068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,507][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0608, 0.1166, 0.0818, 0.0439, 0.0742, 0.1136, 0.0503, 0.0476, 0.0999,
        0.0894, 0.1388, 0.0831], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,508][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([6.9755e-05, 1.7448e-04, 7.8964e-03, 8.3345e-03, 7.3340e-02, 4.4335e-01,
        1.3024e-03, 4.9728e-02, 2.6353e-01, 3.0757e-02, 1.1981e-01, 1.7003e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,510][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0077, 0.0291, 0.0801, 0.0371, 0.1260, 0.1318, 0.0263, 0.1006, 0.1671,
        0.0766, 0.1742, 0.0435], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,511][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0240, 0.0611, 0.1012, 0.0495, 0.0831, 0.1332, 0.0380, 0.0648, 0.1261,
        0.1207, 0.1539, 0.0443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,513][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0032, 0.0274, 0.1083, 0.0349, 0.1121, 0.2104, 0.0137, 0.0789, 0.1667,
        0.1156, 0.1081, 0.0208], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,514][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0007, 0.0196, 0.1602, 0.0272, 0.0667, 0.2359, 0.0040, 0.0939, 0.1680,
        0.0554, 0.1544, 0.0139], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,516][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0614, 0.0014, 0.0329, 0.0034, 0.0018, 0.5357, 0.0044, 0.0222, 0.1473,
        0.0153, 0.0787, 0.0171, 0.0785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,517][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([1.0812e-04, 1.0496e-01, 1.4901e-01, 1.8260e-01, 3.1198e-02, 1.0530e-01,
        2.4130e-03, 3.7404e-02, 1.6757e-01, 3.2526e-02, 9.9045e-02, 7.9015e-02,
        8.8497e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,518][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.1257, 0.0748, 0.0585, 0.0462, 0.0704, 0.0615, 0.0934, 0.0743, 0.0752,
        0.0883, 0.0843, 0.0895, 0.0578], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,520][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0065, 0.1189, 0.1056, 0.1161, 0.0552, 0.0826, 0.0305, 0.0580, 0.1039,
        0.0748, 0.0738, 0.1115, 0.0626], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,521][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0010, 0.0037, 0.0934, 0.0224, 0.0745, 0.2256, 0.0070, 0.0585, 0.1761,
        0.0449, 0.1328, 0.0044, 0.1556], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,522][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0126, 0.0059, 0.0325, 0.0201, 0.1350, 0.2588, 0.0218, 0.0994, 0.1684,
        0.0557, 0.1000, 0.0080, 0.0816], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,524][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0471, 0.1325, 0.1003, 0.0590, 0.0919, 0.1030, 0.0399, 0.0467, 0.0950,
        0.0449, 0.1120, 0.0549, 0.0726], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,525][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0027, 0.0009, 0.0160, 0.0107, 0.0707, 0.3792, 0.0052, 0.0435, 0.1994,
        0.0274, 0.1309, 0.0019, 0.1115], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,527][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0094, 0.0401, 0.1070, 0.0721, 0.1061, 0.1407, 0.0253, 0.0742, 0.1276,
        0.0559, 0.1138, 0.0611, 0.0668], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,528][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0297, 0.0660, 0.1251, 0.0574, 0.0805, 0.1288, 0.0357, 0.0537, 0.1208,
        0.0893, 0.1166, 0.0325, 0.0637], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,530][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0062, 0.0344, 0.0960, 0.0298, 0.1056, 0.1712, 0.0163, 0.0747, 0.1581,
        0.0768, 0.0961, 0.0181, 0.1167], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,531][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0020, 0.0360, 0.1208, 0.0475, 0.0620, 0.2254, 0.0065, 0.0897, 0.1465,
        0.0457, 0.1373, 0.0207, 0.0599], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,533][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1634, 0.0011, 0.0164, 0.0024, 0.0015, 0.4109, 0.0040, 0.0174, 0.1405,
        0.0099, 0.0715, 0.0202, 0.0744, 0.0665], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,534][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0006, 0.0855, 0.0636, 0.1485, 0.0296, 0.0625, 0.0051, 0.0368, 0.1484,
        0.0429, 0.1164, 0.0960, 0.0238, 0.1402], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,536][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.2335, 0.0462, 0.0396, 0.0230, 0.0456, 0.0429, 0.1003, 0.0605, 0.0420,
        0.0588, 0.0683, 0.0566, 0.0543, 0.1284], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,537][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0093, 0.0689, 0.0651, 0.1073, 0.0508, 0.0594, 0.0293, 0.0445, 0.1005,
        0.0592, 0.0867, 0.1288, 0.0354, 0.1548], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,539][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0020, 0.0012, 0.0441, 0.0138, 0.0510, 0.0883, 0.0053, 0.0414, 0.0894,
        0.0256, 0.1403, 0.0034, 0.1176, 0.3766], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,540][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0251, 0.0027, 0.0187, 0.0132, 0.1176, 0.1139, 0.0230, 0.0687, 0.1047,
        0.0480, 0.1079, 0.0060, 0.0904, 0.2601], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,542][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.1197, 0.0721, 0.0496, 0.0263, 0.0757, 0.0575, 0.0490, 0.0399, 0.0615,
        0.0509, 0.1294, 0.0485, 0.0878, 0.1323], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,543][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([5.8370e-03, 2.7649e-04, 5.8029e-03, 5.1284e-03, 4.0931e-02, 1.9188e-01,
        5.0766e-03, 2.9685e-02, 1.0730e-01, 1.6569e-02, 1.2866e-01, 1.1059e-03,
        1.3795e-01, 3.2379e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,544][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0251, 0.0287, 0.0589, 0.0355, 0.0775, 0.0938, 0.0359, 0.0612, 0.0919,
        0.0453, 0.1233, 0.0429, 0.0674, 0.2125], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,546][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0557, 0.0506, 0.0809, 0.0399, 0.0664, 0.0956, 0.0390, 0.0518, 0.0840,
        0.0655, 0.1163, 0.0332, 0.0512, 0.1699], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,547][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0089, 0.0260, 0.0764, 0.0241, 0.0827, 0.1327, 0.0154, 0.0672, 0.1160,
        0.0719, 0.0935, 0.0155, 0.0827, 0.1871], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,548][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0043, 0.0278, 0.0814, 0.0479, 0.0474, 0.1080, 0.0059, 0.0642, 0.0972,
        0.0312, 0.1333, 0.0225, 0.0523, 0.2764], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,550][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0909, 0.0018, 0.0248, 0.0039, 0.0025, 0.4222, 0.0047, 0.0241, 0.1437,
        0.0119, 0.0742, 0.0201, 0.0843, 0.0664, 0.0246], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,552][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0008, 0.0850, 0.0907, 0.1217, 0.0329, 0.0817, 0.0054, 0.0524, 0.1596,
        0.0463, 0.0994, 0.0532, 0.0178, 0.1301, 0.0231], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,553][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.1267, 0.0476, 0.0410, 0.0296, 0.0449, 0.0411, 0.0704, 0.0774, 0.0464,
        0.0454, 0.0597, 0.0572, 0.0579, 0.1195, 0.1352], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,553][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0048, 0.0780, 0.0899, 0.1132, 0.0670, 0.0722, 0.0216, 0.0512, 0.0941,
        0.0475, 0.0536, 0.0955, 0.0383, 0.1107, 0.0624], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,554][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0009, 0.0022, 0.0574, 0.0190, 0.0615, 0.1045, 0.0059, 0.0412, 0.1036,
        0.0323, 0.1094, 0.0044, 0.0906, 0.3225, 0.0447], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,554][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0100, 0.0040, 0.0255, 0.0185, 0.1017, 0.1135, 0.0168, 0.0626, 0.1063,
        0.0498, 0.1066, 0.0076, 0.0805, 0.2257, 0.0709], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,555][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0930, 0.0780, 0.0537, 0.0295, 0.0565, 0.0653, 0.0520, 0.0404, 0.0745,
        0.0386, 0.1019, 0.0408, 0.0758, 0.1062, 0.0939], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,555][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0020, 0.0005, 0.0086, 0.0081, 0.0485, 0.2261, 0.0039, 0.0331, 0.1181,
        0.0213, 0.1114, 0.0019, 0.0896, 0.2876, 0.0393], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,556][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0140, 0.0340, 0.0678, 0.0549, 0.0897, 0.0853, 0.0283, 0.0412, 0.0811,
        0.0405, 0.0990, 0.0493, 0.0452, 0.1691, 0.1006], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,557][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0526, 0.0525, 0.0712, 0.0362, 0.0678, 0.0862, 0.0365, 0.0416, 0.0759,
        0.0676, 0.0967, 0.0338, 0.0476, 0.1366, 0.0972], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,558][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0054, 0.0295, 0.0711, 0.0313, 0.0925, 0.1180, 0.0130, 0.0679, 0.1076,
        0.0673, 0.0745, 0.0162, 0.0778, 0.1540, 0.0740], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,560][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0021, 0.0282, 0.0732, 0.0434, 0.0484, 0.1242, 0.0044, 0.0704, 0.0990,
        0.0288, 0.1009, 0.0203, 0.0415, 0.2359, 0.0793], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,561][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.1318, 0.0008, 0.0144, 0.0020, 0.0016, 0.4084, 0.0046, 0.0155, 0.1291,
        0.0082, 0.0623, 0.0155, 0.0613, 0.0477, 0.0172, 0.0797],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,562][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0007, 0.0580, 0.0625, 0.1977, 0.0290, 0.0713, 0.0043, 0.0440, 0.1456,
        0.0428, 0.1067, 0.0815, 0.0201, 0.0795, 0.0277, 0.0284],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,564][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.2084, 0.0334, 0.0305, 0.0186, 0.0299, 0.0376, 0.0848, 0.0470, 0.0418,
        0.0494, 0.0512, 0.0518, 0.0480, 0.0932, 0.0956, 0.0789],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,565][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0079, 0.0527, 0.0705, 0.0968, 0.0534, 0.0565, 0.0284, 0.0478, 0.0917,
        0.0398, 0.0528, 0.0871, 0.0371, 0.1216, 0.0624, 0.0933],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,567][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0013, 0.0014, 0.0400, 0.0106, 0.0599, 0.1005, 0.0054, 0.0324, 0.1020,
        0.0238, 0.0886, 0.0019, 0.0953, 0.2777, 0.0501, 0.1092],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,568][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0167, 0.0025, 0.0178, 0.0110, 0.0955, 0.1106, 0.0169, 0.0583, 0.0995,
        0.0399, 0.0899, 0.0036, 0.0721, 0.1955, 0.0624, 0.1078],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,569][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0845, 0.0652, 0.0401, 0.0326, 0.0662, 0.0579, 0.0501, 0.0420, 0.0604,
        0.0348, 0.0891, 0.0416, 0.0547, 0.1160, 0.0846, 0.0801],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,570][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([2.5633e-03, 2.2297e-04, 5.6579e-03, 4.4903e-03, 6.4988e-02, 1.8923e-01,
        3.4191e-03, 2.9665e-02, 1.0762e-01, 1.2217e-02, 9.7885e-02, 7.2298e-04,
        8.8424e-02, 2.6618e-01, 3.2293e-02, 9.4421e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,572][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0277, 0.0144, 0.0407, 0.0227, 0.0583, 0.0806, 0.0315, 0.0511, 0.0837,
        0.0321, 0.0956, 0.0277, 0.0521, 0.1628, 0.0991, 0.1199],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,573][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0662, 0.0322, 0.0517, 0.0283, 0.0698, 0.0799, 0.0394, 0.0451, 0.0804,
        0.0541, 0.0820, 0.0258, 0.0520, 0.1287, 0.0830, 0.0813],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,575][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0070, 0.0195, 0.0602, 0.0190, 0.0727, 0.1067, 0.0127, 0.0517, 0.1010,
        0.0575, 0.0668, 0.0102, 0.0777, 0.1380, 0.0494, 0.1500],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,576][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0038, 0.0143, 0.0892, 0.0231, 0.0398, 0.1151, 0.0040, 0.0588, 0.0913,
        0.0207, 0.1110, 0.0088, 0.0406, 0.2488, 0.0713, 0.0591],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,578][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1414, 0.0008, 0.0128, 0.0022, 0.0013, 0.2944, 0.0035, 0.0160, 0.1117,
        0.0086, 0.0607, 0.0187, 0.0626, 0.0539, 0.0194, 0.0795, 0.1124],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,579][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0010, 0.0635, 0.0468, 0.0958, 0.0210, 0.0463, 0.0060, 0.0322, 0.1203,
        0.0324, 0.1042, 0.0683, 0.0191, 0.1152, 0.0227, 0.0625, 0.1426],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,581][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.2126, 0.0309, 0.0262, 0.0163, 0.0267, 0.0297, 0.0652, 0.0448, 0.0319,
        0.0452, 0.0453, 0.0439, 0.0378, 0.0890, 0.0895, 0.0751, 0.0899],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,583][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0077, 0.0478, 0.0503, 0.0759, 0.0361, 0.0443, 0.0246, 0.0320, 0.0742,
        0.0478, 0.0581, 0.0957, 0.0275, 0.1012, 0.0535, 0.1039, 0.1195],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,584][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0006, 0.0008, 0.0317, 0.0158, 0.0421, 0.0633, 0.0031, 0.0243, 0.0670,
        0.0275, 0.1106, 0.0036, 0.0964, 0.2946, 0.0414, 0.0908, 0.0865],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,585][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0090, 0.0017, 0.0120, 0.0109, 0.0825, 0.0798, 0.0128, 0.0463, 0.0788,
        0.0415, 0.0782, 0.0051, 0.0583, 0.1975, 0.0764, 0.1140, 0.0953],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,587][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0868, 0.0552, 0.0348, 0.0186, 0.0516, 0.0416, 0.0365, 0.0297, 0.0467,
        0.0375, 0.1019, 0.0350, 0.0617, 0.1002, 0.0890, 0.0874, 0.0857],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,588][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([1.4967e-03, 1.3232e-04, 3.7392e-03, 3.8631e-03, 3.5738e-02, 1.3217e-01,
        2.8217e-03, 2.0489e-02, 8.0998e-02, 1.3703e-02, 1.1835e-01, 1.0546e-03,
        1.0380e-01, 2.8528e-01, 3.1890e-02, 9.2173e-02, 7.2306e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,589][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0172, 0.0203, 0.0373, 0.0216, 0.0580, 0.0655, 0.0261, 0.0442, 0.0667,
        0.0316, 0.0803, 0.0273, 0.0433, 0.1302, 0.0824, 0.1173, 0.1307],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,591][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0519, 0.0395, 0.0529, 0.0282, 0.0515, 0.0696, 0.0333, 0.0372, 0.0640,
        0.0532, 0.0817, 0.0255, 0.0418, 0.1205, 0.0667, 0.0665, 0.1160],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,592][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0052, 0.0182, 0.0509, 0.0176, 0.0552, 0.0988, 0.0106, 0.0468, 0.0904,
        0.0556, 0.0661, 0.0111, 0.0628, 0.1280, 0.0444, 0.1308, 0.1075],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,594][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0032, 0.0196, 0.0587, 0.0342, 0.0331, 0.0793, 0.0044, 0.0440, 0.0697,
        0.0236, 0.0941, 0.0162, 0.0364, 0.1739, 0.0632, 0.0784, 0.1681],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,595][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:33,597][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[3131],
        [ 839],
        [  49],
        [  31],
        [  10],
        [   7],
        [   6],
        [   2],
        [   1],
        [   1],
        [   3],
        [   4],
        [   2],
        [   3],
        [   1],
        [   2],
        [   2]], device='cuda:0')
[2024-07-24 10:21:33,598][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[2565],
        [ 566],
        [  44],
        [  54],
        [  14],
        [  13],
        [   8],
        [   4],
        [   2],
        [   2],
        [   3],
        [  11],
        [   2],
        [   9],
        [   2],
        [   4],
        [   3]], device='cuda:0')
[2024-07-24 10:21:33,600][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 4670],
        [ 5341],
        [16551],
        [17249],
        [21982],
        [25841],
        [26636],
        [25879],
        [24615],
        [25558],
        [24970],
        [25772],
        [24806],
        [24079],
        [24635],
        [23565],
        [24410]], device='cuda:0')
[2024-07-24 10:21:33,601][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[43251],
        [50250],
        [49938],
        [49514],
        [48459],
        [48325],
        [46592],
        [48063],
        [45725],
        [41357],
        [47090],
        [41916],
        [44323],
        [42685],
        [41534],
        [42443],
        [39673]], device='cuda:0')
[2024-07-24 10:21:33,603][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[18438],
        [22830],
        [25686],
        [21242],
        [20109],
        [18977],
        [20419],
        [20416],
        [19938],
        [16262],
        [17111],
        [16291],
        [16687],
        [17485],
        [19532],
        [17992],
        [18018]], device='cuda:0')
[2024-07-24 10:21:33,604][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[37215],
        [15275],
        [10199],
        [ 3128],
        [ 4792],
        [ 6440],
        [ 6182],
        [ 7842],
        [ 7329],
        [ 8247],
        [ 8151],
        [ 8951],
        [10032],
        [11298],
        [10209],
        [11231],
        [13343]], device='cuda:0')
[2024-07-24 10:21:33,606][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[30514],
        [39000],
        [29270],
        [32371],
        [28969],
        [30683],
        [31517],
        [30251],
        [32708],
        [34447],
        [35059],
        [35065],
        [36585],
        [36287],
        [35755],
        [34848],
        [34926]], device='cuda:0')
[2024-07-24 10:21:33,607][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 8157],
        [34186],
        [35821],
        [31916],
        [40409],
        [41688],
        [41786],
        [41101],
        [41455],
        [42994],
        [42522],
        [42458],
        [43141],
        [45591],
        [46003],
        [46462],
        [47049]], device='cuda:0')
[2024-07-24 10:21:33,609][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[25726],
        [42319],
        [36561],
        [39932],
        [41866],
        [41511],
        [41226],
        [40716],
        [42854],
        [42554],
        [43939],
        [44739],
        [45623],
        [45914],
        [45387],
        [45882],
        [46433]], device='cuda:0')
[2024-07-24 10:21:33,610][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[28531],
        [36104],
        [29268],
        [41221],
        [19365],
        [22433],
        [24883],
        [23239],
        [25379],
        [26508],
        [29733],
        [28722],
        [30320],
        [37522],
        [36311],
        [36850],
        [39028]], device='cuda:0')
[2024-07-24 10:21:33,612][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[35524],
        [46367],
        [30378],
        [34138],
        [30083],
        [23045],
        [24204],
        [23242],
        [20995],
        [19631],
        [20888],
        [19631],
        [21480],
        [19854],
        [21285],
        [19056],
        [19202]], device='cuda:0')
[2024-07-24 10:21:33,613][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[40113],
        [33564],
        [36172],
        [34945],
        [20290],
        [19862],
        [20092],
        [19682],
        [16026],
        [16573],
        [14870],
        [14216],
        [15174],
        [12364],
        [11077],
        [10104],
        [10638]], device='cuda:0')
[2024-07-24 10:21:33,614][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[25207],
        [ 1599],
        [21952],
        [26351],
        [29161],
        [31569],
        [30671],
        [31810],
        [32044],
        [29461],
        [27435],
        [27474],
        [21565],
        [19193],
        [18846],
        [20572],
        [18799]], device='cuda:0')
[2024-07-24 10:21:33,615][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[14037],
        [50257],
        [17413],
        [ 5568],
        [10780],
        [ 6058],
        [ 4239],
        [ 8177],
        [ 7880],
        [ 9244],
        [14175],
        [10746],
        [11741],
        [16737],
        [20279],
        [20148],
        [22387]], device='cuda:0')
[2024-07-24 10:21:33,616][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[23135],
        [ 6851],
        [ 9748],
        [ 7049],
        [ 7102],
        [ 8133],
        [11282],
        [ 6658],
        [ 8077],
        [ 8512],
        [ 6912],
        [ 7364],
        [ 7041],
        [ 6814],
        [ 7234],
        [ 5502],
        [ 7170]], device='cuda:0')
[2024-07-24 10:21:33,617][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[31552],
        [29367],
        [10885],
        [10980],
        [ 5717],
        [24624],
        [23764],
        [23039],
        [24700],
        [23811],
        [21646],
        [21190],
        [20311],
        [21000],
        [19189],
        [21201],
        [21097]], device='cuda:0')
[2024-07-24 10:21:33,619][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 3414],
        [ 4583],
        [ 5045],
        [ 3505],
        [ 6628],
        [ 5106],
        [ 7192],
        [ 8430],
        [ 9461],
        [14084],
        [ 6941],
        [14549],
        [11764],
        [14769],
        [17253],
        [15869],
        [22420]], device='cuda:0')
[2024-07-24 10:21:33,620][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[23221],
        [16453],
        [ 5256],
        [ 7296],
        [ 5280],
        [ 4266],
        [ 1835],
        [ 2193],
        [ 2382],
        [ 2499],
        [ 2781],
        [ 1821],
        [ 2087],
        [ 2287],
        [ 3405],
        [ 2885],
        [ 2877]], device='cuda:0')
[2024-07-24 10:21:33,622][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[39407],
        [12136],
        [13203],
        [13191],
        [12906],
        [14012],
        [14264],
        [13069],
        [12391],
        [10084],
        [12443],
        [13908],
        [14669],
        [16444],
        [17069],
        [17348],
        [16875]], device='cuda:0')
[2024-07-24 10:21:33,623][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[20611],
        [16623],
        [12384],
        [12730],
        [18216],
        [14664],
        [14902],
        [16360],
        [16689],
        [14284],
        [13865],
        [13933],
        [14619],
        [16460],
        [18198],
        [21085],
        [20163]], device='cuda:0')
[2024-07-24 10:21:33,625][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[41140],
        [29499],
        [36962],
        [36746],
        [42056],
        [37132],
        [34127],
        [35015],
        [34411],
        [32860],
        [33256],
        [31838],
        [33086],
        [34459],
        [34622],
        [33525],
        [33001]], device='cuda:0')
[2024-07-24 10:21:33,626][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[10735],
        [ 1202],
        [ 3411],
        [ 3393],
        [ 4655],
        [ 6614],
        [ 5922],
        [ 7898],
        [10499],
        [11618],
        [10948],
        [13229],
        [10314],
        [10670],
        [11013],
        [10976],
        [10376]], device='cuda:0')
[2024-07-24 10:21:33,628][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[1762],
        [3177],
        [1581],
        [1254],
        [3838],
        [1105],
        [ 889],
        [1188],
        [1372],
        [1037],
        [ 985],
        [ 979],
        [ 831],
        [ 626],
        [ 722],
        [ 674],
        [ 608]], device='cuda:0')
[2024-07-24 10:21:33,629][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 4416],
        [15821],
        [33071],
        [29450],
        [29428],
        [29380],
        [25235],
        [26262],
        [24617],
        [25397],
        [26745],
        [26650],
        [24731],
        [25218],
        [26012],
        [24944],
        [26054]], device='cuda:0')
[2024-07-24 10:21:33,631][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[13387],
        [15880],
        [21941],
        [20081],
        [24679],
        [24200],
        [22947],
        [23701],
        [24985],
        [25149],
        [26232],
        [25271],
        [24985],
        [24888],
        [24610],
        [23629],
        [23662]], device='cuda:0')
[2024-07-24 10:21:33,632][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 3314],
        [19003],
        [ 9215],
        [ 6512],
        [ 5044],
        [ 4905],
        [ 4414],
        [ 3404],
        [ 3828],
        [ 3561],
        [ 3733],
        [ 3697],
        [ 4207],
        [ 4918],
        [ 4356],
        [ 5095],
        [ 5171]], device='cuda:0')
[2024-07-24 10:21:33,634][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 9845],
        [19806],
        [ 9062],
        [ 1408],
        [  711],
        [ 1535],
        [ 1098],
        [ 3056],
        [ 2513],
        [ 3682],
        [ 1986],
        [ 4601],
        [ 3137],
        [ 6929],
        [ 7441],
        [12283],
        [14178]], device='cuda:0')
[2024-07-24 10:21:33,635][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[37895],
        [41130],
        [42664],
        [43890],
        [41609],
        [43156],
        [45692],
        [43727],
        [43092],
        [43063],
        [43123],
        [42456],
        [42805],
        [40685],
        [38745],
        [38513],
        [37545]], device='cuda:0')
[2024-07-24 10:21:33,637][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[26220],
        [43478],
        [40384],
        [44542],
        [43878],
        [41211],
        [40327],
        [43097],
        [39557],
        [38646],
        [42947],
        [40742],
        [41184],
        [40998],
        [40717],
        [40551],
        [39330]], device='cuda:0')
[2024-07-24 10:21:33,638][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000],
        [15000]], device='cuda:0')
[2024-07-24 10:21:33,696][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:33,698][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,699][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,700][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,701][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,702][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,704][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,705][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,706][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,707][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,708][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,709][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,710][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,712][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.3510, 0.6490], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,713][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.0169, 0.9831], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,714][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.4044, 0.5956], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,716][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.8855, 0.1145], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,717][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.0782, 0.9218], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,718][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.3915, 0.6085], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,720][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.0023, 0.9977], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,721][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.1567, 0.8433], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,723][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.2782, 0.7218], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,723][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([7.3463e-04, 9.9927e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,725][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([0.1314, 0.8686], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,726][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.0545, 0.9455], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,728][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0578, 0.4259, 0.5163], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,729][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0018, 0.2394, 0.7588], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,730][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.1036, 0.4180, 0.4784], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,732][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.5265, 0.0088, 0.4647], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,733][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0229, 0.6028, 0.3742], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,734][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1420, 0.4726, 0.3854], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,735][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ and] are: tensor([1.1636e-04, 2.8982e-01, 7.1007e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,735][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0202, 0.2252, 0.7547], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,735][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0641, 0.4523, 0.4836], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,735][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ and] are: tensor([1.8524e-05, 1.4592e-01, 8.5406e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,736][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0084, 0.0096, 0.9819], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,736][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0030, 0.3989, 0.5981], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,736][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0729, 0.4095, 0.3657, 0.1519], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,736][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0031, 0.2376, 0.3917, 0.3676], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,737][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0715, 0.3995, 0.2275, 0.3014], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,737][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.7161, 0.0007, 0.1831, 0.1001], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,737][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0192, 0.3527, 0.2936, 0.3345], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,738][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.1238, 0.4096, 0.2568, 0.2099], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,739][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([7.8699e-05, 2.0443e-01, 4.4098e-01, 3.5451e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,740][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0284, 0.1799, 0.6398, 0.1520], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,742][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0260, 0.2129, 0.5561, 0.2050], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,742][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([1.3893e-05, 7.4044e-02, 1.5137e-01, 7.7457e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,744][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([0.0158, 0.0025, 0.5160, 0.4657], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,744][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0043, 0.3122, 0.4461, 0.2374], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:33,745][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.0197, 0.2394, 0.3236, 0.1081, 0.3092], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,747][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0012, 0.2281, 0.4273, 0.2361, 0.1073], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,748][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0295, 0.1529, 0.1916, 0.1927, 0.4333], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,750][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.4320, 0.0006, 0.0212, 0.0101, 0.5362], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,751][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0059, 0.4121, 0.1699, 0.2624, 0.1497], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,752][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0574, 0.2266, 0.1529, 0.0972, 0.4658], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,753][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ got] are: tensor([3.5134e-05, 2.0324e-01, 2.9552e-01, 4.1351e-01, 8.7697e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,755][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0122, 0.0913, 0.5726, 0.0483, 0.2756], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,756][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0177, 0.3182, 0.2099, 0.2510, 0.2032], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,757][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ got] are: tensor([3.1276e-06, 5.2848e-02, 1.3538e-01, 7.0987e-01, 1.0190e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,758][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.0136, 0.0021, 0.1331, 0.0450, 0.8062], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,760][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ got] are: tensor([0.0007, 0.1948, 0.2847, 0.2719, 0.2479], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:33,761][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0517, 0.1600, 0.1660, 0.0640, 0.2477, 0.3105], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,763][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0011, 0.1373, 0.3078, 0.2478, 0.1112, 0.1949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,764][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0354, 0.0994, 0.1357, 0.1119, 0.2379, 0.3797], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,765][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([3.9102e-01, 7.9336e-05, 2.8044e-03, 1.2950e-03, 1.2980e-01, 4.7501e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,766][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0123, 0.2653, 0.1427, 0.2503, 0.0938, 0.2356], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,768][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1038, 0.1651, 0.1124, 0.0484, 0.3893, 0.1811], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,769][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([4.5945e-05, 9.7574e-02, 2.3018e-01, 2.9088e-01, 1.1045e-01, 2.7087e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,770][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0253, 0.0645, 0.2369, 0.0261, 0.1547, 0.4925], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,772][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0271, 0.2614, 0.1655, 0.1735, 0.1536, 0.2190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,772][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([6.2208e-06, 5.4121e-02, 1.5470e-01, 4.1174e-01, 2.0979e-01, 1.6964e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,773][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([3.0670e-03, 1.3661e-04, 1.4036e-02, 2.6142e-03, 1.1881e-01, 8.6133e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,775][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0017, 0.1812, 0.1651, 0.1901, 0.2052, 0.2567], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:33,776][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0135, 0.2259, 0.1794, 0.1215, 0.2400, 0.1929, 0.0268],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,777][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([1.5626e-04, 1.9646e-01, 3.4627e-01, 2.5686e-01, 6.2036e-02, 1.3624e-01,
        1.9798e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,778][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0121, 0.0984, 0.1362, 0.1265, 0.2506, 0.3347, 0.0415],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,780][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0345, 0.0020, 0.0326, 0.0098, 0.2495, 0.6249, 0.0467],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,781][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0016, 0.2536, 0.1538, 0.2347, 0.1292, 0.2167, 0.0104],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,783][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0135, 0.2006, 0.1511, 0.0666, 0.3642, 0.1639, 0.0401],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,784][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([1.4851e-05, 7.3453e-02, 2.2235e-01, 2.9037e-01, 9.4451e-02, 3.1898e-01,
        3.8679e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,785][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0017, 0.0762, 0.2910, 0.0655, 0.1327, 0.4196, 0.0134],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,786][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0064, 0.1257, 0.2048, 0.2588, 0.1678, 0.1935, 0.0429],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,787][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([1.1370e-06, 2.9422e-02, 1.9528e-01, 3.6760e-01, 1.9206e-01, 2.1552e-01,
        1.0753e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,788][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([4.0035e-04, 2.6690e-03, 6.9122e-02, 1.2810e-02, 1.4573e-01, 7.6685e-01,
        2.4220e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,789][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([2.3857e-04, 1.4123e-01, 1.8828e-01, 1.7532e-01, 1.7121e-01, 3.1773e-01,
        5.9827e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:33,791][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0063, 0.1163, 0.1401, 0.0526, 0.1729, 0.2318, 0.0115, 0.2685],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,791][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0010, 0.1495, 0.2533, 0.1686, 0.0713, 0.1990, 0.0059, 0.1515],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,792][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0108, 0.0688, 0.1174, 0.0911, 0.1918, 0.3275, 0.0372, 0.1553],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,792][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ at] are: tensor([2.7278e-02, 9.0569e-05, 4.3700e-03, 3.3040e-03, 2.4378e-01, 6.4805e-01,
        3.1631e-02, 4.1498e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,792][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0052, 0.1842, 0.1396, 0.1113, 0.0803, 0.2225, 0.0177, 0.2393],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,793][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0287, 0.1692, 0.1280, 0.0353, 0.3042, 0.1314, 0.0419, 0.1612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,793][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ at] are: tensor([4.0277e-05, 1.0611e-01, 1.8975e-01, 2.2894e-01, 7.6633e-02, 2.9476e-01,
        9.2976e-04, 1.0283e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,793][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0049, 0.0682, 0.2299, 0.0313, 0.1285, 0.4426, 0.0132, 0.0815],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,794][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0047, 0.2065, 0.1572, 0.2233, 0.1183, 0.1956, 0.0302, 0.0641],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,794][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ at] are: tensor([3.8967e-06, 4.9832e-02, 1.2567e-01, 2.9888e-01, 1.1158e-01, 2.5418e-01,
        3.0139e-04, 1.5956e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,795][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ at] are: tensor([2.8510e-04, 1.1009e-04, 1.1884e-02, 5.8505e-03, 1.1821e-01, 8.2801e-01,
        1.3658e-03, 3.4287e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,796][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0010, 0.1519, 0.1412, 0.1251, 0.1160, 0.1867, 0.0120, 0.2661],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:33,797][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0188, 0.1119, 0.0904, 0.0282, 0.1466, 0.1602, 0.0200, 0.1859, 0.2379],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,799][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0013, 0.1147, 0.2025, 0.2256, 0.0586, 0.1364, 0.0066, 0.1090, 0.1452],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,800][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0237, 0.0690, 0.0729, 0.0780, 0.1752, 0.2150, 0.0452, 0.1229, 0.1982],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,801][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.2264e-01, 3.8728e-05, 2.2836e-03, 1.6473e-03, 1.1500e-01, 5.4823e-01,
        4.4732e-02, 4.2256e-02, 1.2318e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,802][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0070, 0.1491, 0.0833, 0.1139, 0.0808, 0.1423, 0.0186, 0.2450, 0.1600],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,803][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0589, 0.1142, 0.0794, 0.0280, 0.2775, 0.0996, 0.0611, 0.1105, 0.1708],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,804][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([4.3191e-05, 9.9492e-02, 1.5487e-01, 2.2227e-01, 6.9603e-02, 1.7355e-01,
        7.9990e-04, 8.2920e-02, 1.9644e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,806][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0154, 0.0430, 0.1569, 0.0239, 0.1301, 0.3373, 0.0187, 0.0918, 0.1829],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,807][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0115, 0.1905, 0.1274, 0.1504, 0.1152, 0.1485, 0.0446, 0.0823, 0.1296],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,808][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([4.5682e-06, 4.0664e-02, 8.9357e-02, 4.2312e-01, 8.2706e-02, 1.2233e-01,
        2.3556e-04, 9.0738e-02, 1.5085e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,809][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([1.0403e-03, 4.7459e-05, 6.2832e-03, 2.7526e-03, 8.7134e-02, 6.7467e-01,
        1.9319e-03, 4.2471e-02, 1.8367e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,810][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0013, 0.0936, 0.0866, 0.0896, 0.1135, 0.1361, 0.0128, 0.2176, 0.2489],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:33,812][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0316, 0.1004, 0.0719, 0.0362, 0.1456, 0.1718, 0.0226, 0.1541, 0.2038,
        0.0621], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,813][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0013, 0.0984, 0.2099, 0.1912, 0.0692, 0.1220, 0.0073, 0.0838, 0.1440,
        0.0729], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,815][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0183, 0.0555, 0.0713, 0.0720, 0.1512, 0.2267, 0.0317, 0.0987, 0.2070,
        0.0676], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,816][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ station] are: tensor([1.4299e-02, 2.2119e-05, 2.1916e-03, 7.8465e-04, 1.2588e-01, 6.6803e-01,
        1.2872e-02, 2.9530e-02, 1.3353e-01, 1.2868e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,817][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0027, 0.1377, 0.0683, 0.1127, 0.0492, 0.1531, 0.0096, 0.1491, 0.2390,
        0.0785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,818][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0330, 0.1049, 0.0868, 0.0378, 0.2338, 0.0985, 0.0431, 0.0912, 0.1443,
        0.1265], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,819][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ station] are: tensor([4.1949e-05, 6.8853e-02, 1.4696e-01, 2.3939e-01, 7.7544e-02, 1.9627e-01,
        5.5012e-04, 7.7860e-02, 1.8036e-01, 1.2177e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,821][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0041, 0.0276, 0.1308, 0.0090, 0.1398, 0.3610, 0.0085, 0.0831, 0.1789,
        0.0571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,822][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0106, 0.0878, 0.1261, 0.0896, 0.0920, 0.1504, 0.0342, 0.0894, 0.1680,
        0.1518], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,823][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ station] are: tensor([1.1338e-05, 3.7852e-02, 1.1387e-01, 4.2295e-01, 9.1131e-02, 1.0433e-01,
        2.7465e-04, 8.3542e-02, 1.3598e-01, 1.0052e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,824][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ station] are: tensor([2.5706e-04, 4.7193e-05, 5.2762e-03, 1.5215e-03, 7.1255e-02, 7.3403e-01,
        8.8414e-04, 2.5101e-02, 1.4897e-01, 1.2658e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,825][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0010, 0.0565, 0.0947, 0.0674, 0.1086, 0.1673, 0.0108, 0.2074, 0.2193,
        0.0670], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:33,827][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0088, 0.1119, 0.0746, 0.0475, 0.0976, 0.1676, 0.0088, 0.1277, 0.1766,
        0.0450, 0.1339], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,828][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0005, 0.0804, 0.1870, 0.2532, 0.0488, 0.0963, 0.0032, 0.0531, 0.0955,
        0.0471, 0.1350], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,829][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0119, 0.0748, 0.0566, 0.0975, 0.1290, 0.1449, 0.0222, 0.0792, 0.1384,
        0.0683, 0.1772], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,830][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [,] are: tensor([3.7572e-02, 3.6520e-05, 1.6670e-03, 1.5872e-03, 1.0156e-01, 4.8379e-01,
        3.1526e-02, 5.1054e-02, 1.3867e-01, 3.2970e-02, 1.1957e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,832][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0022, 0.1458, 0.0830, 0.1150, 0.0240, 0.1320, 0.0064, 0.1643, 0.2056,
        0.0642, 0.0576], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,833][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0272, 0.1317, 0.0845, 0.0365, 0.2405, 0.0869, 0.0402, 0.0734, 0.1205,
        0.1072, 0.0515], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,834][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [,] are: tensor([1.5302e-05, 1.1935e-01, 1.2243e-01, 2.8138e-01, 4.2998e-02, 1.5837e-01,
        3.9379e-04, 4.4544e-02, 1.7313e-01, 1.5556e-02, 4.1837e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,836][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0051, 0.0439, 0.1418, 0.0280, 0.1062, 0.2929, 0.0117, 0.0657, 0.1412,
        0.0880, 0.0754], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,837][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0051, 0.1339, 0.0869, 0.1776, 0.0670, 0.1038, 0.0195, 0.0512, 0.0983,
        0.1231, 0.1334], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,838][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [,] are: tensor([1.0318e-06, 3.5514e-02, 7.3909e-02, 5.2186e-01, 6.3100e-02, 8.6163e-02,
        9.4502e-05, 5.5399e-02, 1.3249e-01, 7.8538e-03, 2.3609e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,839][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [,] are: tensor([6.8602e-04, 5.5320e-05, 5.8130e-03, 4.0940e-03, 6.8718e-02, 5.9761e-01,
        2.3194e-03, 4.7157e-02, 1.5709e-01, 2.9554e-02, 8.6905e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,840][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0004, 0.0969, 0.0851, 0.1319, 0.0697, 0.1112, 0.0065, 0.1489, 0.1748,
        0.0430, 0.1317], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:33,842][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0140, 0.0575, 0.0572, 0.0319, 0.1319, 0.1351, 0.0108, 0.1398, 0.2287,
        0.0586, 0.0974, 0.0370], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,843][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0015, 0.0772, 0.2244, 0.1095, 0.0544, 0.0963, 0.0050, 0.0775, 0.1055,
        0.0564, 0.1705, 0.0218], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,844][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0129, 0.0460, 0.0441, 0.0469, 0.1493, 0.1696, 0.0231, 0.0563, 0.1622,
        0.0608, 0.1744, 0.0543], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,845][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([1.5983e-04, 3.7559e-06, 1.0735e-03, 9.6574e-04, 7.1961e-02, 6.6213e-01,
        2.5470e-03, 3.3897e-02, 1.2720e-01, 3.4722e-02, 6.5139e-02, 1.9626e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,847][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0015, 0.1166, 0.0581, 0.0819, 0.0352, 0.1044, 0.0071, 0.2053, 0.1891,
        0.0753, 0.0533, 0.0723], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,848][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0164, 0.0761, 0.0634, 0.0282, 0.2893, 0.1042, 0.0252, 0.0907, 0.1142,
        0.1232, 0.0384, 0.0308], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,849][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([3.4081e-05, 9.3725e-02, 1.6046e-01, 1.5480e-01, 5.3029e-02, 1.1976e-01,
        5.4210e-04, 7.7449e-02, 2.2764e-01, 2.0386e-02, 5.6840e-02, 3.5331e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,849][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0014, 0.0213, 0.1017, 0.0163, 0.0958, 0.3593, 0.0064, 0.0761, 0.1367,
        0.1060, 0.0701, 0.0089], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,849][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0068, 0.0339, 0.1484, 0.0592, 0.0586, 0.1232, 0.0199, 0.0870, 0.1441,
        0.1336, 0.1214, 0.0640], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,850][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([1.0160e-05, 5.2199e-02, 1.0844e-01, 4.5198e-01, 3.6302e-02, 6.7724e-02,
        3.0246e-04, 6.3894e-02, 1.0932e-01, 7.7525e-03, 1.5531e-02, 8.6541e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,850][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([7.0305e-06, 9.6433e-06, 1.7917e-03, 1.9369e-03, 4.4813e-02, 6.8752e-01,
        2.5062e-04, 3.5061e-02, 1.5958e-01, 3.5237e-02, 3.3512e-02, 2.7998e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,851][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0013, 0.0774, 0.0615, 0.0489, 0.0618, 0.1065, 0.0108, 0.1155, 0.1928,
        0.0675, 0.1042, 0.1518], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:33,851][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0115, 0.0534, 0.0572, 0.0389, 0.0722, 0.1428, 0.0109, 0.1099, 0.1669,
        0.0314, 0.1101, 0.0340, 0.1608], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,851][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0009, 0.0961, 0.1436, 0.2686, 0.0298, 0.0799, 0.0052, 0.0588, 0.0709,
        0.0383, 0.0950, 0.0690, 0.0438], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,852][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0112, 0.0302, 0.0439, 0.0761, 0.1195, 0.1365, 0.0186, 0.0513, 0.1471,
        0.0686, 0.1518, 0.0766, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,852][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([6.9935e-02, 7.7756e-05, 2.9536e-03, 1.9756e-03, 9.9726e-02, 3.8022e-01,
        4.0157e-02, 3.2568e-02, 9.6146e-02, 2.7336e-02, 8.5974e-02, 5.3281e-04,
        1.6239e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,854][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0013, 0.1530, 0.0529, 0.1622, 0.0372, 0.0973, 0.0077, 0.0977, 0.1646,
        0.0556, 0.0374, 0.0951, 0.0381], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,855][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0201, 0.0705, 0.0594, 0.0261, 0.1529, 0.0632, 0.0317, 0.0635, 0.0963,
        0.0969, 0.0490, 0.0261, 0.2443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,856][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([2.1443e-05, 6.7828e-02, 1.4897e-01, 1.9521e-01, 4.7204e-02, 1.7495e-01,
        4.4554e-04, 6.5976e-02, 1.5980e-01, 1.4662e-02, 4.4159e-02, 6.1573e-02,
        1.9192e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,857][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0038, 0.0411, 0.1543, 0.0152, 0.1112, 0.2783, 0.0090, 0.0675, 0.1247,
        0.0486, 0.0496, 0.0069, 0.0898], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,859][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0142, 0.0634, 0.0651, 0.0596, 0.0763, 0.0812, 0.0314, 0.0619, 0.1200,
        0.1040, 0.1164, 0.0839, 0.1225], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,859][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([4.8496e-06, 3.7996e-02, 9.0732e-02, 2.4740e-01, 8.4244e-02, 1.1682e-01,
        1.9656e-04, 1.1694e-01, 1.2533e-01, 1.5656e-02, 3.4719e-02, 1.0483e-01,
        2.5126e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,860][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([1.3958e-03, 1.8827e-04, 1.0832e-02, 3.0206e-03, 8.9043e-02, 5.0802e-01,
        2.5332e-03, 4.3556e-02, 1.5553e-01, 1.6336e-02, 5.3129e-02, 3.3594e-04,
        1.1608e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,862][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([0.0006, 0.0929, 0.0810, 0.0807, 0.0682, 0.1080, 0.0069, 0.1311, 0.1336,
        0.0472, 0.0869, 0.0907, 0.0722], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:33,863][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0069, 0.0422, 0.0426, 0.0225, 0.0531, 0.0915, 0.0065, 0.0756, 0.1008,
        0.0315, 0.0998, 0.0212, 0.0918, 0.3141], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,865][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0008, 0.0502, 0.1237, 0.1487, 0.0257, 0.0666, 0.0037, 0.0603, 0.0732,
        0.0434, 0.1380, 0.0388, 0.0377, 0.1892], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,866][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0161, 0.0254, 0.0343, 0.0547, 0.0687, 0.0828, 0.0243, 0.0540, 0.0816,
        0.0373, 0.1500, 0.0652, 0.0442, 0.2615], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,867][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([5.1843e-02, 1.7338e-05, 7.0709e-04, 1.1142e-03, 4.7981e-02, 2.4972e-01,
        2.3166e-02, 2.8115e-02, 8.1303e-02, 1.6707e-02, 9.8242e-02, 2.2214e-04,
        2.0270e-01, 1.9816e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,869][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0025, 0.0851, 0.0455, 0.0822, 0.0157, 0.0934, 0.0102, 0.1254, 0.1450,
        0.0797, 0.0622, 0.0619, 0.0190, 0.1723], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,870][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0431, 0.0591, 0.0449, 0.0153, 0.1363, 0.0492, 0.0438, 0.0546, 0.0800,
        0.0769, 0.0443, 0.0219, 0.2054, 0.1253], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,871][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([3.4820e-05, 8.3803e-02, 1.0849e-01, 1.8977e-01, 3.6196e-02, 1.3948e-01,
        5.6394e-04, 5.0073e-02, 1.4262e-01, 1.4860e-02, 5.8492e-02, 4.7075e-02,
        1.4675e-02, 1.1387e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,872][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0100, 0.0303, 0.0865, 0.0151, 0.0681, 0.2050, 0.0131, 0.0497, 0.1092,
        0.0689, 0.0709, 0.0081, 0.1153, 0.1498], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,874][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0102, 0.0965, 0.0624, 0.1075, 0.0673, 0.0612, 0.0270, 0.0303, 0.0728,
        0.1029, 0.0832, 0.0987, 0.0931, 0.0870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,875][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([5.8697e-06, 4.0328e-02, 7.9611e-02, 2.6144e-01, 7.5711e-02, 7.2659e-02,
        2.4438e-04, 5.8665e-02, 1.0807e-01, 1.0784e-02, 3.3478e-02, 1.1918e-01,
        2.0873e-02, 1.1895e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,876][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.0321e-03, 3.8353e-05, 2.7918e-03, 2.9490e-03, 3.2967e-02, 3.5929e-01,
        1.7474e-03, 2.7469e-02, 1.0720e-01, 1.7573e-02, 6.1112e-02, 3.6165e-04,
        1.3256e-01, 2.5291e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,877][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0012, 0.0631, 0.0450, 0.0577, 0.0483, 0.0631, 0.0107, 0.1070, 0.1169,
        0.0339, 0.0995, 0.1053, 0.0484, 0.1998], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:33,879][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0101, 0.0876, 0.0459, 0.0347, 0.0825, 0.0839, 0.0082, 0.0789, 0.0732,
        0.0288, 0.0538, 0.0193, 0.0771, 0.2035, 0.1124], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,880][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0018, 0.0671, 0.1166, 0.0996, 0.0290, 0.0668, 0.0073, 0.0680, 0.0831,
        0.0474, 0.1098, 0.0305, 0.0400, 0.1310, 0.1020], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,881][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0116, 0.0459, 0.0289, 0.0382, 0.0892, 0.0964, 0.0193, 0.0458, 0.0906,
        0.0364, 0.1091, 0.0406, 0.0483, 0.1927, 0.1068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,882][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ give] are: tensor([1.1258e-02, 4.3837e-05, 1.5144e-03, 1.5471e-03, 4.6594e-02, 2.4718e-01,
        1.3127e-02, 3.8284e-02, 8.9859e-02, 2.2943e-02, 9.7589e-02, 3.6354e-04,
        1.8343e-01, 2.2157e-01, 2.4693e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,884][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0019, 0.1016, 0.0408, 0.0665, 0.0398, 0.1088, 0.0096, 0.1162, 0.1409,
        0.0837, 0.0364, 0.0531, 0.0281, 0.1044, 0.0683], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,885][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0189, 0.0586, 0.0573, 0.0149, 0.1090, 0.0528, 0.0283, 0.0615, 0.0809,
        0.0766, 0.0421, 0.0189, 0.1886, 0.1162, 0.0753], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,886][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ give] are: tensor([4.5192e-05, 1.6226e-01, 1.1414e-01, 1.6982e-01, 3.5472e-02, 1.2064e-01,
        6.1911e-04, 4.8842e-02, 1.2936e-01, 1.4713e-02, 3.2039e-02, 3.3642e-02,
        1.2587e-02, 8.9735e-02, 3.6095e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,887][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0040, 0.0271, 0.0892, 0.0180, 0.0954, 0.2141, 0.0086, 0.0571, 0.0939,
        0.0578, 0.0485, 0.0078, 0.0827, 0.1449, 0.0508], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,889][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0073, 0.0869, 0.0520, 0.0866, 0.0567, 0.0649, 0.0216, 0.0342, 0.0930,
        0.0893, 0.0741, 0.0883, 0.0772, 0.0843, 0.0836], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,890][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ give] are: tensor([8.8926e-06, 4.8429e-02, 6.4441e-02, 2.1084e-01, 8.3023e-02, 9.9124e-02,
        2.7451e-04, 6.7723e-02, 1.1498e-01, 1.2424e-02, 2.7399e-02, 1.1499e-01,
        1.8664e-02, 9.8181e-02, 3.9494e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,891][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ give] are: tensor([4.3171e-04, 1.0388e-04, 5.2592e-03, 5.0911e-03, 4.8297e-02, 3.5385e-01,
        1.4909e-03, 2.6166e-02, 1.0101e-01, 2.0452e-02, 5.8735e-02, 6.1006e-04,
        1.2024e-01, 2.2618e-01, 3.2079e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,892][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ give] are: tensor([0.0015, 0.0477, 0.0379, 0.0404, 0.0333, 0.0588, 0.0099, 0.0770, 0.1077,
        0.0322, 0.0667, 0.0932, 0.0382, 0.1843, 0.1712], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:33,893][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0128, 0.0440, 0.0288, 0.0131, 0.0544, 0.0628, 0.0072, 0.0883, 0.0806,
        0.0190, 0.0646, 0.0083, 0.1045, 0.2180, 0.1070, 0.0866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,895][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0017, 0.0460, 0.0854, 0.1110, 0.0247, 0.0677, 0.0047, 0.0645, 0.0754,
        0.0355, 0.1037, 0.0347, 0.0447, 0.1318, 0.0825, 0.0862],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,896][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0147, 0.0111, 0.0157, 0.0180, 0.0547, 0.0703, 0.0208, 0.0417, 0.0673,
        0.0225, 0.1204, 0.0233, 0.0547, 0.2205, 0.1217, 0.1226],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,897][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ it] are: tensor([2.7931e-02, 2.2638e-05, 1.1581e-03, 1.0602e-03, 5.3880e-02, 2.9322e-01,
        1.6866e-02, 2.4460e-02, 7.7326e-02, 1.4006e-02, 7.8984e-02, 1.3967e-04,
        1.4731e-01, 1.8649e-01, 2.5779e-02, 5.1362e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,899][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0033, 0.0934, 0.0449, 0.0783, 0.0430, 0.0940, 0.0111, 0.1216, 0.1284,
        0.0549, 0.0310, 0.0485, 0.0249, 0.0895, 0.0688, 0.0644],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,900][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0308, 0.0403, 0.0380, 0.0136, 0.1037, 0.0468, 0.0343, 0.0505, 0.0621,
        0.0534, 0.0352, 0.0154, 0.1365, 0.0914, 0.0527, 0.1954],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,901][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ it] are: tensor([3.5883e-05, 5.1526e-02, 8.8817e-02, 2.0526e-01, 3.8953e-02, 1.2168e-01,
        5.6523e-04, 6.0291e-02, 1.4691e-01, 1.2766e-02, 4.3020e-02, 3.1197e-02,
        1.7696e-02, 7.7671e-02, 4.1507e-02, 6.2109e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,903][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0103, 0.0155, 0.0468, 0.0089, 0.0636, 0.1848, 0.0100, 0.0348, 0.0877,
        0.0370, 0.0669, 0.0044, 0.1295, 0.1342, 0.0542, 0.1115],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,904][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0108, 0.0798, 0.0759, 0.0833, 0.0574, 0.0518, 0.0306, 0.0252, 0.0607,
        0.0883, 0.0814, 0.0688, 0.0938, 0.0682, 0.0556, 0.0685],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,905][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ it] are: tensor([8.1709e-06, 2.4853e-02, 8.4110e-02, 3.0344e-01, 5.0942e-02, 9.2606e-02,
        2.7413e-04, 6.2060e-02, 1.1492e-01, 1.0931e-02, 2.3000e-02, 7.0832e-02,
        1.7548e-02, 6.6593e-02, 2.7498e-02, 5.0386e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,906][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ it] are: tensor([7.7129e-04, 4.0818e-05, 2.5075e-03, 1.6928e-03, 4.0636e-02, 3.8416e-01,
        1.3451e-03, 1.8071e-02, 9.5735e-02, 1.1666e-02, 4.3442e-02, 1.6690e-04,
        8.2400e-02, 1.8526e-01, 3.0409e-02, 1.0169e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,907][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ it] are: tensor([0.0013, 0.0391, 0.0466, 0.0343, 0.0460, 0.0584, 0.0091, 0.0736, 0.1156,
        0.0345, 0.0731, 0.0668, 0.0302, 0.1426, 0.1055, 0.1233],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:33,907][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0066, 0.0420, 0.0328, 0.0158, 0.0404, 0.0650, 0.0058, 0.0588, 0.0646,
        0.0225, 0.0617, 0.0129, 0.0633, 0.1971, 0.0667, 0.0759, 0.1682],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,908][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0010, 0.0451, 0.0923, 0.1010, 0.0215, 0.0535, 0.0039, 0.0457, 0.0547,
        0.0293, 0.0966, 0.0256, 0.0270, 0.1416, 0.0719, 0.0857, 0.1037],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,908][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0172, 0.0222, 0.0232, 0.0280, 0.0499, 0.0530, 0.0234, 0.0389, 0.0561,
        0.0278, 0.0986, 0.0345, 0.0339, 0.1711, 0.0808, 0.1152, 0.1262],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,908][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([5.4321e-03, 1.2834e-05, 5.3486e-04, 1.8666e-03, 4.6361e-02, 1.8115e-01,
        9.4553e-03, 2.5602e-02, 7.1925e-02, 2.3006e-02, 1.0447e-01, 4.2363e-04,
        1.6697e-01, 2.1615e-01, 3.3950e-02, 8.7266e-02, 2.5415e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,909][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0021, 0.0685, 0.0310, 0.0559, 0.0133, 0.0705, 0.0084, 0.0988, 0.1059,
        0.0541, 0.0379, 0.0353, 0.0123, 0.0860, 0.0527, 0.0642, 0.2032],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,909][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0259, 0.0421, 0.0347, 0.0107, 0.1115, 0.0369, 0.0301, 0.0410, 0.0558,
        0.0540, 0.0328, 0.0150, 0.1373, 0.0787, 0.0412, 0.1623, 0.0899],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,909][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([4.7827e-05, 8.6230e-02, 8.3860e-02, 1.2934e-01, 2.8523e-02, 1.0356e-01,
        6.3306e-04, 4.3002e-02, 1.2161e-01, 1.2082e-02, 4.2221e-02, 3.3115e-02,
        1.2085e-02, 8.9060e-02, 3.6749e-02, 6.3947e-02, 1.1393e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,910][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0058, 0.0229, 0.0626, 0.0116, 0.0613, 0.1509, 0.0102, 0.0413, 0.0785,
        0.0589, 0.0497, 0.0063, 0.0888, 0.1014, 0.0484, 0.1096, 0.0918],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,912][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0093, 0.0759, 0.0484, 0.0875, 0.0527, 0.0502, 0.0228, 0.0247, 0.0641,
        0.0790, 0.0588, 0.0887, 0.0732, 0.0613, 0.0550, 0.0920, 0.0564],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,913][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([7.5868e-06, 3.8158e-02, 4.9196e-02, 1.8693e-01, 4.8200e-02, 5.3530e-02,
        2.5972e-04, 4.4298e-02, 8.0534e-02, 8.5858e-03, 2.5488e-02, 9.0417e-02,
        1.6074e-02, 9.2304e-02, 3.7732e-02, 6.8150e-02, 1.6014e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,913][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.0948e-04, 2.6730e-05, 2.2450e-03, 3.8381e-03, 2.7711e-02, 2.5093e-01,
        6.4180e-04, 2.2054e-02, 8.4143e-02, 1.9381e-02, 5.5174e-02, 4.7738e-04,
        8.3763e-02, 2.3672e-01, 2.5851e-02, 1.5572e-01, 3.1214e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,915][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0010, 0.0431, 0.0238, 0.0274, 0.0231, 0.0367, 0.0083, 0.0608, 0.0783,
        0.0222, 0.0611, 0.0581, 0.0294, 0.1410, 0.0928, 0.0846, 0.2083],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:33,959][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:33,960][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,962][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,963][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,964][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,964][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,965][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,965][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,965][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,966][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,966][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,966][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,967][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:33,967][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.3510, 0.6490], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,968][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.0169, 0.9831], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,969][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.4044, 0.5956], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,970][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.8855, 0.1145], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,972][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.0782, 0.9218], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,982][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.3915, 0.6085], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,984][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.0023, 0.9977], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,985][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.1567, 0.8433], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,986][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.2782, 0.7218], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,987][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([7.3463e-04, 9.9927e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,989][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.1314, 0.8686], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,990][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.0545, 0.9455], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:33,992][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0578, 0.4259, 0.5163], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,993][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0018, 0.2394, 0.7588], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,994][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.1036, 0.4180, 0.4784], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,996][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5265, 0.0088, 0.4647], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,997][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0229, 0.6028, 0.3742], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:33,999][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1420, 0.4726, 0.3854], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,000][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([1.1636e-04, 2.8982e-01, 7.1007e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,001][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0202, 0.2252, 0.7547], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,002][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0641, 0.4523, 0.4836], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,003][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([1.8524e-05, 1.4592e-01, 8.5406e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,005][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0084, 0.0096, 0.9819], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,006][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0030, 0.3989, 0.5981], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,008][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0729, 0.4095, 0.3657, 0.1519], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,009][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0031, 0.2376, 0.3917, 0.3676], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,010][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0715, 0.3995, 0.2275, 0.3014], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,012][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.7161, 0.0007, 0.1831, 0.1001], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,013][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0192, 0.3527, 0.2936, 0.3345], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,014][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.1238, 0.4096, 0.2568, 0.2099], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,015][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([7.8699e-05, 2.0443e-01, 4.4098e-01, 3.5451e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,017][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0284, 0.1799, 0.6398, 0.1520], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,018][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0260, 0.2129, 0.5561, 0.2050], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,019][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([1.3893e-05, 7.4044e-02, 1.5137e-01, 7.7457e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,020][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0158, 0.0025, 0.5160, 0.4657], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,021][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0043, 0.3122, 0.4461, 0.2374], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,021][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.0197, 0.2394, 0.3236, 0.1081, 0.3092], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,022][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0012, 0.2281, 0.4273, 0.2361, 0.1073], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,022][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0295, 0.1529, 0.1916, 0.1927, 0.4333], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,023][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.4320, 0.0006, 0.0212, 0.0101, 0.5362], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,023][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0059, 0.4121, 0.1699, 0.2624, 0.1497], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,023][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0574, 0.2266, 0.1529, 0.0972, 0.4658], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,024][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([3.5134e-05, 2.0324e-01, 2.9552e-01, 4.1351e-01, 8.7697e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,024][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0122, 0.0913, 0.5726, 0.0483, 0.2756], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,024][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0177, 0.3182, 0.2099, 0.2510, 0.2032], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,025][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([3.1276e-06, 5.2848e-02, 1.3538e-01, 7.0987e-01, 1.0190e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,026][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0136, 0.0021, 0.1331, 0.0450, 0.8062], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,028][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0007, 0.1948, 0.2847, 0.2719, 0.2479], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,029][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0517, 0.1600, 0.1660, 0.0640, 0.2477, 0.3105], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,030][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0011, 0.1373, 0.3078, 0.2478, 0.1112, 0.1949], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,031][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0354, 0.0994, 0.1357, 0.1119, 0.2379, 0.3797], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,032][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([3.9102e-01, 7.9336e-05, 2.8044e-03, 1.2950e-03, 1.2980e-01, 4.7501e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,034][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0123, 0.2653, 0.1427, 0.2503, 0.0938, 0.2356], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,035][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1038, 0.1651, 0.1124, 0.0484, 0.3893, 0.1811], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,036][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([4.5945e-05, 9.7574e-02, 2.3018e-01, 2.9088e-01, 1.1045e-01, 2.7087e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,037][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0253, 0.0645, 0.2369, 0.0261, 0.1547, 0.4925], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,039][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0271, 0.2614, 0.1655, 0.1735, 0.1536, 0.2190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,040][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([6.2208e-06, 5.4121e-02, 1.5470e-01, 4.1174e-01, 2.0979e-01, 1.6964e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,041][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([3.0670e-03, 1.3661e-04, 1.4036e-02, 2.6142e-03, 1.1881e-01, 8.6133e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,042][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0017, 0.1812, 0.1651, 0.1901, 0.2052, 0.2567], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,043][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0135, 0.2259, 0.1794, 0.1215, 0.2400, 0.1929, 0.0268],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,044][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([1.5626e-04, 1.9646e-01, 3.4627e-01, 2.5686e-01, 6.2036e-02, 1.3624e-01,
        1.9798e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,046][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0121, 0.0984, 0.1362, 0.1265, 0.2506, 0.3347, 0.0415],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,047][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0345, 0.0020, 0.0326, 0.0098, 0.2495, 0.6249, 0.0467],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,049][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0016, 0.2536, 0.1538, 0.2347, 0.1292, 0.2167, 0.0104],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,050][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0135, 0.2006, 0.1511, 0.0666, 0.3642, 0.1639, 0.0401],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,051][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([1.4851e-05, 7.3453e-02, 2.2235e-01, 2.9037e-01, 9.4451e-02, 3.1898e-01,
        3.8679e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,052][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0017, 0.0762, 0.2910, 0.0655, 0.1327, 0.4196, 0.0134],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,054][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0064, 0.1257, 0.2048, 0.2588, 0.1678, 0.1935, 0.0429],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,055][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([1.1370e-06, 2.9422e-02, 1.9528e-01, 3.6760e-01, 1.9206e-01, 2.1552e-01,
        1.0753e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,056][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([4.0035e-04, 2.6690e-03, 6.9122e-02, 1.2810e-02, 1.4573e-01, 7.6685e-01,
        2.4220e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,057][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([2.3857e-04, 1.4123e-01, 1.8828e-01, 1.7532e-01, 1.7121e-01, 3.1773e-01,
        5.9827e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,058][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0063, 0.1163, 0.1401, 0.0526, 0.1729, 0.2318, 0.0115, 0.2685],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,059][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0010, 0.1495, 0.2533, 0.1686, 0.0713, 0.1990, 0.0059, 0.1515],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,061][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0108, 0.0688, 0.1174, 0.0911, 0.1918, 0.3275, 0.0372, 0.1553],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,062][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([2.7278e-02, 9.0569e-05, 4.3700e-03, 3.3040e-03, 2.4378e-01, 6.4805e-01,
        3.1631e-02, 4.1498e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,063][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0052, 0.1842, 0.1396, 0.1113, 0.0803, 0.2225, 0.0177, 0.2393],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,065][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0287, 0.1692, 0.1280, 0.0353, 0.3042, 0.1314, 0.0419, 0.1612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,066][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([4.0277e-05, 1.0611e-01, 1.8975e-01, 2.2894e-01, 7.6633e-02, 2.9476e-01,
        9.2976e-04, 1.0283e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,067][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0049, 0.0682, 0.2299, 0.0313, 0.1285, 0.4426, 0.0132, 0.0815],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,068][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0047, 0.2065, 0.1572, 0.2233, 0.1183, 0.1956, 0.0302, 0.0641],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,069][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([3.8967e-06, 4.9832e-02, 1.2567e-01, 2.9888e-01, 1.1158e-01, 2.5418e-01,
        3.0139e-04, 1.5956e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,070][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([2.8510e-04, 1.1009e-04, 1.1884e-02, 5.8505e-03, 1.1821e-01, 8.2801e-01,
        1.3658e-03, 3.4287e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,072][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0010, 0.1519, 0.1412, 0.1251, 0.1160, 0.1867, 0.0120, 0.2661],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,073][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0188, 0.1119, 0.0904, 0.0282, 0.1466, 0.1602, 0.0200, 0.1859, 0.2379],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,075][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0013, 0.1147, 0.2025, 0.2256, 0.0586, 0.1364, 0.0066, 0.1090, 0.1452],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,076][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0237, 0.0690, 0.0729, 0.0780, 0.1752, 0.2150, 0.0452, 0.1229, 0.1982],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,077][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.2264e-01, 3.8728e-05, 2.2836e-03, 1.6473e-03, 1.1500e-01, 5.4823e-01,
        4.4732e-02, 4.2256e-02, 1.2318e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,078][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0070, 0.1491, 0.0833, 0.1139, 0.0808, 0.1423, 0.0186, 0.2450, 0.1600],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,079][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0589, 0.1142, 0.0794, 0.0280, 0.2775, 0.0996, 0.0611, 0.1105, 0.1708],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,079][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([4.3191e-05, 9.9492e-02, 1.5487e-01, 2.2227e-01, 6.9603e-02, 1.7355e-01,
        7.9990e-04, 8.2920e-02, 1.9644e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,080][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0154, 0.0430, 0.1569, 0.0239, 0.1301, 0.3373, 0.0187, 0.0918, 0.1829],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,080][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0115, 0.1905, 0.1274, 0.1504, 0.1152, 0.1485, 0.0446, 0.0823, 0.1296],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,080][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([4.5682e-06, 4.0664e-02, 8.9357e-02, 4.2312e-01, 8.2706e-02, 1.2233e-01,
        2.3556e-04, 9.0738e-02, 1.5085e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,081][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.0403e-03, 4.7459e-05, 6.2832e-03, 2.7526e-03, 8.7134e-02, 6.7467e-01,
        1.9319e-03, 4.2471e-02, 1.8367e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,081][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0013, 0.0936, 0.0866, 0.0896, 0.1135, 0.1361, 0.0128, 0.2176, 0.2489],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,081][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0316, 0.1004, 0.0719, 0.0362, 0.1456, 0.1718, 0.0226, 0.1541, 0.2038,
        0.0621], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,083][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0013, 0.0984, 0.2099, 0.1912, 0.0692, 0.1220, 0.0073, 0.0838, 0.1440,
        0.0729], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,084][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0183, 0.0555, 0.0713, 0.0720, 0.1512, 0.2267, 0.0317, 0.0987, 0.2070,
        0.0676], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,085][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([1.4299e-02, 2.2119e-05, 2.1916e-03, 7.8465e-04, 1.2588e-01, 6.6803e-01,
        1.2872e-02, 2.9530e-02, 1.3353e-01, 1.2868e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,086][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0027, 0.1377, 0.0683, 0.1127, 0.0492, 0.1531, 0.0096, 0.1491, 0.2390,
        0.0785], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,088][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0330, 0.1049, 0.0868, 0.0378, 0.2338, 0.0985, 0.0431, 0.0912, 0.1443,
        0.1265], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,089][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([4.1949e-05, 6.8853e-02, 1.4696e-01, 2.3939e-01, 7.7544e-02, 1.9627e-01,
        5.5012e-04, 7.7860e-02, 1.8036e-01, 1.2177e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,090][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0041, 0.0276, 0.1308, 0.0090, 0.1398, 0.3610, 0.0085, 0.0831, 0.1789,
        0.0571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,092][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0106, 0.0878, 0.1261, 0.0896, 0.0920, 0.1504, 0.0342, 0.0894, 0.1680,
        0.1518], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,093][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([1.1338e-05, 3.7852e-02, 1.1387e-01, 4.2295e-01, 9.1131e-02, 1.0433e-01,
        2.7465e-04, 8.3542e-02, 1.3598e-01, 1.0052e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,093][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([2.5706e-04, 4.7193e-05, 5.2762e-03, 1.5215e-03, 7.1255e-02, 7.3403e-01,
        8.8414e-04, 2.5101e-02, 1.4897e-01, 1.2658e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,095][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0010, 0.0565, 0.0947, 0.0674, 0.1086, 0.1673, 0.0108, 0.2074, 0.2193,
        0.0670], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,096][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0088, 0.1119, 0.0746, 0.0475, 0.0976, 0.1676, 0.0088, 0.1277, 0.1766,
        0.0450, 0.1339], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,098][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0005, 0.0804, 0.1870, 0.2532, 0.0488, 0.0963, 0.0032, 0.0531, 0.0955,
        0.0471, 0.1350], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,099][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0119, 0.0748, 0.0566, 0.0975, 0.1290, 0.1449, 0.0222, 0.0792, 0.1384,
        0.0683, 0.1772], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,100][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([3.7572e-02, 3.6520e-05, 1.6670e-03, 1.5872e-03, 1.0156e-01, 4.8379e-01,
        3.1526e-02, 5.1054e-02, 1.3867e-01, 3.2970e-02, 1.1957e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,102][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0022, 0.1458, 0.0830, 0.1150, 0.0240, 0.1320, 0.0064, 0.1643, 0.2056,
        0.0642, 0.0576], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,103][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0272, 0.1317, 0.0845, 0.0365, 0.2405, 0.0869, 0.0402, 0.0734, 0.1205,
        0.1072, 0.0515], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,104][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([1.5302e-05, 1.1935e-01, 1.2243e-01, 2.8138e-01, 4.2998e-02, 1.5837e-01,
        3.9379e-04, 4.4544e-02, 1.7313e-01, 1.5556e-02, 4.1837e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,105][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0051, 0.0439, 0.1418, 0.0280, 0.1062, 0.2929, 0.0117, 0.0657, 0.1412,
        0.0880, 0.0754], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,107][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0051, 0.1339, 0.0869, 0.1776, 0.0670, 0.1038, 0.0195, 0.0512, 0.0983,
        0.1231, 0.1334], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,108][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([1.0318e-06, 3.5514e-02, 7.3909e-02, 5.2186e-01, 6.3100e-02, 8.6163e-02,
        9.4502e-05, 5.5399e-02, 1.3249e-01, 7.8538e-03, 2.3609e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,109][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([6.8602e-04, 5.5320e-05, 5.8130e-03, 4.0940e-03, 6.8718e-02, 5.9761e-01,
        2.3194e-03, 4.7157e-02, 1.5709e-01, 2.9554e-02, 8.6905e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,110][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0004, 0.0969, 0.0851, 0.1319, 0.0697, 0.1112, 0.0065, 0.1489, 0.1748,
        0.0430, 0.1317], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,112][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0140, 0.0575, 0.0572, 0.0319, 0.1319, 0.1351, 0.0108, 0.1398, 0.2287,
        0.0586, 0.0974, 0.0370], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,113][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0015, 0.0772, 0.2244, 0.1095, 0.0544, 0.0963, 0.0050, 0.0775, 0.1055,
        0.0564, 0.1705, 0.0218], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,115][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0129, 0.0460, 0.0441, 0.0469, 0.1493, 0.1696, 0.0231, 0.0563, 0.1622,
        0.0608, 0.1744, 0.0543], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,116][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([1.5983e-04, 3.7559e-06, 1.0735e-03, 9.6574e-04, 7.1961e-02, 6.6213e-01,
        2.5470e-03, 3.3897e-02, 1.2720e-01, 3.4722e-02, 6.5139e-02, 1.9626e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,117][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0015, 0.1166, 0.0581, 0.0819, 0.0352, 0.1044, 0.0071, 0.2053, 0.1891,
        0.0753, 0.0533, 0.0723], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,118][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0164, 0.0761, 0.0634, 0.0282, 0.2893, 0.1042, 0.0252, 0.0907, 0.1142,
        0.1232, 0.0384, 0.0308], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,119][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([3.4081e-05, 9.3725e-02, 1.6046e-01, 1.5480e-01, 5.3029e-02, 1.1976e-01,
        5.4210e-04, 7.7449e-02, 2.2764e-01, 2.0386e-02, 5.6840e-02, 3.5331e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,121][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0014, 0.0213, 0.1017, 0.0163, 0.0958, 0.3593, 0.0064, 0.0761, 0.1367,
        0.1060, 0.0701, 0.0089], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,122][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0068, 0.0339, 0.1484, 0.0592, 0.0586, 0.1232, 0.0199, 0.0870, 0.1441,
        0.1336, 0.1214, 0.0640], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,123][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([1.0160e-05, 5.2199e-02, 1.0844e-01, 4.5198e-01, 3.6302e-02, 6.7724e-02,
        3.0246e-04, 6.3894e-02, 1.0932e-01, 7.7525e-03, 1.5531e-02, 8.6541e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,124][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([7.0305e-06, 9.6433e-06, 1.7917e-03, 1.9369e-03, 4.4813e-02, 6.8752e-01,
        2.5062e-04, 3.5061e-02, 1.5958e-01, 3.5237e-02, 3.3512e-02, 2.7998e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,126][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0013, 0.0774, 0.0615, 0.0489, 0.0618, 0.1065, 0.0108, 0.1155, 0.1928,
        0.0675, 0.1042, 0.1518], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,127][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0115, 0.0534, 0.0572, 0.0389, 0.0722, 0.1428, 0.0109, 0.1099, 0.1669,
        0.0314, 0.1101, 0.0340, 0.1608], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,129][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0009, 0.0961, 0.1436, 0.2686, 0.0298, 0.0799, 0.0052, 0.0588, 0.0709,
        0.0383, 0.0950, 0.0690, 0.0438], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,130][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0112, 0.0302, 0.0439, 0.0761, 0.1195, 0.1365, 0.0186, 0.0513, 0.1471,
        0.0686, 0.1518, 0.0766, 0.0686], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,131][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([6.9935e-02, 7.7756e-05, 2.9536e-03, 1.9756e-03, 9.9726e-02, 3.8022e-01,
        4.0157e-02, 3.2568e-02, 9.6146e-02, 2.7336e-02, 8.5974e-02, 5.3281e-04,
        1.6239e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,133][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0013, 0.1530, 0.0529, 0.1622, 0.0372, 0.0973, 0.0077, 0.0977, 0.1646,
        0.0556, 0.0374, 0.0951, 0.0381], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,134][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0201, 0.0705, 0.0594, 0.0261, 0.1529, 0.0632, 0.0317, 0.0635, 0.0963,
        0.0969, 0.0490, 0.0261, 0.2443], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,135][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([2.1443e-05, 6.7828e-02, 1.4897e-01, 1.9521e-01, 4.7204e-02, 1.7495e-01,
        4.4554e-04, 6.5976e-02, 1.5980e-01, 1.4662e-02, 4.4159e-02, 6.1573e-02,
        1.9192e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,136][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0038, 0.0411, 0.1543, 0.0152, 0.1112, 0.2783, 0.0090, 0.0675, 0.1247,
        0.0486, 0.0496, 0.0069, 0.0898], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,137][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0142, 0.0634, 0.0651, 0.0596, 0.0763, 0.0812, 0.0314, 0.0619, 0.1200,
        0.1040, 0.1164, 0.0839, 0.1225], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,137][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([4.8496e-06, 3.7996e-02, 9.0732e-02, 2.4740e-01, 8.4244e-02, 1.1682e-01,
        1.9656e-04, 1.1694e-01, 1.2533e-01, 1.5656e-02, 3.4719e-02, 1.0483e-01,
        2.5126e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,138][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([1.3958e-03, 1.8827e-04, 1.0832e-02, 3.0206e-03, 8.9043e-02, 5.0802e-01,
        2.5332e-03, 4.3556e-02, 1.5553e-01, 1.6336e-02, 5.3129e-02, 3.3594e-04,
        1.1608e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,138][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0006, 0.0929, 0.0810, 0.0807, 0.0682, 0.1080, 0.0069, 0.1311, 0.1336,
        0.0472, 0.0869, 0.0907, 0.0722], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,138][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0069, 0.0422, 0.0426, 0.0225, 0.0531, 0.0915, 0.0065, 0.0756, 0.1008,
        0.0315, 0.0998, 0.0212, 0.0918, 0.3141], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,139][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0008, 0.0502, 0.1237, 0.1487, 0.0257, 0.0666, 0.0037, 0.0603, 0.0732,
        0.0434, 0.1380, 0.0388, 0.0377, 0.1892], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,139][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0161, 0.0254, 0.0343, 0.0547, 0.0687, 0.0828, 0.0243, 0.0540, 0.0816,
        0.0373, 0.1500, 0.0652, 0.0442, 0.2615], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,140][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([5.1843e-02, 1.7338e-05, 7.0709e-04, 1.1142e-03, 4.7981e-02, 2.4972e-01,
        2.3166e-02, 2.8115e-02, 8.1303e-02, 1.6707e-02, 9.8242e-02, 2.2214e-04,
        2.0270e-01, 1.9816e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,141][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0025, 0.0851, 0.0455, 0.0822, 0.0157, 0.0934, 0.0102, 0.1254, 0.1450,
        0.0797, 0.0622, 0.0619, 0.0190, 0.1723], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,143][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0431, 0.0591, 0.0449, 0.0153, 0.1363, 0.0492, 0.0438, 0.0546, 0.0800,
        0.0769, 0.0443, 0.0219, 0.2054, 0.1253], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,144][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([3.4820e-05, 8.3803e-02, 1.0849e-01, 1.8977e-01, 3.6196e-02, 1.3948e-01,
        5.6394e-04, 5.0073e-02, 1.4262e-01, 1.4860e-02, 5.8492e-02, 4.7075e-02,
        1.4675e-02, 1.1387e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,145][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0100, 0.0303, 0.0865, 0.0151, 0.0681, 0.2050, 0.0131, 0.0497, 0.1092,
        0.0689, 0.0709, 0.0081, 0.1153, 0.1498], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,146][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0102, 0.0965, 0.0624, 0.1075, 0.0673, 0.0612, 0.0270, 0.0303, 0.0728,
        0.1029, 0.0832, 0.0987, 0.0931, 0.0870], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,147][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.8697e-06, 4.0328e-02, 7.9611e-02, 2.6144e-01, 7.5711e-02, 7.2659e-02,
        2.4438e-04, 5.8665e-02, 1.0807e-01, 1.0784e-02, 3.3478e-02, 1.1918e-01,
        2.0873e-02, 1.1895e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,148][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.0321e-03, 3.8353e-05, 2.7918e-03, 2.9490e-03, 3.2967e-02, 3.5929e-01,
        1.7474e-03, 2.7469e-02, 1.0720e-01, 1.7573e-02, 6.1112e-02, 3.6165e-04,
        1.3256e-01, 2.5291e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,150][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0012, 0.0631, 0.0450, 0.0577, 0.0483, 0.0631, 0.0107, 0.1070, 0.1169,
        0.0339, 0.0995, 0.1053, 0.0484, 0.1998], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,151][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0101, 0.0876, 0.0459, 0.0347, 0.0825, 0.0839, 0.0082, 0.0789, 0.0732,
        0.0288, 0.0538, 0.0193, 0.0771, 0.2035, 0.1124], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,153][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0018, 0.0671, 0.1166, 0.0996, 0.0290, 0.0668, 0.0073, 0.0680, 0.0831,
        0.0474, 0.1098, 0.0305, 0.0400, 0.1310, 0.1020], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,154][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0116, 0.0459, 0.0289, 0.0382, 0.0892, 0.0964, 0.0193, 0.0458, 0.0906,
        0.0364, 0.1091, 0.0406, 0.0483, 0.1927, 0.1068], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,155][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([1.1258e-02, 4.3837e-05, 1.5144e-03, 1.5471e-03, 4.6594e-02, 2.4718e-01,
        1.3127e-02, 3.8284e-02, 8.9859e-02, 2.2943e-02, 9.7589e-02, 3.6354e-04,
        1.8343e-01, 2.2157e-01, 2.4693e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,157][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0019, 0.1016, 0.0408, 0.0665, 0.0398, 0.1088, 0.0096, 0.1162, 0.1409,
        0.0837, 0.0364, 0.0531, 0.0281, 0.1044, 0.0683], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,158][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0189, 0.0586, 0.0573, 0.0149, 0.1090, 0.0528, 0.0283, 0.0615, 0.0809,
        0.0766, 0.0421, 0.0189, 0.1886, 0.1162, 0.0753], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,159][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([4.5192e-05, 1.6226e-01, 1.1414e-01, 1.6982e-01, 3.5472e-02, 1.2064e-01,
        6.1911e-04, 4.8842e-02, 1.2936e-01, 1.4713e-02, 3.2039e-02, 3.3642e-02,
        1.2587e-02, 8.9735e-02, 3.6095e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,160][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0040, 0.0271, 0.0892, 0.0180, 0.0954, 0.2141, 0.0086, 0.0571, 0.0939,
        0.0578, 0.0485, 0.0078, 0.0827, 0.1449, 0.0508], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,162][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0073, 0.0869, 0.0520, 0.0866, 0.0567, 0.0649, 0.0216, 0.0342, 0.0930,
        0.0893, 0.0741, 0.0883, 0.0772, 0.0843, 0.0836], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,163][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([8.8926e-06, 4.8429e-02, 6.4441e-02, 2.1084e-01, 8.3023e-02, 9.9124e-02,
        2.7451e-04, 6.7723e-02, 1.1498e-01, 1.2424e-02, 2.7399e-02, 1.1499e-01,
        1.8664e-02, 9.8181e-02, 3.9494e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,164][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([4.3171e-04, 1.0388e-04, 5.2592e-03, 5.0911e-03, 4.8297e-02, 3.5385e-01,
        1.4909e-03, 2.6166e-02, 1.0101e-01, 2.0452e-02, 5.8735e-02, 6.1006e-04,
        1.2024e-01, 2.2618e-01, 3.2079e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,165][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0015, 0.0477, 0.0379, 0.0404, 0.0333, 0.0588, 0.0099, 0.0770, 0.1077,
        0.0322, 0.0667, 0.0932, 0.0382, 0.1843, 0.1712], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,167][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0128, 0.0440, 0.0288, 0.0131, 0.0544, 0.0628, 0.0072, 0.0883, 0.0806,
        0.0190, 0.0646, 0.0083, 0.1045, 0.2180, 0.1070, 0.0866],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,168][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0017, 0.0460, 0.0854, 0.1110, 0.0247, 0.0677, 0.0047, 0.0645, 0.0754,
        0.0355, 0.1037, 0.0347, 0.0447, 0.1318, 0.0825, 0.0862],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,170][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0147, 0.0111, 0.0157, 0.0180, 0.0547, 0.0703, 0.0208, 0.0417, 0.0673,
        0.0225, 0.1204, 0.0233, 0.0547, 0.2205, 0.1217, 0.1226],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,171][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([2.7931e-02, 2.2638e-05, 1.1581e-03, 1.0602e-03, 5.3880e-02, 2.9322e-01,
        1.6866e-02, 2.4460e-02, 7.7326e-02, 1.4006e-02, 7.8984e-02, 1.3967e-04,
        1.4731e-01, 1.8649e-01, 2.5779e-02, 5.1362e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,172][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0033, 0.0934, 0.0449, 0.0783, 0.0430, 0.0940, 0.0111, 0.1216, 0.1284,
        0.0549, 0.0310, 0.0485, 0.0249, 0.0895, 0.0688, 0.0644],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,174][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0308, 0.0403, 0.0380, 0.0136, 0.1037, 0.0468, 0.0343, 0.0505, 0.0621,
        0.0534, 0.0352, 0.0154, 0.1365, 0.0914, 0.0527, 0.1954],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,175][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([3.5883e-05, 5.1526e-02, 8.8817e-02, 2.0526e-01, 3.8953e-02, 1.2168e-01,
        5.6523e-04, 6.0291e-02, 1.4691e-01, 1.2766e-02, 4.3020e-02, 3.1197e-02,
        1.7696e-02, 7.7671e-02, 4.1507e-02, 6.2109e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,176][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0103, 0.0155, 0.0468, 0.0089, 0.0636, 0.1848, 0.0100, 0.0348, 0.0877,
        0.0370, 0.0669, 0.0044, 0.1295, 0.1342, 0.0542, 0.1115],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,178][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0108, 0.0798, 0.0759, 0.0833, 0.0574, 0.0518, 0.0306, 0.0252, 0.0607,
        0.0883, 0.0814, 0.0688, 0.0938, 0.0682, 0.0556, 0.0685],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,179][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([8.1709e-06, 2.4853e-02, 8.4110e-02, 3.0344e-01, 5.0942e-02, 9.2606e-02,
        2.7413e-04, 6.2060e-02, 1.1492e-01, 1.0931e-02, 2.3000e-02, 7.0832e-02,
        1.7548e-02, 6.6593e-02, 2.7498e-02, 5.0386e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,180][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([7.7129e-04, 4.0818e-05, 2.5075e-03, 1.6928e-03, 4.0636e-02, 3.8416e-01,
        1.3451e-03, 1.8071e-02, 9.5735e-02, 1.1666e-02, 4.3442e-02, 1.6690e-04,
        8.2400e-02, 1.8526e-01, 3.0409e-02, 1.0169e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,181][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0013, 0.0391, 0.0466, 0.0343, 0.0460, 0.0584, 0.0091, 0.0736, 0.1156,
        0.0345, 0.0731, 0.0668, 0.0302, 0.1426, 0.1055, 0.1233],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,183][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0066, 0.0420, 0.0328, 0.0158, 0.0404, 0.0650, 0.0058, 0.0588, 0.0646,
        0.0225, 0.0617, 0.0129, 0.0633, 0.1971, 0.0667, 0.0759, 0.1682],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,184][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0010, 0.0451, 0.0923, 0.1010, 0.0215, 0.0535, 0.0039, 0.0457, 0.0547,
        0.0293, 0.0966, 0.0256, 0.0270, 0.1416, 0.0719, 0.0857, 0.1037],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,186][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0172, 0.0222, 0.0232, 0.0280, 0.0499, 0.0530, 0.0234, 0.0389, 0.0561,
        0.0278, 0.0986, 0.0345, 0.0339, 0.1711, 0.0808, 0.1152, 0.1262],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,187][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([5.4321e-03, 1.2834e-05, 5.3486e-04, 1.8666e-03, 4.6361e-02, 1.8115e-01,
        9.4553e-03, 2.5602e-02, 7.1925e-02, 2.3006e-02, 1.0447e-01, 4.2363e-04,
        1.6697e-01, 2.1615e-01, 3.3950e-02, 8.7266e-02, 2.5415e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,188][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0021, 0.0685, 0.0310, 0.0559, 0.0133, 0.0705, 0.0084, 0.0988, 0.1059,
        0.0541, 0.0379, 0.0353, 0.0123, 0.0860, 0.0527, 0.0642, 0.2032],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,190][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0259, 0.0421, 0.0347, 0.0107, 0.1115, 0.0369, 0.0301, 0.0410, 0.0558,
        0.0540, 0.0328, 0.0150, 0.1373, 0.0787, 0.0412, 0.1623, 0.0899],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,191][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([4.7827e-05, 8.6230e-02, 8.3860e-02, 1.2934e-01, 2.8523e-02, 1.0356e-01,
        6.3306e-04, 4.3002e-02, 1.2161e-01, 1.2082e-02, 4.2221e-02, 3.3115e-02,
        1.2085e-02, 8.9060e-02, 3.6749e-02, 6.3947e-02, 1.1393e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,192][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0058, 0.0229, 0.0626, 0.0116, 0.0613, 0.1509, 0.0102, 0.0413, 0.0785,
        0.0589, 0.0497, 0.0063, 0.0888, 0.1014, 0.0484, 0.1096, 0.0918],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,194][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0093, 0.0759, 0.0484, 0.0875, 0.0527, 0.0502, 0.0228, 0.0247, 0.0641,
        0.0790, 0.0588, 0.0887, 0.0732, 0.0613, 0.0550, 0.0920, 0.0564],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,194][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([7.5868e-06, 3.8158e-02, 4.9196e-02, 1.8693e-01, 4.8200e-02, 5.3530e-02,
        2.5972e-04, 4.4298e-02, 8.0534e-02, 8.5858e-03, 2.5488e-02, 9.0417e-02,
        1.6074e-02, 9.2304e-02, 3.7732e-02, 6.8150e-02, 1.6014e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,195][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.0948e-04, 2.6730e-05, 2.2450e-03, 3.8381e-03, 2.7711e-02, 2.5093e-01,
        6.4180e-04, 2.2054e-02, 8.4143e-02, 1.9381e-02, 5.5174e-02, 4.7738e-04,
        8.3763e-02, 2.3672e-01, 2.5851e-02, 1.5572e-01, 3.1214e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,195][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0010, 0.0431, 0.0238, 0.0274, 0.0231, 0.0367, 0.0083, 0.0608, 0.0783,
        0.0222, 0.0611, 0.0581, 0.0294, 0.1410, 0.0928, 0.0846, 0.2083],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,197][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:34,198][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[275],
        [  1],
        [  8],
        [  5],
        [210],
        [ 44],
        [ 26],
        [  6],
        [ 15],
        [  7],
        [  3],
        [  9],
        [ 11],
        [  1],
        [ 13],
        [  4],
        [  1]], device='cuda:0')
[2024-07-24 10:21:34,199][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[203],
        [  1],
        [  8],
        [ 13],
        [303],
        [ 36],
        [ 30],
        [  6],
        [  4],
        [  5],
        [  3],
        [ 11],
        [ 19],
        [  1],
        [ 25],
        [  5],
        [  2]], device='cuda:0')
[2024-07-24 10:21:34,201][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[37607],
        [45393],
        [45533],
        [43930],
        [44226],
        [38455],
        [40449],
        [38314],
        [38617],
        [39493],
        [37945],
        [38186],
        [34568],
        [25693],
        [29633],
        [27371],
        [24749]], device='cuda:0')
[2024-07-24 10:21:34,202][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[25280],
        [11574],
        [23018],
        [35243],
        [28106],
        [31957],
        [32275],
        [31166],
        [33756],
        [31058],
        [32927],
        [27832],
        [35231],
        [32779],
        [29743],
        [32816],
        [32569]], device='cuda:0')
[2024-07-24 10:21:34,204][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[42544],
        [50086],
        [44759],
        [48960],
        [44793],
        [37076],
        [38400],
        [35889],
        [36990],
        [34812],
        [38523],
        [36560],
        [39078],
        [39276],
        [40671],
        [39511],
        [39964]], device='cuda:0')
[2024-07-24 10:21:34,205][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[40574],
        [39157],
        [30288],
        [35152],
        [23178],
        [28658],
        [25463],
        [25013],
        [27181],
        [26553],
        [27686],
        [27461],
        [28466],
        [31213],
        [31283],
        [31210],
        [32420]], device='cuda:0')
[2024-07-24 10:21:34,207][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 5996],
        [10654],
        [27482],
        [22096],
        [22919],
        [30776],
        [31912],
        [35264],
        [33260],
        [32637],
        [31356],
        [29979],
        [26975],
        [28286],
        [29154],
        [28305],
        [24894]], device='cuda:0')
[2024-07-24 10:21:34,208][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[41094],
        [ 7014],
        [25152],
        [21222],
        [19635],
        [26423],
        [26368],
        [33245],
        [35941],
        [36859],
        [35895],
        [36040],
        [32518],
        [34032],
        [34482],
        [35853],
        [34839]], device='cuda:0')
[2024-07-24 10:21:34,210][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[41916],
        [50249],
        [48379],
        [46435],
        [47145],
        [45159],
        [43965],
        [46111],
        [45005],
        [43760],
        [45502],
        [44809],
        [43915],
        [44903],
        [47370],
        [42225],
        [44326]], device='cuda:0')
[2024-07-24 10:21:34,211][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[25337],
        [26818],
        [24715],
        [23789],
        [23037],
        [24643],
        [24458],
        [24242],
        [26218],
        [26952],
        [27176],
        [27417],
        [29146],
        [29999],
        [29645],
        [31091],
        [29999]], device='cuda:0')
[2024-07-24 10:21:34,213][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[  445],
        [50221],
        [50096],
        [41670],
        [43675],
        [41464],
        [18506],
        [33729],
        [35109],
        [20104],
        [23357],
        [14878],
        [14793],
        [15034],
        [17831],
        [15873],
        [13586]], device='cuda:0')
[2024-07-24 10:21:34,214][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[13020],
        [50256],
        [ 7717],
        [17135],
        [14802],
        [ 9560],
        [ 6353],
        [11205],
        [10014],
        [ 9749],
        [11164],
        [11941],
        [ 7940],
        [ 7134],
        [ 8599],
        [ 8138],
        [ 8609]], device='cuda:0')
[2024-07-24 10:21:34,216][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[20140],
        [23993],
        [20335],
        [36178],
        [28429],
        [38725],
        [37728],
        [38746],
        [38651],
        [39011],
        [37536],
        [38805],
        [37161],
        [35705],
        [35984],
        [35849],
        [34559]], device='cuda:0')
[2024-07-24 10:21:34,217][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[35147],
        [50075],
        [43175],
        [45182],
        [41983],
        [40967],
        [38641],
        [44351],
        [39378],
        [36013],
        [40631],
        [39520],
        [40140],
        [40041],
        [35819],
        [36544],
        [38772]], device='cuda:0')
[2024-07-24 10:21:34,218][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[21031],
        [ 1625],
        [13052],
        [ 1935],
        [ 3763],
        [13442],
        [ 7269],
        [ 6548],
        [14408],
        [ 9125],
        [12465],
        [ 4646],
        [ 7953],
        [ 7740],
        [ 4291],
        [11302],
        [ 8457]], device='cuda:0')
[2024-07-24 10:21:34,220][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[18344],
        [24017],
        [26891],
        [27836],
        [24134],
        [21415],
        [22750],
        [15820],
        [19112],
        [21261],
        [25931],
        [26355],
        [34183],
        [38532],
        [36158],
        [39135],
        [40449]], device='cuda:0')
[2024-07-24 10:21:34,222][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[39260],
        [ 2301],
        [24343],
        [30559],
        [27415],
        [29164],
        [28180],
        [26329],
        [28183],
        [28055],
        [28007],
        [24707],
        [27102],
        [19470],
        [14891],
        [14043],
        [12435]], device='cuda:0')
[2024-07-24 10:21:34,223][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[18420],
        [16931],
        [23764],
        [24737],
        [21830],
        [24172],
        [24819],
        [31638],
        [33301],
        [34323],
        [33305],
        [32789],
        [34297],
        [33774],
        [32740],
        [32103],
        [32236]], device='cuda:0')
[2024-07-24 10:21:34,224][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 2471],
        [ 2515],
        [ 8054],
        [ 5065],
        [21220],
        [21087],
        [27751],
        [28137],
        [24427],
        [26182],
        [24142],
        [25572],
        [22048],
        [19608],
        [20129],
        [20688],
        [19777]], device='cuda:0')
[2024-07-24 10:21:34,226][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[10960],
        [  739],
        [ 1090],
        [  969],
        [  698],
        [  385],
        [  392],
        [  471],
        [  497],
        [  426],
        [  521],
        [  577],
        [  506],
        [  711],
        [  636],
        [  612],
        [  784]], device='cuda:0')
[2024-07-24 10:21:34,227][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[34424],
        [22136],
        [ 8669],
        [ 3168],
        [ 3136],
        [ 2555],
        [ 2278],
        [ 2034],
        [ 1877],
        [ 1726],
        [ 1750],
        [ 1551],
        [ 2542],
        [ 2398],
        [ 1874],
        [ 2301],
        [ 2793]], device='cuda:0')
[2024-07-24 10:21:34,229][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 6949],
        [31330],
        [40232],
        [37829],
        [38444],
        [40055],
        [40067],
        [41085],
        [40424],
        [40459],
        [39572],
        [40239],
        [39685],
        [39834],
        [39985],
        [39647],
        [40431]], device='cuda:0')
[2024-07-24 10:21:34,230][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[15087],
        [ 2100],
        [28921],
        [30499],
        [25432],
        [20727],
        [22346],
        [19037],
        [14785],
        [13344],
        [13846],
        [13359],
        [13013],
        [11672],
        [12011],
        [ 9052],
        [ 8562]], device='cuda:0')
[2024-07-24 10:21:34,232][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[19406],
        [17072],
        [ 9733],
        [11998],
        [17575],
        [25723],
        [26562],
        [29730],
        [34236],
        [36345],
        [36325],
        [39903],
        [41406],
        [39350],
        [41581],
        [40352],
        [40661]], device='cuda:0')
[2024-07-24 10:21:34,233][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[44442],
        [11916],
        [38052],
        [41891],
        [40938],
        [36367],
        [36385],
        [37369],
        [40369],
        [40248],
        [41610],
        [42791],
        [41490],
        [44981],
        [44762],
        [43732],
        [47092]], device='cuda:0')
[2024-07-24 10:21:34,235][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 9373],
        [16988],
        [ 4093],
        [ 4051],
        [ 2718],
        [ 8719],
        [ 7139],
        [ 8599],
        [ 8531],
        [ 8954],
        [ 8021],
        [ 8971],
        [ 5066],
        [ 4329],
        [ 4157],
        [ 4555],
        [ 3815]], device='cuda:0')
[2024-07-24 10:21:34,236][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[17590],
        [ 1874],
        [ 1858],
        [ 1700],
        [ 2506],
        [ 3304],
        [ 3660],
        [ 3336],
        [ 4459],
        [ 4977],
        [ 3897],
        [ 4426],
        [ 3975],
        [ 4768],
        [ 6380],
        [ 5510],
        [ 6127]], device='cuda:0')
[2024-07-24 10:21:34,238][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[34881],
        [48053],
        [39694],
        [42679],
        [43786],
        [41366],
        [40217],
        [39100],
        [35761],
        [33658],
        [33309],
        [32818],
        [30214],
        [31088],
        [32414],
        [32904],
        [30306]], device='cuda:0')
[2024-07-24 10:21:34,239][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[24206],
        [44948],
        [41755],
        [42827],
        [45018],
        [41322],
        [47213],
        [42947],
        [44149],
        [37908],
        [42163],
        [35801],
        [33197],
        [42066],
        [41904],
        [40782],
        [42472]], device='cuda:0')
[2024-07-24 10:21:34,240][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552],
        [2552]], device='cuda:0')
[2024-07-24 10:21:34,313][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:34,313][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,314][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,314][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,314][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,315][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,315][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,316][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,317][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,319][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,320][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,321][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,322][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,323][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.0019, 0.9981], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,324][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([7.7139e-05, 9.9992e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,325][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.0426, 0.9574], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,327][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.0093, 0.9907], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,328][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.9433, 0.0567], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,329][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.1184, 0.8816], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,330][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([5.5239e-04, 9.9945e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,332][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.0847, 0.9153], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,333][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.5252, 0.4748], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,335][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.0277, 0.9723], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,335][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([2.5901e-04, 9.9974e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,337][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.0048, 0.9952], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,338][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.0770e-05, 3.6715e-01, 6.3276e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,339][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ and] are: tensor([2.0467e-06, 2.8636e-01, 7.1364e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,340][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0020, 0.4849, 0.5131], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,342][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0034, 0.6668, 0.3298], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,343][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.6845, 0.0523, 0.2632], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,344][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0697, 0.0102, 0.9201], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,345][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ and] are: tensor([2.1635e-05, 1.9789e-01, 8.0209e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,347][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0297, 0.5970, 0.3734], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,348][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0671, 0.4853, 0.4476], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,349][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ and] are: tensor([1.3130e-03, 3.8095e-04, 9.9831e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,350][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ and] are: tensor([3.3994e-05, 9.9870e-01, 1.2644e-03], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,351][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ and] are: tensor([1.9601e-04, 3.7977e-01, 6.2004e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,352][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([2.9873e-05, 3.9734e-01, 3.7589e-01, 2.2674e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,352][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([3.1924e-06, 5.7386e-01, 3.6251e-01, 6.3625e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,354][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0016, 0.4706, 0.2819, 0.2458], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,355][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0020, 0.6925, 0.2902, 0.0153], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,357][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.7378, 0.0290, 0.1246, 0.1086], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,358][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0726, 0.0018, 0.5459, 0.3797], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,359][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([1.1130e-05, 2.5263e-01, 6.4961e-01, 9.7743e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,360][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0172, 0.4348, 0.1479, 0.4001], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,362][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.1134, 0.5192, 0.2331, 0.1342], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,363][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([3.4886e-03, 1.1414e-04, 6.8296e-01, 3.1344e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,363][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([4.0930e-05, 9.9939e-01, 3.7788e-04, 1.8800e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,364][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([1.8626e-04, 3.4140e-01, 1.6102e-01, 4.9739e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,365][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ got] are: tensor([6.6245e-05, 3.3618e-01, 2.2712e-01, 1.5351e-01, 2.8314e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,366][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ got] are: tensor([1.6862e-06, 5.2341e-01, 3.3548e-01, 7.1554e-02, 6.9551e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,368][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0005, 0.4307, 0.1606, 0.1927, 0.2155], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,369][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0019, 0.6329, 0.2809, 0.0162, 0.0680], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,369][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.4423, 0.0367, 0.0883, 0.0914, 0.3414], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,370][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ got] are: tensor([1.7254e-02, 8.6349e-05, 9.3661e-03, 1.5636e-03, 9.7173e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,370][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ got] are: tensor([8.2612e-06, 2.6919e-01, 4.0435e-01, 8.5421e-02, 2.4104e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,370][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0029, 0.5093, 0.0490, 0.1852, 0.2537], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,371][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.0129, 0.1817, 0.3381, 0.1442, 0.3230], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,371][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ got] are: tensor([1.2385e-03, 2.9367e-06, 6.8499e-03, 9.1217e-04, 9.9100e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,371][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ got] are: tensor([9.6099e-06, 9.9930e-01, 3.7556e-04, 1.7246e-04, 1.4164e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,372][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ got] are: tensor([7.5789e-05, 2.7238e-01, 1.9862e-01, 4.0879e-01, 1.2013e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,372][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([5.4280e-05, 1.7745e-01, 1.7200e-01, 1.5526e-01, 3.2242e-01, 1.7282e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,373][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([4.3734e-06, 2.9296e-01, 3.1300e-01, 1.4135e-01, 1.1827e-01, 1.3441e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,374][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0005, 0.2194, 0.1209, 0.1940, 0.3293, 0.1361], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,376][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0023, 0.6557, 0.1486, 0.0166, 0.0725, 0.1044], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,377][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1969, 0.0223, 0.0620, 0.0709, 0.1918, 0.4562], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,378][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([9.6855e-03, 7.6280e-06, 1.0655e-03, 1.7252e-04, 1.2470e-01, 8.6437e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,379][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([8.5204e-06, 1.2610e-01, 2.1529e-01, 6.8162e-02, 2.4917e-01, 3.4128e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,380][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0030, 0.3668, 0.0912, 0.2436, 0.2173, 0.0782], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,382][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0151, 0.1077, 0.1167, 0.0876, 0.2281, 0.4449], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,383][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([8.1433e-04, 1.7292e-07, 3.8188e-04, 3.0463e-05, 4.3369e-02, 9.5540e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,384][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([3.1231e-05, 9.9689e-01, 6.7163e-04, 6.0303e-04, 5.8655e-04, 1.2174e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,385][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([2.3854e-04, 1.0580e-01, 1.2164e-01, 3.6432e-01, 1.9421e-01, 2.1379e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,385][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([7.7025e-05, 1.1666e-01, 1.7188e-01, 1.8976e-01, 3.4234e-01, 1.7787e-01,
        1.4215e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,386][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([2.7819e-06, 2.0700e-01, 3.5922e-01, 1.2341e-01, 1.2537e-01, 1.8495e-01,
        5.6420e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,388][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0009, 0.2558, 0.1531, 0.1663, 0.2486, 0.1696, 0.0057],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,389][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0034, 0.3541, 0.3191, 0.0422, 0.1230, 0.1487, 0.0095],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,390][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.1020, 0.0484, 0.0716, 0.0865, 0.2026, 0.3386, 0.1502],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,391][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([1.9554e-03, 3.6419e-04, 1.0453e-02, 7.0728e-04, 1.5154e-01, 8.2981e-01,
        5.1662e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,392][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([2.1799e-05, 7.9906e-02, 2.1878e-01, 1.5151e-01, 2.7869e-01, 2.7052e-01,
        5.6034e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,394][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0041, 0.5489, 0.0517, 0.2305, 0.0969, 0.0453, 0.0226],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,395][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0037, 0.1289, 0.2381, 0.1249, 0.1397, 0.3539, 0.0108],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,396][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([5.6203e-05, 1.9659e-05, 7.4775e-03, 9.7902e-05, 1.1139e-01, 8.8041e-01,
        5.4460e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,397][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([5.5246e-05, 9.9380e-01, 1.3798e-03, 1.1328e-03, 1.0780e-03, 2.5191e-03,
        3.2836e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,399][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0005, 0.3427, 0.2053, 0.1968, 0.1110, 0.1418, 0.0019],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,399][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ at] are: tensor([1.1179e-05, 1.4728e-01, 1.7617e-01, 1.8478e-01, 2.2463e-01, 2.3234e-01,
        5.6281e-04, 3.4227e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,400][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ at] are: tensor([1.8790e-06, 4.3466e-01, 2.7392e-01, 7.1925e-02, 4.1709e-02, 1.4226e-01,
        3.9822e-05, 3.5480e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,402][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0008, 0.4014, 0.1213, 0.1392, 0.1422, 0.1063, 0.0044, 0.0844],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,403][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0013, 0.6711, 0.1556, 0.0160, 0.0433, 0.0923, 0.0050, 0.0155],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,405][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0648, 0.0316, 0.0447, 0.0777, 0.1276, 0.4931, 0.0760, 0.0845],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,406][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ at] are: tensor([1.1834e-03, 1.2209e-05, 1.0517e-03, 2.4402e-04, 1.3553e-01, 8.4864e-01,
        2.1911e-03, 1.1148e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,406][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ at] are: tensor([1.8062e-06, 1.4619e-01, 2.4462e-01, 7.7102e-02, 1.2269e-01, 3.4181e-01,
        1.6880e-04, 6.7421e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,408][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0009, 0.6237, 0.0512, 0.1006, 0.1527, 0.0486, 0.0052, 0.0171],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,409][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0348, 0.1294, 0.1082, 0.0803, 0.1452, 0.3790, 0.0266, 0.0965],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,410][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ at] are: tensor([1.9869e-05, 1.3374e-07, 3.6722e-04, 2.8924e-05, 3.8263e-02, 9.5544e-01,
        1.9491e-04, 5.6825e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,411][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ at] are: tensor([1.9819e-05, 9.9603e-01, 8.5100e-04, 5.0811e-04, 3.0846e-04, 2.1821e-03,
        1.4558e-05, 9.0454e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,412][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0003, 0.2689, 0.1962, 0.2114, 0.0633, 0.1423, 0.0025, 0.1152],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,413][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([1.9151e-05, 1.6776e-01, 1.2243e-01, 1.7772e-01, 1.9441e-01, 1.3686e-01,
        5.9788e-04, 2.8153e-02, 1.7205e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,414][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([3.1045e-06, 4.6716e-01, 1.9541e-01, 8.3375e-02, 4.6541e-02, 7.9044e-02,
        4.3154e-05, 2.4833e-02, 1.0359e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,416][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0005, 0.3021, 0.0908, 0.1528, 0.1375, 0.0837, 0.0045, 0.0671, 0.1610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,417][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0015, 0.6797, 0.0842, 0.0196, 0.0388, 0.0719, 0.0038, 0.0158, 0.0847],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,419][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0648, 0.0118, 0.0216, 0.0483, 0.0833, 0.2534, 0.0518, 0.0639, 0.4012],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,420][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.5304e-03, 6.5584e-06, 4.8336e-04, 2.0936e-04, 1.1121e-01, 6.8785e-01,
        4.6857e-03, 1.1590e-02, 1.7444e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,421][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([2.4658e-06, 1.1194e-01, 1.2355e-01, 6.2041e-02, 1.2889e-01, 2.6980e-01,
        1.8365e-04, 5.7349e-02, 2.4624e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,422][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0017, 0.4470, 0.0489, 0.1366, 0.1907, 0.0593, 0.0081, 0.0176, 0.0902],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,423][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0142, 0.0777, 0.0965, 0.0580, 0.0816, 0.2664, 0.0146, 0.0719, 0.3191],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,424][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.5006e-04, 9.2509e-08, 2.3540e-04, 6.8855e-05, 3.3250e-02, 8.8402e-01,
        7.2735e-04, 9.9589e-03, 7.1593e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,425][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([9.9434e-06, 9.9887e-01, 2.4714e-04, 2.8057e-04, 1.3069e-04, 4.0485e-04,
        5.7216e-06, 1.6511e-05, 3.8667e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,426][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([8.3077e-05, 1.2380e-01, 1.0603e-01, 2.3040e-01, 7.6221e-02, 1.1817e-01,
        9.0263e-04, 1.3353e-01, 2.1086e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,427][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ station] are: tensor([2.1528e-05, 1.3062e-01, 1.6521e-01, 1.4038e-01, 2.4083e-01, 1.4818e-01,
        5.3141e-04, 2.4586e-02, 1.3064e-01, 1.8993e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,427][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ station] are: tensor([1.6168e-06, 2.4794e-01, 2.3377e-01, 1.1943e-01, 7.1585e-02, 1.4799e-01,
        3.7101e-05, 3.1653e-02, 1.3576e-01, 1.1828e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,427][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ station] are: tensor([1.9827e-04, 1.7406e-01, 1.0381e-01, 1.1808e-01, 1.1581e-01, 1.3767e-01,
        2.5752e-03, 8.5173e-02, 2.1345e-01, 4.9177e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,428][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0016, 0.4334, 0.1640, 0.0264, 0.0656, 0.1425, 0.0033, 0.0148, 0.1192,
        0.0292], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,428][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0227, 0.0068, 0.0263, 0.0480, 0.1325, 0.2477, 0.0375, 0.0638, 0.3389,
        0.0758], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,429][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ station] are: tensor([3.3575e-04, 1.7583e-06, 4.0154e-04, 3.8998e-05, 7.9914e-02, 7.9832e-01,
        4.7082e-04, 5.6205e-03, 1.1113e-01, 3.7633e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,429][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ station] are: tensor([5.8621e-06, 6.0780e-02, 1.7523e-01, 1.0515e-01, 1.8716e-01, 2.9254e-01,
        2.0333e-04, 4.1993e-02, 1.2857e-01, 8.3773e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,429][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0010, 0.4223, 0.0507, 0.1818, 0.1526, 0.0535, 0.0081, 0.0137, 0.0987,
        0.0177], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,431][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.0198, 0.1080, 0.0993, 0.0782, 0.0749, 0.2124, 0.0199, 0.0915, 0.2610,
        0.0349], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,431][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ station] are: tensor([3.9271e-06, 4.8495e-08, 1.9901e-04, 5.6463e-06, 3.5545e-02, 9.1971e-01,
        7.3722e-05, 3.9712e-03, 3.7631e-02, 2.8613e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,432][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ station] are: tensor([2.1660e-05, 9.9572e-01, 7.5001e-04, 1.1254e-03, 5.7247e-04, 1.5923e-03,
        1.2042e-05, 5.4327e-05, 1.4156e-04, 8.8038e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,434][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ station] are: tensor([0.0002, 0.1347, 0.1002, 0.1667, 0.0770, 0.1218, 0.0012, 0.0917, 0.1305,
        0.1760], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,435][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [,] are: tensor([7.9554e-06, 2.5866e-01, 1.1968e-01, 2.7506e-01, 1.1969e-01, 8.2507e-02,
        2.8839e-04, 1.4556e-02, 8.9989e-02, 2.3151e-02, 1.6411e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,435][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [,] are: tensor([4.9668e-07, 4.1698e-01, 2.3949e-01, 1.3317e-01, 4.9607e-02, 6.5444e-02,
        1.3281e-05, 1.5096e-02, 6.9247e-02, 6.0738e-03, 4.8758e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,436][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [,] are: tensor([1.1291e-04, 3.1787e-01, 9.5127e-02, 2.0250e-01, 7.9437e-02, 4.3462e-02,
        1.2938e-03, 4.5169e-02, 1.1866e-01, 3.6583e-02, 5.9785e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,437][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [,] are: tensor([5.2000e-04, 6.6652e-01, 1.1886e-01, 1.9674e-02, 3.6383e-02, 5.2524e-02,
        1.6728e-03, 8.2418e-03, 5.5278e-02, 2.7113e-02, 1.3219e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,438][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0201, 0.0119, 0.0227, 0.0712, 0.0722, 0.2062, 0.0250, 0.0500, 0.2920,
        0.0737, 0.1550], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,439][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [,] are: tensor([1.9856e-03, 8.9128e-06, 5.5531e-04, 5.3548e-04, 1.3978e-01, 6.2215e-01,
        3.0554e-03, 1.5388e-02, 1.3807e-01, 1.1003e-02, 6.7470e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,440][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [,] are: tensor([1.3668e-06, 1.7697e-01, 1.7450e-01, 1.0428e-01, 9.0561e-02, 1.8992e-01,
        1.3466e-04, 4.8156e-02, 1.7610e-01, 1.2024e-02, 2.7365e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,442][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0008, 0.5688, 0.0363, 0.1953, 0.0915, 0.0298, 0.0033, 0.0088, 0.0498,
        0.0043, 0.0113], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,443][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0048, 0.1338, 0.0938, 0.1431, 0.0722, 0.1712, 0.0069, 0.0296, 0.2241,
        0.0187, 0.1019], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,444][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [,] are: tensor([3.5084e-05, 1.3030e-07, 2.1602e-04, 1.7746e-04, 5.2685e-02, 8.2435e-01,
        5.9564e-04, 1.1522e-02, 4.8517e-02, 1.0449e-02, 5.1450e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,445][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [,] are: tensor([1.6963e-06, 9.9855e-01, 3.0875e-04, 6.6423e-04, 9.9187e-05, 3.2596e-04,
        1.9057e-06, 1.1238e-05, 2.8541e-05, 3.1507e-06, 1.5844e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,446][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [,] are: tensor([1.0941e-05, 1.7339e-01, 1.9583e-01, 3.1565e-01, 2.0544e-02, 5.5167e-02,
        1.5507e-04, 2.6059e-02, 6.8367e-02, 1.2417e-01, 2.0653e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,447][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([1.3347e-05, 2.5451e-01, 1.4618e-01, 9.8555e-02, 1.6354e-01, 8.7379e-02,
        3.9265e-04, 2.4031e-02, 9.3973e-02, 3.6898e-02, 2.1497e-02, 7.3021e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,448][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([8.1204e-07, 4.7941e-01, 2.8714e-01, 6.7251e-02, 2.9502e-02, 4.1447e-02,
        1.1164e-05, 1.2702e-02, 6.6724e-02, 8.3970e-03, 3.6926e-03, 3.7232e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,449][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([1.6370e-04, 2.6018e-01, 9.7041e-02, 9.3949e-02, 7.3452e-02, 6.1934e-02,
        1.2240e-03, 6.6542e-02, 1.7108e-01, 5.2693e-02, 7.5553e-02, 4.6183e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,450][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0010, 0.3731, 0.2563, 0.0104, 0.0752, 0.0896, 0.0020, 0.0099, 0.1119,
        0.0493, 0.0173, 0.0039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,452][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0360, 0.0022, 0.0102, 0.0173, 0.0839, 0.1589, 0.0411, 0.0679, 0.2355,
        0.1137, 0.1077, 0.1255], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,453][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([2.0609e-06, 5.4269e-07, 1.8648e-04, 1.2080e-04, 6.5154e-02, 7.6207e-01,
        1.9708e-04, 9.8277e-03, 1.0351e-01, 3.0561e-02, 2.8353e-02, 1.8114e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,454][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([1.5756e-06, 1.0354e-01, 2.4293e-01, 5.2469e-02, 9.7084e-02, 1.6817e-01,
        1.1587e-04, 5.2251e-02, 2.2874e-01, 1.3454e-02, 2.4742e-02, 1.6500e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,455][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0010, 0.5446, 0.0418, 0.1473, 0.0669, 0.0517, 0.0055, 0.0153, 0.0769,
        0.0065, 0.0134, 0.0291], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,456][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.0110, 0.1059, 0.0781, 0.0986, 0.0481, 0.1097, 0.0100, 0.0269, 0.2587,
        0.0530, 0.1132, 0.0868], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,457][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([1.1958e-08, 1.3716e-08, 8.0897e-05, 3.2490e-05, 3.1081e-02, 8.6260e-01,
        2.8432e-05, 7.7530e-03, 4.6004e-02, 3.9080e-02, 1.3334e-02, 4.4375e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,458][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([1.3749e-05, 9.9878e-01, 3.6679e-04, 2.3485e-04, 2.0274e-04, 3.4046e-04,
        4.8859e-06, 1.8087e-05, 3.3135e-05, 5.2717e-06, 9.0414e-07, 1.1050e-07],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,459][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([2.2621e-05, 2.6925e-01, 1.4611e-01, 2.5120e-01, 1.7745e-02, 4.5014e-02,
        1.1892e-04, 4.8144e-02, 9.1953e-02, 7.6501e-02, 1.0207e-02, 4.3734e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,460][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([6.8501e-05, 2.3235e-01, 1.0837e-01, 1.7940e-01, 1.5566e-01, 7.9117e-02,
        5.8466e-04, 1.4619e-02, 7.9198e-02, 2.6868e-02, 2.2360e-02, 6.5196e-02,
        3.6222e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,461][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([2.2195e-06, 3.6071e-01, 2.2805e-01, 7.6688e-02, 6.9064e-02, 7.7828e-02,
        4.4582e-05, 3.5940e-02, 9.2122e-02, 1.5184e-02, 9.4482e-03, 9.0136e-03,
        2.5905e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,463][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0003, 0.2543, 0.1259, 0.1561, 0.0615, 0.0542, 0.0027, 0.0501, 0.1151,
        0.0362, 0.0587, 0.0534, 0.0316], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,464][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0023, 0.4925, 0.1950, 0.0302, 0.0432, 0.0748, 0.0038, 0.0105, 0.0543,
        0.0333, 0.0224, 0.0074, 0.0302], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,465][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0193, 0.0029, 0.0128, 0.0276, 0.0589, 0.1239, 0.0212, 0.0525, 0.1686,
        0.0677, 0.1367, 0.1180, 0.1899], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,466][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([3.8823e-03, 1.3996e-05, 8.5766e-04, 3.9947e-04, 8.6244e-02, 6.3322e-01,
        3.3429e-03, 1.2060e-02, 1.4912e-01, 6.9914e-03, 4.3802e-02, 2.3869e-05,
        6.0043e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,467][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([9.8464e-06, 1.1986e-01, 1.8151e-01, 9.6176e-02, 1.0284e-01, 2.0711e-01,
        2.4751e-04, 4.0444e-02, 1.7114e-01, 1.6503e-02, 2.3727e-02, 3.1548e-02,
        8.8882e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,469][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0027, 0.5079, 0.0346, 0.1901, 0.0868, 0.0324, 0.0074, 0.0059, 0.0606,
        0.0067, 0.0157, 0.0181, 0.0312], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,470][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.0037, 0.0626, 0.1376, 0.0623, 0.0506, 0.1716, 0.0063, 0.0419, 0.2080,
        0.0227, 0.1381, 0.0381, 0.0566], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,471][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([8.8727e-05, 4.3209e-07, 2.1950e-04, 4.5737e-05, 2.1545e-02, 7.7736e-01,
        4.7523e-04, 6.2044e-03, 5.5996e-02, 5.9796e-03, 1.9220e-02, 3.5753e-06,
        1.1286e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,472][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([1.6174e-05, 9.9706e-01, 5.8892e-04, 5.3737e-04, 3.2517e-04, 1.2818e-03,
        1.2553e-05, 3.9322e-05, 1.0573e-04, 1.6565e-05, 7.2709e-06, 7.6507e-07,
        9.7218e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,473][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([1.5552e-04, 2.9135e-01, 1.3209e-01, 1.9511e-01, 2.0631e-02, 7.7239e-02,
        8.5924e-04, 4.5864e-02, 7.5379e-02, 7.9978e-02, 1.8937e-02, 4.6085e-02,
        1.6318e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,474][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([2.6429e-05, 2.0456e-01, 8.0495e-02, 1.8154e-01, 1.0974e-01, 7.4332e-02,
        3.7639e-04, 1.4106e-02, 7.7758e-02, 2.5390e-02, 1.9578e-02, 1.1278e-01,
        2.0805e-02, 7.8511e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,475][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([1.6647e-06, 3.7605e-01, 2.1352e-01, 8.0576e-02, 4.2973e-02, 7.2035e-02,
        2.9803e-05, 2.0946e-02, 7.6702e-02, 9.8163e-03, 9.2937e-03, 7.6140e-03,
        9.2413e-03, 8.1199e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,476][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([2.1345e-04, 2.1967e-01, 6.4840e-02, 1.1256e-01, 5.3526e-02, 4.4122e-02,
        2.2659e-03, 5.1739e-02, 1.2278e-01, 3.7973e-02, 7.3680e-02, 6.8744e-02,
        1.4089e-02, 1.3380e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,478][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0012, 0.5728, 0.1063, 0.0204, 0.0376, 0.0563, 0.0031, 0.0108, 0.0636,
        0.0385, 0.0207, 0.0108, 0.0287, 0.0294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,479][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0382, 0.0035, 0.0104, 0.0296, 0.0334, 0.1049, 0.0283, 0.0351, 0.1724,
        0.0545, 0.1344, 0.1208, 0.1053, 0.1292], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,480][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.4564e-03, 4.4127e-06, 3.4714e-04, 5.8731e-04, 7.4072e-02, 4.6494e-01,
        1.8023e-03, 8.1154e-03, 1.1941e-01, 5.0585e-03, 6.0276e-02, 2.5796e-05,
        9.9517e-02, 1.6439e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,481][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([2.9640e-06, 1.0240e-01, 1.5398e-01, 7.0679e-02, 7.6698e-02, 1.9079e-01,
        1.5352e-04, 4.6898e-02, 1.6127e-01, 9.2802e-03, 2.5153e-02, 2.4025e-02,
        4.5863e-03, 1.3409e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,482][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0015, 0.4365, 0.0309, 0.2173, 0.0943, 0.0366, 0.0068, 0.0094, 0.0553,
        0.0071, 0.0153, 0.0210, 0.0093, 0.0588], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,484][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0098, 0.0711, 0.0656, 0.0736, 0.0580, 0.1321, 0.0081, 0.0274, 0.1801,
        0.0160, 0.1085, 0.0484, 0.0501, 0.1512], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,484][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([3.2907e-05, 1.4832e-07, 1.3236e-04, 1.8807e-04, 1.2088e-02, 6.6846e-01,
        4.3466e-04, 7.0771e-03, 5.0335e-02, 7.3240e-03, 3.9918e-02, 8.7749e-06,
        1.3146e-01, 8.2535e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,485][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.1473e-06, 9.9867e-01, 2.8505e-04, 4.1132e-04, 8.9444e-05, 4.6061e-04,
        4.8193e-06, 1.4042e-05, 3.8808e-05, 4.9244e-06, 2.6148e-06, 2.4064e-07,
        1.7005e-06, 3.1375e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,485][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.1558e-04, 7.4109e-02, 1.0238e-01, 1.9344e-01, 1.7289e-02, 5.9737e-02,
        7.2370e-04, 4.6024e-02, 7.0914e-02, 1.7200e-01, 3.6726e-02, 8.7865e-02,
        1.3049e-02, 1.2564e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,486][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ give] are: tensor([1.5903e-05, 2.9921e-01, 7.2337e-02, 1.2341e-01, 1.2906e-01, 6.6078e-02,
        2.9151e-04, 9.5959e-03, 7.0129e-02, 1.7755e-02, 1.5582e-02, 7.8400e-02,
        1.7134e-02, 6.4430e-02, 3.6568e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,486][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ give] are: tensor([1.9560e-06, 5.6633e-01, 1.5851e-01, 3.3896e-02, 2.9168e-02, 6.0008e-02,
        2.2652e-05, 1.3123e-02, 6.7741e-02, 8.7147e-03, 4.7585e-03, 2.9681e-03,
        7.1512e-03, 3.8384e-02, 9.2247e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,487][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ give] are: tensor([2.5693e-04, 3.9015e-01, 6.6989e-02, 1.0235e-01, 3.5508e-02, 3.2336e-02,
        1.8792e-03, 3.7643e-02, 9.4850e-02, 3.5556e-02, 3.1497e-02, 3.3653e-02,
        1.6299e-02, 8.1233e-02, 3.9806e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,487][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0015, 0.6087, 0.1247, 0.0244, 0.0303, 0.0634, 0.0026, 0.0076, 0.0517,
        0.0252, 0.0131, 0.0058, 0.0101, 0.0158, 0.0151], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,488][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0530, 0.0067, 0.0105, 0.0312, 0.0444, 0.1323, 0.0334, 0.0473, 0.1684,
        0.0442, 0.0861, 0.1058, 0.0922, 0.0984, 0.0461], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,489][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ give] are: tensor([6.1704e-04, 5.5151e-06, 4.8126e-04, 2.9540e-04, 7.3186e-02, 4.3265e-01,
        1.4895e-03, 9.1279e-03, 1.1394e-01, 8.3578e-03, 7.5454e-02, 2.8403e-05,
        9.5472e-02, 1.7490e-01, 1.3991e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,490][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ give] are: tensor([2.3899e-06, 2.3643e-01, 1.6236e-01, 7.0952e-02, 8.6069e-02, 1.6103e-01,
        1.2509e-04, 3.3976e-02, 1.2075e-01, 7.6107e-03, 1.5569e-02, 1.4534e-02,
        4.1430e-03, 7.1458e-02, 1.5002e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,491][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ give] are: tensor([5.3133e-04, 7.5355e-01, 1.6524e-02, 9.2590e-02, 3.1367e-02, 1.6394e-02,
        2.8937e-03, 5.4229e-03, 3.1780e-02, 3.3627e-03, 5.2601e-03, 8.2057e-03,
        3.7554e-03, 1.5593e-02, 1.2767e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,493][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.0135, 0.1028, 0.0597, 0.0571, 0.0429, 0.1077, 0.0117, 0.0363, 0.2074,
        0.0278, 0.0826, 0.0377, 0.0390, 0.1144, 0.0595], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,494][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ give] are: tensor([6.1729e-06, 1.9744e-07, 1.8407e-04, 1.3880e-04, 2.1490e-02, 6.6599e-01,
        1.8709e-04, 7.2187e-03, 4.6087e-02, 1.1264e-02, 3.3248e-02, 1.1283e-05,
        1.2742e-01, 8.4651e-02, 2.1043e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,495][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ give] are: tensor([6.7848e-06, 9.9893e-01, 2.6286e-04, 2.1869e-04, 1.0764e-04, 4.0144e-04,
        3.7775e-06, 1.2865e-05, 3.7231e-05, 6.6861e-06, 2.2428e-06, 2.3891e-07,
        3.1933e-06, 3.4221e-06, 2.8929e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,495][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ give] are: tensor([8.0722e-05, 2.8186e-01, 1.0351e-01, 1.4236e-01, 1.6744e-02, 5.6991e-02,
        7.1586e-04, 4.1570e-02, 7.6360e-02, 1.1612e-01, 2.0680e-02, 3.3532e-02,
        1.1529e-02, 7.9160e-02, 1.8796e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,496][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ it] are: tensor([3.0771e-05, 9.9290e-02, 6.4939e-02, 1.0022e-01, 8.5876e-02, 6.7934e-02,
        4.8086e-04, 1.5430e-02, 9.0694e-02, 2.3650e-02, 1.7072e-02, 4.8977e-02,
        2.1349e-02, 8.0433e-02, 2.9679e-02, 2.5394e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,497][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ it] are: tensor([2.2362e-06, 2.4365e-01, 2.1394e-01, 6.6150e-02, 2.8949e-02, 8.8569e-02,
        3.6503e-05, 2.1984e-02, 1.0673e-01, 1.2384e-02, 9.3392e-03, 4.7511e-03,
        9.8329e-03, 6.9480e-02, 1.9593e-02, 1.0462e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,499][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0004, 0.1203, 0.0717, 0.0944, 0.0663, 0.0575, 0.0026, 0.0671, 0.1344,
        0.0417, 0.0502, 0.0333, 0.0236, 0.1130, 0.0399, 0.0837],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,500][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0035, 0.4438, 0.1289, 0.0211, 0.0452, 0.0743, 0.0047, 0.0120, 0.0640,
        0.0423, 0.0208, 0.0061, 0.0255, 0.0297, 0.0206, 0.0575],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,502][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0672, 0.0037, 0.0086, 0.0303, 0.0295, 0.1123, 0.0295, 0.0343, 0.1375,
        0.0310, 0.0846, 0.0713, 0.0945, 0.1226, 0.0430, 0.1002],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,503][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ it] are: tensor([1.8182e-03, 3.6813e-06, 2.7510e-04, 2.3496e-04, 6.5144e-02, 4.5365e-01,
        1.4468e-03, 5.3818e-03, 1.1313e-01, 3.7506e-03, 3.5918e-02, 9.5768e-06,
        6.2254e-02, 1.3135e-01, 1.1285e-02, 1.1436e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,504][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ it] are: tensor([3.8994e-06, 4.6386e-02, 9.6997e-02, 5.3643e-02, 7.4902e-02, 2.2677e-01,
        1.3846e-04, 3.5802e-02, 1.5447e-01, 5.9575e-03, 1.6222e-02, 1.0489e-02,
        4.2910e-03, 6.7444e-02, 1.2946e-02, 1.9354e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,505][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0014, 0.4738, 0.0257, 0.1250, 0.0600, 0.0252, 0.0059, 0.0057, 0.0361,
        0.0048, 0.0122, 0.0133, 0.0101, 0.0381, 0.0091, 0.1537],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,507][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.0111, 0.0788, 0.0597, 0.0342, 0.0443, 0.0885, 0.0088, 0.0321, 0.1502,
        0.0178, 0.0788, 0.0248, 0.0725, 0.1243, 0.0535, 0.1206],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,508][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ it] are: tensor([2.5169e-05, 9.5706e-08, 9.9459e-05, 6.0196e-05, 1.9384e-02, 7.8088e-01,
        2.4227e-04, 4.1607e-03, 3.2011e-02, 4.4979e-03, 2.0816e-02, 2.7530e-06,
        5.5793e-02, 5.8072e-02, 1.7150e-03, 2.2236e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,509][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ it] are: tensor([2.8278e-05, 9.9719e-01, 6.3419e-04, 4.7336e-04, 2.0830e-04, 1.2030e-03,
        1.3240e-05, 5.1898e-05, 1.1148e-04, 1.4399e-05, 5.4642e-06, 3.1213e-07,
        5.4145e-06, 8.2668e-06, 5.2759e-06, 4.4818e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,510][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ it] are: tensor([1.3170e-04, 9.7955e-02, 1.0744e-01, 1.8269e-01, 4.4783e-02, 6.8722e-02,
        1.1094e-03, 6.3956e-02, 8.3867e-02, 1.1460e-01, 2.6374e-02, 4.3173e-02,
        1.3615e-02, 7.1908e-02, 1.9799e-02, 5.9875e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,511][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([1.4537e-05, 1.3692e-01, 5.3250e-02, 1.2763e-01, 6.8612e-02, 5.3144e-02,
        2.3710e-04, 9.8811e-03, 5.3686e-02, 1.6624e-02, 1.1875e-02, 8.8992e-02,
        1.0668e-02, 5.6842e-02, 1.9309e-02, 2.0285e-01, 8.9474e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,512][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([2.6297e-06, 3.5894e-01, 1.5065e-01, 5.6140e-02, 2.8649e-02, 4.9240e-02,
        2.9442e-05, 1.4787e-02, 6.5567e-02, 7.5294e-03, 6.5519e-03, 5.3094e-03,
        7.9318e-03, 6.5532e-02, 1.3297e-02, 9.1148e-02, 7.8690e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,513][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0002, 0.1669, 0.0407, 0.0646, 0.0334, 0.0337, 0.0018, 0.0373, 0.1116,
        0.0296, 0.0509, 0.0445, 0.0106, 0.0967, 0.0376, 0.0793, 0.1606],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,515][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0013, 0.5388, 0.0819, 0.0177, 0.0273, 0.0419, 0.0031, 0.0094, 0.0534,
        0.0285, 0.0135, 0.0090, 0.0182, 0.0236, 0.0188, 0.0817, 0.0318],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,516][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0218, 0.0058, 0.0115, 0.0427, 0.0350, 0.0906, 0.0181, 0.0272, 0.1468,
        0.0429, 0.0787, 0.1164, 0.0751, 0.0927, 0.0365, 0.0941, 0.0639],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,517][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([7.7902e-05, 3.8719e-06, 2.7694e-04, 9.9981e-04, 7.3384e-02, 3.1922e-01,
        5.3212e-04, 7.0196e-03, 1.1298e-01, 6.9152e-03, 5.8199e-02, 5.7545e-05,
        7.4185e-02, 1.7067e-01, 1.2990e-02, 1.5607e-01, 6.4182e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,518][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([2.0087e-06, 7.2829e-02, 9.0642e-02, 5.5398e-02, 4.9891e-02, 1.2443e-01,
        9.9357e-05, 2.7649e-02, 1.0398e-01, 5.2099e-03, 1.2450e-02, 1.6840e-02,
        2.4578e-03, 6.9961e-02, 8.8263e-03, 2.5439e-01, 1.0494e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,520][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0006, 0.3922, 0.0250, 0.1617, 0.0769, 0.0246, 0.0032, 0.0060, 0.0404,
        0.0052, 0.0101, 0.0105, 0.0063, 0.0401, 0.0056, 0.1478, 0.0436],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,521][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0112, 0.0647, 0.0415, 0.0436, 0.0310, 0.0730, 0.0068, 0.0169, 0.1209,
        0.0119, 0.0723, 0.0316, 0.0314, 0.1038, 0.0442, 0.1223, 0.1729],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,522][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([1.0830e-06, 2.1425e-07, 1.7248e-04, 3.6162e-04, 1.8529e-02, 6.6383e-01,
        1.2067e-04, 6.6301e-03, 5.0932e-02, 1.2912e-02, 4.2597e-02, 1.6157e-05,
        8.3145e-02, 8.0483e-02, 2.6507e-03, 3.5406e-02, 2.2112e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,523][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.6801e-06, 9.9836e-01, 3.1493e-04, 4.7828e-04, 1.0525e-04, 5.7758e-04,
        5.6975e-06, 1.9007e-05, 6.5488e-05, 6.4557e-06, 3.6589e-06, 4.2747e-07,
        2.3369e-06, 4.6774e-06, 3.9983e-06, 3.1192e-05, 7.3428e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,524][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.2270e-04, 8.0703e-02, 7.3668e-02, 1.3774e-01, 1.9092e-02, 4.8295e-02,
        8.3382e-04, 4.3871e-02, 5.2034e-02, 1.3303e-01, 2.5847e-02, 4.9246e-02,
        8.5295e-03, 8.4155e-02, 1.9208e-02, 3.4002e-02, 1.8963e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,583][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:34,584][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,585][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,586][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,588][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,589][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,590][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,591][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,592][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,593][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,594][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,596][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,606][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,607][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.0019, 0.9981], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,608][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([7.7139e-05, 9.9992e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,609][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.0426, 0.9574], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,611][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.0093, 0.9907], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,611][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.9433, 0.0567], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,611][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.1184, 0.8816], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,612][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([5.5239e-04, 9.9945e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,612][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.0847, 0.9153], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,612][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.5252, 0.4748], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,613][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.0277, 0.9723], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,613][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([2.5901e-04, 9.9974e-01], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,613][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.0048, 0.9952], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,613][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([9.0770e-05, 3.6715e-01, 6.3276e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,614][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([2.0467e-06, 2.8636e-01, 7.1364e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,615][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0020, 0.4849, 0.5131], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,616][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0034, 0.6668, 0.3298], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,618][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.6845, 0.0523, 0.2632], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,619][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0697, 0.0102, 0.9201], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,620][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([2.1635e-05, 1.9789e-01, 8.0209e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,621][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0297, 0.5970, 0.3734], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,622][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0671, 0.4853, 0.4476], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,623][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([1.3130e-03, 3.8095e-04, 9.9831e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,624][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([3.3994e-05, 9.9870e-01, 1.2644e-03], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,625][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([1.9601e-04, 3.7977e-01, 6.2004e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,625][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([2.9873e-05, 3.9734e-01, 3.7589e-01, 2.2674e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,626][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([3.1924e-06, 5.7386e-01, 3.6251e-01, 6.3625e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,628][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0016, 0.4706, 0.2819, 0.2458], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,629][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0020, 0.6925, 0.2902, 0.0153], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,630][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.7378, 0.0290, 0.1246, 0.1086], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,632][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0726, 0.0018, 0.5459, 0.3797], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,633][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([1.1130e-05, 2.5263e-01, 6.4961e-01, 9.7743e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,634][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0172, 0.4348, 0.1479, 0.4001], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,636][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.1134, 0.5192, 0.2331, 0.1342], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,636][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([3.4886e-03, 1.1414e-04, 6.8296e-01, 3.1344e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,637][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([4.0930e-05, 9.9939e-01, 3.7788e-04, 1.8800e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,638][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([1.8626e-04, 3.4140e-01, 1.6102e-01, 4.9739e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,639][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([6.6245e-05, 3.3618e-01, 2.2712e-01, 1.5351e-01, 2.8314e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,640][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([1.6862e-06, 5.2341e-01, 3.3548e-01, 7.1554e-02, 6.9551e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,641][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0005, 0.4307, 0.1606, 0.1927, 0.2155], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,643][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0019, 0.6329, 0.2809, 0.0162, 0.0680], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,644][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.4423, 0.0367, 0.0883, 0.0914, 0.3414], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,645][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([1.7254e-02, 8.6349e-05, 9.3661e-03, 1.5636e-03, 9.7173e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,646][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([8.2612e-06, 2.6919e-01, 4.0435e-01, 8.5421e-02, 2.4104e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,647][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0029, 0.5093, 0.0490, 0.1852, 0.2537], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,649][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0129, 0.1817, 0.3381, 0.1442, 0.3230], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,649][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([1.2385e-03, 2.9367e-06, 6.8499e-03, 9.1217e-04, 9.9100e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,650][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([9.6099e-06, 9.9930e-01, 3.7556e-04, 1.7246e-04, 1.4164e-04],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,651][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([7.5789e-05, 2.7238e-01, 1.9862e-01, 4.0879e-01, 1.2013e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,652][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([5.4280e-05, 1.7745e-01, 1.7200e-01, 1.5526e-01, 3.2242e-01, 1.7282e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,653][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([4.3734e-06, 2.9296e-01, 3.1300e-01, 1.4135e-01, 1.1827e-01, 1.3441e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,654][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0005, 0.2194, 0.1209, 0.1940, 0.3293, 0.1361], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,656][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0023, 0.6557, 0.1486, 0.0166, 0.0725, 0.1044], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,657][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1969, 0.0223, 0.0620, 0.0709, 0.1918, 0.4562], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,658][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([9.6855e-03, 7.6280e-06, 1.0655e-03, 1.7252e-04, 1.2470e-01, 8.6437e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,659][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([8.5204e-06, 1.2610e-01, 2.1529e-01, 6.8162e-02, 2.4917e-01, 3.4128e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,660][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0030, 0.3668, 0.0912, 0.2436, 0.2173, 0.0782], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,662][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0151, 0.1077, 0.1167, 0.0876, 0.2281, 0.4449], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,662][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([8.1433e-04, 1.7292e-07, 3.8188e-04, 3.0463e-05, 4.3369e-02, 9.5540e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,663][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([3.1231e-05, 9.9689e-01, 6.7163e-04, 6.0303e-04, 5.8655e-04, 1.2174e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,664][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([2.3854e-04, 1.0580e-01, 1.2164e-01, 3.6432e-01, 1.9421e-01, 2.1379e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,665][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([7.7025e-05, 1.1666e-01, 1.7188e-01, 1.8976e-01, 3.4234e-01, 1.7787e-01,
        1.4215e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,666][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([2.7819e-06, 2.0700e-01, 3.5922e-01, 1.2341e-01, 1.2537e-01, 1.8495e-01,
        5.6420e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,667][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0009, 0.2558, 0.1531, 0.1663, 0.2486, 0.1696, 0.0057],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,668][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0034, 0.3541, 0.3191, 0.0422, 0.1230, 0.1487, 0.0095],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,668][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.1020, 0.0484, 0.0716, 0.0865, 0.2026, 0.3386, 0.1502],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,669][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([1.9554e-03, 3.6419e-04, 1.0453e-02, 7.0728e-04, 1.5154e-01, 8.2981e-01,
        5.1662e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,669][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([2.1799e-05, 7.9906e-02, 2.1878e-01, 1.5151e-01, 2.7869e-01, 2.7052e-01,
        5.6034e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,669][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0041, 0.5489, 0.0517, 0.2305, 0.0969, 0.0453, 0.0226],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,670][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0037, 0.1289, 0.2381, 0.1249, 0.1397, 0.3539, 0.0108],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,670][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([5.6203e-05, 1.9659e-05, 7.4775e-03, 9.7902e-05, 1.1139e-01, 8.8041e-01,
        5.4460e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,670][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([5.5246e-05, 9.9380e-01, 1.3798e-03, 1.1328e-03, 1.0780e-03, 2.5191e-03,
        3.2836e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,671][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0005, 0.3427, 0.2053, 0.1968, 0.1110, 0.1418, 0.0019],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,671][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([1.1179e-05, 1.4728e-01, 1.7617e-01, 1.8478e-01, 2.2463e-01, 2.3234e-01,
        5.6281e-04, 3.4227e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,671][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([1.8790e-06, 4.3466e-01, 2.7392e-01, 7.1925e-02, 4.1709e-02, 1.4226e-01,
        3.9822e-05, 3.5480e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,672][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0008, 0.4014, 0.1213, 0.1392, 0.1422, 0.1063, 0.0044, 0.0844],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,674][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0013, 0.6711, 0.1556, 0.0160, 0.0433, 0.0923, 0.0050, 0.0155],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,675][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0648, 0.0316, 0.0447, 0.0777, 0.1276, 0.4931, 0.0760, 0.0845],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,676][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([1.1834e-03, 1.2209e-05, 1.0517e-03, 2.4402e-04, 1.3553e-01, 8.4864e-01,
        2.1911e-03, 1.1148e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,677][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([1.8062e-06, 1.4619e-01, 2.4462e-01, 7.7102e-02, 1.2269e-01, 3.4181e-01,
        1.6880e-04, 6.7421e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,678][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0009, 0.6237, 0.0512, 0.1006, 0.1527, 0.0486, 0.0052, 0.0171],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,679][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0348, 0.1294, 0.1082, 0.0803, 0.1452, 0.3790, 0.0266, 0.0965],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,680][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([1.9869e-05, 1.3374e-07, 3.6722e-04, 2.8924e-05, 3.8263e-02, 9.5544e-01,
        1.9491e-04, 5.6825e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,681][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([1.9819e-05, 9.9603e-01, 8.5100e-04, 5.0811e-04, 3.0846e-04, 2.1821e-03,
        1.4558e-05, 9.0454e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,683][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0003, 0.2689, 0.1962, 0.2114, 0.0633, 0.1423, 0.0025, 0.1152],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,683][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.9151e-05, 1.6776e-01, 1.2243e-01, 1.7772e-01, 1.9441e-01, 1.3686e-01,
        5.9788e-04, 2.8153e-02, 1.7205e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,684][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([3.1045e-06, 4.6716e-01, 1.9541e-01, 8.3375e-02, 4.6541e-02, 7.9044e-02,
        4.3154e-05, 2.4833e-02, 1.0359e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,685][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0005, 0.3021, 0.0908, 0.1528, 0.1375, 0.0837, 0.0045, 0.0671, 0.1610],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,687][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0015, 0.6797, 0.0842, 0.0196, 0.0388, 0.0719, 0.0038, 0.0158, 0.0847],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,688][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0648, 0.0118, 0.0216, 0.0483, 0.0833, 0.2534, 0.0518, 0.0639, 0.4012],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,689][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.5304e-03, 6.5584e-06, 4.8336e-04, 2.0936e-04, 1.1121e-01, 6.8785e-01,
        4.6857e-03, 1.1590e-02, 1.7444e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,690][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([2.4658e-06, 1.1194e-01, 1.2355e-01, 6.2041e-02, 1.2889e-01, 2.6980e-01,
        1.8365e-04, 5.7349e-02, 2.4624e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,691][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0017, 0.4470, 0.0489, 0.1366, 0.1907, 0.0593, 0.0081, 0.0176, 0.0902],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,693][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0142, 0.0777, 0.0965, 0.0580, 0.0816, 0.2664, 0.0146, 0.0719, 0.3191],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,694][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.5006e-04, 9.2509e-08, 2.3540e-04, 6.8855e-05, 3.3250e-02, 8.8402e-01,
        7.2735e-04, 9.9589e-03, 7.1593e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,694][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([9.9434e-06, 9.9887e-01, 2.4714e-04, 2.8057e-04, 1.3069e-04, 4.0485e-04,
        5.7216e-06, 1.6511e-05, 3.8667e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,695][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([8.3077e-05, 1.2380e-01, 1.0603e-01, 2.3040e-01, 7.6221e-02, 1.1817e-01,
        9.0263e-04, 1.3353e-01, 2.1086e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,696][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([2.1528e-05, 1.3062e-01, 1.6521e-01, 1.4038e-01, 2.4083e-01, 1.4818e-01,
        5.3141e-04, 2.4586e-02, 1.3064e-01, 1.8993e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,697][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([1.6168e-06, 2.4794e-01, 2.3377e-01, 1.1943e-01, 7.1585e-02, 1.4799e-01,
        3.7101e-05, 3.1653e-02, 1.3576e-01, 1.1828e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,698][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([1.9827e-04, 1.7406e-01, 1.0381e-01, 1.1808e-01, 1.1581e-01, 1.3767e-01,
        2.5752e-03, 8.5173e-02, 2.1345e-01, 4.9177e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,699][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0016, 0.4334, 0.1640, 0.0264, 0.0656, 0.1425, 0.0033, 0.0148, 0.1192,
        0.0292], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,701][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0227, 0.0068, 0.0263, 0.0480, 0.1325, 0.2477, 0.0375, 0.0638, 0.3389,
        0.0758], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,702][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([3.3575e-04, 1.7583e-06, 4.0154e-04, 3.8998e-05, 7.9914e-02, 7.9832e-01,
        4.7082e-04, 5.6205e-03, 1.1113e-01, 3.7633e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,703][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([5.8621e-06, 6.0780e-02, 1.7523e-01, 1.0515e-01, 1.8716e-01, 2.9254e-01,
        2.0333e-04, 4.1993e-02, 1.2857e-01, 8.3773e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,704][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0010, 0.4223, 0.0507, 0.1818, 0.1526, 0.0535, 0.0081, 0.0137, 0.0987,
        0.0177], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,705][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0198, 0.1080, 0.0993, 0.0782, 0.0749, 0.2124, 0.0199, 0.0915, 0.2610,
        0.0349], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,706][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([3.9271e-06, 4.8495e-08, 1.9901e-04, 5.6463e-06, 3.5545e-02, 9.1971e-01,
        7.3722e-05, 3.9712e-03, 3.7631e-02, 2.8613e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,707][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([2.1660e-05, 9.9572e-01, 7.5001e-04, 1.1254e-03, 5.7247e-04, 1.5923e-03,
        1.2042e-05, 5.4327e-05, 1.4156e-04, 8.8038e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,709][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0002, 0.1347, 0.1002, 0.1667, 0.0770, 0.1218, 0.0012, 0.0917, 0.1305,
        0.1760], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,710][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([7.9554e-06, 2.5866e-01, 1.1968e-01, 2.7506e-01, 1.1969e-01, 8.2507e-02,
        2.8839e-04, 1.4556e-02, 8.9989e-02, 2.3151e-02, 1.6411e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,710][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([4.9668e-07, 4.1698e-01, 2.3949e-01, 1.3317e-01, 4.9607e-02, 6.5444e-02,
        1.3281e-05, 1.5096e-02, 6.9247e-02, 6.0738e-03, 4.8758e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,711][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([1.1291e-04, 3.1787e-01, 9.5127e-02, 2.0250e-01, 7.9437e-02, 4.3462e-02,
        1.2938e-03, 4.5169e-02, 1.1866e-01, 3.6583e-02, 5.9785e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,712][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([5.2000e-04, 6.6652e-01, 1.1886e-01, 1.9674e-02, 3.6383e-02, 5.2524e-02,
        1.6728e-03, 8.2418e-03, 5.5278e-02, 2.7113e-02, 1.3219e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,714][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0201, 0.0119, 0.0227, 0.0712, 0.0722, 0.2062, 0.0250, 0.0500, 0.2920,
        0.0737, 0.1550], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,715][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([1.9856e-03, 8.9128e-06, 5.5531e-04, 5.3548e-04, 1.3978e-01, 6.2215e-01,
        3.0554e-03, 1.5388e-02, 1.3807e-01, 1.1003e-02, 6.7470e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,716][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([1.3668e-06, 1.7697e-01, 1.7450e-01, 1.0428e-01, 9.0561e-02, 1.8992e-01,
        1.3466e-04, 4.8156e-02, 1.7610e-01, 1.2024e-02, 2.7365e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,717][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0008, 0.5688, 0.0363, 0.1953, 0.0915, 0.0298, 0.0033, 0.0088, 0.0498,
        0.0043, 0.0113], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,718][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0048, 0.1338, 0.0938, 0.1431, 0.0722, 0.1712, 0.0069, 0.0296, 0.2241,
        0.0187, 0.1019], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,719][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([3.5084e-05, 1.3030e-07, 2.1602e-04, 1.7746e-04, 5.2685e-02, 8.2435e-01,
        5.9564e-04, 1.1522e-02, 4.8517e-02, 1.0449e-02, 5.1450e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,720][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([1.6963e-06, 9.9855e-01, 3.0875e-04, 6.6423e-04, 9.9187e-05, 3.2596e-04,
        1.9057e-06, 1.1238e-05, 2.8541e-05, 3.1507e-06, 1.5844e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,721][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([1.0941e-05, 1.7339e-01, 1.9583e-01, 3.1565e-01, 2.0544e-02, 5.5167e-02,
        1.5507e-04, 2.6059e-02, 6.8367e-02, 1.2417e-01, 2.0653e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,722][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([1.3347e-05, 2.5451e-01, 1.4618e-01, 9.8555e-02, 1.6354e-01, 8.7379e-02,
        3.9265e-04, 2.4031e-02, 9.3973e-02, 3.6898e-02, 2.1497e-02, 7.3021e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,723][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([8.1204e-07, 4.7941e-01, 2.8714e-01, 6.7251e-02, 2.9502e-02, 4.1447e-02,
        1.1164e-05, 1.2702e-02, 6.6724e-02, 8.3970e-03, 3.6926e-03, 3.7232e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,724][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([1.6370e-04, 2.6018e-01, 9.7041e-02, 9.3949e-02, 7.3452e-02, 6.1934e-02,
        1.2240e-03, 6.6542e-02, 1.7108e-01, 5.2693e-02, 7.5553e-02, 4.6183e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,725][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0010, 0.3731, 0.2563, 0.0104, 0.0752, 0.0896, 0.0020, 0.0099, 0.1119,
        0.0493, 0.0173, 0.0039], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,727][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0360, 0.0022, 0.0102, 0.0173, 0.0839, 0.1589, 0.0411, 0.0679, 0.2355,
        0.1137, 0.1077, 0.1255], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,727][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([2.0609e-06, 5.4269e-07, 1.8648e-04, 1.2080e-04, 6.5154e-02, 7.6207e-01,
        1.9708e-04, 9.8277e-03, 1.0351e-01, 3.0561e-02, 2.8353e-02, 1.8114e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,728][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([1.5756e-06, 1.0354e-01, 2.4293e-01, 5.2469e-02, 9.7084e-02, 1.6817e-01,
        1.1587e-04, 5.2251e-02, 2.2874e-01, 1.3454e-02, 2.4742e-02, 1.6500e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,730][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0010, 0.5446, 0.0418, 0.1473, 0.0669, 0.0517, 0.0055, 0.0153, 0.0769,
        0.0065, 0.0134, 0.0291], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,731][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0110, 0.1059, 0.0781, 0.0986, 0.0481, 0.1097, 0.0100, 0.0269, 0.2587,
        0.0530, 0.1132, 0.0868], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,731][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([1.1958e-08, 1.3716e-08, 8.0897e-05, 3.2490e-05, 3.1081e-02, 8.6260e-01,
        2.8432e-05, 7.7530e-03, 4.6004e-02, 3.9080e-02, 1.3334e-02, 4.4375e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,731][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([1.3749e-05, 9.9878e-01, 3.6679e-04, 2.3485e-04, 2.0274e-04, 3.4046e-04,
        4.8859e-06, 1.8087e-05, 3.3135e-05, 5.2717e-06, 9.0414e-07, 1.1050e-07],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,732][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([2.2621e-05, 2.6925e-01, 1.4611e-01, 2.5120e-01, 1.7745e-02, 4.5014e-02,
        1.1892e-04, 4.8144e-02, 9.1953e-02, 7.6501e-02, 1.0207e-02, 4.3734e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:34,732][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([6.8501e-05, 2.3235e-01, 1.0837e-01, 1.7940e-01, 1.5566e-01, 7.9117e-02,
        5.8466e-04, 1.4619e-02, 7.9198e-02, 2.6868e-02, 2.2360e-02, 6.5196e-02,
        3.6222e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,732][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([2.2195e-06, 3.6071e-01, 2.2805e-01, 7.6688e-02, 6.9064e-02, 7.7828e-02,
        4.4582e-05, 3.5940e-02, 9.2122e-02, 1.5184e-02, 9.4482e-03, 9.0136e-03,
        2.5905e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,733][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0003, 0.2543, 0.1259, 0.1561, 0.0615, 0.0542, 0.0027, 0.0501, 0.1151,
        0.0362, 0.0587, 0.0534, 0.0316], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,733][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0023, 0.4925, 0.1950, 0.0302, 0.0432, 0.0748, 0.0038, 0.0105, 0.0543,
        0.0333, 0.0224, 0.0074, 0.0302], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,734][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0193, 0.0029, 0.0128, 0.0276, 0.0589, 0.1239, 0.0212, 0.0525, 0.1686,
        0.0677, 0.1367, 0.1180, 0.1899], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,734][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([3.8823e-03, 1.3996e-05, 8.5766e-04, 3.9947e-04, 8.6244e-02, 6.3322e-01,
        3.3429e-03, 1.2060e-02, 1.4912e-01, 6.9914e-03, 4.3802e-02, 2.3869e-05,
        6.0043e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,735][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([9.8464e-06, 1.1986e-01, 1.8151e-01, 9.6176e-02, 1.0284e-01, 2.0711e-01,
        2.4751e-04, 4.0444e-02, 1.7114e-01, 1.6503e-02, 2.3727e-02, 3.1548e-02,
        8.8882e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,736][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0027, 0.5079, 0.0346, 0.1901, 0.0868, 0.0324, 0.0074, 0.0059, 0.0606,
        0.0067, 0.0157, 0.0181, 0.0312], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,737][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0037, 0.0626, 0.1376, 0.0623, 0.0506, 0.1716, 0.0063, 0.0419, 0.2080,
        0.0227, 0.1381, 0.0381, 0.0566], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,738][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([8.8727e-05, 4.3209e-07, 2.1950e-04, 4.5737e-05, 2.1545e-02, 7.7736e-01,
        4.7523e-04, 6.2044e-03, 5.5996e-02, 5.9796e-03, 1.9220e-02, 3.5753e-06,
        1.1286e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,739][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([1.6174e-05, 9.9706e-01, 5.8892e-04, 5.3737e-04, 3.2517e-04, 1.2818e-03,
        1.2553e-05, 3.9322e-05, 1.0573e-04, 1.6565e-05, 7.2709e-06, 7.6507e-07,
        9.7218e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,740][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([1.5552e-04, 2.9135e-01, 1.3209e-01, 1.9511e-01, 2.0631e-02, 7.7239e-02,
        8.5924e-04, 4.5864e-02, 7.5379e-02, 7.9978e-02, 1.8937e-02, 4.6085e-02,
        1.6318e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:34,740][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([2.6429e-05, 2.0456e-01, 8.0495e-02, 1.8154e-01, 1.0974e-01, 7.4332e-02,
        3.7639e-04, 1.4106e-02, 7.7758e-02, 2.5390e-02, 1.9578e-02, 1.1278e-01,
        2.0805e-02, 7.8511e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,741][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([1.6647e-06, 3.7605e-01, 2.1352e-01, 8.0576e-02, 4.2973e-02, 7.2035e-02,
        2.9803e-05, 2.0946e-02, 7.6702e-02, 9.8163e-03, 9.2937e-03, 7.6140e-03,
        9.2413e-03, 8.1199e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,742][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([2.1345e-04, 2.1967e-01, 6.4840e-02, 1.1256e-01, 5.3526e-02, 4.4122e-02,
        2.2659e-03, 5.1739e-02, 1.2278e-01, 3.7973e-02, 7.3680e-02, 6.8744e-02,
        1.4089e-02, 1.3380e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,744][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0012, 0.5728, 0.1063, 0.0204, 0.0376, 0.0563, 0.0031, 0.0108, 0.0636,
        0.0385, 0.0207, 0.0108, 0.0287, 0.0294], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,745][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0382, 0.0035, 0.0104, 0.0296, 0.0334, 0.1049, 0.0283, 0.0351, 0.1724,
        0.0545, 0.1344, 0.1208, 0.1053, 0.1292], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,746][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([1.4564e-03, 4.4127e-06, 3.4714e-04, 5.8731e-04, 7.4072e-02, 4.6494e-01,
        1.8023e-03, 8.1154e-03, 1.1941e-01, 5.0585e-03, 6.0276e-02, 2.5796e-05,
        9.9517e-02, 1.6439e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,747][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([2.9640e-06, 1.0240e-01, 1.5398e-01, 7.0679e-02, 7.6698e-02, 1.9079e-01,
        1.5352e-04, 4.6898e-02, 1.6127e-01, 9.2802e-03, 2.5153e-02, 2.4025e-02,
        4.5863e-03, 1.3409e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,748][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0015, 0.4365, 0.0309, 0.2173, 0.0943, 0.0366, 0.0068, 0.0094, 0.0553,
        0.0071, 0.0153, 0.0210, 0.0093, 0.0588], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,750][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0098, 0.0711, 0.0656, 0.0736, 0.0580, 0.1321, 0.0081, 0.0274, 0.1801,
        0.0160, 0.1085, 0.0484, 0.0501, 0.1512], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,751][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([3.2907e-05, 1.4832e-07, 1.3236e-04, 1.8807e-04, 1.2088e-02, 6.6846e-01,
        4.3466e-04, 7.0771e-03, 5.0335e-02, 7.3240e-03, 3.9918e-02, 8.7749e-06,
        1.3146e-01, 8.2535e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,752][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.1473e-06, 9.9867e-01, 2.8505e-04, 4.1132e-04, 8.9444e-05, 4.6061e-04,
        4.8193e-06, 1.4042e-05, 3.8808e-05, 4.9244e-06, 2.6148e-06, 2.4064e-07,
        1.7005e-06, 3.1375e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,753][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.1558e-04, 7.4109e-02, 1.0238e-01, 1.9344e-01, 1.7289e-02, 5.9737e-02,
        7.2370e-04, 4.6024e-02, 7.0914e-02, 1.7200e-01, 3.6726e-02, 8.7865e-02,
        1.3049e-02, 1.2564e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:34,753][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([1.5903e-05, 2.9921e-01, 7.2337e-02, 1.2341e-01, 1.2906e-01, 6.6078e-02,
        2.9151e-04, 9.5959e-03, 7.0129e-02, 1.7755e-02, 1.5582e-02, 7.8400e-02,
        1.7134e-02, 6.4430e-02, 3.6568e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,754][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([1.9560e-06, 5.6633e-01, 1.5851e-01, 3.3896e-02, 2.9168e-02, 6.0008e-02,
        2.2652e-05, 1.3123e-02, 6.7741e-02, 8.7147e-03, 4.7585e-03, 2.9681e-03,
        7.1512e-03, 3.8384e-02, 9.2247e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,755][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([2.5693e-04, 3.9015e-01, 6.6989e-02, 1.0235e-01, 3.5508e-02, 3.2336e-02,
        1.8792e-03, 3.7643e-02, 9.4850e-02, 3.5556e-02, 3.1497e-02, 3.3653e-02,
        1.6299e-02, 8.1233e-02, 3.9806e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,757][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0015, 0.6087, 0.1247, 0.0244, 0.0303, 0.0634, 0.0026, 0.0076, 0.0517,
        0.0252, 0.0131, 0.0058, 0.0101, 0.0158, 0.0151], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,758][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0530, 0.0067, 0.0105, 0.0312, 0.0444, 0.1323, 0.0334, 0.0473, 0.1684,
        0.0442, 0.0861, 0.1058, 0.0922, 0.0984, 0.0461], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,759][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([6.1704e-04, 5.5151e-06, 4.8126e-04, 2.9540e-04, 7.3186e-02, 4.3265e-01,
        1.4895e-03, 9.1279e-03, 1.1394e-01, 8.3578e-03, 7.5454e-02, 2.8403e-05,
        9.5472e-02, 1.7490e-01, 1.3991e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,760][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([2.3899e-06, 2.3643e-01, 1.6236e-01, 7.0952e-02, 8.6069e-02, 1.6103e-01,
        1.2509e-04, 3.3976e-02, 1.2075e-01, 7.6107e-03, 1.5569e-02, 1.4534e-02,
        4.1430e-03, 7.1458e-02, 1.5002e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,761][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([5.3133e-04, 7.5355e-01, 1.6524e-02, 9.2590e-02, 3.1367e-02, 1.6394e-02,
        2.8937e-03, 5.4229e-03, 3.1780e-02, 3.3627e-03, 5.2601e-03, 8.2057e-03,
        3.7554e-03, 1.5593e-02, 1.2767e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,762][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0135, 0.1028, 0.0597, 0.0571, 0.0429, 0.1077, 0.0117, 0.0363, 0.2074,
        0.0278, 0.0826, 0.0377, 0.0390, 0.1144, 0.0595], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,763][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([6.1729e-06, 1.9744e-07, 1.8407e-04, 1.3880e-04, 2.1490e-02, 6.6599e-01,
        1.8709e-04, 7.2187e-03, 4.6087e-02, 1.1264e-02, 3.3248e-02, 1.1283e-05,
        1.2742e-01, 8.4651e-02, 2.1043e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,764][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([6.7848e-06, 9.9893e-01, 2.6286e-04, 2.1869e-04, 1.0764e-04, 4.0144e-04,
        3.7775e-06, 1.2865e-05, 3.7231e-05, 6.6861e-06, 2.2428e-06, 2.3891e-07,
        3.1933e-06, 3.4221e-06, 2.8929e-06], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,765][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([8.0722e-05, 2.8186e-01, 1.0351e-01, 1.4236e-01, 1.6744e-02, 5.6991e-02,
        7.1586e-04, 4.1570e-02, 7.6360e-02, 1.1612e-01, 2.0680e-02, 3.3532e-02,
        1.1529e-02, 7.9160e-02, 1.8796e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:34,766][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([3.0771e-05, 9.9290e-02, 6.4939e-02, 1.0022e-01, 8.5876e-02, 6.7934e-02,
        4.8086e-04, 1.5430e-02, 9.0694e-02, 2.3650e-02, 1.7072e-02, 4.8977e-02,
        2.1349e-02, 8.0433e-02, 2.9679e-02, 2.5394e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,767][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([2.2362e-06, 2.4365e-01, 2.1394e-01, 6.6150e-02, 2.8949e-02, 8.8569e-02,
        3.6503e-05, 2.1984e-02, 1.0673e-01, 1.2384e-02, 9.3392e-03, 4.7511e-03,
        9.8329e-03, 6.9480e-02, 1.9593e-02, 1.0462e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,768][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0004, 0.1203, 0.0717, 0.0944, 0.0663, 0.0575, 0.0026, 0.0671, 0.1344,
        0.0417, 0.0502, 0.0333, 0.0236, 0.1130, 0.0399, 0.0837],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,770][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0035, 0.4438, 0.1289, 0.0211, 0.0452, 0.0743, 0.0047, 0.0120, 0.0640,
        0.0423, 0.0208, 0.0061, 0.0255, 0.0297, 0.0206, 0.0575],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,771][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0672, 0.0037, 0.0086, 0.0303, 0.0295, 0.1123, 0.0295, 0.0343, 0.1375,
        0.0310, 0.0846, 0.0713, 0.0945, 0.1226, 0.0430, 0.1002],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,772][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([1.8182e-03, 3.6813e-06, 2.7510e-04, 2.3496e-04, 6.5144e-02, 4.5365e-01,
        1.4468e-03, 5.3818e-03, 1.1313e-01, 3.7506e-03, 3.5918e-02, 9.5768e-06,
        6.2254e-02, 1.3135e-01, 1.1285e-02, 1.1436e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,773][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([3.8994e-06, 4.6386e-02, 9.6997e-02, 5.3643e-02, 7.4902e-02, 2.2677e-01,
        1.3846e-04, 3.5802e-02, 1.5447e-01, 5.9575e-03, 1.6222e-02, 1.0489e-02,
        4.2910e-03, 6.7444e-02, 1.2946e-02, 1.9354e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,775][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0014, 0.4738, 0.0257, 0.1250, 0.0600, 0.0252, 0.0059, 0.0057, 0.0361,
        0.0048, 0.0122, 0.0133, 0.0101, 0.0381, 0.0091, 0.1537],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,776][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0111, 0.0788, 0.0597, 0.0342, 0.0443, 0.0885, 0.0088, 0.0321, 0.1502,
        0.0178, 0.0788, 0.0248, 0.0725, 0.1243, 0.0535, 0.1206],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,777][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([2.5169e-05, 9.5706e-08, 9.9459e-05, 6.0196e-05, 1.9384e-02, 7.8088e-01,
        2.4227e-04, 4.1607e-03, 3.2011e-02, 4.4979e-03, 2.0816e-02, 2.7530e-06,
        5.5793e-02, 5.8072e-02, 1.7150e-03, 2.2236e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,778][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([2.8278e-05, 9.9719e-01, 6.3419e-04, 4.7336e-04, 2.0830e-04, 1.2030e-03,
        1.3240e-05, 5.1898e-05, 1.1148e-04, 1.4399e-05, 5.4642e-06, 3.1213e-07,
        5.4145e-06, 8.2668e-06, 5.2759e-06, 4.4818e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,779][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([1.3170e-04, 9.7955e-02, 1.0744e-01, 1.8269e-01, 4.4783e-02, 6.8722e-02,
        1.1094e-03, 6.3956e-02, 8.3867e-02, 1.1460e-01, 2.6374e-02, 4.3173e-02,
        1.3615e-02, 7.1908e-02, 1.9799e-02, 5.9875e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:34,780][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([1.4537e-05, 1.3692e-01, 5.3250e-02, 1.2763e-01, 6.8612e-02, 5.3144e-02,
        2.3710e-04, 9.8811e-03, 5.3686e-02, 1.6624e-02, 1.1875e-02, 8.8992e-02,
        1.0668e-02, 5.6842e-02, 1.9309e-02, 2.0285e-01, 8.9474e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,781][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([2.6297e-06, 3.5894e-01, 1.5065e-01, 5.6140e-02, 2.8649e-02, 4.9240e-02,
        2.9442e-05, 1.4787e-02, 6.5567e-02, 7.5294e-03, 6.5519e-03, 5.3094e-03,
        7.9318e-03, 6.5532e-02, 1.3297e-02, 9.1148e-02, 7.8690e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,782][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0002, 0.1669, 0.0407, 0.0646, 0.0334, 0.0337, 0.0018, 0.0373, 0.1116,
        0.0296, 0.0509, 0.0445, 0.0106, 0.0967, 0.0376, 0.0793, 0.1606],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,784][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0013, 0.5388, 0.0819, 0.0177, 0.0273, 0.0419, 0.0031, 0.0094, 0.0534,
        0.0285, 0.0135, 0.0090, 0.0182, 0.0236, 0.0188, 0.0817, 0.0318],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,785][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0218, 0.0058, 0.0115, 0.0427, 0.0350, 0.0906, 0.0181, 0.0272, 0.1468,
        0.0429, 0.0787, 0.1164, 0.0751, 0.0927, 0.0365, 0.0941, 0.0639],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,786][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([7.7902e-05, 3.8719e-06, 2.7694e-04, 9.9981e-04, 7.3384e-02, 3.1922e-01,
        5.3212e-04, 7.0196e-03, 1.1298e-01, 6.9152e-03, 5.8199e-02, 5.7545e-05,
        7.4185e-02, 1.7067e-01, 1.2990e-02, 1.5607e-01, 6.4182e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,787][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([2.0087e-06, 7.2829e-02, 9.0642e-02, 5.5398e-02, 4.9891e-02, 1.2443e-01,
        9.9357e-05, 2.7649e-02, 1.0398e-01, 5.2099e-03, 1.2450e-02, 1.6840e-02,
        2.4578e-03, 6.9961e-02, 8.8263e-03, 2.5439e-01, 1.0494e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,789][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0006, 0.3922, 0.0250, 0.1617, 0.0769, 0.0246, 0.0032, 0.0060, 0.0404,
        0.0052, 0.0101, 0.0105, 0.0063, 0.0401, 0.0056, 0.1478, 0.0436],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,789][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0112, 0.0647, 0.0415, 0.0436, 0.0310, 0.0730, 0.0068, 0.0169, 0.1209,
        0.0119, 0.0723, 0.0316, 0.0314, 0.1038, 0.0442, 0.1223, 0.1729],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,789][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([1.0830e-06, 2.1425e-07, 1.7248e-04, 3.6162e-04, 1.8529e-02, 6.6383e-01,
        1.2067e-04, 6.6301e-03, 5.0932e-02, 1.2912e-02, 4.2597e-02, 1.6157e-05,
        8.3145e-02, 8.0483e-02, 2.6507e-03, 3.5406e-02, 2.2112e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,790][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.6801e-06, 9.9836e-01, 3.1493e-04, 4.7828e-04, 1.0525e-04, 5.7758e-04,
        5.6975e-06, 1.9007e-05, 6.5488e-05, 6.4557e-06, 3.6589e-06, 4.2747e-07,
        2.3369e-06, 4.6774e-06, 3.9983e-06, 3.1192e-05, 7.3428e-06],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,790][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([1.2270e-04, 8.0703e-02, 7.3668e-02, 1.3774e-01, 1.9092e-02, 4.8295e-02,
        8.3382e-04, 4.3871e-02, 5.2034e-02, 1.3303e-01, 2.5847e-02, 4.9246e-02,
        8.5295e-03, 8.4155e-02, 1.9208e-02, 3.4002e-02, 1.8963e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:34,791][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:34,792][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 763],
        [   2],
        [  60],
        [  68],
        [2221],
        [ 346],
        [ 261],
        [  32],
        [  40],
        [ 156],
        [   2],
        [  71],
        [ 148],
        [   3],
        [  87],
        [   8],
        [   1]], device='cuda:0')
[2024-07-24 10:21:34,794][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 689],
        [   3],
        [  88],
        [  60],
        [2029],
        [ 410],
        [ 298],
        [  34],
        [  49],
        [ 163],
        [   2],
        [  64],
        [ 166],
        [   2],
        [  85],
        [  11],
        [   1]], device='cuda:0')
[2024-07-24 10:21:34,795][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[27848],
        [50180],
        [46396],
        [47697],
        [47078],
        [38768],
        [33073],
        [35251],
        [33924],
        [30437],
        [42866],
        [40321],
        [39902],
        [33602],
        [43060],
        [13994],
        [18766]], device='cuda:0')
[2024-07-24 10:21:34,797][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[15836],
        [12942],
        [ 4788],
        [ 7226],
        [ 7417],
        [ 7015],
        [ 6574],
        [ 7572],
        [ 8810],
        [ 8314],
        [ 8301],
        [ 8066],
        [ 8918],
        [ 9292],
        [10186],
        [11193],
        [12853]], device='cuda:0')
[2024-07-24 10:21:34,798][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[24374],
        [50248],
        [48563],
        [49640],
        [49270],
        [45185],
        [46344],
        [49174],
        [48006],
        [43706],
        [48706],
        [47249],
        [47611],
        [46346],
        [49392],
        [41193],
        [43589]], device='cuda:0')
[2024-07-24 10:21:34,799][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[14428],
        [24997],
        [18055],
        [18839],
        [18704],
        [19165],
        [14457],
        [19311],
        [19722],
        [15295],
        [19706],
        [14572],
        [16308],
        [18552],
        [18895],
        [16990],
        [19425]], device='cuda:0')
[2024-07-24 10:21:34,801][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[29004],
        [28490],
        [25226],
        [22777],
        [ 9690],
        [12669],
        [14014],
        [15443],
        [18221],
        [16231],
        [17256],
        [16347],
        [16020],
        [17593],
        [17353],
        [18011],
        [17999]], device='cuda:0')
[2024-07-24 10:21:34,802][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[30837],
        [33092],
        [32142],
        [31747],
        [33116],
        [35450],
        [35413],
        [35392],
        [35229],
        [35328],
        [34239],
        [34717],
        [33811],
        [32439],
        [32210],
        [32183],
        [30874]], device='cuda:0')
[2024-07-24 10:21:34,804][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 4361],
        [50256],
        [48229],
        [49314],
        [49698],
        [47419],
        [45909],
        [48148],
        [47718],
        [45453],
        [49154],
        [47650],
        [48126],
        [47102],
        [49631],
        [45686],
        [47024]], device='cuda:0')
[2024-07-24 10:21:34,805][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[49515],
        [    8],
        [  404],
        [ 2171],
        [  375],
        [ 2592],
        [  287],
        [   88],
        [  786],
        [ 1143],
        [  200],
        [  252],
        [  368],
        [  958],
        [   19],
        [  603],
        [ 1638]], device='cuda:0')
[2024-07-24 10:21:34,807][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[35542],
        [44286],
        [45192],
        [45764],
        [47087],
        [44873],
        [45216],
        [43716],
        [41399],
        [42206],
        [43977],
        [43908],
        [43704],
        [44172],
        [43824],
        [43697],
        [43353]], device='cuda:0')
[2024-07-24 10:21:34,808][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[47441],
        [44400],
        [46797],
        [46734],
        [42062],
        [47263],
        [47093],
        [47322],
        [47674],
        [47510],
        [47850],
        [47814],
        [47821],
        [48175],
        [48140],
        [48002],
        [48393]], device='cuda:0')
[2024-07-24 10:21:34,810][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[39528],
        [49297],
        [49295],
        [49296],
        [49296],
        [49294],
        [49288],
        [49292],
        [49296],
        [49292],
        [49294],
        [49296],
        [49294],
        [49294],
        [49296],
        [49294],
        [49294]], device='cuda:0')
[2024-07-24 10:21:34,811][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[15476],
        [33816],
        [23135],
        [30389],
        [28712],
        [25182],
        [26269],
        [24851],
        [22795],
        [24249],
        [26725],
        [27537],
        [27461],
        [26860],
        [27733],
        [25112],
        [25004]], device='cuda:0')
[2024-07-24 10:21:34,812][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[11303],
        [ 3328],
        [ 1735],
        [ 1713],
        [ 3115],
        [ 3719],
        [ 4997],
        [ 3632],
        [ 2304],
        [ 5451],
        [ 2189],
        [ 1735],
        [ 3393],
        [ 3384],
        [ 4113],
        [ 4042],
        [ 3056]], device='cuda:0')
[2024-07-24 10:21:34,814][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 7378],
        [29180],
        [38456],
        [37827],
        [40302],
        [41009],
        [40901],
        [39984],
        [38680],
        [38842],
        [37893],
        [37134],
        [37395],
        [35307],
        [37318],
        [33912],
        [33091]], device='cuda:0')
[2024-07-24 10:21:34,815][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[41610],
        [ 4143],
        [ 6576],
        [ 5794],
        [ 6297],
        [ 8106],
        [ 8657],
        [ 7375],
        [ 7113],
        [ 9064],
        [ 7320],
        [ 6669],
        [ 8448],
        [ 8151],
        [ 6618],
        [10328],
        [ 8973]], device='cuda:0')
[2024-07-24 10:21:34,817][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[35730],
        [ 3736],
        [ 5690],
        [ 6626],
        [ 6125],
        [ 6879],
        [ 6592],
        [ 5555],
        [ 5817],
        [ 6581],
        [ 6215],
        [ 6337],
        [ 6668],
        [ 6542],
        [ 5317],
        [ 7014],
        [ 6536]], device='cuda:0')
[2024-07-24 10:21:34,818][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[4961],
        [9823],
        [5697],
        [5929],
        [5433],
        [5435],
        [4103],
        [5592],
        [5339],
        [3836],
        [5529],
        [3782],
        [4136],
        [4417],
        [4828],
        [3728],
        [4167]], device='cuda:0')
[2024-07-24 10:21:34,820][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[11302],
        [14756],
        [15167],
        [15367],
        [17389],
        [15538],
        [15240],
        [14811],
        [15691],
        [15823],
        [17018],
        [17384],
        [21047],
        [19677],
        [19403],
        [20274],
        [19526]], device='cuda:0')
[2024-07-24 10:21:34,821][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[12955],
        [29991],
        [12641],
        [14982],
        [12630],
        [19498],
        [19299],
        [19145],
        [18841],
        [19288],
        [17965],
        [18802],
        [18320],
        [16570],
        [16242],
        [16583],
        [15563]], device='cuda:0')
[2024-07-24 10:21:34,823][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[18177],
        [ 6981],
        [ 3886],
        [ 4151],
        [ 3303],
        [ 4385],
        [ 5185],
        [ 4335],
        [ 4192],
        [ 4880],
        [ 3897],
        [ 3777],
        [ 4170],
        [ 3221],
        [ 3073],
        [ 2849],
        [ 2235]], device='cuda:0')
[2024-07-24 10:21:34,824][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[17721],
        [14313],
        [20553],
        [25031],
        [23460],
        [27302],
        [22520],
        [19899],
        [24888],
        [24870],
        [21510],
        [21463],
        [21569],
        [24509],
        [16940],
        [20531],
        [23065]], device='cuda:0')
[2024-07-24 10:21:34,825][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 8339],
        [ 5241],
        [ 6115],
        [ 6006],
        [ 7041],
        [ 8232],
        [ 7712],
        [ 7727],
        [ 7873],
        [ 7700],
        [ 8167],
        [ 9363],
        [10034],
        [10778],
        [ 9836],
        [ 9826],
        [ 9933]], device='cuda:0')
[2024-07-24 10:21:34,827][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[8877],
        [4811],
        [4597],
        [5389],
        [5708],
        [4697],
        [4660],
        [4723],
        [4757],
        [4712],
        [4663],
        [4594],
        [4445],
        [4513],
        [4511],
        [4575],
        [4514]], device='cuda:0')
[2024-07-24 10:21:34,828][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[32734],
        [28834],
        [28831],
        [28836],
        [28837],
        [28856],
        [28868],
        [28857],
        [28844],
        [28861],
        [28846],
        [28843],
        [28855],
        [28846],
        [28844],
        [28853],
        [28849]], device='cuda:0')
[2024-07-24 10:21:34,830][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[14321],
        [ 5682],
        [ 8843],
        [ 7110],
        [ 8776],
        [11686],
        [10298],
        [11097],
        [13237],
        [12182],
        [ 9375],
        [ 9223],
        [ 9151],
        [ 9402],
        [ 9280],
        [10745],
        [ 9938]], device='cuda:0')
[2024-07-24 10:21:34,831][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[20049],
        [43685],
        [41968],
        [40925],
        [39559],
        [35076],
        [36609],
        [38443],
        [37343],
        [36620],
        [39083],
        [39834],
        [37646],
        [38859],
        [40835],
        [39413],
        [40760]], device='cuda:0')
[2024-07-24 10:21:34,833][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[34156],
        [39694],
        [43930],
        [47740],
        [47018],
        [42715],
        [41107],
        [46500],
        [45786],
        [34751],
        [46514],
        [46158],
        [38075],
        [43519],
        [41697],
        [40983],
        [44062]], device='cuda:0')
[2024-07-24 10:21:34,834][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490],
        [10490]], device='cuda:0')
[2024-07-24 10:21:34,883][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:34,884][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,885][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,886][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,887][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,887][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,887][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,887][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,888][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,888][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,888][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,889][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,889][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:34,889][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Benjamin] are: tensor([0.2861, 0.7139], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,890][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Benjamin] are: tensor([0.3995, 0.6005], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,890][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Benjamin] are: tensor([0.1291, 0.8709], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,890][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Benjamin] are: tensor([0.5901, 0.4099], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,891][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Benjamin] are: tensor([0.0174, 0.9826], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,891][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Benjamin] are: tensor([0.2223, 0.7777], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,891][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Benjamin] are: tensor([0.0149, 0.9851], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,892][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Benjamin] are: tensor([0.3673, 0.6327], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,892][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Benjamin] are: tensor([0.8360, 0.1640], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,892][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Benjamin] are: tensor([0.1929, 0.8071], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,892][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Benjamin] are: tensor([9.9982e-01, 1.7670e-04], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,893][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Benjamin] are: tensor([0.7492, 0.2508], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:34,893][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.1338, 0.6062, 0.2600], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,893][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0593, 0.6784, 0.2623], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,894][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0343, 0.6496, 0.3160], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,894][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0255, 0.3314, 0.6432], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,894][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0355, 0.6724, 0.2921], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,896][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0258, 0.3118, 0.6624], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,897][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0055, 0.3654, 0.6291], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,898][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.1031, 0.4811, 0.4158], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,900][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6831, 0.1927, 0.1242], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,901][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0387, 0.9119, 0.0495], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,902][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ and] are: tensor([9.9655e-01, 1.6231e-04, 3.2877e-03], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,903][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ and] are: tensor([2.1683e-03, 4.2613e-04, 9.9741e-01], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:34,904][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0527, 0.1985, 0.1053, 0.6435], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,905][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.1118, 0.5854, 0.1993, 0.1036], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,907][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0164, 0.1996, 0.1005, 0.6835], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,908][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0319, 0.3096, 0.6234, 0.0351], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,909][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0219, 0.2795, 0.1122, 0.5864], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,911][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0475, 0.2138, 0.3211, 0.4176], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,912][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0037, 0.1961, 0.4821, 0.3181], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,913][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.2131, 0.3147, 0.2380, 0.2343], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,915][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.7720, 0.1158, 0.0625, 0.0497], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,916][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0184, 0.1213, 0.0162, 0.8440], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,917][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([9.9929e-01, 4.1678e-05, 6.2404e-04, 4.0970e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,917][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([0.0052, 0.0010, 0.8807, 0.1131], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:34,917][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ got] are: tensor([0.1948, 0.1778, 0.0416, 0.3051, 0.2808], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,918][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ got] are: tensor([0.0270, 0.4286, 0.1509, 0.1243, 0.2692], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,918][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ got] are: tensor([0.0053, 0.1654, 0.0508, 0.6027, 0.1759], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,918][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ got] are: tensor([0.0410, 0.2898, 0.5270, 0.0517, 0.0904], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,918][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ got] are: tensor([0.0118, 0.1586, 0.0669, 0.3118, 0.4509], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,919][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ got] are: tensor([0.0242, 0.1517, 0.2833, 0.3443, 0.1965], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,919][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ got] are: tensor([0.0006, 0.1413, 0.3444, 0.1703, 0.3434], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,919][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ got] are: tensor([0.0725, 0.2579, 0.2252, 0.1272, 0.3172], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,920][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ got] are: tensor([0.8624, 0.0656, 0.0335, 0.0239, 0.0145], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,920][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ got] are: tensor([0.0156, 0.2435, 0.0106, 0.7086, 0.0217], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,921][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ got] are: tensor([0.9656, 0.0025, 0.0110, 0.0016, 0.0192], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,922][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ got] are: tensor([7.7003e-05, 1.4464e-04, 7.4074e-01, 2.2771e-02, 2.3627e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:34,924][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0953, 0.1260, 0.0812, 0.2936, 0.2150, 0.1890], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,925][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0118, 0.3553, 0.1191, 0.1184, 0.1626, 0.2329], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,926][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0035, 0.1604, 0.0749, 0.6483, 0.0920, 0.0210], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,928][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0062, 0.1332, 0.3800, 0.0273, 0.1427, 0.3106], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,929][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0127, 0.1100, 0.0641, 0.2960, 0.4526, 0.0646], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,930][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0069, 0.0882, 0.1410, 0.2651, 0.3977, 0.1011], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,932][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0009, 0.1109, 0.1905, 0.2166, 0.2551, 0.2261], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,933][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0215, 0.1517, 0.3170, 0.1311, 0.1799, 0.1988], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,934][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.8891, 0.0497, 0.0221, 0.0158, 0.0097, 0.0136], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,936][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0184, 0.1853, 0.0028, 0.7513, 0.0410, 0.0012], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,937][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([9.7436e-01, 6.3511e-04, 4.1786e-03, 3.4111e-04, 5.3785e-03, 1.5111e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,938][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([2.3962e-05, 4.3705e-05, 7.1721e-02, 9.0527e-03, 1.9874e-02, 8.9928e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:34,939][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0782, 0.1862, 0.0407, 0.2722, 0.1534, 0.0856, 0.1837],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,940][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0248, 0.1712, 0.0728, 0.0697, 0.2385, 0.2977, 0.1252],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,942][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0092, 0.0842, 0.0760, 0.5251, 0.1450, 0.0711, 0.0894],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,943][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0764, 0.2428, 0.1909, 0.0460, 0.0796, 0.3138, 0.0505],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,945][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0053, 0.0305, 0.0140, 0.0673, 0.1170, 0.0047, 0.7612],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,946][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0084, 0.0185, 0.0527, 0.1248, 0.6084, 0.1243, 0.0630],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,947][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0003, 0.1216, 0.2859, 0.0753, 0.1826, 0.1781, 0.1562],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,949][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.1178, 0.0758, 0.1011, 0.0882, 0.2375, 0.1966, 0.1831],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,950][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.5063, 0.1262, 0.0682, 0.0706, 0.0423, 0.0514, 0.1350],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,952][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0116, 0.0513, 0.0021, 0.5019, 0.0020, 0.0005, 0.4306],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,953][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.8382, 0.0214, 0.0128, 0.0060, 0.0176, 0.0217, 0.0822],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,954][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([1.2278e-05, 4.6037e-05, 1.0740e-01, 4.1168e-04, 8.7219e-03, 8.8337e-01,
        3.9995e-05], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:34,955][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0788, 0.1513, 0.0429, 0.2148, 0.1873, 0.1086, 0.1344, 0.0820],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,957][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0127, 0.2198, 0.1060, 0.0764, 0.1154, 0.1871, 0.0536, 0.2289],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,958][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0031, 0.0872, 0.0631, 0.4752, 0.0856, 0.0379, 0.0546, 0.1933],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,959][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0043, 0.1225, 0.2351, 0.0226, 0.1111, 0.3247, 0.0224, 0.1573],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,961][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0046, 0.0374, 0.0213, 0.0698, 0.1639, 0.0168, 0.6625, 0.0237],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,962][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0062, 0.1173, 0.1811, 0.2593, 0.1758, 0.1020, 0.0638, 0.0945],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,963][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ at] are: tensor([2.2549e-04, 6.4196e-02, 9.2262e-02, 9.9605e-02, 1.2718e-01, 8.6115e-02,
        4.9584e-01, 3.4575e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,965][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0238, 0.1251, 0.1704, 0.0877, 0.1749, 0.1591, 0.1179, 0.1411],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,966][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.8517, 0.0420, 0.0179, 0.0130, 0.0078, 0.0104, 0.0467, 0.0105],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,967][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ at] are: tensor([2.6194e-03, 6.4643e-02, 1.8723e-03, 1.1888e-01, 8.2331e-03, 6.2528e-04,
        7.9818e-01, 4.9488e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,968][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.6718, 0.0124, 0.0321, 0.0033, 0.0470, 0.1261, 0.0429, 0.0644],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,969][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ at] are: tensor([1.5103e-06, 1.7419e-05, 6.1717e-02, 1.5561e-03, 1.0164e-02, 9.2525e-01,
        1.0843e-04, 1.1875e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:34,971][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0671, 0.0882, 0.0406, 0.1599, 0.1120, 0.1069, 0.0961, 0.0804, 0.2488],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,972][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0088, 0.1779, 0.0562, 0.0760, 0.0706, 0.0999, 0.0670, 0.1447, 0.2989],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,974][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0032, 0.0802, 0.0444, 0.4655, 0.0462, 0.0162, 0.0736, 0.0688, 0.2019],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,974][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0017, 0.0425, 0.0829, 0.0083, 0.0527, 0.1363, 0.0076, 0.0651, 0.6026],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,975][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0101, 0.0404, 0.0273, 0.1013, 0.1536, 0.0161, 0.6133, 0.0286, 0.0093],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,975][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0041, 0.0716, 0.0893, 0.2457, 0.1799, 0.0682, 0.0694, 0.0907, 0.1812],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,975][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0007, 0.0624, 0.1010, 0.1039, 0.1371, 0.1064, 0.2779, 0.0394, 0.1713],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,975][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0215, 0.0643, 0.1288, 0.0558, 0.0820, 0.0974, 0.0980, 0.0813, 0.3708],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,976][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.6848, 0.0729, 0.0340, 0.0253, 0.0178, 0.0256, 0.0781, 0.0249, 0.0367],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,976][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.8317e-03, 1.9596e-02, 8.0494e-04, 8.3101e-02, 7.4855e-03, 2.9359e-04,
        8.8474e-01, 1.8222e-03, 3.2342e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,976][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.8446, 0.0022, 0.0085, 0.0009, 0.0149, 0.0479, 0.0180, 0.0205, 0.0424],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,977][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.1978e-05, 3.7768e-05, 5.7335e-02, 4.0564e-03, 2.1950e-02, 8.8289e-01,
        3.0225e-04, 2.4058e-03, 3.1005e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:34,977][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ station] are: tensor([0.0492, 0.1240, 0.0243, 0.1283, 0.1197, 0.0602, 0.1521, 0.0865, 0.1444,
        0.1114], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,978][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ station] are: tensor([0.0028, 0.0610, 0.0379, 0.0307, 0.0774, 0.1583, 0.0371, 0.1337, 0.2948,
        0.1662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,979][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ station] are: tensor([0.0021, 0.0348, 0.0375, 0.2935, 0.0892, 0.0397, 0.0446, 0.0745, 0.2372,
        0.1470], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,981][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ station] are: tensor([0.0044, 0.0480, 0.0888, 0.0114, 0.0559, 0.1875, 0.0130, 0.0636, 0.4299,
        0.0976], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,982][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ station] are: tensor([0.0056, 0.0100, 0.0099, 0.0177, 0.0458, 0.0052, 0.0934, 0.0092, 0.0028,
        0.8005], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,983][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ station] are: tensor([0.0012, 0.0187, 0.0449, 0.0825, 0.1912, 0.1107, 0.0200, 0.0687, 0.1700,
        0.2922], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,984][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ station] are: tensor([0.0004, 0.0512, 0.0962, 0.0335, 0.0806, 0.1000, 0.0426, 0.0265, 0.2162,
        0.3529], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,985][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ station] are: tensor([0.0103, 0.0192, 0.0453, 0.0265, 0.0997, 0.0708, 0.0450, 0.0599, 0.2732,
        0.3502], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,987][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ station] are: tensor([0.8553, 0.0357, 0.0145, 0.0104, 0.0058, 0.0091, 0.0410, 0.0085, 0.0130,
        0.0067], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,988][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ station] are: tensor([3.1106e-03, 1.4846e-02, 5.5421e-04, 5.1760e-02, 2.8110e-03, 4.7280e-04,
        1.4604e-01, 1.4926e-03, 4.9745e-04, 7.7841e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,989][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ station] are: tensor([0.8879, 0.0024, 0.0081, 0.0012, 0.0156, 0.0257, 0.0178, 0.0097, 0.0233,
        0.0083], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,990][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ station] are: tensor([2.7252e-05, 5.8000e-05, 1.1125e-01, 1.3963e-03, 9.4440e-03, 8.2427e-01,
        4.9796e-05, 1.2010e-03, 5.1273e-02, 1.0346e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:34,991][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0326, 0.0869, 0.0334, 0.2000, 0.0891, 0.0937, 0.0699, 0.0723, 0.1984,
        0.0672, 0.0565], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,993][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0029, 0.1568, 0.0356, 0.0467, 0.0410, 0.0733, 0.0367, 0.0857, 0.2209,
        0.1635, 0.1369], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,994][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0012, 0.0658, 0.0164, 0.3653, 0.0260, 0.0077, 0.0312, 0.0301, 0.0896,
        0.3333, 0.0333], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,996][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0027, 0.0601, 0.0926, 0.0123, 0.0301, 0.1157, 0.0102, 0.0581, 0.3749,
        0.2084, 0.0349], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,997][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0052, 0.0125, 0.0116, 0.0368, 0.0527, 0.0057, 0.1198, 0.0095, 0.0036,
        0.7245, 0.0181], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:34,999][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0008, 0.0256, 0.0374, 0.1237, 0.1286, 0.0574, 0.0216, 0.0469, 0.1388,
        0.3662, 0.0529], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,000][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [,] are: tensor([3.7196e-04, 3.0830e-02, 3.8549e-02, 4.8853e-02, 6.1954e-02, 4.1898e-02,
        8.6993e-02, 1.8002e-02, 7.9946e-02, 5.1217e-01, 8.0436e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,001][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0046, 0.0443, 0.0604, 0.0361, 0.0405, 0.0491, 0.0457, 0.0327, 0.2197,
        0.4174, 0.0495], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,002][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.6181, 0.0756, 0.0389, 0.0265, 0.0171, 0.0251, 0.0784, 0.0287, 0.0345,
        0.0224, 0.0348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,003][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [,] are: tensor([2.3543e-04, 2.1570e-02, 4.6799e-04, 5.4140e-02, 5.5028e-03, 2.8433e-04,
        1.5244e-01, 1.5084e-03, 4.4135e-04, 7.6188e-01, 1.5251e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,005][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.5844, 0.0049, 0.0152, 0.0038, 0.0252, 0.0545, 0.0241, 0.0304, 0.0456,
        0.0202, 0.1918], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,006][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [,] are: tensor([1.4417e-05, 1.8627e-05, 4.1777e-02, 2.9000e-03, 1.4580e-02, 4.9249e-01,
        8.6267e-05, 1.4287e-03, 3.0046e-02, 3.3407e-03, 4.1332e-01],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,007][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Lindsay] are: tensor([0.0119, 0.0585, 0.0178, 0.1364, 0.1146, 0.0625, 0.0668, 0.0543, 0.1114,
        0.0870, 0.0377, 0.2411], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,009][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Lindsay] are: tensor([0.0023, 0.0450, 0.0182, 0.0100, 0.0507, 0.0930, 0.0368, 0.1244, 0.1877,
        0.1887, 0.1222, 0.1210], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,010][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Lindsay] are: tensor([0.0014, 0.0262, 0.0186, 0.0727, 0.0438, 0.0169, 0.0311, 0.0607, 0.1241,
        0.2621, 0.0703, 0.2721], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,011][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Lindsay] are: tensor([0.0013, 0.0474, 0.1094, 0.0077, 0.0287, 0.1324, 0.0096, 0.0734, 0.3458,
        0.1477, 0.0602, 0.0363], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,013][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Lindsay] are: tensor([0.0055, 0.0174, 0.0138, 0.0349, 0.0321, 0.0046, 0.0670, 0.0080, 0.0026,
        0.7611, 0.0136, 0.0394], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,014][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Lindsay] are: tensor([0.0035, 0.0467, 0.0522, 0.0838, 0.1580, 0.0603, 0.0203, 0.0473, 0.1322,
        0.2271, 0.0328, 0.1358], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,016][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Lindsay] are: tensor([0.0005, 0.0558, 0.1088, 0.0694, 0.1108, 0.0905, 0.0376, 0.0188, 0.1269,
        0.2706, 0.0725, 0.0377], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,017][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Lindsay] are: tensor([0.0031, 0.0130, 0.0202, 0.0166, 0.0338, 0.0387, 0.0270, 0.0293, 0.1254,
        0.5348, 0.0473, 0.1108], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,018][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Lindsay] are: tensor([0.6554, 0.0671, 0.0295, 0.0223, 0.0122, 0.0204, 0.0846, 0.0212, 0.0304,
        0.0199, 0.0244, 0.0126], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,020][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Lindsay] are: tensor([0.0039, 0.0382, 0.0039, 0.1954, 0.0040, 0.0025, 0.0695, 0.0015, 0.0025,
        0.3636, 0.0047, 0.3104], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,021][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Lindsay] are: tensor([7.9639e-01, 6.6266e-04, 6.5235e-03, 8.7063e-04, 1.0736e-02, 2.8433e-02,
        1.0498e-02, 1.0567e-02, 2.4308e-02, 8.5541e-03, 9.9908e-02, 2.5535e-03],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,022][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Lindsay] are: tensor([2.2691e-05, 2.3697e-04, 9.1277e-02, 4.8390e-03, 1.2309e-02, 5.7687e-01,
        1.3132e-04, 9.2766e-04, 6.6059e-02, 1.2561e-02, 2.3475e-01, 1.4849e-05],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,023][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ decided] are: tensor([0.0493, 0.1000, 0.0133, 0.1424, 0.0772, 0.0315, 0.1087, 0.0391, 0.0685,
        0.0786, 0.0384, 0.1874, 0.0657], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,025][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ decided] are: tensor([0.0028, 0.0502, 0.0312, 0.0249, 0.0530, 0.0664, 0.0190, 0.1034, 0.1372,
        0.1627, 0.1015, 0.1130, 0.1348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,026][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ decided] are: tensor([0.0010, 0.0312, 0.0174, 0.1132, 0.0367, 0.0108, 0.0193, 0.0530, 0.1058,
        0.2631, 0.0377, 0.2897, 0.0210], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,027][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ decided] are: tensor([0.0139, 0.0492, 0.0743, 0.0110, 0.0279, 0.1132, 0.0099, 0.0660, 0.3796,
        0.1314, 0.0483, 0.0288, 0.0466], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,029][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ decided] are: tensor([0.0030, 0.0108, 0.0057, 0.0241, 0.0232, 0.0022, 0.0807, 0.0044, 0.0013,
        0.6720, 0.0082, 0.0269, 0.1375], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,030][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ decided] are: tensor([0.0027, 0.0298, 0.0466, 0.0511, 0.0895, 0.0550, 0.0197, 0.0691, 0.1251,
        0.2586, 0.0358, 0.1138, 0.1033], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,032][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ decided] are: tensor([0.0004, 0.0354, 0.0599, 0.0388, 0.0706, 0.0516, 0.0604, 0.0191, 0.0955,
        0.2721, 0.0817, 0.0253, 0.1892], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,032][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ decided] are: tensor([0.0061, 0.0224, 0.0417, 0.0252, 0.0478, 0.0441, 0.0279, 0.0385, 0.2170,
        0.2957, 0.0460, 0.0879, 0.0996], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,032][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ decided] are: tensor([0.8805, 0.0280, 0.0111, 0.0079, 0.0040, 0.0056, 0.0295, 0.0058, 0.0099,
        0.0047, 0.0070, 0.0032, 0.0027], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,033][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ decided] are: tensor([0.0029, 0.0147, 0.0011, 0.0742, 0.0028, 0.0009, 0.0667, 0.0008, 0.0007,
        0.6298, 0.0026, 0.1864, 0.0164], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,033][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ decided] are: tensor([0.7165, 0.0021, 0.0061, 0.0015, 0.0125, 0.0256, 0.0151, 0.0088, 0.0209,
        0.0059, 0.0576, 0.0015, 0.1258], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,034][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ decided] are: tensor([1.1393e-04, 2.7343e-05, 4.8969e-02, 1.4929e-03, 9.0687e-03, 5.6872e-01,
        8.4275e-05, 1.3812e-03, 4.0925e-02, 1.8346e-03, 3.2565e-01, 2.1343e-06,
        1.7383e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,034][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0317, 0.0576, 0.0238, 0.1201, 0.0762, 0.0596, 0.0679, 0.0598, 0.1300,
        0.0583, 0.0516, 0.1580, 0.0483, 0.0571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,034][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0044, 0.0827, 0.0242, 0.0203, 0.0350, 0.0624, 0.0247, 0.0700, 0.1762,
        0.0871, 0.1083, 0.1025, 0.0916, 0.1107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,035][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0012, 0.0376, 0.0172, 0.1410, 0.0238, 0.0076, 0.0158, 0.0264, 0.0785,
        0.1815, 0.0303, 0.2924, 0.0217, 0.1251], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,035][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0046, 0.0484, 0.0654, 0.0066, 0.0490, 0.1004, 0.0102, 0.0422, 0.3318,
        0.1440, 0.0346, 0.0242, 0.0672, 0.0715], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,036][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0037, 0.0083, 0.0076, 0.0216, 0.0380, 0.0048, 0.0816, 0.0084, 0.0030,
        0.5866, 0.0124, 0.0297, 0.1608, 0.0336], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,038][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0013, 0.0256, 0.0595, 0.0910, 0.0675, 0.0504, 0.0153, 0.0399, 0.1107,
        0.1858, 0.0573, 0.1464, 0.0528, 0.0967], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,039][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([3.6649e-04, 1.9498e-02, 3.4268e-02, 2.8582e-02, 4.2483e-02, 3.0621e-02,
        8.4040e-02, 1.0236e-02, 4.9374e-02, 3.6878e-01, 6.3653e-02, 3.2368e-02,
        2.0077e-01, 3.4954e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,040][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0041, 0.0312, 0.0548, 0.0192, 0.0345, 0.0425, 0.0373, 0.0266, 0.1575,
        0.3139, 0.0502, 0.0736, 0.0719, 0.0828], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,041][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.7170, 0.0541, 0.0243, 0.0186, 0.0097, 0.0148, 0.0622, 0.0158, 0.0230,
        0.0132, 0.0200, 0.0096, 0.0074, 0.0103], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,042][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([4.6595e-04, 3.2845e-02, 2.2189e-03, 4.0193e-02, 5.9045e-03, 1.0022e-03,
        1.5831e-01, 1.7786e-03, 1.0882e-03, 5.9015e-01, 3.8448e-03, 1.2809e-01,
        2.6913e-02, 7.1921e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,043][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.5990, 0.0023, 0.0052, 0.0010, 0.0090, 0.0225, 0.0149, 0.0119, 0.0172,
        0.0082, 0.0771, 0.0010, 0.0983, 0.1324], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,044][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([2.8411e-05, 6.0799e-06, 1.7168e-02, 1.1567e-03, 6.4603e-03, 4.6986e-01,
        2.4703e-04, 1.6052e-03, 2.2627e-02, 6.1326e-03, 4.2879e-01, 6.8890e-06,
        2.7746e-03, 4.3139e-02], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,045][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ give] are: tensor([0.0350, 0.0598, 0.0139, 0.0893, 0.0804, 0.0410, 0.1002, 0.0464, 0.0942,
        0.0555, 0.0416, 0.1394, 0.0558, 0.0465, 0.1010], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,047][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ give] are: tensor([0.0050, 0.1031, 0.0181, 0.0286, 0.0348, 0.0591, 0.0227, 0.0466, 0.1800,
        0.0999, 0.0668, 0.1165, 0.0840, 0.0824, 0.0523], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,048][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ give] are: tensor([0.0008, 0.0474, 0.0107, 0.2033, 0.0267, 0.0054, 0.0177, 0.0215, 0.0620,
        0.1351, 0.0241, 0.3304, 0.0139, 0.0728, 0.0281], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,050][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ give] are: tensor([0.0040, 0.0325, 0.0597, 0.0063, 0.0272, 0.1288, 0.0148, 0.0405, 0.3985,
        0.0931, 0.0348, 0.0224, 0.0489, 0.0696, 0.0190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,051][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ give] are: tensor([0.0044, 0.0076, 0.0059, 0.0152, 0.0312, 0.0032, 0.0734, 0.0048, 0.0017,
        0.5153, 0.0073, 0.0180, 0.1419, 0.0257, 0.1444], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,053][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ give] are: tensor([0.0046, 0.0673, 0.0628, 0.0924, 0.0755, 0.0361, 0.0187, 0.0322, 0.0982,
        0.1696, 0.0360, 0.1024, 0.0521, 0.0701, 0.0818], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,054][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ give] are: tensor([0.0004, 0.0229, 0.0373, 0.0230, 0.0439, 0.0366, 0.0637, 0.0143, 0.0645,
        0.3335, 0.0660, 0.0212, 0.1667, 0.0418, 0.0643], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,055][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ give] are: tensor([0.0058, 0.0439, 0.0568, 0.0308, 0.0517, 0.0423, 0.0264, 0.0294, 0.2200,
        0.1920, 0.0419, 0.0537, 0.0934, 0.0722, 0.0397], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,057][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ give] are: tensor([0.8134, 0.0382, 0.0162, 0.0109, 0.0056, 0.0086, 0.0417, 0.0095, 0.0146,
        0.0084, 0.0124, 0.0050, 0.0045, 0.0067, 0.0042], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,058][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ give] are: tensor([2.2964e-03, 9.1526e-03, 3.0219e-04, 3.3782e-02, 1.2902e-03, 1.1245e-04,
        2.1674e-01, 6.1964e-04, 1.6342e-04, 5.6799e-01, 6.8780e-04, 1.3274e-01,
        2.7113e-02, 2.1730e-03, 4.8257e-03], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,059][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ give] are: tensor([0.4790, 0.0095, 0.0088, 0.0055, 0.0165, 0.0329, 0.0218, 0.0128, 0.0350,
        0.0104, 0.0731, 0.0037, 0.1179, 0.1395, 0.0336], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,060][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ give] are: tensor([1.4244e-05, 2.2181e-05, 2.3979e-02, 9.7935e-04, 4.1905e-03, 5.9365e-01,
        1.8184e-04, 1.3474e-03, 4.0613e-02, 3.1667e-03, 2.7478e-01, 4.0375e-06,
        2.2504e-03, 5.4258e-02, 5.5999e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,062][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ it] are: tensor([0.0456, 0.0675, 0.0137, 0.0744, 0.0807, 0.0380, 0.1083, 0.0398, 0.0903,
        0.0486, 0.0391, 0.1084, 0.0659, 0.0368, 0.0657, 0.0772],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,063][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ it] are: tensor([0.0023, 0.0356, 0.0192, 0.0195, 0.0449, 0.0750, 0.0239, 0.0614, 0.1177,
        0.0739, 0.0874, 0.0892, 0.0856, 0.0981, 0.0655, 0.1007],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,064][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ it] are: tensor([0.0020, 0.0182, 0.0198, 0.0815, 0.0458, 0.0114, 0.0177, 0.0325, 0.0768,
        0.1275, 0.0442, 0.1707, 0.0151, 0.1282, 0.0396, 0.1692],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,066][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ it] are: tensor([0.0117, 0.0329, 0.0433, 0.0068, 0.0329, 0.0858, 0.0122, 0.0546, 0.2859,
        0.1191, 0.0292, 0.0236, 0.0615, 0.0675, 0.0303, 0.1027],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,067][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ it] are: tensor([0.0021, 0.0052, 0.0034, 0.0119, 0.0164, 0.0024, 0.0622, 0.0041, 0.0016,
        0.5966, 0.0071, 0.0199, 0.1083, 0.0235, 0.1189, 0.0164],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,069][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ it] are: tensor([0.0026, 0.0277, 0.0458, 0.0674, 0.0610, 0.0423, 0.0177, 0.0357, 0.0861,
        0.1579, 0.0375, 0.1096, 0.0372, 0.0653, 0.0701, 0.1361],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,070][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ it] are: tensor([1.8300e-04, 1.4803e-02, 2.8592e-02, 1.7033e-02, 2.9274e-02, 2.9426e-02,
        5.3937e-02, 1.2702e-02, 6.5047e-02, 3.2225e-01, 6.2255e-02, 2.1205e-02,
        1.2352e-01, 3.7390e-02, 5.7524e-02, 1.2486e-01], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,071][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ it] are: tensor([0.0031, 0.0196, 0.0497, 0.0338, 0.0654, 0.0495, 0.0254, 0.0223, 0.1055,
        0.1712, 0.0435, 0.0779, 0.0382, 0.0653, 0.0321, 0.1975],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,073][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ it] are: tensor([0.8320, 0.0345, 0.0132, 0.0100, 0.0057, 0.0081, 0.0415, 0.0079, 0.0127,
        0.0071, 0.0090, 0.0039, 0.0032, 0.0050, 0.0033, 0.0029],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,074][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ it] are: tensor([1.5701e-03, 1.5163e-02, 4.9613e-04, 1.7657e-02, 3.3317e-03, 3.9025e-04,
        1.3709e-01, 1.0241e-03, 4.5421e-04, 6.9500e-01, 1.4749e-03, 9.3705e-02,
        1.3239e-02, 6.8268e-03, 1.1883e-02, 7.0224e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,075][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ it] are: tensor([0.5686, 0.0009, 0.0049, 0.0009, 0.0083, 0.0202, 0.0108, 0.0093, 0.0189,
        0.0063, 0.0772, 0.0012, 0.1009, 0.1346, 0.0141, 0.0229],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,076][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ it] are: tensor([1.8506e-05, 1.4688e-05, 2.8716e-02, 1.3091e-03, 8.4869e-03, 6.0179e-01,
        2.6657e-04, 1.3268e-03, 2.0237e-02, 4.4986e-03, 2.6427e-01, 3.9842e-06,
        2.6124e-03, 6.5030e-02, 8.0579e-04, 6.1045e-04], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,077][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0186, 0.0541, 0.0178, 0.1066, 0.0612, 0.0445, 0.0477, 0.0461, 0.0988,
        0.0483, 0.0343, 0.1273, 0.0380, 0.0413, 0.0593, 0.1097, 0.0464],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,079][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0029, 0.0493, 0.0195, 0.0138, 0.0275, 0.0463, 0.0164, 0.0602, 0.1324,
        0.0544, 0.0941, 0.0676, 0.0677, 0.0955, 0.0491, 0.1232, 0.0800],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,080][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0011, 0.0308, 0.0141, 0.1107, 0.0218, 0.0065, 0.0136, 0.0202, 0.0554,
        0.1112, 0.0238, 0.2085, 0.0147, 0.0919, 0.0292, 0.1717, 0.0747],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,082][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0054, 0.0501, 0.0493, 0.0068, 0.0501, 0.0787, 0.0092, 0.0376, 0.2300,
        0.1250, 0.0264, 0.0228, 0.0568, 0.0514, 0.0368, 0.1254, 0.0383],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,083][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0031, 0.0057, 0.0046, 0.0125, 0.0240, 0.0029, 0.0635, 0.0050, 0.0018,
        0.5400, 0.0086, 0.0152, 0.1096, 0.0216, 0.1168, 0.0174, 0.0479],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,085][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0011, 0.0181, 0.0575, 0.0723, 0.0503, 0.0420, 0.0096, 0.0280, 0.0919,
        0.1182, 0.0427, 0.1017, 0.0342, 0.0687, 0.0517, 0.1508, 0.0611],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,086][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0004, 0.0168, 0.0262, 0.0209, 0.0325, 0.0253, 0.0693, 0.0090, 0.0468,
        0.2989, 0.0518, 0.0247, 0.1676, 0.0296, 0.0489, 0.1060, 0.0253],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,088][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0040, 0.0277, 0.0422, 0.0173, 0.0306, 0.0311, 0.0263, 0.0208, 0.0852,
        0.1875, 0.0404, 0.0546, 0.0506, 0.0614, 0.0313, 0.2228, 0.0663],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,089][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.6729, 0.0569, 0.0256, 0.0213, 0.0111, 0.0159, 0.0615, 0.0171, 0.0253,
        0.0138, 0.0201, 0.0107, 0.0077, 0.0109, 0.0085, 0.0075, 0.0132],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,090][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0007, 0.0288, 0.0021, 0.0377, 0.0055, 0.0009, 0.1286, 0.0018, 0.0009,
        0.6432, 0.0033, 0.1032, 0.0202, 0.0076, 0.0112, 0.0021, 0.0022],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,090][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.4669, 0.0029, 0.0051, 0.0012, 0.0103, 0.0204, 0.0135, 0.0115, 0.0173,
        0.0090, 0.0837, 0.0013, 0.0902, 0.1395, 0.0189, 0.0261, 0.0820],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,091][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([9.8157e-06, 5.1936e-06, 1.5421e-02, 6.1603e-04, 5.2266e-03, 4.6026e-01,
        1.4713e-04, 1.3033e-03, 2.4238e-02, 2.6357e-03, 4.2701e-01, 3.0335e-06,
        1.4949e-03, 4.1865e-02, 2.9049e-04, 4.7704e-04, 1.9003e-02],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,139][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:21:35,140][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,141][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,143][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,144][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,145][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,146][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,147][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,148][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,148][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,148][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,149][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,149][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [When] are: tensor([1.], device='cuda:0') for source tokens [When]
[2024-07-24 10:21:35,149][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Benjamin] are: tensor([0.2861, 0.7139], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,150][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Benjamin] are: tensor([0.3995, 0.6005], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,150][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Benjamin] are: tensor([0.1174, 0.8826], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,150][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Benjamin] are: tensor([0.5901, 0.4099], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,151][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Benjamin] are: tensor([0.0891, 0.9109], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,151][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Benjamin] are: tensor([0.2223, 0.7777], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,152][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Benjamin] are: tensor([0.2179, 0.7821], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,153][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Benjamin] are: tensor([0.3673, 0.6327], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,154][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Benjamin] are: tensor([0.1078, 0.8922], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,156][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Benjamin] are: tensor([0.1427, 0.8573], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,157][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Benjamin] are: tensor([0.0240, 0.9760], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,158][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Benjamin] are: tensor([0.0747, 0.9253], device='cuda:0') for source tokens [When Benjamin]
[2024-07-24 10:21:35,159][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.1338, 0.6062, 0.2600], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,160][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0593, 0.6784, 0.2623], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,162][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0287, 0.5270, 0.4443], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,163][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0255, 0.3314, 0.6432], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,164][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0285, 0.2253, 0.7462], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,166][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0258, 0.3118, 0.6624], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,167][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0631, 0.6903, 0.2466], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,169][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.1031, 0.4811, 0.4158], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,170][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0099, 0.3418, 0.6483], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,171][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0315, 0.7536, 0.2149], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,173][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0124, 0.7656, 0.2220], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,174][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0383, 0.9209, 0.0407], device='cuda:0') for source tokens [When Benjamin and]
[2024-07-24 10:21:35,175][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0527, 0.1985, 0.1053, 0.6435], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,177][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.1118, 0.5854, 0.1993, 0.1036], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,178][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0222, 0.3735, 0.2094, 0.3950], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,179][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0319, 0.3096, 0.6234, 0.0351], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,181][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0163, 0.1751, 0.6439, 0.1647], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,182][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0475, 0.2138, 0.3211, 0.4176], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,184][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0741, 0.4416, 0.1520, 0.3323], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,185][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.2131, 0.3147, 0.2380, 0.2343], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,186][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0082, 0.3859, 0.4504, 0.1555], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,187][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0279, 0.6244, 0.1382, 0.2095], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,189][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0077, 0.5386, 0.1500, 0.3038], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,190][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0132, 0.8353, 0.0575, 0.0941], device='cuda:0') for source tokens [When Benjamin and Lindsay]
[2024-07-24 10:21:35,192][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ got] are: tensor([0.1948, 0.1778, 0.0416, 0.3051, 0.2808], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,193][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ got] are: tensor([0.0270, 0.4286, 0.1509, 0.1243, 0.2692], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,194][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ got] are: tensor([0.0063, 0.3423, 0.1319, 0.3282, 0.1912], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,196][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ got] are: tensor([0.0410, 0.2898, 0.5270, 0.0517, 0.0904], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,197][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ got] are: tensor([0.0231, 0.1872, 0.3159, 0.1640, 0.3098], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,198][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ got] are: tensor([0.0242, 0.1517, 0.2833, 0.3443, 0.1965], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,200][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ got] are: tensor([0.0292, 0.2906, 0.1234, 0.3089, 0.2478], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,201][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ got] are: tensor([0.0725, 0.2579, 0.2252, 0.1272, 0.3172], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,202][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ got] are: tensor([0.0458, 0.2747, 0.3061, 0.1103, 0.2630], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,204][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ got] are: tensor([0.0685, 0.3639, 0.1264, 0.2534, 0.1878], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,205][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ got] are: tensor([0.0055, 0.2466, 0.0924, 0.3117, 0.3437], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,207][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ got] are: tensor([0.0702, 0.6913, 0.0544, 0.0318, 0.1523], device='cuda:0') for source tokens [When Benjamin and Lindsay got]
[2024-07-24 10:21:35,207][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0953, 0.1260, 0.0812, 0.2936, 0.2150, 0.1890], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,207][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0118, 0.3553, 0.1191, 0.1184, 0.1626, 0.2329], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,208][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0044, 0.2717, 0.1950, 0.3441, 0.1222, 0.0626], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,208][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0062, 0.1332, 0.3800, 0.0273, 0.1427, 0.3106], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,208][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0090, 0.1532, 0.2214, 0.1462, 0.2862, 0.1840], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,209][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0069, 0.0882, 0.1410, 0.2651, 0.3977, 0.1011], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,209][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0124, 0.2745, 0.1251, 0.2914, 0.2460, 0.0506], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,209][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0215, 0.1517, 0.3170, 0.1311, 0.1799, 0.1988], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,209][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0285, 0.2422, 0.1926, 0.0837, 0.2273, 0.2257], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,210][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0225, 0.4430, 0.1117, 0.1689, 0.2183, 0.0356], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,210][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0025, 0.4427, 0.0734, 0.2158, 0.2129, 0.0527], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,212][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0302, 0.6259, 0.0641, 0.0338, 0.2126, 0.0335], device='cuda:0') for source tokens [When Benjamin and Lindsay got a]
[2024-07-24 10:21:35,213][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0782, 0.1862, 0.0407, 0.2722, 0.1534, 0.0856, 0.1837],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,214][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0248, 0.1712, 0.0728, 0.0697, 0.2385, 0.2977, 0.1252],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,216][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0159, 0.1713, 0.1553, 0.2603, 0.1961, 0.1209, 0.0802],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,217][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0764, 0.2428, 0.1909, 0.0460, 0.0796, 0.3138, 0.0505],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,218][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0096, 0.0559, 0.2038, 0.0721, 0.2099, 0.4306, 0.0181],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,220][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0084, 0.0185, 0.0527, 0.1248, 0.6084, 0.1243, 0.0630],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,221][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0098, 0.1699, 0.1150, 0.1025, 0.3455, 0.1082, 0.1491],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,223][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.1178, 0.0758, 0.1011, 0.0882, 0.2375, 0.1966, 0.1831],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,224][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0059, 0.1978, 0.1352, 0.1319, 0.2975, 0.1858, 0.0459],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,225][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0629, 0.1287, 0.1450, 0.0852, 0.3792, 0.1029, 0.0961],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,227][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0029, 0.5734, 0.0161, 0.3079, 0.0610, 0.0114, 0.0274],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,228][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0261, 0.6500, 0.0303, 0.0377, 0.1795, 0.0548, 0.0216],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack]
[2024-07-24 10:21:35,229][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0788, 0.1513, 0.0429, 0.2148, 0.1873, 0.1086, 0.1344, 0.0820],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,230][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0127, 0.2198, 0.1060, 0.0764, 0.1154, 0.1871, 0.0536, 0.2289],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,232][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0023, 0.1460, 0.1038, 0.2312, 0.0782, 0.0756, 0.0387, 0.3243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,233][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0043, 0.1225, 0.2351, 0.0226, 0.1111, 0.3247, 0.0224, 0.1573],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,235][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0110, 0.1591, 0.1945, 0.1039, 0.1856, 0.2057, 0.0159, 0.1243],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,236][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0062, 0.1173, 0.1811, 0.2593, 0.1758, 0.1020, 0.0638, 0.0945],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,237][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0117, 0.2696, 0.0877, 0.2105, 0.0999, 0.0440, 0.1667, 0.1099],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,239][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0238, 0.1251, 0.1704, 0.0877, 0.1749, 0.1591, 0.1179, 0.1411],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,240][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0227, 0.1867, 0.1395, 0.0656, 0.1748, 0.1401, 0.0990, 0.1715],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,241][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0254, 0.5215, 0.0796, 0.1403, 0.0596, 0.0190, 0.0883, 0.0663],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,243][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0018, 0.4667, 0.0435, 0.2553, 0.1308, 0.0421, 0.0196, 0.0402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,244][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0092, 0.6753, 0.0634, 0.0320, 0.1653, 0.0264, 0.0161, 0.0122],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at]
[2024-07-24 10:21:35,245][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0671, 0.0882, 0.0406, 0.1599, 0.1120, 0.1069, 0.0961, 0.0804, 0.2488],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,247][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0088, 0.1779, 0.0562, 0.0760, 0.0706, 0.0999, 0.0670, 0.1447, 0.2989],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,248][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0019, 0.1151, 0.0700, 0.1785, 0.0407, 0.0330, 0.0333, 0.1104, 0.4171],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,250][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0017, 0.0425, 0.0829, 0.0083, 0.0527, 0.1363, 0.0076, 0.0651, 0.6026],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,251][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0059, 0.0824, 0.1135, 0.0823, 0.0981, 0.1169, 0.0107, 0.0698, 0.4203],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,253][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0041, 0.0716, 0.0893, 0.2457, 0.1799, 0.0682, 0.0694, 0.0907, 0.1812],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,254][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0097, 0.2219, 0.0576, 0.2000, 0.1064, 0.0269, 0.0980, 0.0832, 0.1963],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,255][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0215, 0.0643, 0.1288, 0.0558, 0.0820, 0.0974, 0.0980, 0.0813, 0.3708],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,257][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0101, 0.1343, 0.1020, 0.0381, 0.1644, 0.1779, 0.0543, 0.1825, 0.1364],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,258][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0166, 0.2857, 0.0687, 0.1461, 0.0998, 0.0203, 0.1425, 0.1054, 0.1148],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,260][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0021, 0.5055, 0.0437, 0.1747, 0.1232, 0.0417, 0.0117, 0.0340, 0.0634],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,261][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0221, 0.6586, 0.0398, 0.0282, 0.1638, 0.0237, 0.0192, 0.0089, 0.0357],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the]
[2024-07-24 10:21:35,262][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ station] are: tensor([0.0492, 0.1240, 0.0243, 0.1283, 0.1197, 0.0602, 0.1521, 0.0865, 0.1444,
        0.1114], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,264][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ station] are: tensor([0.0028, 0.0610, 0.0379, 0.0307, 0.0774, 0.1583, 0.0371, 0.1337, 0.2948,
        0.1662], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,265][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ station] are: tensor([0.0011, 0.0717, 0.0564, 0.1426, 0.0695, 0.0604, 0.0238, 0.1007, 0.3531,
        0.1206], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,266][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ station] are: tensor([0.0044, 0.0480, 0.0888, 0.0114, 0.0559, 0.1875, 0.0130, 0.0636, 0.4299,
        0.0976], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,266][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ station] are: tensor([0.0021, 0.0497, 0.1085, 0.0819, 0.0961, 0.1533, 0.0063, 0.0657, 0.3621,
        0.0742], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,266][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ station] are: tensor([0.0012, 0.0187, 0.0449, 0.0825, 0.1912, 0.1107, 0.0200, 0.0687, 0.1700,
        0.2922], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,267][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ station] are: tensor([0.0025, 0.0820, 0.0446, 0.1259, 0.0852, 0.0354, 0.0687, 0.0804, 0.1921,
        0.2832], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,267][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ station] are: tensor([0.0103, 0.0192, 0.0453, 0.0265, 0.0997, 0.0708, 0.0450, 0.0599, 0.2732,
        0.3502], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,267][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ station] are: tensor([0.0298, 0.1453, 0.0916, 0.0429, 0.1096, 0.1549, 0.0976, 0.1576, 0.1079,
        0.0628], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,268][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ station] are: tensor([0.0063, 0.1627, 0.0551, 0.0754, 0.0958, 0.0286, 0.0639, 0.0710, 0.0864,
        0.3548], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,268][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ station] are: tensor([0.0007, 0.4586, 0.0265, 0.2649, 0.0725, 0.0237, 0.0067, 0.0154, 0.0348,
        0.0961], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,268][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ station] are: tensor([0.0092, 0.6103, 0.0608, 0.0335, 0.1668, 0.0271, 0.0100, 0.0106, 0.0346,
        0.0371], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station]
[2024-07-24 10:21:35,269][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0326, 0.0869, 0.0334, 0.2000, 0.0891, 0.0937, 0.0699, 0.0723, 0.1984,
        0.0672, 0.0565], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,271][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0029, 0.1568, 0.0356, 0.0467, 0.0410, 0.0733, 0.0367, 0.0857, 0.2209,
        0.1635, 0.1369], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,272][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0011, 0.1235, 0.0352, 0.1785, 0.0277, 0.0181, 0.0199, 0.0540, 0.1905,
        0.2614, 0.0902], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,273][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.0027, 0.0601, 0.0926, 0.0123, 0.0301, 0.1157, 0.0102, 0.0581, 0.3749,
        0.2084, 0.0349], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,275][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0038, 0.0614, 0.0712, 0.0572, 0.0437, 0.1364, 0.0071, 0.0435, 0.4479,
        0.0676, 0.0602], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,276][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.0008, 0.0256, 0.0374, 0.1237, 0.1286, 0.0574, 0.0216, 0.0469, 0.1388,
        0.3662, 0.0529], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,277][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0023, 0.1331, 0.0282, 0.1372, 0.0534, 0.0147, 0.0488, 0.0408, 0.1368,
        0.3387, 0.0658], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,278][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.0046, 0.0443, 0.0604, 0.0361, 0.0405, 0.0491, 0.0457, 0.0327, 0.2197,
        0.4174, 0.0495], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,280][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0023, 0.1003, 0.1067, 0.0276, 0.1368, 0.1173, 0.0194, 0.1892, 0.0702,
        0.0678, 0.1623], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,281][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0051, 0.3138, 0.0302, 0.0895, 0.0406, 0.0083, 0.0397, 0.0473, 0.0455,
        0.3587, 0.0213], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,282][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0011, 0.4930, 0.0264, 0.2494, 0.0725, 0.0257, 0.0072, 0.0176, 0.0308,
        0.0446, 0.0317], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,284][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0085, 0.7072, 0.0411, 0.0338, 0.1262, 0.0131, 0.0123, 0.0075, 0.0175,
        0.0300, 0.0029], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station,]
[2024-07-24 10:21:35,285][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Lindsay] are: tensor([0.0119, 0.0585, 0.0178, 0.1364, 0.1146, 0.0625, 0.0668, 0.0543, 0.1114,
        0.0870, 0.0377, 0.2411], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,287][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Lindsay] are: tensor([0.0023, 0.0450, 0.0182, 0.0100, 0.0507, 0.0930, 0.0368, 0.1244, 0.1877,
        0.1887, 0.1222, 0.1210], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,288][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Lindsay] are: tensor([0.0006, 0.0533, 0.0284, 0.0444, 0.0325, 0.0227, 0.0169, 0.0753, 0.1722,
        0.2297, 0.1025, 0.2216], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,290][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Lindsay] are: tensor([0.0013, 0.0474, 0.1094, 0.0077, 0.0287, 0.1324, 0.0096, 0.0734, 0.3458,
        0.1477, 0.0602, 0.0363], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,291][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Lindsay] are: tensor([0.0018, 0.0263, 0.0803, 0.0368, 0.0348, 0.1070, 0.0058, 0.0431, 0.3760,
        0.0946, 0.0729, 0.1206], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,293][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Lindsay] are: tensor([0.0035, 0.0467, 0.0522, 0.0838, 0.1580, 0.0603, 0.0203, 0.0473, 0.1322,
        0.2271, 0.0328, 0.1358], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,294][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Lindsay] are: tensor([0.0017, 0.0569, 0.0177, 0.0555, 0.0814, 0.0120, 0.0591, 0.0374, 0.0778,
        0.3384, 0.0437, 0.2185], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,295][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Lindsay] are: tensor([0.0031, 0.0130, 0.0202, 0.0166, 0.0338, 0.0387, 0.0270, 0.0293, 0.1254,
        0.5348, 0.0473, 0.1108], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,297][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Lindsay] are: tensor([0.0009, 0.0912, 0.0694, 0.0265, 0.1020, 0.1298, 0.0307, 0.1565, 0.0849,
        0.1043, 0.1137, 0.0901], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,298][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Lindsay] are: tensor([0.0099, 0.3003, 0.0395, 0.0741, 0.0626, 0.0110, 0.0664, 0.0322, 0.0510,
        0.2927, 0.0187, 0.0415], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,299][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Lindsay] are: tensor([0.0005, 0.2536, 0.0433, 0.2558, 0.0729, 0.0324, 0.0061, 0.0183, 0.0488,
        0.0809, 0.0504, 0.1369], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,301][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Lindsay] are: tensor([0.0025, 0.5509, 0.0949, 0.0961, 0.1087, 0.0160, 0.0133, 0.0127, 0.0256,
        0.0642, 0.0059, 0.0092], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay]
[2024-07-24 10:21:35,302][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ decided] are: tensor([0.0493, 0.1000, 0.0133, 0.1424, 0.0772, 0.0315, 0.1087, 0.0391, 0.0685,
        0.0786, 0.0384, 0.1874, 0.0657], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,303][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ decided] are: tensor([0.0028, 0.0502, 0.0312, 0.0249, 0.0530, 0.0664, 0.0190, 0.1034, 0.1372,
        0.1627, 0.1015, 0.1130, 0.1348], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,305][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ decided] are: tensor([0.0009, 0.0709, 0.0332, 0.0720, 0.0362, 0.0201, 0.0129, 0.0694, 0.1682,
        0.1902, 0.0706, 0.2201, 0.0354], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,306][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ decided] are: tensor([0.0139, 0.0492, 0.0743, 0.0110, 0.0279, 0.1132, 0.0099, 0.0660, 0.3796,
        0.1314, 0.0483, 0.0288, 0.0466], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,308][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ decided] are: tensor([0.0036, 0.0685, 0.0675, 0.0540, 0.0488, 0.0668, 0.0062, 0.0469, 0.2880,
        0.0861, 0.0343, 0.0798, 0.1494], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,309][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ decided] are: tensor([0.0027, 0.0298, 0.0466, 0.0511, 0.0895, 0.0550, 0.0197, 0.0691, 0.1251,
        0.2586, 0.0358, 0.1138, 0.1033], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,311][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ decided] are: tensor([0.0034, 0.0894, 0.0281, 0.0650, 0.0441, 0.0237, 0.0407, 0.0592, 0.1725,
        0.2138, 0.0433, 0.1475, 0.0692], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,312][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ decided] are: tensor([0.0061, 0.0224, 0.0417, 0.0252, 0.0478, 0.0441, 0.0279, 0.0385, 0.2170,
        0.2957, 0.0460, 0.0879, 0.0996], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,314][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ decided] are: tensor([0.0297, 0.1256, 0.0814, 0.0431, 0.0844, 0.0736, 0.0657, 0.0991, 0.0968,
        0.0468, 0.1101, 0.0646, 0.0791], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,315][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ decided] are: tensor([0.0132, 0.1176, 0.0630, 0.0553, 0.0508, 0.0277, 0.0370, 0.0780, 0.0745,
        0.2469, 0.0384, 0.0417, 0.1560], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,317][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ decided] are: tensor([0.0026, 0.3814, 0.0307, 0.1715, 0.0817, 0.0287, 0.0102, 0.0168, 0.0444,
        0.0471, 0.0332, 0.0661, 0.0856], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,318][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ decided] are: tensor([0.0147, 0.5277, 0.0539, 0.0204, 0.1222, 0.0309, 0.0117, 0.0160, 0.0667,
        0.0788, 0.0078, 0.0057, 0.0435], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided]
[2024-07-24 10:21:35,319][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0317, 0.0576, 0.0238, 0.1201, 0.0762, 0.0596, 0.0679, 0.0598, 0.1300,
        0.0583, 0.0516, 0.1580, 0.0483, 0.0571], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,321][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0044, 0.0827, 0.0242, 0.0203, 0.0350, 0.0624, 0.0247, 0.0700, 0.1762,
        0.0871, 0.1083, 0.1025, 0.0916, 0.1107], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,322][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0010, 0.0736, 0.0276, 0.0785, 0.0198, 0.0129, 0.0098, 0.0371, 0.1220,
        0.1366, 0.0602, 0.2105, 0.0288, 0.1816], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,323][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0046, 0.0484, 0.0654, 0.0066, 0.0490, 0.1004, 0.0102, 0.0422, 0.3318,
        0.1440, 0.0346, 0.0242, 0.0672, 0.0715], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,324][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0034, 0.0565, 0.0474, 0.0341, 0.0382, 0.0625, 0.0055, 0.0387, 0.2650,
        0.0744, 0.0473, 0.0832, 0.0952, 0.1486], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,324][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0013, 0.0256, 0.0595, 0.0910, 0.0675, 0.0504, 0.0153, 0.0399, 0.1107,
        0.1858, 0.0573, 0.1464, 0.0528, 0.0967], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,324][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0019, 0.0866, 0.0240, 0.0900, 0.0282, 0.0102, 0.0283, 0.0263, 0.0850,
        0.2165, 0.0476, 0.1860, 0.0424, 0.1271], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,325][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0041, 0.0312, 0.0548, 0.0192, 0.0345, 0.0425, 0.0373, 0.0266, 0.1575,
        0.3139, 0.0502, 0.0736, 0.0719, 0.0828], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,325][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0027, 0.0838, 0.0730, 0.0327, 0.0876, 0.0877, 0.0270, 0.1197, 0.0686,
        0.0546, 0.1197, 0.0780, 0.0965, 0.0683], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,325][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0064, 0.2133, 0.0412, 0.0413, 0.0253, 0.0105, 0.0333, 0.0458, 0.0574,
        0.2688, 0.0303, 0.0312, 0.1445, 0.0507], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,326][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0025, 0.4254, 0.0186, 0.1303, 0.0611, 0.0184, 0.0096, 0.0169, 0.0284,
        0.0422, 0.0305, 0.0484, 0.0745, 0.0933], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,326][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0118, 0.6841, 0.0336, 0.0191, 0.1303, 0.0136, 0.0107, 0.0071, 0.0227,
        0.0305, 0.0028, 0.0016, 0.0202, 0.0118], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to]
[2024-07-24 10:21:35,327][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ give] are: tensor([0.0350, 0.0598, 0.0139, 0.0893, 0.0804, 0.0410, 0.1002, 0.0464, 0.0942,
        0.0555, 0.0416, 0.1394, 0.0558, 0.0465, 0.1010], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,328][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ give] are: tensor([0.0050, 0.1031, 0.0181, 0.0286, 0.0348, 0.0591, 0.0227, 0.0466, 0.1800,
        0.0999, 0.0668, 0.1165, 0.0840, 0.0824, 0.0523], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,329][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ give] are: tensor([0.0006, 0.1153, 0.0214, 0.1144, 0.0264, 0.0115, 0.0113, 0.0322, 0.1144,
        0.1127, 0.0530, 0.2105, 0.0215, 0.1178, 0.0371], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,331][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ give] are: tensor([0.0040, 0.0325, 0.0597, 0.0063, 0.0272, 0.1288, 0.0148, 0.0405, 0.3985,
        0.0931, 0.0348, 0.0224, 0.0489, 0.0696, 0.0190], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,332][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ give] are: tensor([0.0031, 0.0510, 0.0604, 0.0524, 0.0434, 0.0592, 0.0041, 0.0302, 0.2347,
        0.0390, 0.0325, 0.0646, 0.0722, 0.0922, 0.1609], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,334][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ give] are: tensor([0.0046, 0.0673, 0.0628, 0.0924, 0.0755, 0.0361, 0.0187, 0.0322, 0.0982,
        0.1696, 0.0360, 0.1024, 0.0521, 0.0701, 0.0818], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,335][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ give] are: tensor([0.0023, 0.1168, 0.0237, 0.0834, 0.0425, 0.0131, 0.0320, 0.0244, 0.0927,
        0.1684, 0.0343, 0.1404, 0.0344, 0.1310, 0.0609], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,337][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ give] are: tensor([0.0058, 0.0439, 0.0568, 0.0308, 0.0517, 0.0423, 0.0264, 0.0294, 0.2200,
        0.1920, 0.0419, 0.0537, 0.0934, 0.0722, 0.0397], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,338][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ give] are: tensor([0.0105, 0.0940, 0.0702, 0.0246, 0.0586, 0.0672, 0.0394, 0.1067, 0.0715,
        0.0616, 0.1318, 0.0473, 0.0923, 0.0903, 0.0341], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,340][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ give] are: tensor([0.0177, 0.3074, 0.0209, 0.0333, 0.0290, 0.0080, 0.0327, 0.0319, 0.0401,
        0.2036, 0.0126, 0.0178, 0.0988, 0.0236, 0.1227], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,341][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ give] are: tensor([0.0017, 0.3610, 0.0185, 0.2284, 0.0519, 0.0273, 0.0108, 0.0103, 0.0393,
        0.0312, 0.0210, 0.0856, 0.0512, 0.0519, 0.0099], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,342][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ give] are: tensor([0.0184, 0.6196, 0.0333, 0.0227, 0.1161, 0.0234, 0.0141, 0.0086, 0.0405,
        0.0398, 0.0035, 0.0031, 0.0214, 0.0171, 0.0184], device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give]
[2024-07-24 10:21:35,344][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ it] are: tensor([0.0456, 0.0675, 0.0137, 0.0744, 0.0807, 0.0380, 0.1083, 0.0398, 0.0903,
        0.0486, 0.0391, 0.1084, 0.0659, 0.0368, 0.0657, 0.0772],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,345][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ it] are: tensor([0.0023, 0.0356, 0.0192, 0.0195, 0.0449, 0.0750, 0.0239, 0.0614, 0.1177,
        0.0739, 0.0874, 0.0892, 0.0856, 0.0981, 0.0655, 0.1007],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,347][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ it] are: tensor([0.0013, 0.0367, 0.0247, 0.0484, 0.0274, 0.0141, 0.0098, 0.0341, 0.0932,
        0.0922, 0.0614, 0.1203, 0.0188, 0.1518, 0.0368, 0.2288],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,348][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ it] are: tensor([0.0117, 0.0329, 0.0433, 0.0068, 0.0329, 0.0858, 0.0122, 0.0546, 0.2859,
        0.1191, 0.0292, 0.0236, 0.0615, 0.0675, 0.0303, 0.1027],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,350][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ it] are: tensor([0.0023, 0.0274, 0.0378, 0.0300, 0.0304, 0.0419, 0.0046, 0.0239, 0.1597,
        0.0503, 0.0254, 0.0600, 0.0994, 0.0778, 0.1276, 0.2015],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,351][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ it] are: tensor([0.0026, 0.0277, 0.0458, 0.0674, 0.0610, 0.0423, 0.0177, 0.0357, 0.0861,
        0.1579, 0.0375, 0.1096, 0.0372, 0.0653, 0.0701, 0.1361],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,353][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ it] are: tensor([0.0028, 0.0398, 0.0230, 0.0566, 0.0409, 0.0195, 0.0288, 0.0340, 0.0986,
        0.1498, 0.0405, 0.1218, 0.0428, 0.1133, 0.0579, 0.1300],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,354][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ it] are: tensor([0.0031, 0.0196, 0.0497, 0.0338, 0.0654, 0.0495, 0.0254, 0.0223, 0.1055,
        0.1712, 0.0435, 0.0779, 0.0382, 0.0653, 0.0321, 0.1975],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,356][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ it] are: tensor([0.0100, 0.1073, 0.0578, 0.0307, 0.1212, 0.0932, 0.0547, 0.0984, 0.0716,
        0.0630, 0.0734, 0.0362, 0.0522, 0.0564, 0.0254, 0.0485],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,357][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ it] are: tensor([0.0104, 0.1580, 0.0205, 0.0317, 0.0421, 0.0113, 0.0414, 0.0365, 0.0345,
        0.1993, 0.0143, 0.0155, 0.0878, 0.0276, 0.2000, 0.0692],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,359][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ it] are: tensor([0.0027, 0.2396, 0.0248, 0.2277, 0.0767, 0.0254, 0.0073, 0.0154, 0.0412,
        0.0268, 0.0300, 0.0834, 0.0510, 0.0850, 0.0115, 0.0516],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,360][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ it] are: tensor([0.0211, 0.4111, 0.0679, 0.0300, 0.1688, 0.0249, 0.0161, 0.0143, 0.0392,
        0.0498, 0.0071, 0.0046, 0.0306, 0.0289, 0.0692, 0.0164],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it]
[2024-07-24 10:21:35,361][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0186, 0.0541, 0.0178, 0.1066, 0.0612, 0.0445, 0.0477, 0.0461, 0.0988,
        0.0483, 0.0343, 0.1273, 0.0380, 0.0413, 0.0593, 0.1097, 0.0464],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,363][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0029, 0.0493, 0.0195, 0.0138, 0.0275, 0.0463, 0.0164, 0.0602, 0.1324,
        0.0544, 0.0941, 0.0676, 0.0677, 0.0955, 0.0491, 0.1232, 0.0800],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,364][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0009, 0.0556, 0.0191, 0.0571, 0.0144, 0.0084, 0.0072, 0.0230, 0.0736,
        0.0747, 0.0382, 0.1361, 0.0157, 0.1160, 0.0272, 0.2331, 0.0996],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,366][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0054, 0.0501, 0.0493, 0.0068, 0.0501, 0.0787, 0.0092, 0.0376, 0.2300,
        0.1250, 0.0264, 0.0228, 0.0568, 0.0514, 0.0368, 0.1254, 0.0383],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,367][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0022, 0.0359, 0.0277, 0.0201, 0.0205, 0.0344, 0.0029, 0.0197, 0.1588,
        0.0426, 0.0281, 0.0462, 0.0557, 0.0825, 0.0983, 0.2351, 0.0891],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,369][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0011, 0.0181, 0.0575, 0.0723, 0.0503, 0.0420, 0.0096, 0.0280, 0.0919,
        0.1182, 0.0427, 0.1017, 0.0342, 0.0687, 0.0517, 0.1508, 0.0611],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,370][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0017, 0.0646, 0.0200, 0.0632, 0.0246, 0.0087, 0.0231, 0.0225, 0.0678,
        0.1680, 0.0427, 0.1387, 0.0308, 0.1118, 0.0456, 0.1101, 0.0563],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,372][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0040, 0.0277, 0.0422, 0.0173, 0.0306, 0.0311, 0.0263, 0.0208, 0.0852,
        0.1875, 0.0404, 0.0546, 0.0506, 0.0614, 0.0313, 0.2228, 0.0663],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,373][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0021, 0.0754, 0.0618, 0.0339, 0.0995, 0.0778, 0.0182, 0.1015, 0.0634,
        0.0423, 0.0803, 0.0646, 0.0716, 0.0523, 0.0406, 0.0745, 0.0402],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,375][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0063, 0.1485, 0.0352, 0.0222, 0.0185, 0.0080, 0.0199, 0.0359, 0.0472,
        0.1618, 0.0232, 0.0152, 0.1140, 0.0354, 0.1610, 0.1216, 0.0262],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,376][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0018, 0.4579, 0.0122, 0.1973, 0.0420, 0.0101, 0.0062, 0.0099, 0.0138,
        0.0312, 0.0153, 0.0502, 0.0440, 0.0502, 0.0080, 0.0258, 0.0244],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,377][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0076, 0.6486, 0.0295, 0.0198, 0.1490, 0.0131, 0.0092, 0.0071, 0.0173,
        0.0291, 0.0028, 0.0017, 0.0170, 0.0104, 0.0227, 0.0099, 0.0053],
       device='cuda:0') for source tokens [When Benjamin and Lindsay got a snack at the station, Lindsay decided to give it to]
[2024-07-24 10:21:35,378][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:21:35,381][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[1140],
        [   1],
        [  76],
        [  51],
        [ 820],
        [ 229],
        [  95],
        [  22],
        [  40],
        [  51],
        [   2],
        [  13],
        [  32],
        [   7],
        [  10],
        [   3],
        [   1]], device='cuda:0')
[2024-07-24 10:21:35,382][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[1304],
        [   1],
        [  45],
        [  35],
        [ 726],
        [ 164],
        [  92],
        [   5],
        [  27],
        [  40],
        [   1],
        [  13],
        [  23],
        [   1],
        [   7],
        [   2],
        [   1]], device='cuda:0')
[2024-07-24 10:21:35,383][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[12930],
        [ 8972],
        [ 8010],
        [10374],
        [ 8877],
        [ 7608],
        [ 8385],
        [ 7828],
        [ 6528],
        [ 7207],
        [ 6679],
        [ 6732],
        [ 7107],
        [ 6352],
        [ 6206],
        [ 6315],
        [ 6060]], device='cuda:0')
[2024-07-24 10:21:35,384][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 2648],
        [50251],
        [50246],
        [50250],
        [50218],
        [50173],
        [48890],
        [49887],
        [49639],
        [47394],
        [49727],
        [49616],
        [50061],
        [50082],
        [50102],
        [49941],
        [49936]], device='cuda:0')
[2024-07-24 10:21:35,385][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[14289],
        [50257],
        [50250],
        [32194],
        [31507],
        [28696],
        [21810],
        [24977],
        [22693],
        [21625],
        [28703],
        [29048],
        [28638],
        [27367],
        [28030],
        [29887],
        [30930]], device='cuda:0')
[2024-07-24 10:21:35,386][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[39689],
        [49050],
        [44825],
        [44537],
        [44030],
        [39667],
        [42131],
        [39295],
        [38664],
        [38382],
        [38993],
        [38847],
        [39282],
        [39162],
        [38674],
        [38954],
        [39265]], device='cuda:0')
[2024-07-24 10:21:35,387][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[11868],
        [ 7774],
        [ 7744],
        [ 7741],
        [ 7700],
        [ 7503],
        [ 9290],
        [ 8866],
        [ 8710],
        [16382],
        [15705],
        [16168],
        [13856],
        [12755],
        [12215],
        [13276],
        [12860]], device='cuda:0')
[2024-07-24 10:21:35,389][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[41963],
        [42753],
        [47508],
        [48942],
        [48711],
        [47647],
        [45191],
        [48064],
        [47501],
        [44330],
        [44262],
        [44607],
        [41651],
        [42898],
        [44069],
        [42186],
        [41851]], device='cuda:0')
[2024-07-24 10:21:35,390][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[33816],
        [ 2306],
        [ 6067],
        [ 8076],
        [ 6921],
        [ 6604],
        [ 5643],
        [ 4901],
        [ 4771],
        [11395],
        [16385],
        [11591],
        [12523],
        [14525],
        [13848],
        [12516],
        [12219]], device='cuda:0')
[2024-07-24 10:21:35,392][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[46076],
        [29062],
        [30686],
        [21355],
        [28868],
        [32742],
        [35455],
        [33374],
        [38429],
        [33100],
        [30364],
        [24994],
        [30893],
        [31827],
        [34778],
        [35448],
        [37013]], device='cuda:0')
[2024-07-24 10:21:35,393][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 9589],
        [ 9661],
        [ 9759],
        [ 9706],
        [ 9649],
        [ 9639],
        [10053],
        [ 9660],
        [ 9788],
        [ 9656],
        [ 9872],
        [ 9832],
        [ 9649],
        [ 9778],
        [ 9691],
        [ 9675],
        [ 9828]], device='cuda:0')
[2024-07-24 10:21:35,395][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[21987],
        [  760],
        [  753],
        [ 4707],
        [ 3054],
        [ 3766],
        [ 8528],
        [12508],
        [15011],
        [ 4153],
        [ 4107],
        [ 4515],
        [ 4208],
        [ 4302],
        [ 4958],
        [ 4393],
        [ 4237]], device='cuda:0')
[2024-07-24 10:21:35,396][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[39298],
        [39259],
        [39590],
        [39365],
        [41376],
        [41453],
        [42166],
        [46859],
        [46705],
        [45299],
        [44710],
        [44795],
        [44552],
        [44592],
        [44157],
        [45050],
        [44906]], device='cuda:0')
[2024-07-24 10:21:35,398][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[41825],
        [ 6626],
        [11271],
        [11639],
        [13365],
        [11959],
        [11781],
        [11893],
        [11884],
        [11638],
        [11582],
        [11397],
        [11529],
        [11355],
        [11309],
        [11344],
        [11312]], device='cuda:0')
[2024-07-24 10:21:35,399][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[11481],
        [21862],
        [12554],
        [24673],
        [15022],
        [ 9685],
        [11651],
        [ 8545],
        [ 9629],
        [11751],
        [12573],
        [13827],
        [ 9228],
        [ 6010],
        [ 7033],
        [ 9127],
        [ 5370]], device='cuda:0')
[2024-07-24 10:21:35,400][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[13298],
        [10468],
        [11561],
        [16576],
        [15067],
        [14550],
        [14953],
        [14330],
        [14933],
        [15098],
        [15539],
        [18307],
        [18493],
        [17673],
        [18073],
        [18054],
        [18384]], device='cuda:0')
[2024-07-24 10:21:35,402][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[23938],
        [18364],
        [18252],
        [19325],
        [20912],
        [21713],
        [23170],
        [21319],
        [21157],
        [23188],
        [22780],
        [22805],
        [23075],
        [22681],
        [23160],
        [22574],
        [22014]], device='cuda:0')
[2024-07-24 10:21:35,403][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[31773],
        [30778],
        [37729],
        [34766],
        [37280],
        [37326],
        [38945],
        [38156],
        [38790],
        [39740],
        [40153],
        [40283],
        [40166],
        [38830],
        [38274],
        [37953],
        [37245]], device='cuda:0')
[2024-07-24 10:21:35,405][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[16481],
        [12725],
        [16071],
        [16569],
        [16312],
        [15337],
        [14076],
        [13601],
        [12204],
        [11784],
        [11432],
        [11913],
        [11406],
        [11213],
        [11332],
        [10532],
        [10642]], device='cuda:0')
[2024-07-24 10:21:35,406][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[13297],
        [10761],
        [ 6717],
        [ 5617],
        [ 9088],
        [11381],
        [14572],
        [11858],
        [15936],
        [15742],
        [17043],
        [15515],
        [15567],
        [15119],
        [15229],
        [16659],
        [17754]], device='cuda:0')
[2024-07-24 10:21:35,408][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[11095],
        [ 8546],
        [ 6928],
        [ 9120],
        [10276],
        [11858],
        [13195],
        [13761],
        [15675],
        [14520],
        [15747],
        [18986],
        [19613],
        [19943],
        [17910],
        [20326],
        [19938]], device='cuda:0')
[2024-07-24 10:21:35,409][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[4914],
        [2135],
        [1814],
        [2392],
        [2224],
        [2143],
        [1323],
        [1495],
        [1313],
        [2006],
        [2536],
        [3074],
        [1870],
        [2525],
        [2250],
        [2102],
        [2419]], device='cuda:0')
[2024-07-24 10:21:35,410][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[14881],
        [ 7114],
        [ 8269],
        [10176],
        [ 7934],
        [10718],
        [10171],
        [10188],
        [12666],
        [17096],
        [18007],
        [19290],
        [16383],
        [16177],
        [14226],
        [13893],
        [13920]], device='cuda:0')
[2024-07-24 10:21:35,412][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[21983],
        [23053],
        [25872],
        [25918],
        [25643],
        [25958],
        [26264],
        [25366],
        [26047],
        [25202],
        [27185],
        [27494],
        [25900],
        [27675],
        [27098],
        [26674],
        [27862]], device='cuda:0')
[2024-07-24 10:21:35,414][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[19492],
        [10533],
        [10271],
        [ 9564],
        [13660],
        [14432],
        [19594],
        [12884],
        [17354],
        [11787],
        [ 9933],
        [11100],
        [14407],
        [12704],
        [11623],
        [11969],
        [12780]], device='cuda:0')
[2024-07-24 10:21:35,415][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[12084],
        [18429],
        [17258],
        [24715],
        [24096],
        [22543],
        [26559],
        [23293],
        [20771],
        [21629],
        [21535],
        [20402],
        [18785],
        [16567],
        [19978],
        [18718],
        [19320]], device='cuda:0')
[2024-07-24 10:21:35,416][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12057],
        [11944],
        [11793],
        [12149],
        [10596],
        [10197],
        [10211],
        [10176],
        [10066],
        [10199],
        [10526],
        [10854],
        [ 9891],
        [10199],
        [10028],
        [ 9228],
        [ 9829]], device='cuda:0')
[2024-07-24 10:21:35,418][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[23089],
        [32527],
        [30171],
        [26652],
        [24948],
        [24032],
        [21898],
        [24846],
        [23267],
        [22158],
        [21506],
        [20043],
        [21376],
        [22628],
        [23009],
        [24316],
        [23391]], device='cuda:0')
[2024-07-24 10:21:35,419][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[34900],
        [20701],
        [31730],
        [18689],
        [27122],
        [32039],
        [30568],
        [35284],
        [30834],
        [28071],
        [29052],
        [26985],
        [31038],
        [37887],
        [32283],
        [32029],
        [40491]], device='cuda:0')
[2024-07-24 10:21:35,421][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659],
        [7659]], device='cuda:0')
