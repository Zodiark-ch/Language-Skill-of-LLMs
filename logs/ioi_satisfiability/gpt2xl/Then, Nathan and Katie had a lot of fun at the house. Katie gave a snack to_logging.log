[2024-07-24 10:25:12,760][explain_satisfiability.py][line:287][INFO] ############ CASE TEXT isThen, Nathan and Katie had a lot of fun at the house. Katie gave a snack to
[2024-07-24 10:25:12,760][explain_satisfiability.py][line:288][INFO] ############ CASE Prediction is  Nathan
[2024-07-24 10:25:12,760][explain_satisfiability.py][line:289][INFO] ############ Refined Forward Graph
[2024-07-24 10:25:12,760][explain_satisfiability.py][line:290][INFO] ****** Layer 1
[2024-07-24 10:25:12,760][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 0
[2024-07-24 10:25:12,760][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 1
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 2
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 3
[2024-07-24 10:25:12,761][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 4
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 5
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 6
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,762][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 7
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 8
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 9
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,763][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 10
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 11
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 12
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 13
[2024-07-24 10:25:12,764][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 14
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 15
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 16
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,765][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 17
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit11', 'circuit13', 'circuit17', 'circuit19', 'circuit21', 'circuit26']
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 18
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 19
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:25:12,766][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 20
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 21
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 22
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 23
[2024-07-24 10:25:12,767][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 24
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 25
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 26
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,768][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 27
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:296][INFO] Layer 1 and circuit 28
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 0
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit5', 'circuit6', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,769][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 1
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 2
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit8', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 3
[2024-07-24 10:25:12,770][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 4
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit19', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit23']
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 5
[2024-07-24 10:25:12,771][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 6
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 7
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:25:12,772][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 8
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit18']
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 9
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit3', 'circuit10', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,773][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 10
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14']
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 11
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,774][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 12
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit5', 'circuit14']
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 13
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 14
[2024-07-24 10:25:12,775][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 15
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 16
[2024-07-24 10:25:12,776][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 17
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit22']
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 18
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit19', 'circuit22', 'circuit23', 'circuit27']
[2024-07-24 10:25:12,777][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit4', 'circuit13', 'circuit15']
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 19
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 20
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,778][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 21
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 22
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit11', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 23
[2024-07-24 10:25:12,779][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 24
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 25
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit16', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,780][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20']
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 26
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 27
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,781][explain_satisfiability.py][line:296][INFO] Layer 2 and circuit 28
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 0
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,782][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 1
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 2
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,783][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit7', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 3
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 4
[2024-07-24 10:25:12,784][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 5
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,785][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 6
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit24']
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 7
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,786][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 8
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 9
[2024-07-24 10:25:12,787][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 10
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit26']
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,788][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 11
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 12
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,789][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22']
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 13
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 14
[2024-07-24 10:25:12,790][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 15
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,791][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 16
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19']
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 17
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,792][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit10', 'circuit20']
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 18
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 19
[2024-07-24 10:25:12,793][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 20
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit6', 'circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,794][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 21
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 22
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,795][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 23
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit16', 'circuit20']
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit16']
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 24
[2024-07-24 10:25:12,796][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit20', 'circuit21', 'circuit26']
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit4', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 25
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,797][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 26
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 27
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,798][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:296][INFO] Layer 3 and circuit 28
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 0
[2024-07-24 10:25:12,799][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 1
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,800][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit20', 'circuit21', 'circuit22', 'circuit27']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit6', 'circuit12', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 2
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,801][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit7', 'circuit9', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 3
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit26']
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit21', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 4
[2024-07-24 10:25:12,802][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit8']
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit10', 'circuit14', 'circuit26']
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 5
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,803][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit7', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 6
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,804][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 7
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit5', 'circuit13', 'circuit14', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,805][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 8
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit27']
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 9
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,806][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit7', 'circuit10', 'circuit12', 'circuit13']
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 10
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17']
[2024-07-24 10:25:12,807][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit26']
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 11
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,808][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 12
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 13
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,809][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit1', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 14
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,810][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 15
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,811][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 16
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit7', 'circuit11', 'circuit14', 'circuit20', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit22']
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 17
[2024-07-24 10:25:12,812][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 18
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit19']
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,813][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 19
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,814][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 20
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 21
[2024-07-24 10:25:12,815][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 22
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15']
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit25']
[2024-07-24 10:25:12,816][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 23
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,817][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 24
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 25
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,818][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 26
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,819][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 27
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,820][explain_satisfiability.py][line:296][INFO] Layer 4 and circuit 28
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 0
[2024-07-24 10:25:12,821][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 1
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:25:12,822][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 2
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,823][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit19']
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21']
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 3
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,824][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 4
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,825][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 5
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,826][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit26']
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 6
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit5', 'circuit6', 'circuit12', 'circuit13', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit2', 'circuit5', 'circuit7', 'circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,827][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 7
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit15', 'circuit18']
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 8
[2024-07-24 10:25:12,828][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit19']
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16']
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit26']
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 9
[2024-07-24 10:25:12,829][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18']
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit24']
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 10
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,830][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 11
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,831][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 12
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:25:12,832][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 13
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit7', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,833][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 14
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,834][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 15
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,835][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 16
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,836][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 17
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 18
[2024-07-24 10:25:12,837][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 19
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,838][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 20
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,839][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 21
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,840][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 22
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,841][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 23
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,842][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 24
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,843][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 25
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 26
[2024-07-24 10:25:12,844][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 27
[2024-07-24 10:25:12,845][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:296][INFO] Layer 5 and circuit 28
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,846][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 0
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,847][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit7', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit7', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 1
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit22']
[2024-07-24 10:25:12,848][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15']
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 2
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,849][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit25']
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 3
[2024-07-24 10:25:12,850][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit4', 'circuit14', 'circuit16', 'circuit21']
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit19', 'circuit22', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit15', 'circuit20', 'circuit26']
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 4
[2024-07-24 10:25:12,851][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit14']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit16', 'circuit18', 'circuit19', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit5', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit23']
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 5
[2024-07-24 10:25:12,852][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit21', 'circuit26']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit10', 'circuit19', 'circuit27']
[2024-07-24 10:25:12,853][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 6
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18']
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,854][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 7
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit26']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-24 10:25:12,855][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 8
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,856][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 9
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,857][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 10
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit15']
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-24 10:25:12,858][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 11
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit19', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,859][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 12
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,860][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 13
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,861][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 14
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,862][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 15
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,863][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit19']
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit9', 'circuit13', 'circuit14']
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 16
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit11', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,864][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 17
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit23', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,865][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit18']
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 18
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,866][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 19
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,867][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 20
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20']
[2024-07-24 10:25:12,868][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 21
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,869][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 22
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,870][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit16']
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 23
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,871][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 24
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,872][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 25
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,873][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 26
[2024-07-24 10:25:12,874][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 27
[2024-07-24 10:25:12,875][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,876][explain_satisfiability.py][line:296][INFO] Layer 6 and circuit 28
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,877][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 0
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,878][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 1
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit18', 'circuit19', 'circuit21', 'circuit25']
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,879][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 2
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit23']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit24']
[2024-07-24 10:25:12,880][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21']
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit21', 'circuit22']
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 3
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,881][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit27']
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit20', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 4
[2024-07-24 10:25:12,882][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,883][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 5
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,884][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 6
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit22']
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit25']
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,885][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit17', 'circuit19']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 7
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit8']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit7', 'circuit13', 'circuit15', 'circuit19', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,886][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit3', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 8
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,887][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit22']
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 9
[2024-07-24 10:25:12,888][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,889][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 10
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20']
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit26']
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,890][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 11
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20']
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21']
[2024-07-24 10:25:12,891][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit24', 'circuit27']
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit26']
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit6', 'circuit8', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 12
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,892][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit24']
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 13
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit21', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,893][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit26']
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 14
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,894][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,895][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 15
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,896][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 16
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,897][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 17
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,898][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 18
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,899][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 19
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,900][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 20
[2024-07-24 10:25:12,901][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,902][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 21
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,903][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 22
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,904][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 23
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,905][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 24
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,906][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 25
[2024-07-24 10:25:12,907][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,908][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 26
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,909][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 27
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,910][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:296][INFO] Layer 7 and circuit 28
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,911][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 0
[2024-07-24 10:25:12,912][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit7', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,913][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 1
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit19']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,914][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit17', 'circuit18']
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 2
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13']
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit18']
[2024-07-24 10:25:12,915][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16']
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15']
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 3
[2024-07-24 10:25:12,916][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit10', 'circuit14', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,917][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 4
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit20']
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit25']
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit20', 'circuit25']
[2024-07-24 10:25:12,918][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit14', 'circuit15', 'circuit19']
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 5
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0']
[2024-07-24 10:25:12,919][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit17']
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit21']
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 6
[2024-07-24 10:25:12,920][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,921][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit6']
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit4', 'circuit5', 'circuit8', 'circuit12']
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 7
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit19']
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,922][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit26']
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 8
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit22']
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,923][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit23']
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit20', 'circuit22']
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 9
[2024-07-24 10:25:12,924][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit21', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,925][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 10
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit20']
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit25']
[2024-07-24 10:25:12,926][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 11
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,927][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,928][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 12
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit18', 'circuit20']
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,929][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 13
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,930][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 14
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,931][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,932][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 15
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit9', 'circuit13', 'circuit26']
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,933][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 16
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,934][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 17
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,935][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,936][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 18
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,937][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 19
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,938][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 20
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,939][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,940][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 21
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,941][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 22
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,942][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 23
[2024-07-24 10:25:12,943][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit9', 'circuit10', 'circuit12', 'circuit16', 'circuit17', 'circuit18', 'circuit22']
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit15']
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,944][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 24
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,945][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 25
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,946][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 26
[2024-07-24 10:25:12,947][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,948][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 27
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,949][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:296][INFO] Layer 8 and circuit 28
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,950][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,951][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 0
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit6', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,952][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit7', 'circuit8', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 1
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,953][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,954][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 2
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,955][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 3
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit17', 'circuit18']
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15']
[2024-07-24 10:25:12,956][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,957][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 4
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,958][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 5
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit17', 'circuit20', 'circuit24', 'circuit27']
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,959][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-24 10:25:12,960][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 6
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit21', 'circuit23', 'circuit25']
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,961][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 7
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit8', 'circuit10', 'circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit25']
[2024-07-24 10:25:12,962][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit19']
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit23']
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit23']
[2024-07-24 10:25:12,963][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 8
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit22']
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,964][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 9
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit18']
[2024-07-24 10:25:12,965][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit15', 'circuit16', 'circuit17']
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,966][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 10
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit24']
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit25']
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,967][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit17', 'circuit19', 'circuit20']
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 11
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,968][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit14']
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,969][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 12
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,970][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 13
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit18', 'circuit27']
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,971][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit21', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit21', 'circuit25']
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit23']
[2024-07-24 10:25:12,972][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 14
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,973][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 15
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,974][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 16
[2024-07-24 10:25:12,975][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,976][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 17
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,977][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 18
[2024-07-24 10:25:12,978][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,979][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 19
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,980][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 20
[2024-07-24 10:25:12,981][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,982][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 21
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,983][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 22
[2024-07-24 10:25:12,984][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,985][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 23
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,986][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 24
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,987][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,988][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 25
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,989][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 26
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,990][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,991][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 27
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,992][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:296][INFO] Layer 9 and circuit 28
[2024-07-24 10:25:12,993][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,994][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 0
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,995][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit9', 'circuit10', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit10', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit14', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit7', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit21', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit26', 'circuit27']
[2024-07-24 10:25:12,996][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit8', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit25']
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 1
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,997][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 2
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:12,998][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:12,999][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 3
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,000][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,001][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 4
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,002][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 5
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,003][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,004][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 6
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,005][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,006][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 7
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,007][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 8
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,008][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,009][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 9
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,010][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 10
[2024-07-24 10:25:13,011][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,012][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 11
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-24 10:25:13,013][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14']
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,014][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit15']
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 12
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,015][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 13
[2024-07-24 10:25:13,016][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,017][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit22']
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 14
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,018][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,019][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 15
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,020][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 16
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,021][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,022][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 17
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,023][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,024][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 18
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,025][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 19
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,026][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,027][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 20
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,028][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 21
[2024-07-24 10:25:13,029][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,030][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 22
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,031][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,032][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 23
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,033][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 24
[2024-07-24 10:25:13,034][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,035][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 25
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,036][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,037][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 26
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,038][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 27
[2024-07-24 10:25:13,039][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,040][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:296][INFO] Layer 10 and circuit 28
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,041][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,042][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 0
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit27']
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit7', 'circuit10', 'circuit12', 'circuit13', 'circuit16', 'circuit17', 'circuit24', 'circuit27']
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit25']
[2024-07-24 10:25:13,043][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit3', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit27']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit13', 'circuit27']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit7', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-24 10:25:13,044][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 1
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,045][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 2
[2024-07-24 10:25:13,046][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,047][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 3
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,048][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,049][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 4
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,050][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,051][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 5
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,052][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit24']
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 6
[2024-07-24 10:25:13,053][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,054][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 7
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-24 10:25:13,055][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit19', 'circuit20']
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,056][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 8
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,057][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,058][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 9
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit16']
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,059][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit16', 'circuit19']
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit25']
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit17', 'circuit20', 'circuit25']
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:13,060][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 10
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit20']
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit23']
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,061][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit25']
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 11
[2024-07-24 10:25:13,062][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,063][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 12
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,064][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,065][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 13
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit23']
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit23', 'circuit24']
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,066][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit19', 'circuit25']
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit21']
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit13', 'circuit15', 'circuit18', 'circuit19']
[2024-07-24 10:25:13,067][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit19', 'circuit21', 'circuit22', 'circuit23']
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 14
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,068][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 15
[2024-07-24 10:25:13,069][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,070][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 16
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,071][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,072][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 17
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,073][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,074][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 18
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,075][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,076][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 19
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,077][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 20
[2024-07-24 10:25:13,078][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,079][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 21
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,080][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,081][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 22
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,082][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,083][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 23
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,084][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,085][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 24
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,086][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 25
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are []
[2024-07-24 10:25:13,087][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are []
[2024-07-24 10:25:13,088][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are []
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are []
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are []
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are []
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 26
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,089][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,090][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 27
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,091][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,092][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:296][INFO] Layer 11 and circuit 28
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,093][explain_satisfiability.py][line:302][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:13,094][explain_satisfiability.py][line:302][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-24 10:25:14,656][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:14,657][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,658][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,658][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,659][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,660][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,660][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,661][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,662][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,662][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,663][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,664][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,664][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:14,665][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.9675, 0.0325], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,666][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0065, 0.9935], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,666][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.6358, 0.3642], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,668][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.6853, 0.3147], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,672][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.9772, 0.0228], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,672][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.2539, 0.7461], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,673][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.9881, 0.0119], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,674][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.9119, 0.0881], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,675][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.3215, 0.6785], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,676][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.7118, 0.2882], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,678][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.7102, 0.2898], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,681][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.6916, 0.3084], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:14,685][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.4955, 0.3224, 0.1821], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,688][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([1.3917e-05, 2.4242e-05, 9.9996e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,691][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.4903, 0.2362, 0.2735], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,693][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([2.5928e-02, 6.8828e-04, 9.7338e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,694][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.1191, 0.0084, 0.8725], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,694][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([9.9620e-03, 6.6615e-07, 9.9004e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,695][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.3383, 0.3619, 0.2998], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,696][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.5098, 0.3692, 0.1211], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,698][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.5817, 0.2880, 0.1303], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,700][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.6510, 0.3097, 0.0393], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,705][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.4051, 0.2464, 0.3485], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,709][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.4845, 0.2793, 0.2361], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:14,712][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.7250, 0.0821, 0.1288, 0.0640], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,714][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0023, 0.0392, 0.0010, 0.9575], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,715][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.2201, 0.1648, 0.1082, 0.5070], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,715][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.1145, 0.3914, 0.0153, 0.4788], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,716][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.3621, 0.1640, 0.1769, 0.2971], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,717][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1236, 0.1978, 0.0029, 0.6757], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,719][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.6931, 0.0331, 0.2486, 0.0251], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,722][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.2479, 0.1885, 0.2798, 0.2838], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,726][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0670, 0.4692, 0.0239, 0.4399], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,730][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.4163, 0.2325, 0.1353, 0.2160], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,734][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.4084, 0.3042, 0.0868, 0.2007], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,735][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.4377, 0.1911, 0.1214, 0.2498], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:14,736][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.2983, 0.2479, 0.1048, 0.1901, 0.1590], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,736][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([4.6413e-04, 9.0983e-05, 6.0294e-04, 3.2117e-04, 9.9852e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,738][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.3680, 0.2217, 0.1073, 0.1190, 0.1839], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,740][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([2.3447e-02, 3.1144e-04, 1.3347e-02, 5.0622e-04, 9.6239e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,744][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0297, 0.0029, 0.0551, 0.0022, 0.9101], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,746][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([2.9624e-02, 3.0556e-06, 9.6966e-06, 1.0744e-06, 9.7036e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,750][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.1965, 0.2724, 0.1560, 0.1458, 0.2294], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,754][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.2892, 0.1491, 0.1102, 0.3639, 0.0876], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,755][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.3421, 0.1825, 0.1288, 0.1353, 0.2113], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,756][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.3919, 0.2275, 0.1620, 0.1911, 0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,757][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.2945, 0.2148, 0.0660, 0.1358, 0.2889], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,757][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.2205, 0.1773, 0.1570, 0.2179, 0.2274], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:14,759][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.4376, 0.0385, 0.0815, 0.0383, 0.0670, 0.3371], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,761][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ had] are: tensor([3.2143e-04, 1.6716e-03, 2.5927e-03, 2.8552e-03, 1.3322e-04, 9.9243e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,765][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.4551, 0.1161, 0.0933, 0.1373, 0.0622, 0.1360], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,768][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ had] are: tensor([4.9393e-03, 5.4287e-03, 4.2666e-04, 1.1487e-02, 1.9679e-03, 9.7575e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,771][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.2175, 0.0619, 0.0545, 0.1120, 0.0923, 0.4619], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,773][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ had] are: tensor([3.9777e-02, 1.8562e-03, 1.4180e-03, 1.2905e-03, 6.7154e-05, 9.5559e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,775][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.2864, 0.0214, 0.2106, 0.0201, 0.4328, 0.0287], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,776][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.1153, 0.0898, 0.0723, 0.2084, 0.2550, 0.2592], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,777][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.0947, 0.2660, 0.0222, 0.3494, 0.0515, 0.2162], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,778][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.3004, 0.1811, 0.1247, 0.1867, 0.0809, 0.1261], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,778][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.2531, 0.1952, 0.0644, 0.1430, 0.0338, 0.3104], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,780][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.4287, 0.1432, 0.0890, 0.1786, 0.0703, 0.0902], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:14,783][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.4892, 0.0564, 0.1086, 0.0426, 0.1953, 0.0702, 0.0377],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,785][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([8.5139e-04, 4.2787e-03, 6.4922e-03, 4.2767e-03, 3.8004e-04, 4.6599e-04,
        9.8326e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,789][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2843, 0.1763, 0.1050, 0.2079, 0.0648, 0.1177, 0.0439],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,793][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0293, 0.0179, 0.0112, 0.0330, 0.0147, 0.1785, 0.7154],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,796][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1410, 0.0310, 0.0631, 0.0437, 0.0672, 0.4408, 0.2132],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,797][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0560, 0.1299, 0.0039, 0.0972, 0.0007, 0.0449, 0.6675],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,798][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3653, 0.0121, 0.1971, 0.0103, 0.3495, 0.0537, 0.0119],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,798][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0939, 0.0564, 0.0639, 0.1262, 0.1566, 0.2117, 0.2913],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,801][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0240, 0.1307, 0.0064, 0.1941, 0.0126, 0.1418, 0.4904],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,804][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.2550, 0.1633, 0.0965, 0.1657, 0.0862, 0.1028, 0.1307],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,807][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.2205, 0.1880, 0.0877, 0.1594, 0.0425, 0.0816, 0.2204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,811][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.3275, 0.1313, 0.1069, 0.1410, 0.1002, 0.0846, 0.1086],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:14,816][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.3644, 0.0806, 0.0690, 0.0851, 0.1363, 0.0760, 0.0901, 0.0984],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,816][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([2.6490e-04, 1.0292e-03, 6.5056e-04, 1.2099e-03, 3.7711e-04, 1.3685e-03,
        5.0367e-04, 9.9460e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,817][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.2635, 0.1019, 0.0740, 0.1821, 0.1392, 0.1353, 0.0702, 0.0337],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,818][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([1.0774e-03, 1.2864e-04, 1.0028e-04, 2.5224e-04, 1.3201e-04, 2.0045e-03,
        1.6744e-03, 9.9463e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,819][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.0463, 0.0144, 0.0158, 0.0159, 0.0139, 0.0611, 0.0720, 0.7607],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,820][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([3.2516e-03, 1.1955e-04, 4.8794e-05, 1.7790e-05, 6.6889e-05, 9.6462e-06,
        5.2872e-06, 9.9648e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,822][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.2834, 0.0612, 0.1638, 0.0357, 0.1595, 0.0405, 0.0529, 0.2028],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,825][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.0901, 0.0486, 0.0925, 0.0956, 0.0253, 0.1551, 0.2866, 0.2063],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,830][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.1227, 0.1490, 0.0656, 0.1377, 0.0244, 0.1124, 0.3338, 0.0544],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,833][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.2278, 0.1445, 0.0891, 0.1398, 0.1261, 0.0939, 0.1205, 0.0583],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,837][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.2109, 0.1515, 0.0486, 0.1252, 0.0342, 0.0845, 0.0843, 0.2608],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,839][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.4441, 0.0851, 0.1177, 0.0881, 0.0584, 0.0601, 0.0443, 0.1021],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:14,840][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.3650, 0.0574, 0.0971, 0.0413, 0.1395, 0.0753, 0.0501, 0.1424, 0.0319],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,841][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ of] are: tensor([8.9829e-04, 1.5853e-02, 5.4463e-04, 2.1554e-02, 2.7802e-05, 6.7570e-04,
        3.8719e-04, 9.5655e-05, 9.5996e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,842][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.2911, 0.1379, 0.0939, 0.1282, 0.0377, 0.0742, 0.0588, 0.0817, 0.0965],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,844][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0052, 0.0030, 0.0019, 0.0070, 0.0027, 0.0454, 0.0611, 0.3736, 0.5001],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,846][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.1569, 0.0214, 0.0498, 0.0255, 0.0451, 0.1129, 0.0731, 0.3386, 0.1768],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,849][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ of] are: tensor([2.2640e-02, 9.1235e-02, 2.7995e-03, 1.0706e-01, 2.0609e-04, 6.5621e-02,
        2.3558e-01, 7.2063e-03, 4.6765e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,853][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2257, 0.0192, 0.1646, 0.0170, 0.2746, 0.0440, 0.0177, 0.2251, 0.0121],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,857][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0551, 0.0267, 0.0189, 0.0558, 0.0451, 0.1102, 0.1866, 0.2769, 0.2247],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,859][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0229, 0.1175, 0.0092, 0.1836, 0.0118, 0.1237, 0.3534, 0.0215, 0.1564],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,860][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.2003, 0.1308, 0.0791, 0.1339, 0.0644, 0.0839, 0.1092, 0.0781, 0.1203],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,861][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.1678, 0.1555, 0.0606, 0.1471, 0.0231, 0.0759, 0.0903, 0.0587, 0.2211],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,862][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.2492, 0.1084, 0.1200, 0.1278, 0.0964, 0.0745, 0.0804, 0.0956, 0.0477],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:14,864][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.2460, 0.0992, 0.0920, 0.0910, 0.1070, 0.0347, 0.0870, 0.0965, 0.0536,
        0.0930], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,866][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([1.4076e-04, 4.5683e-05, 3.8527e-04, 3.7480e-05, 1.8489e-03, 2.0406e-05,
        1.1337e-03, 8.8403e-04, 1.7963e-05, 9.9549e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,870][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.1719, 0.0597, 0.1131, 0.0953, 0.1558, 0.0866, 0.0794, 0.0641, 0.0879,
        0.0862], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,873][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([4.4575e-04, 8.2962e-06, 1.0093e-04, 8.7105e-06, 4.3981e-05, 2.8328e-04,
        2.2892e-04, 1.8482e-03, 2.9446e-04, 9.9674e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,877][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0123, 0.0021, 0.0024, 0.0024, 0.0033, 0.0103, 0.0108, 0.0139, 0.0095,
        0.9329], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,879][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([4.2285e-03, 2.8178e-05, 1.3689e-05, 3.7846e-06, 5.6268e-05, 4.2784e-07,
        1.7980e-06, 1.4492e-04, 2.7548e-07, 9.9552e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,880][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.1845, 0.0391, 0.2034, 0.0305, 0.1958, 0.0256, 0.0278, 0.1217, 0.0332,
        0.1384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,881][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.0951, 0.0237, 0.0117, 0.0511, 0.0103, 0.0706, 0.1410, 0.1985, 0.2667,
        0.1313], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,882][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.1330, 0.1229, 0.0184, 0.1824, 0.0245, 0.1075, 0.2012, 0.0660, 0.1060,
        0.0382], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,884][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.1919, 0.1172, 0.0721, 0.1120, 0.0860, 0.0842, 0.0966, 0.0782, 0.1035,
        0.0583], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,886][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.1294, 0.1008, 0.0537, 0.0979, 0.0638, 0.0554, 0.0814, 0.0849, 0.0703,
        0.2624], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,891][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.3126, 0.1556, 0.0694, 0.1160, 0.0429, 0.0707, 0.0654, 0.0461, 0.0545,
        0.0669], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:14,895][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.2494, 0.0353, 0.0723, 0.0326, 0.1235, 0.0458, 0.0510, 0.1348, 0.0318,
        0.1996, 0.0241], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,897][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ at] are: tensor([7.4864e-04, 1.5246e-03, 3.0011e-04, 3.7641e-03, 6.4807e-04, 1.5070e-04,
        1.4839e-03, 3.0373e-05, 4.3018e-03, 3.3091e-05, 9.8701e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,900][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1927, 0.0994, 0.0687, 0.1176, 0.0372, 0.0778, 0.0436, 0.0615, 0.1138,
        0.0775, 0.1101], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,901][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ at] are: tensor([1.0218e-03, 3.5255e-04, 3.7753e-05, 6.9202e-04, 7.8188e-04, 4.8586e-03,
        7.4725e-03, 3.9645e-03, 2.9986e-02, 1.3044e-01, 8.2039e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,901][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0617, 0.0063, 0.0058, 0.0095, 0.0168, 0.0579, 0.0341, 0.0466, 0.0704,
        0.3767, 0.3143], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,902][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0546, 0.0231, 0.0020, 0.0097, 0.0013, 0.0364, 0.0161, 0.0012, 0.0039,
        0.0009, 0.8508], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,904][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.1609, 0.0123, 0.1738, 0.0120, 0.2278, 0.0394, 0.0154, 0.1708, 0.0118,
        0.1566, 0.0191], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,908][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0367, 0.0125, 0.0128, 0.0282, 0.0128, 0.0469, 0.0824, 0.1102, 0.1269,
        0.1938, 0.3367], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,911][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0239, 0.1159, 0.0083, 0.2101, 0.0104, 0.1117, 0.1968, 0.0245, 0.1660,
        0.0261, 0.1064], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,915][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.1499, 0.1046, 0.0594, 0.1132, 0.0499, 0.0783, 0.0905, 0.0696, 0.1091,
        0.0727, 0.1028], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,919][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.1416, 0.1095, 0.0486, 0.1127, 0.0439, 0.0821, 0.0949, 0.0342, 0.0910,
        0.0299, 0.2116], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,920][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.2512, 0.0861, 0.0899, 0.0894, 0.0564, 0.0533, 0.0466, 0.0841, 0.0468,
        0.0816, 0.1147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:14,921][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.4017, 0.0340, 0.0856, 0.0220, 0.0997, 0.0445, 0.0195, 0.0866, 0.0195,
        0.1422, 0.0209, 0.0238], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,922][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.9748e-03, 1.8208e-02, 4.8488e-04, 3.9237e-02, 1.6895e-04, 2.7148e-04,
        3.3095e-02, 8.6429e-05, 6.8745e-02, 7.4382e-05, 4.1111e-02, 7.9554e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,924][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1791, 0.0876, 0.0498, 0.0978, 0.0353, 0.0910, 0.0284, 0.0673, 0.1091,
        0.0606, 0.1650, 0.0289], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,927][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0108, 0.0017, 0.0009, 0.0019, 0.0007, 0.0073, 0.0189, 0.0121, 0.0616,
        0.0559, 0.1826, 0.6457], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,931][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1108, 0.0123, 0.0145, 0.0163, 0.0227, 0.0775, 0.0332, 0.0500, 0.0976,
        0.1405, 0.2303, 0.1942], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,935][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0666, 0.1044, 0.0079, 0.0855, 0.0026, 0.0639, 0.2095, 0.0509, 0.0549,
        0.0114, 0.0586, 0.2838], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,940][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1829, 0.0031, 0.1547, 0.0029, 0.2771, 0.0176, 0.0034, 0.1541, 0.0025,
        0.1931, 0.0066, 0.0020], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,941][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0379, 0.0089, 0.0097, 0.0176, 0.0241, 0.0228, 0.0364, 0.0571, 0.0940,
        0.0966, 0.2919, 0.3030], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,942][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0084, 0.0712, 0.0041, 0.1144, 0.0075, 0.0746, 0.2401, 0.0077, 0.0763,
        0.0134, 0.0509, 0.3314], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,943][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1645, 0.0929, 0.0631, 0.0964, 0.0575, 0.0675, 0.0770, 0.0630, 0.0895,
        0.0485, 0.0907, 0.0894], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,945][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1749, 0.1071, 0.0566, 0.1067, 0.0300, 0.0545, 0.1120, 0.0416, 0.0642,
        0.0301, 0.0737, 0.1486], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,948][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1913, 0.0770, 0.0867, 0.0807, 0.0833, 0.0499, 0.0835, 0.0735, 0.0305,
        0.0765, 0.0761, 0.0908], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:14,951][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.1677, 0.0584, 0.0553, 0.0644, 0.0998, 0.0219, 0.0733, 0.0405, 0.0549,
        0.0444, 0.0322, 0.0746, 0.2125], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,954][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ house] are: tensor([7.2587e-05, 1.2392e-03, 3.3619e-04, 8.5886e-04, 1.0352e-04, 8.9760e-04,
        3.2079e-04, 1.4536e-03, 1.1995e-03, 3.4712e-04, 2.9569e-04, 1.8757e-04,
        9.9269e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,958][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.1497, 0.0393, 0.1171, 0.0810, 0.1905, 0.0484, 0.0700, 0.0301, 0.0500,
        0.0464, 0.0849, 0.0726, 0.0198], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,960][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ house] are: tensor([3.1485e-03, 2.7352e-05, 2.2292e-04, 5.1175e-05, 4.5504e-04, 1.3823e-04,
        1.6475e-04, 2.6683e-02, 1.1132e-03, 7.6100e-03, 4.8281e-03, 3.5776e-03,
        9.5198e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,961][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0533, 0.0019, 0.0025, 0.0023, 0.0106, 0.0033, 0.0037, 0.0156, 0.0117,
        0.0235, 0.0177, 0.0231, 0.8308], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,962][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ house] are: tensor([3.7486e-03, 3.0089e-05, 1.0262e-04, 6.5236e-06, 9.2650e-06, 8.3637e-06,
        1.0965e-05, 1.9986e-03, 1.6165e-06, 1.6521e-05, 8.9260e-07, 6.0808e-06,
        9.9406e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,963][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.1369, 0.0239, 0.1932, 0.0161, 0.2551, 0.0083, 0.0115, 0.0559, 0.0097,
        0.1189, 0.0086, 0.0120, 0.1500], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,965][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.0431, 0.0093, 0.0096, 0.0243, 0.0022, 0.0315, 0.0463, 0.0409, 0.0604,
        0.2175, 0.1300, 0.3196, 0.0653], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,967][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.1467, 0.0704, 0.0480, 0.0862, 0.0811, 0.0646, 0.0711, 0.0462, 0.0723,
        0.1023, 0.0575, 0.1023, 0.0513], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,971][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.1632, 0.0925, 0.0534, 0.0876, 0.0709, 0.0739, 0.0779, 0.0597, 0.0812,
        0.0662, 0.0732, 0.0861, 0.0141], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,975][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.0912, 0.0769, 0.0492, 0.0887, 0.0418, 0.0740, 0.0806, 0.0542, 0.0722,
        0.0261, 0.0463, 0.0606, 0.2383], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,979][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.2584, 0.0794, 0.1089, 0.0507, 0.0598, 0.0228, 0.0324, 0.0891, 0.0536,
        0.0970, 0.0594, 0.0233, 0.0651], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:14,980][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.4542, 0.0142, 0.0477, 0.0118, 0.0843, 0.0494, 0.0222, 0.0802, 0.0091,
        0.1105, 0.0172, 0.0378, 0.0510, 0.0103], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,981][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [.] are: tensor([2.9666e-03, 2.9770e-02, 2.9895e-04, 5.5113e-03, 2.2136e-04, 1.1793e-04,
        2.6019e-04, 2.2069e-04, 2.5129e-03, 7.6652e-05, 1.1372e-03, 2.5233e-04,
        2.8909e-04, 9.5637e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,982][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.3301, 0.0421, 0.0572, 0.0361, 0.0476, 0.1848, 0.0167, 0.0446, 0.0795,
        0.0352, 0.0491, 0.0205, 0.0138, 0.0427], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,984][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [.] are: tensor([1.4020e-02, 1.6669e-03, 4.6325e-04, 1.6151e-03, 1.6303e-03, 5.0362e-03,
        5.4290e-03, 2.2060e-02, 2.1123e-02, 2.2439e-02, 7.2589e-02, 1.9717e-01,
        9.7704e-02, 5.3705e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,986][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0305, 0.0103, 0.0058, 0.0196, 0.0084, 0.0213, 0.0182, 0.0450, 0.0600,
        0.0681, 0.1390, 0.1189, 0.1236, 0.3314], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,991][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.1023, 0.1627, 0.0393, 0.1042, 0.0054, 0.0721, 0.0783, 0.0328, 0.0515,
        0.0108, 0.0543, 0.0630, 0.0141, 0.2094], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,994][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.2551, 0.0065, 0.0997, 0.0049, 0.1976, 0.0134, 0.0065, 0.0991, 0.0040,
        0.2093, 0.0085, 0.0043, 0.0879, 0.0032], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:14,998][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0184, 0.0044, 0.0071, 0.0082, 0.0104, 0.0123, 0.0188, 0.0307, 0.0320,
        0.0679, 0.0954, 0.1474, 0.1376, 0.4096], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,000][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0143, 0.1265, 0.0049, 0.1008, 0.0030, 0.0225, 0.0843, 0.0064, 0.0605,
        0.0048, 0.0399, 0.1364, 0.0117, 0.3838], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,001][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.1401, 0.0783, 0.0637, 0.0814, 0.0529, 0.0621, 0.0627, 0.0591, 0.0655,
        0.0585, 0.0687, 0.0706, 0.0502, 0.0862], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,002][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.1039, 0.0877, 0.0595, 0.0944, 0.0438, 0.0714, 0.0585, 0.0614, 0.0732,
        0.0514, 0.0723, 0.0587, 0.0403, 0.1234], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,003][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.2166, 0.0755, 0.0745, 0.0717, 0.0560, 0.0506, 0.0438, 0.0699, 0.0357,
        0.0637, 0.0755, 0.0339, 0.0389, 0.0939], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,005][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.1650, 0.1008, 0.0557, 0.0822, 0.0860, 0.0182, 0.0362, 0.0162, 0.0682,
        0.0365, 0.0650, 0.0488, 0.0406, 0.0833, 0.0974], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,007][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([2.3840e-04, 1.8784e-05, 9.7520e-05, 8.6870e-05, 5.1253e-01, 1.1592e-05,
        2.6742e-05, 4.6056e-05, 1.1895e-05, 4.3572e-04, 4.8300e-04, 7.3759e-06,
        1.6465e-05, 2.8538e-06, 4.8598e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,011][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.1567, 0.0799, 0.0507, 0.0562, 0.0996, 0.0581, 0.0768, 0.0249, 0.0629,
        0.0248, 0.0470, 0.0835, 0.0287, 0.0565, 0.0937], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,013][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([1.8959e-03, 2.0685e-06, 6.7934e-05, 1.2104e-06, 3.4314e-03, 4.5710e-06,
        5.8958e-06, 2.1427e-05, 1.9077e-05, 8.2374e-04, 7.8668e-05, 7.5162e-05,
        3.3184e-03, 2.4529e-04, 9.9001e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,016][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([3.8950e-03, 2.1279e-04, 2.9079e-03, 1.2316e-04, 4.6709e-02, 2.5002e-04,
        1.8042e-04, 3.1130e-04, 6.5129e-04, 3.4356e-04, 1.8363e-03, 8.4368e-04,
        2.5457e-03, 2.6826e-03, 9.3651e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,018][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([6.0337e-03, 4.5202e-07, 2.0228e-06, 1.7918e-07, 6.2780e-01, 2.2215e-07,
        1.8633e-08, 6.5437e-06, 1.0067e-08, 5.5789e-06, 1.7657e-07, 8.9191e-09,
        8.4902e-08, 7.8677e-09, 3.6615e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,021][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0872, 0.1081, 0.0943, 0.0668, 0.1504, 0.0166, 0.0370, 0.0183, 0.0472,
        0.0255, 0.0204, 0.0806, 0.0517, 0.0402, 0.1557], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,022][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0557, 0.0087, 0.0059, 0.0165, 0.0040, 0.0160, 0.0245, 0.0153, 0.0562,
        0.0267, 0.1091, 0.1346, 0.0556, 0.4023, 0.0690], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,023][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1708, 0.0782, 0.0690, 0.0680, 0.1295, 0.0243, 0.0488, 0.0241, 0.0528,
        0.0197, 0.0439, 0.0495, 0.0273, 0.0514, 0.1429], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,025][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.1684, 0.0981, 0.0832, 0.0878, 0.0141, 0.0485, 0.0597, 0.0490, 0.0692,
        0.0319, 0.0705, 0.0685, 0.0529, 0.0839, 0.0144], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,027][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0812, 0.0599, 0.0305, 0.0565, 0.2395, 0.0331, 0.0433, 0.0244, 0.0409,
        0.0296, 0.0468, 0.0332, 0.0202, 0.0453, 0.2158], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,031][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0656, 0.0510, 0.0584, 0.0552, 0.0726, 0.0496, 0.0468, 0.0636, 0.0327,
        0.0913, 0.0708, 0.0587, 0.0743, 0.1120, 0.0973], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,036][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.2181, 0.0215, 0.0320, 0.0258, 0.0491, 0.1070, 0.0204, 0.0520, 0.0252,
        0.0417, 0.0206, 0.0191, 0.0509, 0.0171, 0.0643, 0.2353],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,039][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([3.7431e-04, 5.8112e-04, 2.7942e-03, 1.2637e-03, 9.4087e-05, 6.5247e-03,
        1.4091e-04, 3.2776e-04, 7.6421e-04, 1.2306e-04, 3.5720e-04, 1.7541e-04,
        1.5129e-04, 5.0774e-05, 6.4635e-05, 9.8621e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,040][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1676, 0.0420, 0.0364, 0.0506, 0.0455, 0.0754, 0.0435, 0.0567, 0.0723,
        0.0635, 0.0713, 0.0535, 0.0281, 0.0711, 0.0473, 0.0751],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,041][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([8.5871e-04, 4.6006e-06, 1.5856e-06, 3.9968e-06, 3.0338e-06, 4.5570e-05,
        1.1799e-05, 3.8633e-05, 4.7150e-05, 2.9226e-04, 1.4114e-04, 2.0651e-04,
        6.3361e-04, 6.5208e-04, 9.8672e-04, 9.9607e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,042][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0812, 0.0063, 0.0023, 0.0063, 0.0054, 0.0104, 0.0073, 0.0099, 0.0185,
        0.0222, 0.0385, 0.0209, 0.0748, 0.0420, 0.0559, 0.5979],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,043][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([3.9492e-02, 2.1587e-05, 1.1947e-03, 2.1849e-05, 5.3602e-05, 1.8019e-03,
        1.2753e-05, 7.0335e-06, 5.9570e-06, 5.3200e-05, 1.5017e-05, 7.0719e-06,
        4.2984e-06, 1.0694e-06, 1.8500e-05, 9.5729e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,045][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0867, 0.0109, 0.0804, 0.0106, 0.1524, 0.0129, 0.0155, 0.0699, 0.0080,
        0.1340, 0.0094, 0.0120, 0.1236, 0.0111, 0.2359, 0.0266],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,049][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0167, 0.0037, 0.0026, 0.0055, 0.0079, 0.0055, 0.0105, 0.0123, 0.0136,
        0.0215, 0.0498, 0.0685, 0.0587, 0.2742, 0.2496, 0.1995],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,052][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0658, 0.0650, 0.0092, 0.0823, 0.0111, 0.0356, 0.0936, 0.0322, 0.0725,
        0.0583, 0.0675, 0.1135, 0.1581, 0.0881, 0.0151, 0.0321],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,056][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.1129, 0.0619, 0.0396, 0.0657, 0.0424, 0.0611, 0.0549, 0.0548, 0.0606,
        0.0745, 0.0549, 0.0591, 0.0560, 0.0890, 0.0525, 0.0603],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,060][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0790, 0.0557, 0.0530, 0.0628, 0.0246, 0.1047, 0.0512, 0.0305, 0.0539,
        0.0338, 0.0557, 0.0492, 0.0371, 0.0662, 0.0245, 0.2181],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,061][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.2216, 0.0782, 0.0518, 0.0935, 0.0388, 0.0411, 0.0375, 0.0473, 0.0396,
        0.0458, 0.0695, 0.0337, 0.0252, 0.0842, 0.0368, 0.0552],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,062][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2204, 0.0174, 0.0426, 0.0135, 0.0840, 0.0281, 0.0121, 0.0537, 0.0114,
        0.0927, 0.0113, 0.0213, 0.1101, 0.0159, 0.1323, 0.1147, 0.0185],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,064][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([4.3877e-04, 1.1422e-03, 1.3065e-03, 1.2506e-03, 1.2135e-04, 1.3479e-04,
        4.3277e-01, 1.8424e-04, 1.1676e-03, 9.9607e-04, 3.4081e-03, 1.4658e-02,
        2.1884e-04, 2.8378e-04, 8.7797e-05, 1.1736e-04, 5.4171e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,067][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1001, 0.0512, 0.0391, 0.0780, 0.0261, 0.0469, 0.0159, 0.0593, 0.0786,
        0.0443, 0.1192, 0.0185, 0.0197, 0.1659, 0.0307, 0.0887, 0.0179],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,069][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([3.5994e-03, 2.7353e-04, 1.0571e-04, 1.7511e-04, 6.1948e-05, 3.7664e-04,
        1.3116e-03, 5.5530e-04, 1.7493e-03, 1.7577e-03, 4.7781e-03, 1.9530e-02,
        7.9901e-03, 2.8989e-02, 1.2062e-02, 1.3243e-01, 7.8426e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,073][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0232, 0.0027, 0.0035, 0.0029, 0.0027, 0.0193, 0.0077, 0.0072, 0.0116,
        0.0268, 0.0253, 0.0193, 0.0304, 0.0225, 0.0354, 0.5619, 0.1976],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,076][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([1.6529e-02, 5.2664e-02, 1.6509e-03, 4.6213e-02, 3.5810e-04, 2.4732e-02,
        4.1456e-01, 3.8326e-03, 2.4810e-02, 4.2673e-03, 3.1376e-02, 1.0094e-01,
        8.2149e-03, 1.3132e-02, 1.2415e-04, 3.8450e-03, 2.5276e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,080][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1095, 0.0025, 0.0702, 0.0025, 0.1272, 0.0161, 0.0028, 0.1046, 0.0027,
        0.1325, 0.0055, 0.0022, 0.1134, 0.0023, 0.2293, 0.0726, 0.0042],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,081][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0134, 0.0025, 0.0024, 0.0038, 0.0040, 0.0034, 0.0039, 0.0071, 0.0092,
        0.0115, 0.0290, 0.0340, 0.0364, 0.1929, 0.1320, 0.2649, 0.2497],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,082][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0068, 0.0336, 0.0018, 0.0589, 0.0039, 0.0458, 0.1356, 0.0064, 0.0434,
        0.0111, 0.0352, 0.1941, 0.0160, 0.0993, 0.0067, 0.0233, 0.2780],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,083][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1038, 0.0636, 0.0433, 0.0699, 0.0418, 0.0470, 0.0546, 0.0461, 0.0646,
        0.0406, 0.0638, 0.0657, 0.0475, 0.0690, 0.0499, 0.0591, 0.0696],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,085][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0726, 0.0565, 0.0410, 0.0666, 0.0290, 0.0442, 0.1235, 0.0332, 0.0529,
        0.0379, 0.0682, 0.0798, 0.0417, 0.0538, 0.0284, 0.0352, 0.1356],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,088][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1754, 0.0658, 0.0629, 0.0628, 0.0493, 0.0426, 0.0482, 0.0556, 0.0275,
        0.0540, 0.0548, 0.0356, 0.0320, 0.0709, 0.0488, 0.0666, 0.0472],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,092][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0952, 0.0585, 0.0389, 0.0560, 0.0713, 0.0350, 0.0384, 0.0159, 0.0525,
        0.0261, 0.0376, 0.0498, 0.1243, 0.0481, 0.0831, 0.0559, 0.0458, 0.0675],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,094][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([1.3653e-04, 2.3869e-04, 7.6705e-04, 1.2754e-04, 4.7235e-03, 2.6149e-04,
        3.8705e-05, 2.1743e-04, 3.3389e-05, 1.9585e-03, 3.5639e-04, 1.6595e-05,
        9.8398e-05, 6.0592e-05, 3.2311e-03, 4.1758e-04, 2.2928e-05, 9.8729e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,098][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.1524, 0.0719, 0.0443, 0.0524, 0.0298, 0.0329, 0.0857, 0.0568, 0.0395,
        0.0175, 0.0549, 0.0834, 0.0375, 0.0605, 0.0294, 0.0208, 0.0953, 0.0351],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,100][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([7.7344e-04, 4.6170e-07, 4.9354e-06, 2.3885e-07, 4.0227e-06, 5.2092e-06,
        3.7190e-07, 8.1551e-06, 2.6885e-06, 1.1950e-04, 6.6422e-06, 4.4023e-06,
        1.8602e-04, 2.5255e-05, 7.7414e-04, 2.5009e-02, 1.6349e-04, 9.7291e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,101][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([2.1255e-02, 6.2510e-04, 2.2978e-03, 4.4701e-04, 1.1072e-03, 4.5982e-04,
        5.9259e-04, 2.1885e-03, 1.2519e-03, 7.3288e-03, 2.5854e-03, 2.4147e-03,
        3.5062e-03, 2.8195e-03, 9.0223e-03, 2.5955e-02, 8.9668e-03, 9.0718e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,102][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([4.1296e-03, 3.8952e-07, 4.7451e-05, 5.1771e-08, 5.3528e-05, 4.9143e-07,
        3.4114e-08, 3.8939e-06, 1.9325e-08, 3.2353e-05, 8.3800e-08, 1.0189e-08,
        6.3644e-07, 1.4325e-08, 1.7628e-05, 8.8207e-06, 9.6571e-09, 9.9570e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,104][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0747, 0.0508, 0.0845, 0.0427, 0.1205, 0.0069, 0.0282, 0.0196, 0.0266,
        0.0718, 0.0186, 0.0366, 0.0641, 0.0368, 0.1332, 0.0116, 0.0317, 0.1413],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,107][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0191, 0.0041, 0.0040, 0.0070, 0.0011, 0.0050, 0.0085, 0.0044, 0.0141,
        0.0142, 0.0349, 0.0470, 0.0290, 0.1235, 0.0180, 0.2407, 0.3237, 0.1020],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,111][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.1174, 0.0551, 0.0455, 0.0601, 0.0667, 0.0379, 0.0420, 0.0401, 0.0488,
        0.0469, 0.0529, 0.0603, 0.0616, 0.0457, 0.0771, 0.0379, 0.0493, 0.0548],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,115][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.1141, 0.0624, 0.0547, 0.0567, 0.0548, 0.0367, 0.0421, 0.0361, 0.0441,
        0.0770, 0.0417, 0.0467, 0.1056, 0.0568, 0.0607, 0.0472, 0.0490, 0.0135],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,120][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0813, 0.0544, 0.0357, 0.0555, 0.0504, 0.0346, 0.0440, 0.0365, 0.0433,
        0.0348, 0.0385, 0.0410, 0.0316, 0.0456, 0.0418, 0.0298, 0.0372, 0.2638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,121][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0814, 0.0655, 0.0335, 0.0572, 0.0307, 0.0465, 0.0425, 0.0269, 0.0474,
        0.0338, 0.0711, 0.0348, 0.0288, 0.1358, 0.0368, 0.1063, 0.0523, 0.0685],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,122][circuit_model.py][line:2294][INFO] ##0-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.1916, 0.0168, 0.0300, 0.0141, 0.0400, 0.0468, 0.0149, 0.0481, 0.0155,
        0.0531, 0.0143, 0.0224, 0.0478, 0.0155, 0.0607, 0.1253, 0.0242, 0.2055,
        0.0135], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,123][circuit_model.py][line:2297][INFO] ##0-th layer ##Weight##: The head2 weight for token [ to] are: tensor([4.0642e-03, 8.9850e-03, 8.8169e-05, 2.6124e-02, 3.8333e-05, 2.3096e-04,
        1.0627e-03, 3.7217e-05, 2.5761e-02, 1.8241e-04, 1.4921e-02, 1.4074e-03,
        6.4354e-05, 6.9822e-03, 2.8243e-05, 6.6513e-04, 9.9167e-04, 3.9291e-05,
        9.0833e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,125][circuit_model.py][line:2300][INFO] ##0-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.1016, 0.0414, 0.0356, 0.0603, 0.0197, 0.0371, 0.0154, 0.0443, 0.0651,
        0.0344, 0.0888, 0.0182, 0.0205, 0.1825, 0.0218, 0.0521, 0.0171, 0.0221,
        0.1221], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,127][circuit_model.py][line:2303][INFO] ##0-th layer ##Weight##: The head4 weight for token [ to] are: tensor([1.1909e-03, 6.2803e-05, 7.6753e-05, 3.0114e-05, 5.1139e-06, 7.1861e-05,
        1.0019e-04, 2.9883e-05, 3.9034e-04, 2.6210e-04, 5.9296e-04, 2.4093e-03,
        9.5663e-04, 4.3639e-03, 8.1937e-04, 2.4931e-01, 7.0855e-02, 2.6383e-02,
        6.4209e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,129][circuit_model.py][line:2306][INFO] ##0-th layer ##Weight##: The head5 weight for token [ to] are: tensor([2.0702e-02, 1.1075e-03, 7.4660e-04, 1.6438e-03, 3.3637e-04, 1.1075e-02,
        2.9788e-03, 4.6436e-03, 4.0585e-03, 2.6537e-02, 1.0395e-02, 7.7488e-03,
        1.5890e-02, 1.2660e-02, 3.9206e-03, 3.7214e-01, 8.3521e-02, 1.4372e-01,
        2.7617e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,132][circuit_model.py][line:2309][INFO] ##0-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.9681e-02, 4.3468e-02, 1.2988e-03, 5.8532e-02, 1.9535e-04, 2.4954e-02,
        1.8516e-01, 7.4579e-03, 3.9367e-02, 9.5769e-03, 1.6708e-02, 1.3526e-01,
        6.6728e-03, 1.6423e-02, 7.1421e-05, 4.8662e-03, 1.1447e-01, 6.7658e-04,
        3.0516e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,136][circuit_model.py][line:2312][INFO] ##0-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0805, 0.0036, 0.0419, 0.0031, 0.0483, 0.0108, 0.0029, 0.0345, 0.0043,
        0.0983, 0.0157, 0.0025, 0.0703, 0.0043, 0.0913, 0.0522, 0.0048, 0.1474,
        0.2834], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,140][circuit_model.py][line:2315][INFO] ##0-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0157, 0.0022, 0.0033, 0.0031, 0.0027, 0.0028, 0.0033, 0.0046, 0.0041,
        0.0119, 0.0119, 0.0197, 0.0172, 0.0980, 0.0619, 0.1297, 0.1707, 0.1408,
        0.2964], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,141][circuit_model.py][line:2318][INFO] ##0-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0053, 0.0382, 0.0018, 0.0686, 0.0033, 0.0310, 0.0840, 0.0056, 0.0488,
        0.0057, 0.0333, 0.1499, 0.0152, 0.1157, 0.0055, 0.0172, 0.1652, 0.0164,
        0.1892], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,142][circuit_model.py][line:2321][INFO] ##0-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0892, 0.0531, 0.0445, 0.0600, 0.0330, 0.0408, 0.0509, 0.0405, 0.0543,
        0.0431, 0.0544, 0.0584, 0.0399, 0.0612, 0.0392, 0.0536, 0.0653, 0.0370,
        0.0817], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,143][circuit_model.py][line:2324][INFO] ##0-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0707, 0.0540, 0.0287, 0.0695, 0.0232, 0.0545, 0.0627, 0.0343, 0.0620,
        0.0396, 0.0625, 0.0626, 0.0346, 0.0669, 0.0238, 0.0547, 0.0697, 0.0286,
        0.0975], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,145][circuit_model.py][line:2327][INFO] ##0-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.1560, 0.0601, 0.0606, 0.0607, 0.0418, 0.0447, 0.0393, 0.0478, 0.0283,
        0.0468, 0.0566, 0.0282, 0.0261, 0.0603, 0.0393, 0.0624, 0.0366, 0.0477,
        0.0567], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,157][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:15,158][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,159][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,160][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,160][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,161][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,162][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,162][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,163][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,164][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,164][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,165][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,166][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,166][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.9675, 0.0325], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,167][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0065, 0.9935], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,168][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.6358, 0.3642], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,168][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.6853, 0.3147], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,170][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.9772, 0.0228], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,172][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.2539, 0.7461], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,173][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.9881, 0.0119], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,175][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.9119, 0.0881], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,176][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.3215, 0.6785], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,178][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.7118, 0.2882], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,180][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.7102, 0.2898], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,181][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.6916, 0.3084], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,183][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.4955, 0.3224, 0.1821], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,184][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([1.3917e-05, 2.4242e-05, 9.9996e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,185][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.4903, 0.2362, 0.2735], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,187][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([2.5928e-02, 6.8828e-04, 9.7338e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,188][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.1191, 0.0084, 0.8725], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,189][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([9.9620e-03, 6.6615e-07, 9.9004e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,191][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.3383, 0.3619, 0.2998], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,192][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.5098, 0.3692, 0.1211], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,194][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.5817, 0.2880, 0.1303], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,196][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.6510, 0.3097, 0.0393], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,197][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.4051, 0.2464, 0.3485], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,199][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.4845, 0.2793, 0.2361], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,200][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7250, 0.0821, 0.1288, 0.0640], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,201][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0023, 0.0392, 0.0010, 0.9575], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,201][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.2201, 0.1648, 0.1082, 0.5070], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,202][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.1145, 0.3914, 0.0153, 0.4788], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,204][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.3621, 0.1640, 0.1769, 0.2971], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,205][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1236, 0.1978, 0.0029, 0.6757], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,207][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.6931, 0.0331, 0.2486, 0.0251], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,209][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.2479, 0.1885, 0.2798, 0.2838], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,210][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0670, 0.4692, 0.0239, 0.4399], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,212][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.4163, 0.2325, 0.1353, 0.2160], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,213][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.4084, 0.3042, 0.0868, 0.2007], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,215][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.4377, 0.1911, 0.1214, 0.2498], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,217][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.2983, 0.2479, 0.1048, 0.1901, 0.1590], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,218][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([4.6413e-04, 9.0983e-05, 6.0294e-04, 3.2117e-04, 9.9852e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,219][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.3680, 0.2217, 0.1073, 0.1190, 0.1839], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,220][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([2.3447e-02, 3.1144e-04, 1.3347e-02, 5.0622e-04, 9.6239e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,222][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0297, 0.0029, 0.0551, 0.0022, 0.9101], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,223][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([2.9624e-02, 3.0556e-06, 9.6966e-06, 1.0744e-06, 9.7036e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,225][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.1965, 0.2724, 0.1560, 0.1458, 0.2294], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,226][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.2892, 0.1491, 0.1102, 0.3639, 0.0876], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,228][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.3421, 0.1825, 0.1288, 0.1353, 0.2113], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,230][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.3919, 0.2275, 0.1620, 0.1911, 0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,231][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2945, 0.2148, 0.0660, 0.1358, 0.2889], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,233][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.2205, 0.1773, 0.1570, 0.2179, 0.2274], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,234][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.4376, 0.0385, 0.0815, 0.0383, 0.0670, 0.3371], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,236][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([3.2143e-04, 1.6716e-03, 2.5927e-03, 2.8552e-03, 1.3322e-04, 9.9243e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,237][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.4551, 0.1161, 0.0933, 0.1373, 0.0622, 0.1360], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,238][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([4.9393e-03, 5.4287e-03, 4.2666e-04, 1.1487e-02, 1.9679e-03, 9.7575e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,240][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.2175, 0.0619, 0.0545, 0.1120, 0.0923, 0.4619], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,241][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([3.9777e-02, 1.8562e-03, 1.4180e-03, 1.2905e-03, 6.7154e-05, 9.5559e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,242][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.2864, 0.0214, 0.2106, 0.0201, 0.4328, 0.0287], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,242][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1153, 0.0898, 0.0723, 0.2084, 0.2550, 0.2592], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,243][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.0947, 0.2660, 0.0222, 0.3494, 0.0515, 0.2162], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,244][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.3004, 0.1811, 0.1247, 0.1867, 0.0809, 0.1261], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,246][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.2531, 0.1952, 0.0644, 0.1430, 0.0338, 0.3104], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,247][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.4287, 0.1432, 0.0890, 0.1786, 0.0703, 0.0902], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,249][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.4892, 0.0564, 0.1086, 0.0426, 0.1953, 0.0702, 0.0377],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,250][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([8.5139e-04, 4.2787e-03, 6.4922e-03, 4.2767e-03, 3.8004e-04, 4.6599e-04,
        9.8326e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,252][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2843, 0.1763, 0.1050, 0.2079, 0.0648, 0.1177, 0.0439],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,253][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0293, 0.0179, 0.0112, 0.0330, 0.0147, 0.1785, 0.7154],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,255][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1410, 0.0310, 0.0631, 0.0437, 0.0672, 0.4408, 0.2132],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,257][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0560, 0.1299, 0.0039, 0.0972, 0.0007, 0.0449, 0.6675],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,258][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.3653, 0.0121, 0.1971, 0.0103, 0.3495, 0.0537, 0.0119],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,260][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0939, 0.0564, 0.0639, 0.1262, 0.1566, 0.2117, 0.2913],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,262][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0240, 0.1307, 0.0064, 0.1941, 0.0126, 0.1418, 0.4904],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,263][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2550, 0.1633, 0.0965, 0.1657, 0.0862, 0.1028, 0.1307],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,265][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.2205, 0.1880, 0.0877, 0.1594, 0.0425, 0.0816, 0.2204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,267][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.3275, 0.1313, 0.1069, 0.1410, 0.1002, 0.0846, 0.1086],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,268][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.3644, 0.0806, 0.0690, 0.0851, 0.1363, 0.0760, 0.0901, 0.0984],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,270][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([2.6490e-04, 1.0292e-03, 6.5056e-04, 1.2099e-03, 3.7711e-04, 1.3685e-03,
        5.0367e-04, 9.9460e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,271][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.2635, 0.1019, 0.0740, 0.1821, 0.1392, 0.1353, 0.0702, 0.0337],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,273][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([1.0774e-03, 1.2864e-04, 1.0028e-04, 2.5224e-04, 1.3201e-04, 2.0045e-03,
        1.6744e-03, 9.9463e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,274][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.0463, 0.0144, 0.0158, 0.0159, 0.0139, 0.0611, 0.0720, 0.7607],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,275][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([3.2516e-03, 1.1955e-04, 4.8794e-05, 1.7790e-05, 6.6889e-05, 9.6462e-06,
        5.2872e-06, 9.9648e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,277][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.2834, 0.0612, 0.1638, 0.0357, 0.1595, 0.0405, 0.0529, 0.2028],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,278][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.0901, 0.0486, 0.0925, 0.0956, 0.0253, 0.1551, 0.2866, 0.2063],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,280][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.1227, 0.1490, 0.0656, 0.1377, 0.0244, 0.1124, 0.3338, 0.0544],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,282][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.2278, 0.1445, 0.0891, 0.1398, 0.1261, 0.0939, 0.1205, 0.0583],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,283][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.2109, 0.1515, 0.0486, 0.1252, 0.0342, 0.0845, 0.0843, 0.2608],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,284][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.4441, 0.0851, 0.1177, 0.0881, 0.0584, 0.0601, 0.0443, 0.1021],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,284][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.3650, 0.0574, 0.0971, 0.0413, 0.1395, 0.0753, 0.0501, 0.1424, 0.0319],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,285][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([8.9829e-04, 1.5853e-02, 5.4463e-04, 2.1554e-02, 2.7802e-05, 6.7570e-04,
        3.8719e-04, 9.5655e-05, 9.5996e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,286][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.2911, 0.1379, 0.0939, 0.1282, 0.0377, 0.0742, 0.0588, 0.0817, 0.0965],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,288][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0052, 0.0030, 0.0019, 0.0070, 0.0027, 0.0454, 0.0611, 0.3736, 0.5001],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,289][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.1569, 0.0214, 0.0498, 0.0255, 0.0451, 0.1129, 0.0731, 0.3386, 0.1768],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,290][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([2.2640e-02, 9.1235e-02, 2.7995e-03, 1.0706e-01, 2.0609e-04, 6.5621e-02,
        2.3558e-01, 7.2063e-03, 4.6765e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,292][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.2257, 0.0192, 0.1646, 0.0170, 0.2746, 0.0440, 0.0177, 0.2251, 0.0121],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,294][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0551, 0.0267, 0.0189, 0.0558, 0.0451, 0.1102, 0.1866, 0.2769, 0.2247],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,295][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.0229, 0.1175, 0.0092, 0.1836, 0.0118, 0.1237, 0.3534, 0.0215, 0.1564],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,297][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.2003, 0.1308, 0.0791, 0.1339, 0.0644, 0.0839, 0.1092, 0.0781, 0.1203],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,299][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.1678, 0.1555, 0.0606, 0.1471, 0.0231, 0.0759, 0.0903, 0.0587, 0.2211],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,300][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.2492, 0.1084, 0.1200, 0.1278, 0.0964, 0.0745, 0.0804, 0.0956, 0.0477],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,302][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.2460, 0.0992, 0.0920, 0.0910, 0.1070, 0.0347, 0.0870, 0.0965, 0.0536,
        0.0930], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,303][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([1.4076e-04, 4.5683e-05, 3.8527e-04, 3.7480e-05, 1.8489e-03, 2.0406e-05,
        1.1337e-03, 8.8403e-04, 1.7963e-05, 9.9549e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,305][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.1719, 0.0597, 0.1131, 0.0953, 0.1558, 0.0866, 0.0794, 0.0641, 0.0879,
        0.0862], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,306][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([4.4575e-04, 8.2962e-06, 1.0093e-04, 8.7105e-06, 4.3981e-05, 2.8328e-04,
        2.2892e-04, 1.8482e-03, 2.9446e-04, 9.9674e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,308][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.0123, 0.0021, 0.0024, 0.0024, 0.0033, 0.0103, 0.0108, 0.0139, 0.0095,
        0.9329], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,309][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([4.2285e-03, 2.8178e-05, 1.3689e-05, 3.7846e-06, 5.6268e-05, 4.2784e-07,
        1.7980e-06, 1.4492e-04, 2.7548e-07, 9.9552e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,311][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.1845, 0.0391, 0.2034, 0.0305, 0.1958, 0.0256, 0.0278, 0.1217, 0.0332,
        0.1384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,313][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.0951, 0.0237, 0.0117, 0.0511, 0.0103, 0.0706, 0.1410, 0.1985, 0.2667,
        0.1313], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,314][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.1330, 0.1229, 0.0184, 0.1824, 0.0245, 0.1075, 0.2012, 0.0660, 0.1060,
        0.0382], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,316][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.1919, 0.1172, 0.0721, 0.1120, 0.0860, 0.0842, 0.0966, 0.0782, 0.1035,
        0.0583], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,318][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.1294, 0.1008, 0.0537, 0.0979, 0.0638, 0.0554, 0.0814, 0.0849, 0.0703,
        0.2624], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,320][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.3126, 0.1556, 0.0694, 0.1160, 0.0429, 0.0707, 0.0654, 0.0461, 0.0545,
        0.0669], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,321][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.2494, 0.0353, 0.0723, 0.0326, 0.1235, 0.0458, 0.0510, 0.1348, 0.0318,
        0.1996, 0.0241], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,322][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([7.4864e-04, 1.5246e-03, 3.0011e-04, 3.7641e-03, 6.4807e-04, 1.5070e-04,
        1.4839e-03, 3.0373e-05, 4.3018e-03, 3.3091e-05, 9.8701e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,324][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.1927, 0.0994, 0.0687, 0.1176, 0.0372, 0.0778, 0.0436, 0.0615, 0.1138,
        0.0775, 0.1101], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,325][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([1.0218e-03, 3.5255e-04, 3.7753e-05, 6.9202e-04, 7.8188e-04, 4.8586e-03,
        7.4725e-03, 3.9645e-03, 2.9986e-02, 1.3044e-01, 8.2039e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,326][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0617, 0.0063, 0.0058, 0.0095, 0.0168, 0.0579, 0.0341, 0.0466, 0.0704,
        0.3767, 0.3143], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,327][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0546, 0.0231, 0.0020, 0.0097, 0.0013, 0.0364, 0.0161, 0.0012, 0.0039,
        0.0009, 0.8508], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,328][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1609, 0.0123, 0.1738, 0.0120, 0.2278, 0.0394, 0.0154, 0.1708, 0.0118,
        0.1566, 0.0191], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,330][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0367, 0.0125, 0.0128, 0.0282, 0.0128, 0.0469, 0.0824, 0.1102, 0.1269,
        0.1938, 0.3367], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,332][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0239, 0.1159, 0.0083, 0.2101, 0.0104, 0.1117, 0.1968, 0.0245, 0.1660,
        0.0261, 0.1064], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,333][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.1499, 0.1046, 0.0594, 0.1132, 0.0499, 0.0783, 0.0905, 0.0696, 0.1091,
        0.0727, 0.1028], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,335][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.1416, 0.1095, 0.0486, 0.1127, 0.0439, 0.0821, 0.0949, 0.0342, 0.0910,
        0.0299, 0.2116], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,336][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.2512, 0.0861, 0.0899, 0.0894, 0.0564, 0.0533, 0.0466, 0.0841, 0.0468,
        0.0816, 0.1147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,338][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.4017, 0.0340, 0.0856, 0.0220, 0.0997, 0.0445, 0.0195, 0.0866, 0.0195,
        0.1422, 0.0209, 0.0238], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,340][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([2.9748e-03, 1.8208e-02, 4.8488e-04, 3.9237e-02, 1.6895e-04, 2.7148e-04,
        3.3095e-02, 8.6429e-05, 6.8745e-02, 7.4382e-05, 4.1111e-02, 7.9554e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,341][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1791, 0.0876, 0.0498, 0.0978, 0.0353, 0.0910, 0.0284, 0.0673, 0.1091,
        0.0606, 0.1650, 0.0289], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,343][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0108, 0.0017, 0.0009, 0.0019, 0.0007, 0.0073, 0.0189, 0.0121, 0.0616,
        0.0559, 0.1826, 0.6457], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,345][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1108, 0.0123, 0.0145, 0.0163, 0.0227, 0.0775, 0.0332, 0.0500, 0.0976,
        0.1405, 0.2303, 0.1942], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,346][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0666, 0.1044, 0.0079, 0.0855, 0.0026, 0.0639, 0.2095, 0.0509, 0.0549,
        0.0114, 0.0586, 0.2838], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,348][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1829, 0.0031, 0.1547, 0.0029, 0.2771, 0.0176, 0.0034, 0.1541, 0.0025,
        0.1931, 0.0066, 0.0020], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,350][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0379, 0.0089, 0.0097, 0.0176, 0.0241, 0.0228, 0.0364, 0.0571, 0.0940,
        0.0966, 0.2919, 0.3030], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,352][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0084, 0.0712, 0.0041, 0.1144, 0.0075, 0.0746, 0.2401, 0.0077, 0.0763,
        0.0134, 0.0509, 0.3314], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,354][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1645, 0.0929, 0.0631, 0.0964, 0.0575, 0.0675, 0.0770, 0.0630, 0.0895,
        0.0485, 0.0907, 0.0894], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,355][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1749, 0.1071, 0.0566, 0.1067, 0.0300, 0.0545, 0.1120, 0.0416, 0.0642,
        0.0301, 0.0737, 0.1486], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,357][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1913, 0.0770, 0.0867, 0.0807, 0.0833, 0.0499, 0.0835, 0.0735, 0.0305,
        0.0765, 0.0761, 0.0908], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,359][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.1677, 0.0584, 0.0553, 0.0644, 0.0998, 0.0219, 0.0733, 0.0405, 0.0549,
        0.0444, 0.0322, 0.0746, 0.2125], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,360][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([7.2587e-05, 1.2392e-03, 3.3619e-04, 8.5886e-04, 1.0352e-04, 8.9760e-04,
        3.2079e-04, 1.4536e-03, 1.1995e-03, 3.4712e-04, 2.9569e-04, 1.8757e-04,
        9.9269e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,362][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.1497, 0.0393, 0.1171, 0.0810, 0.1905, 0.0484, 0.0700, 0.0301, 0.0500,
        0.0464, 0.0849, 0.0726, 0.0198], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,363][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([3.1485e-03, 2.7352e-05, 2.2292e-04, 5.1175e-05, 4.5504e-04, 1.3823e-04,
        1.6475e-04, 2.6683e-02, 1.1132e-03, 7.6100e-03, 4.8281e-03, 3.5776e-03,
        9.5198e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,365][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0533, 0.0019, 0.0025, 0.0023, 0.0106, 0.0033, 0.0037, 0.0156, 0.0117,
        0.0235, 0.0177, 0.0231, 0.8308], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,366][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([3.7486e-03, 3.0089e-05, 1.0262e-04, 6.5236e-06, 9.2650e-06, 8.3637e-06,
        1.0965e-05, 1.9986e-03, 1.6165e-06, 1.6521e-05, 8.9260e-07, 6.0808e-06,
        9.9406e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,367][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.1369, 0.0239, 0.1932, 0.0161, 0.2551, 0.0083, 0.0115, 0.0559, 0.0097,
        0.1189, 0.0086, 0.0120, 0.1500], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,367][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.0431, 0.0093, 0.0096, 0.0243, 0.0022, 0.0315, 0.0463, 0.0409, 0.0604,
        0.2175, 0.1300, 0.3196, 0.0653], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,368][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.1467, 0.0704, 0.0480, 0.0862, 0.0811, 0.0646, 0.0711, 0.0462, 0.0723,
        0.1023, 0.0575, 0.1023, 0.0513], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,370][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.1632, 0.0925, 0.0534, 0.0876, 0.0709, 0.0739, 0.0779, 0.0597, 0.0812,
        0.0662, 0.0732, 0.0861, 0.0141], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,372][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.0912, 0.0769, 0.0492, 0.0887, 0.0418, 0.0740, 0.0806, 0.0542, 0.0722,
        0.0261, 0.0463, 0.0606, 0.2383], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,374][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.2584, 0.0794, 0.1089, 0.0507, 0.0598, 0.0228, 0.0324, 0.0891, 0.0536,
        0.0970, 0.0594, 0.0233, 0.0651], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,375][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.4542, 0.0142, 0.0477, 0.0118, 0.0843, 0.0494, 0.0222, 0.0802, 0.0091,
        0.1105, 0.0172, 0.0378, 0.0510, 0.0103], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,376][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([2.9666e-03, 2.9770e-02, 2.9895e-04, 5.5113e-03, 2.2136e-04, 1.1793e-04,
        2.6019e-04, 2.2069e-04, 2.5129e-03, 7.6652e-05, 1.1372e-03, 2.5233e-04,
        2.8909e-04, 9.5637e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,378][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.3301, 0.0421, 0.0572, 0.0361, 0.0476, 0.1848, 0.0167, 0.0446, 0.0795,
        0.0352, 0.0491, 0.0205, 0.0138, 0.0427], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,379][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([1.4020e-02, 1.6669e-03, 4.6325e-04, 1.6151e-03, 1.6303e-03, 5.0362e-03,
        5.4290e-03, 2.2060e-02, 2.1123e-02, 2.2439e-02, 7.2589e-02, 1.9717e-01,
        9.7704e-02, 5.3705e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,381][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.0305, 0.0103, 0.0058, 0.0196, 0.0084, 0.0213, 0.0182, 0.0450, 0.0600,
        0.0681, 0.1390, 0.1189, 0.1236, 0.3314], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,383][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.1023, 0.1627, 0.0393, 0.1042, 0.0054, 0.0721, 0.0783, 0.0328, 0.0515,
        0.0108, 0.0543, 0.0630, 0.0141, 0.2094], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,385][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.2551, 0.0065, 0.0997, 0.0049, 0.1976, 0.0134, 0.0065, 0.0991, 0.0040,
        0.2093, 0.0085, 0.0043, 0.0879, 0.0032], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,386][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0184, 0.0044, 0.0071, 0.0082, 0.0104, 0.0123, 0.0188, 0.0307, 0.0320,
        0.0679, 0.0954, 0.1474, 0.1376, 0.4096], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,388][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.0143, 0.1265, 0.0049, 0.1008, 0.0030, 0.0225, 0.0843, 0.0064, 0.0605,
        0.0048, 0.0399, 0.1364, 0.0117, 0.3838], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,390][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.1401, 0.0783, 0.0637, 0.0814, 0.0529, 0.0621, 0.0627, 0.0591, 0.0655,
        0.0585, 0.0687, 0.0706, 0.0502, 0.0862], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,392][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.1039, 0.0877, 0.0595, 0.0944, 0.0438, 0.0714, 0.0585, 0.0614, 0.0732,
        0.0514, 0.0723, 0.0587, 0.0403, 0.1234], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,393][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.2166, 0.0755, 0.0745, 0.0717, 0.0560, 0.0506, 0.0438, 0.0699, 0.0357,
        0.0637, 0.0755, 0.0339, 0.0389, 0.0939], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,395][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.1650, 0.1008, 0.0557, 0.0822, 0.0860, 0.0182, 0.0362, 0.0162, 0.0682,
        0.0365, 0.0650, 0.0488, 0.0406, 0.0833, 0.0974], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,396][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([2.3840e-04, 1.8784e-05, 9.7520e-05, 8.6870e-05, 5.1253e-01, 1.1592e-05,
        2.6742e-05, 4.6056e-05, 1.1895e-05, 4.3572e-04, 4.8300e-04, 7.3759e-06,
        1.6465e-05, 2.8538e-06, 4.8598e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,398][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.1567, 0.0799, 0.0507, 0.0562, 0.0996, 0.0581, 0.0768, 0.0249, 0.0629,
        0.0248, 0.0470, 0.0835, 0.0287, 0.0565, 0.0937], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,399][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([1.8959e-03, 2.0685e-06, 6.7934e-05, 1.2104e-06, 3.4314e-03, 4.5710e-06,
        5.8958e-06, 2.1427e-05, 1.9077e-05, 8.2374e-04, 7.8668e-05, 7.5162e-05,
        3.3184e-03, 2.4529e-04, 9.9001e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,401][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([3.8950e-03, 2.1279e-04, 2.9079e-03, 1.2316e-04, 4.6709e-02, 2.5002e-04,
        1.8042e-04, 3.1130e-04, 6.5129e-04, 3.4356e-04, 1.8363e-03, 8.4368e-04,
        2.5457e-03, 2.6826e-03, 9.3651e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,402][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([6.0337e-03, 4.5202e-07, 2.0228e-06, 1.7918e-07, 6.2780e-01, 2.2215e-07,
        1.8633e-08, 6.5437e-06, 1.0067e-08, 5.5789e-06, 1.7657e-07, 8.9191e-09,
        8.4902e-08, 7.8677e-09, 3.6615e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,403][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0872, 0.1081, 0.0943, 0.0668, 0.1504, 0.0166, 0.0370, 0.0183, 0.0472,
        0.0255, 0.0204, 0.0806, 0.0517, 0.0402, 0.1557], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,405][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0557, 0.0087, 0.0059, 0.0165, 0.0040, 0.0160, 0.0245, 0.0153, 0.0562,
        0.0267, 0.1091, 0.1346, 0.0556, 0.4023, 0.0690], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,407][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1708, 0.0782, 0.0690, 0.0680, 0.1295, 0.0243, 0.0488, 0.0241, 0.0528,
        0.0197, 0.0439, 0.0495, 0.0273, 0.0514, 0.1429], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,408][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.1684, 0.0981, 0.0832, 0.0878, 0.0141, 0.0485, 0.0597, 0.0490, 0.0692,
        0.0319, 0.0705, 0.0685, 0.0529, 0.0839, 0.0144], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,409][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.0812, 0.0599, 0.0305, 0.0565, 0.2395, 0.0331, 0.0433, 0.0244, 0.0409,
        0.0296, 0.0468, 0.0332, 0.0202, 0.0453, 0.2158], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,410][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0656, 0.0510, 0.0584, 0.0552, 0.0726, 0.0496, 0.0468, 0.0636, 0.0327,
        0.0913, 0.0708, 0.0587, 0.0743, 0.1120, 0.0973], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,412][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.2181, 0.0215, 0.0320, 0.0258, 0.0491, 0.1070, 0.0204, 0.0520, 0.0252,
        0.0417, 0.0206, 0.0191, 0.0509, 0.0171, 0.0643, 0.2353],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,413][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([3.7431e-04, 5.8112e-04, 2.7942e-03, 1.2637e-03, 9.4087e-05, 6.5247e-03,
        1.4091e-04, 3.2776e-04, 7.6421e-04, 1.2306e-04, 3.5720e-04, 1.7541e-04,
        1.5129e-04, 5.0774e-05, 6.4635e-05, 9.8621e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,415][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.1676, 0.0420, 0.0364, 0.0506, 0.0455, 0.0754, 0.0435, 0.0567, 0.0723,
        0.0635, 0.0713, 0.0535, 0.0281, 0.0711, 0.0473, 0.0751],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,416][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([8.5871e-04, 4.6006e-06, 1.5856e-06, 3.9968e-06, 3.0338e-06, 4.5570e-05,
        1.1799e-05, 3.8633e-05, 4.7150e-05, 2.9226e-04, 1.4114e-04, 2.0651e-04,
        6.3361e-04, 6.5208e-04, 9.8672e-04, 9.9607e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,417][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0812, 0.0063, 0.0023, 0.0063, 0.0054, 0.0104, 0.0073, 0.0099, 0.0185,
        0.0222, 0.0385, 0.0209, 0.0748, 0.0420, 0.0559, 0.5979],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,418][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([3.9492e-02, 2.1587e-05, 1.1947e-03, 2.1849e-05, 5.3602e-05, 1.8019e-03,
        1.2753e-05, 7.0335e-06, 5.9570e-06, 5.3200e-05, 1.5017e-05, 7.0719e-06,
        4.2984e-06, 1.0694e-06, 1.8500e-05, 9.5729e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,420][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0867, 0.0109, 0.0804, 0.0106, 0.1524, 0.0129, 0.0155, 0.0699, 0.0080,
        0.1340, 0.0094, 0.0120, 0.1236, 0.0111, 0.2359, 0.0266],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,422][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0167, 0.0037, 0.0026, 0.0055, 0.0079, 0.0055, 0.0105, 0.0123, 0.0136,
        0.0215, 0.0498, 0.0685, 0.0587, 0.2742, 0.2496, 0.1995],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,424][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0658, 0.0650, 0.0092, 0.0823, 0.0111, 0.0356, 0.0936, 0.0322, 0.0725,
        0.0583, 0.0675, 0.1135, 0.1581, 0.0881, 0.0151, 0.0321],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,426][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.1129, 0.0619, 0.0396, 0.0657, 0.0424, 0.0611, 0.0549, 0.0548, 0.0606,
        0.0745, 0.0549, 0.0591, 0.0560, 0.0890, 0.0525, 0.0603],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,427][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0790, 0.0557, 0.0530, 0.0628, 0.0246, 0.1047, 0.0512, 0.0305, 0.0539,
        0.0338, 0.0557, 0.0492, 0.0371, 0.0662, 0.0245, 0.2181],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,429][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.2216, 0.0782, 0.0518, 0.0935, 0.0388, 0.0411, 0.0375, 0.0473, 0.0396,
        0.0458, 0.0695, 0.0337, 0.0252, 0.0842, 0.0368, 0.0552],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,431][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2204, 0.0174, 0.0426, 0.0135, 0.0840, 0.0281, 0.0121, 0.0537, 0.0114,
        0.0927, 0.0113, 0.0213, 0.1101, 0.0159, 0.1323, 0.1147, 0.0185],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,432][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([4.3877e-04, 1.1422e-03, 1.3065e-03, 1.2506e-03, 1.2135e-04, 1.3479e-04,
        4.3277e-01, 1.8424e-04, 1.1676e-03, 9.9607e-04, 3.4081e-03, 1.4658e-02,
        2.1884e-04, 2.8378e-04, 8.7797e-05, 1.1736e-04, 5.4171e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,434][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1001, 0.0512, 0.0391, 0.0780, 0.0261, 0.0469, 0.0159, 0.0593, 0.0786,
        0.0443, 0.1192, 0.0185, 0.0197, 0.1659, 0.0307, 0.0887, 0.0179],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,435][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([3.5994e-03, 2.7353e-04, 1.0571e-04, 1.7511e-04, 6.1948e-05, 3.7664e-04,
        1.3116e-03, 5.5530e-04, 1.7493e-03, 1.7577e-03, 4.7781e-03, 1.9530e-02,
        7.9901e-03, 2.8989e-02, 1.2062e-02, 1.3243e-01, 7.8426e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,437][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0232, 0.0027, 0.0035, 0.0029, 0.0027, 0.0193, 0.0077, 0.0072, 0.0116,
        0.0268, 0.0253, 0.0193, 0.0304, 0.0225, 0.0354, 0.5619, 0.1976],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,438][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([1.6529e-02, 5.2664e-02, 1.6509e-03, 4.6213e-02, 3.5810e-04, 2.4732e-02,
        4.1456e-01, 3.8326e-03, 2.4810e-02, 4.2673e-03, 3.1376e-02, 1.0094e-01,
        8.2149e-03, 1.3132e-02, 1.2415e-04, 3.8450e-03, 2.5276e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,440][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1095, 0.0025, 0.0702, 0.0025, 0.1272, 0.0161, 0.0028, 0.1046, 0.0027,
        0.1325, 0.0055, 0.0022, 0.1134, 0.0023, 0.2293, 0.0726, 0.0042],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,442][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0134, 0.0025, 0.0024, 0.0038, 0.0040, 0.0034, 0.0039, 0.0071, 0.0092,
        0.0115, 0.0290, 0.0340, 0.0364, 0.1929, 0.1320, 0.2649, 0.2497],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,444][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0068, 0.0336, 0.0018, 0.0589, 0.0039, 0.0458, 0.1356, 0.0064, 0.0434,
        0.0111, 0.0352, 0.1941, 0.0160, 0.0993, 0.0067, 0.0233, 0.2780],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,446][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1038, 0.0636, 0.0433, 0.0699, 0.0418, 0.0470, 0.0546, 0.0461, 0.0646,
        0.0406, 0.0638, 0.0657, 0.0475, 0.0690, 0.0499, 0.0591, 0.0696],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,447][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0726, 0.0565, 0.0410, 0.0666, 0.0290, 0.0442, 0.1235, 0.0332, 0.0529,
        0.0379, 0.0682, 0.0798, 0.0417, 0.0538, 0.0284, 0.0352, 0.1356],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,449][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1754, 0.0658, 0.0629, 0.0628, 0.0493, 0.0426, 0.0482, 0.0556, 0.0275,
        0.0540, 0.0548, 0.0356, 0.0320, 0.0709, 0.0488, 0.0666, 0.0472],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,450][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0952, 0.0585, 0.0389, 0.0560, 0.0713, 0.0350, 0.0384, 0.0159, 0.0525,
        0.0261, 0.0376, 0.0498, 0.1243, 0.0481, 0.0831, 0.0559, 0.0458, 0.0675],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,451][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([1.3653e-04, 2.3869e-04, 7.6705e-04, 1.2754e-04, 4.7235e-03, 2.6149e-04,
        3.8705e-05, 2.1743e-04, 3.3389e-05, 1.9585e-03, 3.5639e-04, 1.6595e-05,
        9.8398e-05, 6.0592e-05, 3.2311e-03, 4.1758e-04, 2.2928e-05, 9.8729e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,452][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.1524, 0.0719, 0.0443, 0.0524, 0.0298, 0.0329, 0.0857, 0.0568, 0.0395,
        0.0175, 0.0549, 0.0834, 0.0375, 0.0605, 0.0294, 0.0208, 0.0953, 0.0351],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,453][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([7.7344e-04, 4.6170e-07, 4.9354e-06, 2.3885e-07, 4.0227e-06, 5.2092e-06,
        3.7190e-07, 8.1551e-06, 2.6885e-06, 1.1950e-04, 6.6422e-06, 4.4023e-06,
        1.8602e-04, 2.5255e-05, 7.7414e-04, 2.5009e-02, 1.6349e-04, 9.7291e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,455][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([2.1255e-02, 6.2510e-04, 2.2978e-03, 4.4701e-04, 1.1072e-03, 4.5982e-04,
        5.9259e-04, 2.1885e-03, 1.2519e-03, 7.3288e-03, 2.5854e-03, 2.4147e-03,
        3.5062e-03, 2.8195e-03, 9.0223e-03, 2.5955e-02, 8.9668e-03, 9.0718e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,456][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([4.1296e-03, 3.8952e-07, 4.7451e-05, 5.1771e-08, 5.3528e-05, 4.9143e-07,
        3.4114e-08, 3.8939e-06, 1.9325e-08, 3.2353e-05, 8.3800e-08, 1.0189e-08,
        6.3644e-07, 1.4325e-08, 1.7628e-05, 8.8207e-06, 9.6571e-09, 9.9570e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,458][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0747, 0.0508, 0.0845, 0.0427, 0.1205, 0.0069, 0.0282, 0.0196, 0.0266,
        0.0718, 0.0186, 0.0366, 0.0641, 0.0368, 0.1332, 0.0116, 0.0317, 0.1413],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,460][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0191, 0.0041, 0.0040, 0.0070, 0.0011, 0.0050, 0.0085, 0.0044, 0.0141,
        0.0142, 0.0349, 0.0470, 0.0290, 0.1235, 0.0180, 0.2407, 0.3237, 0.1020],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,461][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.1174, 0.0551, 0.0455, 0.0601, 0.0667, 0.0379, 0.0420, 0.0401, 0.0488,
        0.0469, 0.0529, 0.0603, 0.0616, 0.0457, 0.0771, 0.0379, 0.0493, 0.0548],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,463][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.1141, 0.0624, 0.0547, 0.0567, 0.0548, 0.0367, 0.0421, 0.0361, 0.0441,
        0.0770, 0.0417, 0.0467, 0.1056, 0.0568, 0.0607, 0.0472, 0.0490, 0.0135],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,465][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0813, 0.0544, 0.0357, 0.0555, 0.0504, 0.0346, 0.0440, 0.0365, 0.0433,
        0.0348, 0.0385, 0.0410, 0.0316, 0.0456, 0.0418, 0.0298, 0.0372, 0.2638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,467][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0814, 0.0655, 0.0335, 0.0572, 0.0307, 0.0465, 0.0425, 0.0269, 0.0474,
        0.0338, 0.0711, 0.0348, 0.0288, 0.1358, 0.0368, 0.1063, 0.0523, 0.0685],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,469][circuit_model.py][line:2332][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1916, 0.0168, 0.0300, 0.0141, 0.0400, 0.0468, 0.0149, 0.0481, 0.0155,
        0.0531, 0.0143, 0.0224, 0.0478, 0.0155, 0.0607, 0.1253, 0.0242, 0.2055,
        0.0135], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,470][circuit_model.py][line:2335][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([4.0642e-03, 8.9850e-03, 8.8169e-05, 2.6124e-02, 3.8333e-05, 2.3096e-04,
        1.0627e-03, 3.7217e-05, 2.5761e-02, 1.8241e-04, 1.4921e-02, 1.4074e-03,
        6.4354e-05, 6.9822e-03, 2.8243e-05, 6.6513e-04, 9.9167e-04, 3.9291e-05,
        9.0833e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,472][circuit_model.py][line:2338][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1016, 0.0414, 0.0356, 0.0603, 0.0197, 0.0371, 0.0154, 0.0443, 0.0651,
        0.0344, 0.0888, 0.0182, 0.0205, 0.1825, 0.0218, 0.0521, 0.0171, 0.0221,
        0.1221], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,473][circuit_model.py][line:2341][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.1909e-03, 6.2803e-05, 7.6753e-05, 3.0114e-05, 5.1139e-06, 7.1861e-05,
        1.0019e-04, 2.9883e-05, 3.9034e-04, 2.6210e-04, 5.9296e-04, 2.4093e-03,
        9.5663e-04, 4.3639e-03, 8.1937e-04, 2.4931e-01, 7.0855e-02, 2.6383e-02,
        6.4209e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,474][circuit_model.py][line:2344][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([2.0702e-02, 1.1075e-03, 7.4660e-04, 1.6438e-03, 3.3637e-04, 1.1075e-02,
        2.9788e-03, 4.6436e-03, 4.0585e-03, 2.6537e-02, 1.0395e-02, 7.7488e-03,
        1.5890e-02, 1.2660e-02, 3.9206e-03, 3.7214e-01, 8.3521e-02, 1.4372e-01,
        2.7617e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,476][circuit_model.py][line:2347][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([2.9681e-02, 4.3468e-02, 1.2988e-03, 5.8532e-02, 1.9535e-04, 2.4954e-02,
        1.8516e-01, 7.4579e-03, 3.9367e-02, 9.5769e-03, 1.6708e-02, 1.3526e-01,
        6.6728e-03, 1.6423e-02, 7.1421e-05, 4.8662e-03, 1.1447e-01, 6.7658e-04,
        3.0516e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,478][circuit_model.py][line:2350][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0805, 0.0036, 0.0419, 0.0031, 0.0483, 0.0108, 0.0029, 0.0345, 0.0043,
        0.0983, 0.0157, 0.0025, 0.0703, 0.0043, 0.0913, 0.0522, 0.0048, 0.1474,
        0.2834], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,479][circuit_model.py][line:2353][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0157, 0.0022, 0.0033, 0.0031, 0.0027, 0.0028, 0.0033, 0.0046, 0.0041,
        0.0119, 0.0119, 0.0197, 0.0172, 0.0980, 0.0619, 0.1297, 0.1707, 0.1408,
        0.2964], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,481][circuit_model.py][line:2356][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0053, 0.0382, 0.0018, 0.0686, 0.0033, 0.0310, 0.0840, 0.0056, 0.0488,
        0.0057, 0.0333, 0.1499, 0.0152, 0.1157, 0.0055, 0.0172, 0.1652, 0.0164,
        0.1892], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,483][circuit_model.py][line:2359][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0892, 0.0531, 0.0445, 0.0600, 0.0330, 0.0408, 0.0509, 0.0405, 0.0543,
        0.0431, 0.0544, 0.0584, 0.0399, 0.0612, 0.0392, 0.0536, 0.0653, 0.0370,
        0.0817], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,485][circuit_model.py][line:2362][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0707, 0.0540, 0.0287, 0.0695, 0.0232, 0.0545, 0.0627, 0.0343, 0.0620,
        0.0396, 0.0625, 0.0626, 0.0346, 0.0669, 0.0238, 0.0547, 0.0697, 0.0286,
        0.0975], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,487][circuit_model.py][line:2365][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.1560, 0.0601, 0.0606, 0.0607, 0.0418, 0.0447, 0.0393, 0.0478, 0.0283,
        0.0468, 0.0566, 0.0282, 0.0261, 0.0603, 0.0393, 0.0624, 0.0366, 0.0477,
        0.0567], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,490][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:15,492][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 6820],
        [ 5887],
        [    1],
        [11436],
        [ 1213],
        [ 6752],
        [18795],
        [11212],
        [17540],
        [13002],
        [10093],
        [27517],
        [33786],
        [ 2929],
        [ 2019],
        [ 2575],
        [17091],
        [ 6118],
        [10605]], device='cuda:0')
[2024-07-24 10:25:15,493][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[26155],
        [32809],
        [    1],
        [42492],
        [ 4797],
        [43587],
        [35190],
        [42095],
        [40490],
        [34964],
        [33662],
        [28035],
        [40769],
        [41007],
        [ 2228],
        [28838],
        [25647],
        [19678],
        [34812]], device='cuda:0')
[2024-07-24 10:25:15,495][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 7497],
        [ 7772],
        [12245],
        [ 8829],
        [15276],
        [ 8780],
        [ 9708],
        [12528],
        [11018],
        [13453],
        [10202],
        [ 8645],
        [ 9722],
        [ 7343],
        [17675],
        [ 7685],
        [ 6491],
        [12932],
        [ 7142]], device='cuda:0')
[2024-07-24 10:25:15,497][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 9824],
        [ 4827],
        [13921],
        [13969],
        [ 5323],
        [31724],
        [43166],
        [13021],
        [32739],
        [30884],
        [35061],
        [45884],
        [45359],
        [ 3651],
        [ 5212],
        [ 8430],
        [43126],
        [13080],
        [30713]], device='cuda:0')
[2024-07-24 10:25:15,499][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[13834],
        [ 9629],
        [ 7539],
        [15888],
        [12938],
        [14989],
        [14596],
        [17740],
        [15662],
        [17979],
        [18770],
        [20336],
        [20229],
        [20126],
        [21951],
        [21925],
        [26322],
        [18016],
        [31000]], device='cuda:0')
[2024-07-24 10:25:15,501][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[32021],
        [30820],
        [  194],
        [16214],
        [19281],
        [ 4382],
        [12829],
        [19128],
        [25617],
        [ 3821],
        [23883],
        [19911],
        [15942],
        [30238],
        [18357],
        [17680],
        [17696],
        [ 6160],
        [20961]], device='cuda:0')
[2024-07-24 10:25:15,502][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[19961],
        [19723],
        [   38],
        [ 5517],
        [ 4819],
        [13644],
        [13821],
        [ 4765],
        [ 4635],
        [ 1320],
        [ 2785],
        [ 7720],
        [38218],
        [ 6370],
        [ 5551],
        [28441],
        [25475],
        [26697],
        [20550]], device='cuda:0')
[2024-07-24 10:25:15,504][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[26822],
        [26873],
        [ 7199],
        [32699],
        [20337],
        [15006],
        [25274],
        [20186],
        [30073],
        [19381],
        [30526],
        [28074],
        [23290],
        [30595],
        [20989],
        [29796],
        [26441],
        [17120],
        [30841]], device='cuda:0')
[2024-07-24 10:25:15,506][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[35919],
        [35708],
        [ 5405],
        [ 9649],
        [ 1878],
        [  438],
        [  548],
        [ 1935],
        [  807],
        [  599],
        [  878],
        [ 1144],
        [  832],
        [ 3355],
        [  922],
        [  845],
        [ 1195],
        [  968],
        [ 4734]], device='cuda:0')
[2024-07-24 10:25:15,508][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[13582],
        [13454],
        [10252],
        [17579],
        [26227],
        [35275],
        [44197],
        [42786],
        [39846],
        [35616],
        [26610],
        [35950],
        [40528],
        [42367],
        [38680],
        [34763],
        [43457],
        [42857],
        [39619]], device='cuda:0')
[2024-07-24 10:25:15,510][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[16062],
        [20536],
        [14132],
        [21429],
        [14662],
        [20625],
        [15254],
        [12466],
        [15276],
        [13022],
        [13039],
        [11975],
        [ 7840],
        [ 8391],
        [10182],
        [ 7288],
        [ 9833],
        [ 7877],
        [ 9823]], device='cuda:0')
[2024-07-24 10:25:15,512][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[15410],
        [28184],
        [28334],
        [31713],
        [29502],
        [31779],
        [30871],
        [27202],
        [31058],
        [26643],
        [29279],
        [28703],
        [26771],
        [26197],
        [28076],
        [24874],
        [27429],
        [16653],
        [27214]], device='cuda:0')
[2024-07-24 10:25:15,514][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[15269],
        [ 7543],
        [29136],
        [ 5728],
        [16055],
        [ 6422],
        [13796],
        [11172],
        [ 5463],
        [27447],
        [ 7437],
        [13822],
        [28588],
        [ 5877],
        [32317],
        [ 8785],
        [18406],
        [28572],
        [ 8257]], device='cuda:0')
[2024-07-24 10:25:15,516][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[23335],
        [16820],
        [16550],
        [12263],
        [11369],
        [11267],
        [12675],
        [14235],
        [10650],
        [10471],
        [ 9355],
        [12191],
        [10866],
        [10884],
        [11280],
        [10027],
        [11242],
        [ 9354],
        [ 9241]], device='cuda:0')
[2024-07-24 10:25:15,518][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[11031],
        [ 5138],
        [    1],
        [15227],
        [  422],
        [25471],
        [21954],
        [14649],
        [13710],
        [26017],
        [ 3864],
        [40155],
        [44941],
        [ 1471],
        [  336],
        [12427],
        [21322],
        [ 8120],
        [13416]], device='cuda:0')
[2024-07-24 10:25:15,520][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[10420],
        [10056],
        [ 5812],
        [ 7586],
        [ 9195],
        [10888],
        [ 7806],
        [ 8029],
        [ 7656],
        [ 8845],
        [ 8503],
        [ 7760],
        [13275],
        [ 8709],
        [11346],
        [10409],
        [11406],
        [12926],
        [13948]], device='cuda:0')
[2024-07-24 10:25:15,521][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[19092],
        [42792],
        [31288],
        [33720],
        [37871],
        [35137],
        [27284],
        [39069],
        [34442],
        [38424],
        [28700],
        [26300],
        [34210],
        [41464],
        [35826],
        [32365],
        [26371],
        [35023],
        [24230]], device='cuda:0')
[2024-07-24 10:25:15,523][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[46652],
        [43328],
        [44216],
        [45037],
        [45400],
        [46003],
        [45772],
        [46533],
        [45594],
        [46029],
        [44621],
        [44209],
        [46406],
        [44742],
        [46309],
        [45100],
        [42989],
        [45671],
        [40382]], device='cuda:0')
[2024-07-24 10:25:15,525][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[36815],
        [43347],
        [42298],
        [38536],
        [35789],
        [41925],
        [34709],
        [47185],
        [40034],
        [47867],
        [35458],
        [34286],
        [42712],
        [40703],
        [39449],
        [41293],
        [33731],
        [41420],
        [37429]], device='cuda:0')
[2024-07-24 10:25:15,527][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[28790],
        [29403],
        [40278],
        [37163],
        [14519],
        [29236],
        [26903],
        [32053],
        [32427],
        [31840],
        [31196],
        [31016],
        [27786],
        [25975],
        [10958],
        [25582],
        [24852],
        [29675],
        [22312]], device='cuda:0')
[2024-07-24 10:25:15,529][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[24039],
        [20837],
        [24937],
        [22504],
        [15836],
        [17963],
        [18641],
        [17613],
        [22192],
        [ 4216],
        [20479],
        [15776],
        [15594],
        [20843],
        [14393],
        [ 8808],
        [16168],
        [ 8643],
        [17897]], device='cuda:0')
[2024-07-24 10:25:15,531][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[42741],
        [42674],
        [15416],
        [23313],
        [13517],
        [ 8645],
        [11246],
        [13839],
        [12327],
        [ 5920],
        [ 6773],
        [ 5914],
        [ 3421],
        [ 8194],
        [ 8649],
        [ 5531],
        [ 7901],
        [ 6176],
        [23659]], device='cuda:0')
[2024-07-24 10:25:15,533][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[10869],
        [10879],
        [ 8069],
        [ 5404],
        [ 7840],
        [15454],
        [ 7848],
        [ 4783],
        [ 5982],
        [ 5821],
        [ 5940],
        [ 6805],
        [ 5392],
        [10676],
        [14312],
        [25556],
        [17741],
        [10227],
        [ 9293]], device='cuda:0')
[2024-07-24 10:25:15,535][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[33283],
        [17512],
        [22873],
        [22848],
        [19323],
        [24324],
        [20493],
        [22787],
        [23524],
        [25310],
        [26159],
        [22120],
        [25094],
        [25530],
        [21623],
        [26223],
        [22745],
        [23863],
        [25925]], device='cuda:0')
[2024-07-24 10:25:15,537][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[14764],
        [25276],
        [25653],
        [30127],
        [29157],
        [32174],
        [35246],
        [35263],
        [36604],
        [36841],
        [37992],
        [37745],
        [38341],
        [39295],
        [38482],
        [39865],
        [39881],
        [41846],
        [40128]], device='cuda:0')
[2024-07-24 10:25:15,538][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[46184],
        [49926],
        [45017],
        [50130],
        [47829],
        [50069],
        [50064],
        [49574],
        [50147],
        [46806],
        [50164],
        [50084],
        [47107],
        [50023],
        [44948],
        [49806],
        [49904],
        [44553],
        [50015]], device='cuda:0')
[2024-07-24 10:25:15,540][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[24576],
        [10197],
        [ 6542],
        [ 6381],
        [16384],
        [ 6022],
        [11818],
        [ 4351],
        [11499],
        [10533],
        [ 9306],
        [13861],
        [10252],
        [10005],
        [14204],
        [ 8908],
        [10859],
        [12191],
        [10748]], device='cuda:0')
[2024-07-24 10:25:15,542][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[3160],
        [1579],
        [7773],
        [2935],
        [6449],
        [3201],
        [5019],
        [4103],
        [4405],
        [7203],
        [5464],
        [5718],
        [7524],
        [4417],
        [8187],
        [5061],
        [5735],
        [8733],
        [4184]], device='cuda:0')
[2024-07-24 10:25:15,544][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[28967],
        [37444],
        [50256],
        [25173],
        [49017],
        [16114],
        [19807],
        [27628],
        [27319],
        [16350],
        [41891],
        [ 4966],
        [ 3148],
        [45888],
        [49260],
        [29324],
        [20598],
        [35013],
        [27927]], device='cuda:0')
[2024-07-24 10:25:15,546][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173],
        [11173]], device='cuda:0')
[2024-07-24 10:25:15,576][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:15,577][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,579][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,580][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,581][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,581][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,582][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,583][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,584][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,584][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,585][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,586][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,586][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,587][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.2808, 0.7192], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,588][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [,] are: tensor([3.4444e-06, 1.0000e+00], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,588][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.8572, 0.1428], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,589][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.3691, 0.6309], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,590][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.7070, 0.2930], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,590][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.7870, 0.2130], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,592][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.9878, 0.0122], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,594][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.9913, 0.0087], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,595][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.8399, 0.1601], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,596][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1527, 0.8473], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,597][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.0337, 0.9663], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,597][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2057, 0.7943], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,599][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.0429, 0.3797, 0.5774], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,600][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([4.5787e-05, 9.9788e-01, 2.0750e-03], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,602][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.5250, 0.2313, 0.2437], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,603][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.3406, 0.4920, 0.1674], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,604][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.4448, 0.3530, 0.2022], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,604][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.6250, 0.2976, 0.0774], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,605][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.9926, 0.0034, 0.0040], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,606][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.8230, 0.1388, 0.0382], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,607][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.7206, 0.2239, 0.0555], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,609][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.0406, 0.8653, 0.0941], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,610][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.0386, 0.4609, 0.5005], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,612][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.2409, 0.6524, 0.1067], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,614][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0392, 0.2118, 0.5394, 0.2096], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,615][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ and] are: tensor([1.1229e-03, 9.9441e-01, 4.1991e-03, 2.6516e-04], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,616][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.4034, 0.1289, 0.2087, 0.2590], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,618][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2182, 0.2845, 0.1273, 0.3700], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,619][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.3950, 0.1531, 0.2432, 0.2087], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,621][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.2695, 0.3801, 0.0089, 0.3415], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,623][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.9869, 0.0048, 0.0066, 0.0018], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,624][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.5067, 0.0458, 0.1949, 0.2526], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,626][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.6071, 0.1168, 0.0919, 0.1843], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,628][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0074, 0.0271, 0.0327, 0.9328], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,629][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0260, 0.3018, 0.4419, 0.2303], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,631][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0679, 0.1473, 0.0011, 0.7837], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,633][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0108, 0.1231, 0.4659, 0.1426, 0.2576], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,634][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([3.8507e-03, 9.9347e-01, 2.0229e-03, 6.5497e-04, 5.2376e-07],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,635][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.3073, 0.1578, 0.1670, 0.2643, 0.1035], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,637][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.1700, 0.2684, 0.0969, 0.3131, 0.1516], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,638][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.3244, 0.2348, 0.1464, 0.1813, 0.1130], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,640][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.2801, 0.1840, 0.2436, 0.2095, 0.0828], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,642][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.8816, 0.0259, 0.0220, 0.0130, 0.0574], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,643][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.3702, 0.0614, 0.3103, 0.2073, 0.0508], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,645][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.5296, 0.2581, 0.0465, 0.1282, 0.0376], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,646][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0119, 0.3899, 0.0960, 0.3638, 0.1384], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,646][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0226, 0.2686, 0.2911, 0.1979, 0.2199], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,647][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0470, 0.1900, 0.0022, 0.7506, 0.0103], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,648][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.0411, 0.1186, 0.2207, 0.1580, 0.1664, 0.2951], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,649][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ had] are: tensor([5.7426e-04, 9.9631e-01, 2.1993e-03, 5.4673e-04, 3.5468e-07, 3.7142e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,651][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.2344, 0.1473, 0.1694, 0.1831, 0.1226, 0.1431], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,652][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.1314, 0.1959, 0.0841, 0.2346, 0.1512, 0.2028], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,654][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.2859, 0.1286, 0.1364, 0.1389, 0.1246, 0.1857], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,656][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.3302, 0.4304, 0.0496, 0.0713, 0.0263, 0.0923], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,657][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ had] are: tensor([9.3024e-01, 1.2348e-02, 1.7950e-02, 4.1549e-03, 3.4419e-02, 8.9215e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,658][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.2596, 0.0356, 0.1564, 0.1429, 0.2396, 0.1660], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,660][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.4412, 0.1571, 0.1097, 0.0923, 0.0957, 0.1041], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,662][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0072, 0.0622, 0.0733, 0.3356, 0.5143, 0.0074], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,663][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.0117, 0.1983, 0.2489, 0.1286, 0.2190, 0.1934], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,664][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ had] are: tensor([4.2128e-02, 2.1065e-01, 2.1337e-03, 6.2181e-01, 6.0504e-04, 1.2267e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:15,666][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0124, 0.0679, 0.1496, 0.0837, 0.1242, 0.3317, 0.2305],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,667][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([6.0378e-03, 9.9188e-01, 1.7018e-03, 2.0364e-04, 2.0889e-07, 1.7768e-04,
        1.9695e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,669][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2464, 0.1078, 0.1423, 0.1664, 0.1003, 0.0968, 0.1401],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,671][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1118, 0.1480, 0.0669, 0.1805, 0.1132, 0.1521, 0.2275],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,672][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2754, 0.1107, 0.1052, 0.1208, 0.1174, 0.1446, 0.1258],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,674][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.2198, 0.3768, 0.0333, 0.1010, 0.1002, 0.0384, 0.1306],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,676][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.8202, 0.0273, 0.0534, 0.0134, 0.0768, 0.0025, 0.0064],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,677][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2289, 0.0400, 0.1295, 0.1206, 0.1492, 0.1391, 0.1927],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,679][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2984, 0.0794, 0.0560, 0.0691, 0.0494, 0.0576, 0.3902],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,681][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0048, 0.0281, 0.0596, 0.3711, 0.5267, 0.0038, 0.0060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,682][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0152, 0.1596, 0.2350, 0.1194, 0.1945, 0.1603, 0.1160],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,684][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0473, 0.1480, 0.0027, 0.5004, 0.0007, 0.0100, 0.2908],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:15,686][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0074, 0.0397, 0.1088, 0.0543, 0.0832, 0.1277, 0.3925, 0.1865],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,686][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([2.4028e-05, 8.5223e-01, 3.7001e-03, 2.5611e-03, 2.4808e-06, 2.2349e-03,
        5.5196e-05, 1.3920e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,687][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.2479, 0.1125, 0.1263, 0.1369, 0.0885, 0.0863, 0.0995, 0.1020],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,688][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.1068, 0.1437, 0.0567, 0.1714, 0.0966, 0.1335, 0.1803, 0.1109],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,689][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.1965, 0.1120, 0.1077, 0.1171, 0.1173, 0.1291, 0.1168, 0.1035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,690][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.1723, 0.3738, 0.0050, 0.0812, 0.0251, 0.0260, 0.1262, 0.1903],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,692][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.8215, 0.0491, 0.0546, 0.0166, 0.0434, 0.0024, 0.0055, 0.0069],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,694][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.2738, 0.0362, 0.1310, 0.0929, 0.1389, 0.1345, 0.1514, 0.0413],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,695][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.2345, 0.0691, 0.0357, 0.0646, 0.0393, 0.0823, 0.4008, 0.0737],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,697][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0055, 0.1583, 0.0596, 0.4991, 0.2226, 0.0172, 0.0355, 0.0023],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,698][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.0151, 0.1356, 0.2237, 0.0994, 0.1822, 0.1287, 0.0930, 0.1224],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,700][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.0412, 0.1782, 0.0009, 0.6346, 0.0010, 0.0144, 0.0290, 0.1007],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:15,702][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0055, 0.0324, 0.1013, 0.0448, 0.0629, 0.0994, 0.2640, 0.3560, 0.0336],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,703][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ of] are: tensor([1.2829e-02, 7.5132e-01, 2.7332e-03, 2.9584e-04, 7.0810e-07, 3.2717e-04,
        7.4816e-06, 2.3248e-01, 1.1664e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,705][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1876, 0.0867, 0.1297, 0.1226, 0.1012, 0.0908, 0.0991, 0.0893, 0.0930],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,706][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0754, 0.1021, 0.0455, 0.1203, 0.0820, 0.1035, 0.1308, 0.0873, 0.2531],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,708][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2079, 0.0982, 0.1053, 0.1057, 0.1095, 0.1236, 0.1007, 0.0617, 0.0875],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,709][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ of] are: tensor([1.0174e-02, 5.1103e-03, 3.5272e-04, 5.8382e-03, 1.0043e-03, 3.1548e-04,
        1.5685e-02, 6.9967e-04, 9.6082e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,711][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.7140, 0.0906, 0.0719, 0.0275, 0.0611, 0.0036, 0.0086, 0.0088, 0.0139],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,712][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.2406, 0.0536, 0.1091, 0.0968, 0.1080, 0.0714, 0.1365, 0.0715, 0.1125],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,714][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.2460, 0.0509, 0.0474, 0.0663, 0.0522, 0.0388, 0.3759, 0.0786, 0.0439],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,716][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0025, 0.0204, 0.0547, 0.3691, 0.5410, 0.0023, 0.0060, 0.0019, 0.0020],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,718][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0127, 0.1299, 0.1899, 0.0958, 0.1701, 0.1366, 0.0818, 0.1227, 0.0605],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,719][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ of] are: tensor([1.9728e-02, 7.1081e-02, 5.3444e-04, 3.0855e-01, 2.9890e-04, 6.3096e-03,
        2.2291e-02, 2.3435e-03, 5.6886e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:15,720][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.0029, 0.0294, 0.0793, 0.0443, 0.0661, 0.1836, 0.2341, 0.2599, 0.0415,
        0.0590], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,722][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([2.8276e-03, 8.1709e-01, 1.1918e-03, 2.8701e-04, 2.3583e-07, 1.8653e-04,
        3.2294e-06, 1.7841e-01, 7.3432e-07, 3.8929e-07], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,723][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.1785, 0.1047, 0.1041, 0.1318, 0.0737, 0.0777, 0.0850, 0.0746, 0.0910,
        0.0788], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,725][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.0704, 0.1019, 0.0372, 0.1263, 0.0734, 0.1026, 0.1311, 0.0773, 0.2348,
        0.0450], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,727][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.2034, 0.1003, 0.0766, 0.1068, 0.0672, 0.1078, 0.0893, 0.0808, 0.0876,
        0.0803], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,728][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.1562, 0.1979, 0.0189, 0.0762, 0.0104, 0.0247, 0.1395, 0.1440, 0.0802,
        0.1521], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,729][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.6918, 0.0898, 0.0709, 0.0227, 0.0683, 0.0029, 0.0067, 0.0071, 0.0120,
        0.0279], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,729][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.1665, 0.0429, 0.0435, 0.0939, 0.0962, 0.2059, 0.1231, 0.1328, 0.0857,
        0.0096], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,730][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.1925, 0.0624, 0.0712, 0.0659, 0.0856, 0.0756, 0.2815, 0.0825, 0.0366,
        0.0463], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,731][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0064, 0.0464, 0.0520, 0.4367, 0.4203, 0.0098, 0.0167, 0.0046, 0.0022,
        0.0049], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,733][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.0121, 0.1239, 0.1559, 0.0891, 0.1339, 0.1125, 0.0761, 0.1004, 0.0495,
        0.1467], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,734][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([2.4659e-02, 1.0450e-01, 4.2780e-04, 4.3030e-01, 3.2727e-04, 4.9126e-03,
        1.3446e-02, 4.6993e-03, 4.0697e-01, 9.7535e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:15,736][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0080, 0.0467, 0.0787, 0.0568, 0.0566, 0.1406, 0.1791, 0.2632, 0.0598,
        0.0713, 0.0391], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,737][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ at] are: tensor([2.7479e-02, 7.0122e-01, 3.9332e-03, 4.8826e-04, 1.8607e-06, 4.9089e-04,
        1.3812e-05, 2.6636e-01, 2.9061e-06, 3.2161e-06, 1.3609e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,739][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1725, 0.0732, 0.1047, 0.1019, 0.0808, 0.0690, 0.0840, 0.0792, 0.0676,
        0.0731, 0.0940], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,740][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0704, 0.0900, 0.0388, 0.1082, 0.0679, 0.0859, 0.1148, 0.0734, 0.2023,
        0.0460, 0.1022], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,742][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.1837, 0.0751, 0.0850, 0.0795, 0.0730, 0.1005, 0.0776, 0.0717, 0.0705,
        0.0805, 0.1028], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,744][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0211, 0.0178, 0.0019, 0.0102, 0.0038, 0.0010, 0.0346, 0.0043, 0.0479,
        0.0025, 0.8550], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,745][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.5975, 0.1289, 0.0607, 0.0249, 0.0698, 0.0032, 0.0086, 0.0084, 0.0181,
        0.0373, 0.0427], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,747][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1458, 0.0237, 0.0694, 0.0887, 0.1043, 0.1775, 0.1041, 0.0710, 0.0784,
        0.0597, 0.0776], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,749][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.2297, 0.0794, 0.0530, 0.0409, 0.0483, 0.0467, 0.2750, 0.0678, 0.0340,
        0.0477, 0.0775], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,751][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0035, 0.0324, 0.0942, 0.3460, 0.4856, 0.0061, 0.0143, 0.0037, 0.0020,
        0.0078, 0.0044], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,752][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0125, 0.1034, 0.1383, 0.0783, 0.1267, 0.1000, 0.0653, 0.1021, 0.0420,
        0.1372, 0.0940], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,754][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0231, 0.1110, 0.0010, 0.3700, 0.0009, 0.0076, 0.0098, 0.0037, 0.2985,
        0.0012, 0.1732], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:15,756][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0079, 0.0403, 0.0979, 0.0519, 0.0671, 0.1326, 0.1730, 0.1698, 0.0541,
        0.0562, 0.0606, 0.0887], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,757][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([4.5011e-02, 7.0488e-01, 3.2509e-03, 6.0913e-04, 2.6053e-06, 5.2510e-04,
        1.6342e-05, 2.4569e-01, 4.8138e-06, 3.6016e-06, 2.4452e-06, 3.5999e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,759][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1335, 0.0620, 0.0959, 0.0909, 0.0746, 0.0629, 0.0811, 0.0895, 0.0618,
        0.0801, 0.0761, 0.0916], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,760][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0610, 0.0723, 0.0345, 0.0950, 0.0603, 0.0759, 0.1115, 0.0710, 0.1716,
        0.0421, 0.0829, 0.1219], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,762][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1865, 0.0715, 0.0671, 0.0714, 0.0718, 0.0826, 0.0742, 0.0724, 0.0569,
        0.0775, 0.0836, 0.0846], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,764][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0619, 0.2164, 0.0238, 0.0383, 0.0268, 0.0090, 0.0725, 0.0247, 0.2789,
        0.0095, 0.1561, 0.0820], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,766][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4763, 0.0702, 0.0735, 0.0364, 0.1008, 0.0062, 0.0112, 0.0181, 0.0246,
        0.0616, 0.0635, 0.0575], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,768][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1189, 0.0176, 0.0697, 0.0644, 0.0822, 0.0940, 0.1254, 0.0537, 0.0675,
        0.0675, 0.0755, 0.1635], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,769][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1869, 0.0556, 0.0428, 0.0340, 0.0381, 0.0277, 0.2440, 0.0559, 0.0277,
        0.0409, 0.0607, 0.1856], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,771][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0020, 0.0056, 0.0566, 0.3444, 0.5607, 0.0020, 0.0043, 0.0083, 0.0006,
        0.0075, 0.0011, 0.0069], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,773][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0112, 0.0910, 0.1336, 0.0693, 0.1201, 0.0956, 0.0683, 0.0941, 0.0369,
        0.1314, 0.0780, 0.0706], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,774][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0462, 0.1062, 0.0008, 0.3460, 0.0005, 0.0055, 0.0295, 0.0040, 0.3281,
        0.0019, 0.0108, 0.1204], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:15,775][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0031, 0.0345, 0.0663, 0.0432, 0.0788, 0.1067, 0.2113, 0.1092, 0.0521,
        0.0597, 0.0737, 0.1122, 0.0492], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,776][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ house] are: tensor([4.3662e-03, 6.8347e-01, 4.9973e-03, 7.4236e-04, 2.4553e-06, 7.5830e-04,
        2.5075e-05, 3.0561e-01, 4.2625e-06, 3.4616e-06, 2.2828e-06, 4.0565e-06,
        1.2876e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,777][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.1360, 0.0807, 0.0802, 0.1068, 0.0608, 0.0670, 0.0683, 0.0634, 0.0624,
        0.0651, 0.0704, 0.0649, 0.0740], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,778][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0580, 0.0766, 0.0293, 0.1030, 0.0546, 0.0751, 0.0984, 0.0659, 0.1782,
        0.0386, 0.0855, 0.0964, 0.0402], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,780][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.1829, 0.0732, 0.0720, 0.0697, 0.0636, 0.0796, 0.0643, 0.0568, 0.0550,
        0.0600, 0.0820, 0.0688, 0.0720], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,781][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0627, 0.0930, 0.0033, 0.0491, 0.0313, 0.0188, 0.0900, 0.0102, 0.0503,
        0.0105, 0.3263, 0.1589, 0.0956], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,783][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.3460, 0.0653, 0.0635, 0.0289, 0.1080, 0.0049, 0.0085, 0.0124, 0.0188,
        0.0430, 0.0447, 0.0420, 0.2140], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,784][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.1124, 0.0134, 0.0977, 0.0343, 0.1153, 0.1490, 0.0782, 0.1208, 0.0562,
        0.0727, 0.0507, 0.0923, 0.0070], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,786][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.1612, 0.0623, 0.0369, 0.0346, 0.0404, 0.0651, 0.1760, 0.0847, 0.0339,
        0.0473, 0.0731, 0.1315, 0.0528], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,788][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0106, 0.0587, 0.1124, 0.1819, 0.4206, 0.0167, 0.0511, 0.0116, 0.0039,
        0.0169, 0.0212, 0.0909, 0.0036], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,789][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.0087, 0.0915, 0.1257, 0.0688, 0.1034, 0.0903, 0.0585, 0.0796, 0.0374,
        0.1162, 0.0857, 0.0533, 0.0810], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,791][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ house] are: tensor([2.3842e-02, 1.0023e-01, 5.9278e-04, 4.6374e-01, 3.7978e-04, 6.1632e-03,
        8.2932e-03, 3.3097e-03, 3.5941e-01, 9.3808e-04, 1.6285e-02, 1.2424e-02,
        4.4026e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:15,792][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.0067, 0.0325, 0.0836, 0.0408, 0.0559, 0.0904, 0.1486, 0.1408, 0.0447,
        0.0640, 0.0527, 0.0819, 0.0949, 0.0625], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,794][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [.] are: tensor([3.6940e-02, 6.9627e-01, 3.5648e-03, 4.6035e-04, 1.9343e-06, 3.8771e-04,
        1.1895e-05, 2.6234e-01, 2.6627e-06, 2.3479e-06, 1.4629e-06, 2.4217e-06,
        1.3571e-05, 3.4412e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,795][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.1111, 0.0475, 0.0838, 0.0752, 0.0714, 0.0571, 0.0662, 0.0749, 0.0572,
        0.0707, 0.0610, 0.0664, 0.0870, 0.0704], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,797][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0485, 0.0589, 0.0265, 0.0814, 0.0482, 0.0587, 0.0855, 0.0518, 0.1495,
        0.0335, 0.0689, 0.0829, 0.0365, 0.1690], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,799][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.1444, 0.0756, 0.0670, 0.0588, 0.0640, 0.0722, 0.0529, 0.0669, 0.0451,
        0.0607, 0.0636, 0.0531, 0.0785, 0.0971], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,800][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.1176, 0.0819, 0.0103, 0.0685, 0.0278, 0.0097, 0.0661, 0.0144, 0.0702,
        0.0105, 0.0426, 0.1106, 0.0435, 0.3262], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,802][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.6666, 0.0162, 0.0275, 0.0103, 0.0589, 0.0037, 0.0055, 0.0072, 0.0077,
        0.0167, 0.0219, 0.0243, 0.0727, 0.0609], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,804][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0856, 0.0104, 0.0976, 0.0637, 0.1202, 0.0610, 0.0567, 0.0696, 0.0660,
        0.0895, 0.0529, 0.0756, 0.0885, 0.0628], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,806][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.2282, 0.0721, 0.0394, 0.0501, 0.0385, 0.0310, 0.1633, 0.0688, 0.0211,
        0.0390, 0.0335, 0.1067, 0.0434, 0.0649], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,807][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [.] are: tensor([3.3559e-03, 9.9723e-03, 8.9057e-02, 2.5019e-01, 5.9733e-01, 4.9175e-03,
        5.3042e-03, 6.7902e-03, 4.5803e-04, 8.1066e-03, 1.9881e-03, 6.7932e-03,
        2.0082e-03, 1.3734e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,809][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.0092, 0.0871, 0.1137, 0.0647, 0.1055, 0.0853, 0.0540, 0.0787, 0.0348,
        0.1159, 0.0790, 0.0514, 0.0845, 0.0362], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,810][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0375, 0.0754, 0.0010, 0.2592, 0.0009, 0.0089, 0.0173, 0.0042, 0.1974,
        0.0027, 0.0229, 0.0364, 0.0020, 0.3342], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:15,812][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0023, 0.0283, 0.0970, 0.0331, 0.0520, 0.1374, 0.1752, 0.0989, 0.0411,
        0.0442, 0.0323, 0.0938, 0.0699, 0.0631, 0.0316], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,813][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([2.0318e-02, 7.9937e-01, 5.8529e-03, 2.7990e-03, 1.0666e-05, 2.1037e-03,
        1.3374e-04, 1.6912e-01, 3.6966e-05, 2.0608e-05, 2.2880e-05, 3.3911e-05,
        1.1068e-04, 5.7265e-05, 2.3779e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,815][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0998, 0.0663, 0.0687, 0.1016, 0.0442, 0.0611, 0.0663, 0.0586, 0.0645,
        0.0566, 0.0695, 0.0581, 0.0678, 0.0670, 0.0499], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,816][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0445, 0.0640, 0.0244, 0.0799, 0.0379, 0.0626, 0.0812, 0.0433, 0.1527,
        0.0286, 0.0670, 0.0792, 0.0301, 0.1654, 0.0392], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,817][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.1488, 0.1126, 0.0641, 0.0719, 0.0398, 0.0812, 0.0609, 0.0444, 0.0410,
        0.0412, 0.0533, 0.0568, 0.0456, 0.1000, 0.0386], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,818][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0351, 0.0612, 0.0760, 0.0672, 0.0350, 0.0042, 0.0313, 0.0127, 0.0175,
        0.0045, 0.1815, 0.0726, 0.0113, 0.2842, 0.1058], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,819][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.1995, 0.0256, 0.0247, 0.0249, 0.0802, 0.0078, 0.0080, 0.0126, 0.0146,
        0.0322, 0.0606, 0.0335, 0.2006, 0.1304, 0.1448], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,821][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0344, 0.0159, 0.0311, 0.0240, 0.0062, 0.2183, 0.0759, 0.1175, 0.0336,
        0.0427, 0.0459, 0.0841, 0.1008, 0.1648, 0.0048], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,822][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0880, 0.0913, 0.0106, 0.0130, 0.0096, 0.0926, 0.3286, 0.0240, 0.0112,
        0.0118, 0.0224, 0.1911, 0.0226, 0.0676, 0.0154], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,824][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0027, 0.2083, 0.1421, 0.0742, 0.1683, 0.0169, 0.0993, 0.0019, 0.0028,
        0.0057, 0.0403, 0.1168, 0.0017, 0.0590, 0.0602], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,826][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0105, 0.0917, 0.1066, 0.0719, 0.0786, 0.0788, 0.0544, 0.0679, 0.0395,
        0.0956, 0.0748, 0.0496, 0.0700, 0.0348, 0.0753], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,827][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([1.2939e-02, 5.9475e-02, 7.2552e-04, 2.2822e-01, 3.1858e-03, 2.0998e-03,
        4.4816e-03, 1.8065e-03, 1.5397e-01, 9.6462e-04, 1.0156e-02, 6.7563e-03,
        4.7005e-04, 5.1330e-01, 1.4448e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:15,829][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0052, 0.0323, 0.0640, 0.0377, 0.0402, 0.0986, 0.1354, 0.1380, 0.0406,
        0.0525, 0.0512, 0.0774, 0.0556, 0.0726, 0.0269, 0.0720],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,830][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([3.2623e-02, 7.1573e-01, 3.3081e-03, 6.3340e-04, 1.7746e-06, 4.7623e-04,
        1.3462e-05, 2.4718e-01, 3.2926e-06, 2.5595e-06, 2.0363e-06, 3.6142e-06,
        1.4776e-05, 5.0524e-06, 3.5820e-07, 1.3550e-07], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,832][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0966, 0.0664, 0.0694, 0.0817, 0.0544, 0.0523, 0.0515, 0.0517, 0.0529,
        0.0478, 0.0603, 0.0508, 0.0668, 0.0622, 0.0600, 0.0751],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,833][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0444, 0.0553, 0.0238, 0.0738, 0.0403, 0.0595, 0.0747, 0.0445, 0.1463,
        0.0282, 0.0699, 0.0750, 0.0295, 0.1607, 0.0426, 0.0314],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,835][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.1350, 0.0738, 0.0579, 0.0662, 0.0457, 0.0630, 0.0497, 0.0341, 0.0473,
        0.0411, 0.0632, 0.0505, 0.0455, 0.1152, 0.0465, 0.0653],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,837][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.1384, 0.0638, 0.0072, 0.0248, 0.0076, 0.0957, 0.0997, 0.0226, 0.0727,
        0.0149, 0.0464, 0.0973, 0.0277, 0.2054, 0.0115, 0.0644],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,839][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.4332, 0.0171, 0.0132, 0.0050, 0.0299, 0.0016, 0.0032, 0.0029, 0.0046,
        0.0093, 0.0175, 0.0217, 0.0721, 0.0862, 0.2402, 0.0423],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,841][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0648, 0.0313, 0.0498, 0.0581, 0.0551, 0.0410, 0.0648, 0.0559, 0.0616,
        0.0788, 0.0590, 0.0838, 0.1287, 0.1311, 0.0185, 0.0178],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,843][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1513, 0.0622, 0.0253, 0.0312, 0.0236, 0.0451, 0.1713, 0.0496, 0.0247,
        0.0271, 0.0457, 0.1358, 0.0356, 0.0850, 0.0328, 0.0536],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,844][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0038, 0.0512, 0.1272, 0.1374, 0.3536, 0.0183, 0.0462, 0.0052, 0.0018,
        0.0095, 0.0152, 0.0873, 0.0023, 0.0308, 0.0869, 0.0230],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,846][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0069, 0.0739, 0.1000, 0.0524, 0.0818, 0.0733, 0.0438, 0.0652, 0.0313,
        0.0905, 0.0713, 0.0424, 0.0668, 0.0301, 0.0790, 0.0913],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,847][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([2.1598e-02, 7.1314e-02, 8.0965e-04, 2.7090e-01, 1.9902e-04, 7.1400e-03,
        7.4732e-03, 2.4350e-03, 1.7310e-01, 1.4526e-03, 4.2906e-03, 5.8539e-03,
        3.2621e-04, 4.2447e-01, 8.8231e-05, 8.5471e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:15,849][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0043, 0.0256, 0.0480, 0.0313, 0.0425, 0.1233, 0.0898, 0.1467, 0.0390,
        0.0394, 0.0374, 0.0767, 0.0636, 0.0583, 0.0297, 0.0752, 0.0692],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,851][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([8.5765e-02, 6.7837e-01, 3.4433e-03, 5.2389e-04, 2.5767e-06, 4.2043e-04,
        1.1286e-05, 2.3142e-01, 4.0780e-06, 3.1997e-06, 2.1010e-06, 3.6823e-06,
        2.0627e-05, 5.6705e-06, 6.1309e-07, 2.3684e-07, 3.1222e-07],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,852][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0867, 0.0461, 0.0641, 0.0667, 0.0471, 0.0428, 0.0580, 0.0623, 0.0481,
        0.0569, 0.0537, 0.0575, 0.0726, 0.0592, 0.0550, 0.0610, 0.0623],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,854][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0390, 0.0482, 0.0223, 0.0615, 0.0384, 0.0514, 0.0782, 0.0473, 0.1200,
        0.0263, 0.0581, 0.0793, 0.0303, 0.1479, 0.0407, 0.0285, 0.0827],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,856][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1436, 0.0654, 0.0443, 0.0550, 0.0384, 0.0559, 0.0512, 0.0446, 0.0396,
        0.0493, 0.0512, 0.0528, 0.0543, 0.0960, 0.0417, 0.0608, 0.0558],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,858][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0327, 0.0809, 0.0076, 0.0199, 0.0306, 0.0163, 0.0465, 0.0157, 0.1360,
        0.0122, 0.1015, 0.0891, 0.0246, 0.2385, 0.0528, 0.0329, 0.0623],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,858][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1707, 0.0268, 0.0341, 0.0133, 0.0528, 0.0024, 0.0046, 0.0055, 0.0086,
        0.0188, 0.0272, 0.0249, 0.1253, 0.1428, 0.2545, 0.0570, 0.0308],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,859][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0697, 0.0239, 0.0433, 0.0501, 0.0513, 0.0524, 0.0699, 0.0715, 0.0594,
        0.0432, 0.0494, 0.0901, 0.0766, 0.1211, 0.0108, 0.0487, 0.0689],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,860][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.1139, 0.0420, 0.0281, 0.0205, 0.0281, 0.0221, 0.1692, 0.0338, 0.0143,
        0.0237, 0.0365, 0.0906, 0.0351, 0.0636, 0.0387, 0.0359, 0.2040],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,861][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.9642e-03, 5.2056e-03, 9.9586e-02, 1.3523e-01, 6.4727e-01, 3.7831e-03,
        9.3844e-03, 6.9815e-03, 3.0869e-04, 7.9707e-03, 1.2440e-03, 1.3647e-02,
        2.5636e-03, 5.5627e-03, 5.0588e-02, 5.5621e-03, 3.1467e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,863][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0075, 0.0615, 0.0947, 0.0481, 0.0776, 0.0651, 0.0479, 0.0701, 0.0265,
        0.0923, 0.0554, 0.0445, 0.0720, 0.0272, 0.0768, 0.0880, 0.0450],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,864][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([1.7263e-02, 5.7563e-02, 9.5899e-04, 1.9595e-01, 2.7185e-04, 3.5035e-03,
        9.9165e-02, 2.8074e-03, 2.0900e-01, 1.2156e-03, 6.8107e-03, 2.2524e-02,
        3.9219e-04, 3.1016e-01, 1.2888e-04, 6.8658e-04, 7.1599e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:15,866][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0020, 0.0175, 0.0582, 0.0249, 0.0360, 0.0956, 0.1373, 0.1080, 0.0267,
        0.0416, 0.0307, 0.0650, 0.0502, 0.0459, 0.0229, 0.1156, 0.1012, 0.0206],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,867][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([9.4152e-02, 6.4211e-01, 7.7206e-03, 2.4359e-03, 1.9060e-05, 2.0810e-03,
        1.3332e-04, 2.5097e-01, 4.0677e-05, 3.1636e-05, 3.3067e-05, 4.0789e-05,
        1.3680e-04, 7.0236e-05, 5.4899e-06, 3.9740e-06, 6.7290e-06, 1.0362e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,869][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0825, 0.0510, 0.0644, 0.0669, 0.0439, 0.0499, 0.0473, 0.0511, 0.0508,
        0.0511, 0.0568, 0.0478, 0.0591, 0.0566, 0.0491, 0.0644, 0.0493, 0.0579],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,871][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0384, 0.0520, 0.0207, 0.0696, 0.0357, 0.0521, 0.0690, 0.0383, 0.1219,
        0.0239, 0.0594, 0.0720, 0.0256, 0.1545, 0.0372, 0.0266, 0.0723, 0.0306],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,873][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.1480, 0.0834, 0.0572, 0.0500, 0.0419, 0.0598, 0.0448, 0.0347, 0.0334,
        0.0406, 0.0479, 0.0421, 0.0440, 0.0796, 0.0411, 0.0576, 0.0467, 0.0473],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,874][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0798, 0.0335, 0.0007, 0.0115, 0.0050, 0.0192, 0.0553, 0.0200, 0.0121,
        0.0643, 0.0263, 0.0702, 0.0519, 0.0738, 0.0037, 0.0260, 0.0536, 0.3932],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,876][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1117, 0.0436, 0.0430, 0.0299, 0.0475, 0.0041, 0.0057, 0.0079, 0.0113,
        0.0277, 0.0369, 0.0272, 0.1616, 0.1422, 0.1472, 0.0464, 0.0270, 0.0792],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,878][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0328, 0.0143, 0.0751, 0.0262, 0.0593, 0.0491, 0.1110, 0.0710, 0.0269,
        0.0191, 0.0646, 0.0608, 0.0865, 0.0780, 0.0216, 0.0630, 0.1403, 0.0003],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,880][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.1281, 0.0316, 0.0086, 0.0433, 0.0140, 0.0537, 0.1299, 0.0339, 0.0221,
        0.0104, 0.0315, 0.1225, 0.0188, 0.0899, 0.0298, 0.0708, 0.1529, 0.0081],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,881][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([1.2828e-03, 6.4460e-02, 4.4217e-02, 5.1422e-02, 1.4955e-01, 1.4197e-02,
        1.6441e-02, 1.6642e-03, 5.0467e-04, 3.9649e-03, 5.5121e-03, 2.8520e-02,
        7.7462e-04, 1.5714e-02, 4.6690e-02, 2.8509e-02, 7.4132e-03, 5.1917e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,883][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0070, 0.0649, 0.0888, 0.0487, 0.0793, 0.0653, 0.0402, 0.0555, 0.0273,
        0.0843, 0.0615, 0.0365, 0.0612, 0.0287, 0.0769, 0.0875, 0.0367, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,884][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([1.3940e-02, 5.6104e-02, 2.8706e-04, 2.1751e-01, 2.2550e-04, 1.7380e-03,
        4.4894e-03, 2.3864e-03, 1.4950e-01, 1.7806e-03, 4.1910e-03, 6.9112e-03,
        3.5089e-04, 5.3220e-01, 9.8128e-05, 3.1896e-04, 3.4261e-03, 4.5482e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:15,886][circuit_model.py][line:2294][INFO] ##1-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0037, 0.0230, 0.0462, 0.0248, 0.0320, 0.1130, 0.1020, 0.1273, 0.0365,
        0.0483, 0.0342, 0.0541, 0.0476, 0.0523, 0.0207, 0.1074, 0.0779, 0.0372,
        0.0119], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,887][circuit_model.py][line:2297][INFO] ##1-th layer ##Weight##: The head2 weight for token [ to] are: tensor([8.3762e-02, 6.3325e-01, 5.8586e-03, 9.1163e-04, 5.9240e-06, 8.2183e-04,
        3.2151e-05, 2.7526e-01, 9.2024e-06, 6.8488e-06, 5.3878e-06, 8.8480e-06,
        4.8612e-05, 1.4042e-05, 1.3840e-06, 6.6657e-07, 1.0313e-06, 1.7494e-07,
        1.2023e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,889][circuit_model.py][line:2300][INFO] ##1-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0654, 0.0369, 0.0595, 0.0500, 0.0505, 0.0433, 0.0476, 0.0535, 0.0426,
        0.0421, 0.0521, 0.0492, 0.0563, 0.0501, 0.0590, 0.0695, 0.0522, 0.0617,
        0.0585], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,891][circuit_model.py][line:2303][INFO] ##1-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0363, 0.0424, 0.0204, 0.0544, 0.0347, 0.0492, 0.0629, 0.0397, 0.1142,
        0.0246, 0.0562, 0.0641, 0.0269, 0.1322, 0.0369, 0.0274, 0.0669, 0.0331,
        0.0776], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,893][circuit_model.py][line:2306][INFO] ##1-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.1418, 0.0588, 0.0469, 0.0453, 0.0378, 0.0536, 0.0427, 0.0327, 0.0352,
        0.0361, 0.0474, 0.0433, 0.0396, 0.0791, 0.0397, 0.0634, 0.0482, 0.0478,
        0.0603], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,894][circuit_model.py][line:2309][INFO] ##1-th layer ##Weight##: The head6 weight for token [ to] are: tensor([1.6482e-02, 5.4564e-03, 1.8245e-03, 8.7197e-03, 3.5384e-03, 1.6142e-03,
        1.5786e-02, 3.1161e-04, 3.4612e-02, 6.7538e-04, 2.5099e-02, 2.5510e-02,
        4.2219e-03, 3.7651e-02, 4.4969e-03, 1.4821e-03, 2.7247e-02, 1.3269e-03,
        7.8394e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,896][circuit_model.py][line:2312][INFO] ##1-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0776, 0.0426, 0.0244, 0.0118, 0.0281, 0.0015, 0.0041, 0.0041, 0.0104,
        0.0264, 0.0299, 0.0305, 0.1216, 0.1184, 0.2009, 0.0478, 0.0298, 0.1105,
        0.0795], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,898][circuit_model.py][line:2315][INFO] ##1-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0826, 0.0267, 0.0354, 0.0527, 0.0495, 0.0478, 0.0758, 0.0355, 0.0631,
        0.0339, 0.0528, 0.0745, 0.0321, 0.0999, 0.0105, 0.0498, 0.0678, 0.0390,
        0.0706], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,900][circuit_model.py][line:2318][INFO] ##1-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.1058, 0.0439, 0.0285, 0.0191, 0.0291, 0.0260, 0.1659, 0.0318, 0.0141,
        0.0213, 0.0338, 0.0807, 0.0287, 0.0608, 0.0367, 0.0340, 0.1861, 0.0264,
        0.0273], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,901][circuit_model.py][line:2321][INFO] ##1-th layer ##Weight##: The head10 weight for token [ to] are: tensor([4.9514e-04, 1.6134e-03, 4.0799e-02, 5.1807e-02, 1.8862e-01, 9.1258e-04,
        4.1642e-03, 3.0337e-03, 5.4932e-05, 3.2410e-03, 4.8967e-04, 7.1868e-03,
        5.8454e-04, 1.4402e-03, 6.4636e-03, 2.0593e-03, 1.1182e-03, 6.8591e-01,
        1.2316e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,902][circuit_model.py][line:2324][INFO] ##1-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0069, 0.0603, 0.0875, 0.0429, 0.0759, 0.0614, 0.0401, 0.0574, 0.0258,
        0.0781, 0.0589, 0.0375, 0.0600, 0.0253, 0.0739, 0.0800, 0.0375, 0.0521,
        0.0384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,902][circuit_model.py][line:2327][INFO] ##1-th layer ##Weight##: The head12 weight for token [ to] are: tensor([1.6725e-02, 5.8496e-02, 4.7654e-04, 2.3877e-01, 1.6554e-04, 3.5255e-03,
        8.6180e-03, 1.3648e-03, 1.8396e-01, 7.4582e-04, 1.0356e-02, 1.0857e-02,
        2.5954e-04, 3.2945e-01, 8.3112e-05, 4.5414e-04, 6.2796e-03, 3.7971e-04,
        1.2904e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:15,921][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:15,922][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,923][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,925][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,926][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,927][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,927][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,928][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,929][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,929][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,930][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,931][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,931][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:15,932][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.4330, 0.5670], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,933][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.1901, 0.8099], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,934][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.6310, 0.3690], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,934][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.7095, 0.2905], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,935][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.9411, 0.0589], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,936][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.4147, 0.5853], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,938][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.9614, 0.0386], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,940][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.9654, 0.0346], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,941][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.8460, 0.1540], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,942][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([2.5270e-04, 9.9975e-01], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,944][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0026, 0.9974], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,945][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.1186, 0.8814], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:15,947][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.3030, 0.4275, 0.2696], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,948][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.0618, 0.5303, 0.4079], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,950][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.4221, 0.2738, 0.3041], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,952][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.3599, 0.1521, 0.4880], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,953][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.8274, 0.0857, 0.0870], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,955][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.3798, 0.5746, 0.0456], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,957][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.5485, 0.1984, 0.2531], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,958][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.8312, 0.1165, 0.0524], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,960][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.7736, 0.1561, 0.0703], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,961][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([4.0257e-04, 9.9010e-01, 9.4947e-03], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,962][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.0021, 0.4966, 0.5012], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,962][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.0170, 0.0184, 0.9645], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:15,963][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.1393, 0.2023, 0.1847, 0.4737], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,964][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0529, 0.2964, 0.3798, 0.2709], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,965][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.3290, 0.2005, 0.2516, 0.2188], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,967][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2710, 0.0800, 0.5247, 0.1243], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,968][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.6185, 0.0826, 0.1117, 0.1872], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,970][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1655, 0.2564, 0.0336, 0.5445], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,971][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5509, 0.1980, 0.0264, 0.2247], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,973][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.5824, 0.0737, 0.1721, 0.1718], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,974][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.7507, 0.1504, 0.0697, 0.0293], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,975][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([7.6630e-05, 9.8838e-01, 1.0181e-02, 1.3593e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,977][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0018, 0.2273, 0.2606, 0.5102], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,978][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([5.6612e-03, 6.4649e-03, 1.5667e-07, 9.8787e-01], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:15,980][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0952, 0.1600, 0.1019, 0.4432, 0.1997], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,982][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0269, 0.2593, 0.2309, 0.3381, 0.1449], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,983][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.2782, 0.1765, 0.1945, 0.1869, 0.1638], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,985][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.2358, 0.0960, 0.4033, 0.1205, 0.1445], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,986][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.5836, 0.0728, 0.0860, 0.1246, 0.1329], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,988][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.1506, 0.2466, 0.0153, 0.5775, 0.0100], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,990][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.2977, 0.1378, 0.0069, 0.1698, 0.3879], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,991][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.4882, 0.0753, 0.2143, 0.1680, 0.0542], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,993][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.7813, 0.1323, 0.0517, 0.0250, 0.0097], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,994][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([3.6286e-04, 8.6952e-01, 9.5495e-03, 3.3662e-03, 1.1720e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,996][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.0011, 0.1606, 0.1738, 0.3454, 0.3191], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,997][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([5.3775e-05, 1.7722e-03, 4.4185e-06, 4.6962e-03, 9.9347e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:15,999][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.0842, 0.1193, 0.0905, 0.2510, 0.1633, 0.2918], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,000][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0314, 0.1814, 0.1953, 0.2022, 0.1587, 0.2310], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,002][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.2216, 0.1501, 0.1776, 0.1527, 0.1517, 0.1462], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,003][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.1901, 0.0668, 0.3337, 0.0901, 0.1455, 0.1738], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,004][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.2926, 0.0705, 0.0894, 0.1177, 0.1687, 0.2611], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,005][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.1107, 0.1463, 0.0189, 0.3112, 0.0164, 0.3964], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,006][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.3798, 0.0907, 0.0294, 0.2130, 0.1726, 0.1144], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,007][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.3958, 0.0497, 0.1252, 0.1076, 0.1763, 0.1454], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,008][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.7365, 0.1435, 0.0657, 0.0265, 0.0120, 0.0158], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,009][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([3.4855e-05, 5.0680e-01, 1.0100e-02, 2.5622e-04, 4.7897e-01, 3.8381e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,011][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.0011, 0.0931, 0.1112, 0.2190, 0.2158, 0.3598], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,012][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([1.3273e-04, 1.6142e-02, 9.2162e-06, 5.4971e-02, 5.4661e-07, 9.2874e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,014][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0607, 0.0863, 0.0665, 0.1816, 0.1184, 0.2143, 0.2722],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,015][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0251, 0.1362, 0.1345, 0.1572, 0.1158, 0.2310, 0.2003],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,017][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2040, 0.1273, 0.1523, 0.1351, 0.1293, 0.1216, 0.1305],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,019][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1790, 0.0549, 0.3018, 0.0783, 0.1262, 0.1433, 0.1165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,020][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.4288, 0.0601, 0.0646, 0.1112, 0.0991, 0.1458, 0.0904],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,022][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0806, 0.1030, 0.0128, 0.2187, 0.0111, 0.2658, 0.3080],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,024][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2271, 0.1422, 0.1429, 0.2327, 0.0687, 0.0552, 0.1311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,025][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.3542, 0.0566, 0.1108, 0.0996, 0.1341, 0.1312, 0.1135],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,027][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.7232, 0.1339, 0.0552, 0.0265, 0.0110, 0.0150, 0.0352],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,028][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([3.5980e-05, 6.3620e-01, 5.5805e-03, 3.6635e-04, 3.5595e-01, 1.3723e-03,
        4.9678e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,030][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0009, 0.0631, 0.0797, 0.1559, 0.1579, 0.2576, 0.2850],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,031][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([3.3467e-05, 5.4791e-04, 5.2495e-06, 3.7347e-03, 1.9578e-07, 1.4438e-06,
        9.9568e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,033][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.0400, 0.0640, 0.0522, 0.1576, 0.0968, 0.1859, 0.2504, 0.1531],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,034][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.0164, 0.1137, 0.1032, 0.1353, 0.0702, 0.2145, 0.1941, 0.1527],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,036][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.1930, 0.1153, 0.1305, 0.1169, 0.1120, 0.1066, 0.1088, 0.1170],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,038][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.1713, 0.0537, 0.2834, 0.0748, 0.1081, 0.1263, 0.0881, 0.0944],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,039][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.4715, 0.0461, 0.0552, 0.0785, 0.0919, 0.1288, 0.0632, 0.0648],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,041][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.0678, 0.0924, 0.0091, 0.1936, 0.0072, 0.2285, 0.2760, 0.1255],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,043][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.2227, 0.0991, 0.0060, 0.1846, 0.0701, 0.0845, 0.2941, 0.0388],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,044][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.4238, 0.0405, 0.0977, 0.0722, 0.1162, 0.1170, 0.0853, 0.0472],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,045][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.7272, 0.1342, 0.0529, 0.0243, 0.0094, 0.0128, 0.0320, 0.0072],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,046][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([6.9960e-05, 4.4594e-01, 7.0455e-03, 7.0094e-04, 5.4460e-01, 1.2210e-03,
        4.1065e-04, 3.8577e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,047][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.0008, 0.0497, 0.0623, 0.1240, 0.1263, 0.2064, 0.2291, 0.2015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,048][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([4.0074e-06, 2.3818e-04, 1.8873e-08, 5.4435e-04, 4.9218e-07, 3.9433e-07,
        3.1965e-07, 9.9921e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,049][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0406, 0.0577, 0.0475, 0.1130, 0.0852, 0.1389, 0.1691, 0.1249, 0.2230],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,050][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0180, 0.0849, 0.1151, 0.0996, 0.1017, 0.1537, 0.1658, 0.1727, 0.0887],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,052][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.1624, 0.1003, 0.1246, 0.1047, 0.1075, 0.0994, 0.1007, 0.1062, 0.0942],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,054][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1497, 0.0457, 0.2768, 0.0663, 0.1133, 0.1271, 0.0837, 0.1003, 0.0372],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,055][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.3299, 0.0534, 0.0709, 0.1019, 0.1033, 0.1338, 0.0779, 0.0610, 0.0679],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,056][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0506, 0.0622, 0.0088, 0.1318, 0.0077, 0.1686, 0.1940, 0.0900, 0.2863],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,058][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.1654, 0.1602, 0.0583, 0.1832, 0.0137, 0.0376, 0.1835, 0.0933, 0.1048],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,060][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.2851, 0.0704, 0.1010, 0.0850, 0.1091, 0.0896, 0.0828, 0.0766, 0.1004],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,062][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.7268, 0.1280, 0.0511, 0.0241, 0.0095, 0.0131, 0.0311, 0.0074, 0.0089],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,063][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([1.1071e-05, 5.8273e-01, 4.3082e-03, 4.4163e-04, 4.1200e-01, 2.9172e-04,
        1.8292e-04, 8.1225e-07, 4.2156e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,065][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0008, 0.0365, 0.0482, 0.0951, 0.0990, 0.1610, 0.1758, 0.1542, 0.2292],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,066][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([1.8892e-05, 1.0030e-03, 9.5073e-08, 3.5116e-02, 4.0205e-08, 8.4909e-07,
        2.8316e-05, 2.2161e-07, 9.6383e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,067][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.0200, 0.0362, 0.0253, 0.1017, 0.0511, 0.1332, 0.1827, 0.1131, 0.2797,
        0.0570], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,069][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0112, 0.0866, 0.0821, 0.1035, 0.0546, 0.1439, 0.1525, 0.1636, 0.1498,
        0.0522], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,071][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.1547, 0.0968, 0.1055, 0.0987, 0.0912, 0.0883, 0.0903, 0.0929, 0.0863,
        0.0953], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,072][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.1458, 0.0483, 0.2319, 0.0698, 0.1028, 0.1224, 0.0779, 0.0851, 0.0332,
        0.0828], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,074][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.3772, 0.0498, 0.0456, 0.0798, 0.0719, 0.1131, 0.0607, 0.0518, 0.0593,
        0.0906], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,076][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.0441, 0.0632, 0.0045, 0.1399, 0.0030, 0.1575, 0.1931, 0.0803, 0.2953,
        0.0191], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,077][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.1388, 0.0374, 0.3065, 0.0670, 0.0502, 0.0985, 0.1139, 0.1226, 0.0286,
        0.0364], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,079][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.2641, 0.0479, 0.0477, 0.0770, 0.0982, 0.1751, 0.0768, 0.1096, 0.0747,
        0.0290], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,081][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.7160, 0.1252, 0.0531, 0.0260, 0.0106, 0.0138, 0.0332, 0.0079, 0.0096,
        0.0047], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,082][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([2.0344e-04, 7.5170e-01, 9.6881e-03, 2.3865e-03, 2.3021e-01, 3.0470e-03,
        1.8684e-03, 2.8265e-05, 5.2421e-04, 3.3718e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,084][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.0007, 0.0298, 0.0383, 0.0761, 0.0777, 0.1277, 0.1405, 0.1244, 0.1819,
        0.2028], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,085][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([6.2249e-05, 8.3430e-03, 5.7993e-07, 3.1998e-02, 7.0752e-06, 4.7382e-06,
        5.4612e-05, 5.7457e-05, 2.1034e-02, 9.3844e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,087][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0353, 0.0490, 0.0376, 0.0962, 0.0665, 0.1119, 0.1389, 0.1013, 0.1866,
        0.0666, 0.1103], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,088][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0144, 0.0785, 0.0924, 0.0919, 0.0632, 0.1338, 0.1376, 0.1329, 0.1039,
        0.0731, 0.0785], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,088][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.1399, 0.0834, 0.1025, 0.0871, 0.0890, 0.0800, 0.0836, 0.0887, 0.0763,
        0.0897, 0.0798], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,089][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1426, 0.0436, 0.2108, 0.0589, 0.0889, 0.0962, 0.0697, 0.0835, 0.0313,
        0.0819, 0.0928], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,090][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1898, 0.0474, 0.0569, 0.0885, 0.0888, 0.1289, 0.0696, 0.0516, 0.0600,
        0.0962, 0.1224], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,092][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0414, 0.0541, 0.0067, 0.1146, 0.0057, 0.1415, 0.1676, 0.0758, 0.2500,
        0.0276, 0.1149], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,093][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0920, 0.0545, 0.1389, 0.0953, 0.0943, 0.0432, 0.0622, 0.0095, 0.1099,
        0.2781, 0.0220], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,095][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1948, 0.0336, 0.0672, 0.0682, 0.1007, 0.1493, 0.0643, 0.0701, 0.0699,
        0.1010, 0.0807], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,097][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.7026, 0.1292, 0.0524, 0.0258, 0.0106, 0.0140, 0.0329, 0.0081, 0.0098,
        0.0048, 0.0097], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,098][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([2.6807e-05, 2.8415e-01, 7.3270e-03, 4.7155e-04, 3.7319e-01, 7.8469e-04,
        6.3795e-04, 3.2914e-06, 6.8910e-05, 1.9311e-04, 3.3314e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,099][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0006, 0.0217, 0.0293, 0.0575, 0.0606, 0.0984, 0.1064, 0.0943, 0.1399,
        0.1565, 0.2348], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,101][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([7.3898e-06, 1.1648e-03, 1.1708e-07, 5.3913e-03, 1.7543e-06, 9.3449e-07,
        7.5471e-08, 1.9753e-07, 6.5365e-04, 4.9828e-09, 9.9278e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,102][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0265, 0.0388, 0.0306, 0.0795, 0.0571, 0.0961, 0.1188, 0.0894, 0.1638,
        0.0587, 0.0959, 0.1447], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,104][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0136, 0.0672, 0.0697, 0.0815, 0.0541, 0.1200, 0.1128, 0.1234, 0.1013,
        0.0621, 0.1096, 0.0849], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,106][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1234, 0.0766, 0.0946, 0.0806, 0.0821, 0.0749, 0.0788, 0.0849, 0.0712,
        0.0856, 0.0720, 0.0751], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,108][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1273, 0.0344, 0.2068, 0.0540, 0.0859, 0.0911, 0.0729, 0.0869, 0.0269,
        0.0826, 0.0802, 0.0509], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,109][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2564, 0.0423, 0.0426, 0.0743, 0.0652, 0.0955, 0.0638, 0.0492, 0.0529,
        0.0777, 0.1025, 0.0777], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,111][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0359, 0.0460, 0.0059, 0.0982, 0.0053, 0.1189, 0.1423, 0.0655, 0.2055,
        0.0250, 0.0950, 0.1563], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,113][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0764, 0.0484, 0.1894, 0.1489, 0.0389, 0.0388, 0.0725, 0.0321, 0.0986,
        0.0648, 0.1123, 0.0789], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,114][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1780, 0.0275, 0.0671, 0.0551, 0.0842, 0.0922, 0.0686, 0.0573, 0.0614,
        0.1085, 0.0770, 0.1232], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,116][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.6717, 0.1325, 0.0518, 0.0260, 0.0102, 0.0141, 0.0320, 0.0083, 0.0101,
        0.0047, 0.0099, 0.0286], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,118][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.3050e-05, 2.2183e-01, 3.0435e-03, 2.5351e-04, 2.8395e-01, 3.7807e-04,
        1.6927e-04, 2.5642e-06, 2.3632e-05, 1.2269e-04, 4.8711e-01, 3.1135e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,119][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0005, 0.0166, 0.0231, 0.0456, 0.0483, 0.0794, 0.0871, 0.0757, 0.1139,
        0.1289, 0.1949, 0.1860], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,120][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([4.6562e-03, 1.5248e-02, 2.7821e-06, 1.0207e-01, 3.2157e-06, 6.8248e-06,
        1.3505e-03, 1.6156e-05, 3.9687e-02, 4.5810e-06, 1.0239e-05, 8.3695e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,122][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.0169, 0.0293, 0.0189, 0.0741, 0.0365, 0.0957, 0.1242, 0.0750, 0.1801,
        0.0383, 0.0953, 0.1498, 0.0659], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,124][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0074, 0.0577, 0.0623, 0.0678, 0.0451, 0.1176, 0.1293, 0.1205, 0.0961,
        0.0442, 0.1047, 0.1074, 0.0400], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,126][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.1223, 0.0751, 0.0830, 0.0766, 0.0723, 0.0693, 0.0707, 0.0733, 0.0656,
        0.0754, 0.0655, 0.0664, 0.0845], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,128][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.1244, 0.0375, 0.2066, 0.0557, 0.0800, 0.0958, 0.0621, 0.0720, 0.0254,
        0.0709, 0.0827, 0.0363, 0.0505], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,129][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.2924, 0.0358, 0.0392, 0.0619, 0.0601, 0.0950, 0.0511, 0.0382, 0.0464,
        0.0692, 0.0939, 0.0571, 0.0597], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,129][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.0322, 0.0467, 0.0036, 0.1002, 0.0026, 0.1175, 0.1440, 0.0597, 0.2152,
        0.0161, 0.0918, 0.1507, 0.0197], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,130][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0841, 0.0304, 0.3981, 0.0626, 0.0941, 0.0452, 0.0292, 0.0902, 0.0416,
        0.0019, 0.0505, 0.0406, 0.0315], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,131][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.1973, 0.0188, 0.0708, 0.0326, 0.0956, 0.1223, 0.0496, 0.0886, 0.0521,
        0.1024, 0.0555, 0.0789, 0.0354], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,133][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.7114, 0.1208, 0.0453, 0.0221, 0.0084, 0.0110, 0.0270, 0.0061, 0.0075,
        0.0036, 0.0075, 0.0252, 0.0039], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,134][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([3.8663e-05, 2.2070e-01, 2.1782e-03, 7.4881e-04, 2.5031e-01, 6.2348e-04,
        1.7481e-04, 4.9321e-06, 5.0303e-05, 2.8020e-04, 5.2356e-01, 1.3322e-03,
        3.5368e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,136][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.0004, 0.0143, 0.0195, 0.0386, 0.0403, 0.0670, 0.0734, 0.0649, 0.0976,
        0.1107, 0.1671, 0.1573, 0.1488], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,137][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([2.0390e-05, 3.0749e-03, 1.5284e-06, 1.6304e-02, 5.1792e-06, 6.1762e-06,
        1.8782e-06, 3.2118e-06, 2.1745e-03, 1.2133e-07, 3.2967e-05, 8.8034e-08,
        9.7838e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,139][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.0197, 0.0300, 0.0224, 0.0665, 0.0442, 0.0822, 0.1064, 0.0717, 0.1430,
        0.0441, 0.0832, 0.1306, 0.0724, 0.0836], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,141][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0132, 0.0565, 0.0632, 0.0620, 0.0533, 0.0902, 0.1001, 0.1081, 0.0782,
        0.0602, 0.0880, 0.0845, 0.0621, 0.0805], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,142][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.1074, 0.0645, 0.0808, 0.0683, 0.0709, 0.0640, 0.0669, 0.0710, 0.0610,
        0.0721, 0.0605, 0.0623, 0.0810, 0.0693], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,144][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.1219, 0.0339, 0.1992, 0.0543, 0.0784, 0.0876, 0.0638, 0.0720, 0.0255,
        0.0742, 0.0792, 0.0386, 0.0553, 0.0162], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,146][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.2383, 0.0297, 0.0512, 0.0507, 0.0703, 0.0858, 0.0438, 0.0439, 0.0344,
        0.0727, 0.0770, 0.0525, 0.0589, 0.0907], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,148][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.0291, 0.0397, 0.0049, 0.0887, 0.0043, 0.1088, 0.1304, 0.0579, 0.1869,
        0.0198, 0.0845, 0.1418, 0.0225, 0.0806], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,150][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0707, 0.0235, 0.0460, 0.0777, 0.3356, 0.0245, 0.0244, 0.0153, 0.1009,
        0.0375, 0.0560, 0.0424, 0.0648, 0.0807], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,151][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.1265, 0.0144, 0.0673, 0.0412, 0.0878, 0.0555, 0.0326, 0.0567, 0.0479,
        0.1058, 0.0491, 0.0584, 0.1824, 0.0744], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,153][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.7025, 0.1175, 0.0447, 0.0225, 0.0085, 0.0119, 0.0281, 0.0069, 0.0084,
        0.0040, 0.0085, 0.0259, 0.0043, 0.0063], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,154][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([2.0593e-05, 2.3660e-01, 2.5066e-03, 6.4104e-04, 4.9465e-01, 3.7191e-05,
        1.8035e-05, 3.0685e-06, 1.2244e-05, 2.3020e-04, 2.6195e-01, 2.9146e-04,
        1.9245e-06, 3.0323e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,156][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.0005, 0.0116, 0.0160, 0.0319, 0.0336, 0.0553, 0.0600, 0.0524, 0.0799,
        0.0896, 0.1369, 0.1286, 0.1209, 0.1828], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,157][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([4.3912e-02, 7.0577e-02, 1.5159e-04, 4.9866e-01, 9.7220e-04, 1.4575e-03,
        1.6119e-03, 2.9997e-04, 5.9845e-02, 4.0707e-04, 2.2966e-02, 1.2398e-02,
        5.4508e-04, 2.8620e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,159][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0140, 0.0241, 0.0133, 0.0629, 0.0271, 0.0822, 0.1118, 0.0701, 0.1723,
        0.0305, 0.0877, 0.1462, 0.0602, 0.0751, 0.0226], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,161][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0051, 0.0524, 0.0463, 0.0694, 0.0287, 0.0999, 0.1077, 0.0913, 0.0932,
        0.0389, 0.0899, 0.0992, 0.0520, 0.0996, 0.0263], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,163][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.1035, 0.0655, 0.0719, 0.0682, 0.0610, 0.0610, 0.0626, 0.0635, 0.0579,
        0.0653, 0.0576, 0.0573, 0.0733, 0.0652, 0.0660], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,164][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.1172, 0.0417, 0.1702, 0.0551, 0.0646, 0.0901, 0.0609, 0.0632, 0.0290,
        0.0631, 0.0754, 0.0390, 0.0508, 0.0180, 0.0616], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,166][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.2081, 0.0319, 0.0394, 0.0501, 0.0599, 0.1062, 0.0429, 0.0358, 0.0328,
        0.0595, 0.0682, 0.0530, 0.0505, 0.0848, 0.0769], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,168][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.0281, 0.0457, 0.0026, 0.1092, 0.0017, 0.1119, 0.1344, 0.0506, 0.2108,
        0.0100, 0.0795, 0.1385, 0.0129, 0.0634, 0.0008], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,170][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0363, 0.0784, 0.0012, 0.0421, 0.0997, 0.0881, 0.0425, 0.0051, 0.0578,
        0.0486, 0.0619, 0.0721, 0.0211, 0.2522, 0.0929], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,171][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0572, 0.0194, 0.0281, 0.0270, 0.0103, 0.1449, 0.0455, 0.0826, 0.0353,
        0.0673, 0.0513, 0.0699, 0.1923, 0.1561, 0.0128], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,171][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.7176, 0.1197, 0.0396, 0.0208, 0.0076, 0.0098, 0.0248, 0.0055, 0.0070,
        0.0031, 0.0071, 0.0252, 0.0037, 0.0061, 0.0024], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,172][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([1.4268e-04, 4.0824e-01, 4.9588e-03, 1.7140e-03, 1.0786e-01, 5.6200e-03,
        1.9411e-04, 2.4353e-05, 1.1305e-04, 1.4973e-03, 3.7714e-01, 9.1220e-04,
        2.6414e-05, 2.3440e-02, 6.8118e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,173][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.0003, 0.0106, 0.0142, 0.0288, 0.0292, 0.0498, 0.0529, 0.0464, 0.0705,
        0.0788, 0.1209, 0.1132, 0.1057, 0.1590, 0.1195], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,174][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([1.6791e-05, 8.6496e-04, 2.5679e-06, 1.6832e-03, 6.5153e-01, 4.3124e-07,
        8.1323e-07, 8.9926e-07, 1.6528e-04, 2.6954e-06, 4.1478e-05, 1.6680e-07,
        8.8715e-07, 7.0217e-03, 3.3867e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,176][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0188, 0.0291, 0.0196, 0.0645, 0.0358, 0.0744, 0.0980, 0.0683, 0.1411,
        0.0383, 0.0807, 0.1220, 0.0672, 0.0751, 0.0320, 0.0352],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,178][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0070, 0.0469, 0.0644, 0.0528, 0.0445, 0.0814, 0.0972, 0.1073, 0.0695,
        0.0433, 0.0695, 0.0947, 0.0477, 0.0832, 0.0416, 0.0490],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,180][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0952, 0.0601, 0.0682, 0.0613, 0.0595, 0.0557, 0.0561, 0.0595, 0.0535,
        0.0597, 0.0534, 0.0528, 0.0694, 0.0602, 0.0639, 0.0715],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,181][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.1046, 0.0311, 0.1452, 0.0460, 0.0634, 0.0750, 0.0507, 0.0593, 0.0256,
        0.0596, 0.0725, 0.0334, 0.0469, 0.0163, 0.0631, 0.1072],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,183][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.1409, 0.0278, 0.0362, 0.0425, 0.0672, 0.1127, 0.0385, 0.0330, 0.0283,
        0.0647, 0.0656, 0.0483, 0.0506, 0.0745, 0.0834, 0.0860],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,185][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0293, 0.0431, 0.0043, 0.0915, 0.0032, 0.1098, 0.1245, 0.0556, 0.1844,
        0.0167, 0.0821, 0.1302, 0.0193, 0.0707, 0.0017, 0.0337],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,187][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.1022, 0.0449, 0.0211, 0.1041, 0.0223, 0.0298, 0.0782, 0.0351, 0.0870,
        0.0324, 0.0704, 0.1202, 0.0500, 0.1342, 0.0125, 0.0557],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,188][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0980, 0.0309, 0.0427, 0.0410, 0.0503, 0.0412, 0.0379, 0.0476, 0.0466,
        0.0920, 0.0504, 0.0577, 0.1950, 0.1106, 0.0281, 0.0299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,190][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.6535, 0.1312, 0.0523, 0.0246, 0.0105, 0.0134, 0.0331, 0.0073, 0.0090,
        0.0047, 0.0096, 0.0300, 0.0051, 0.0077, 0.0027, 0.0054],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,192][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([1.6941e-05, 2.1353e-01, 1.6973e-03, 2.8736e-04, 4.9962e-02, 5.2354e-04,
        7.3118e-04, 1.8392e-06, 1.1245e-04, 4.8321e-05, 6.8337e-01, 7.7139e-03,
        1.8455e-06, 1.0747e-02, 3.0459e-02, 8.0164e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,193][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0003, 0.0078, 0.0110, 0.0224, 0.0235, 0.0396, 0.0430, 0.0371, 0.0579,
        0.0650, 0.1023, 0.0951, 0.0880, 0.1371, 0.1009, 0.1689],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,195][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([7.4730e-04, 8.7915e-03, 1.5841e-05, 3.9605e-02, 7.8569e-07, 1.3556e-04,
        1.1038e-05, 2.6196e-06, 2.9067e-03, 1.5649e-05, 8.4386e-07, 7.5347e-08,
        4.1124e-08, 2.6708e-01, 4.0402e-07, 6.8069e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,196][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0205, 0.0298, 0.0212, 0.0594, 0.0385, 0.0706, 0.0846, 0.0635, 0.1166,
        0.0386, 0.0697, 0.1045, 0.0642, 0.0702, 0.0349, 0.0366, 0.0766],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,198][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0079, 0.0455, 0.0458, 0.0521, 0.0381, 0.0799, 0.0688, 0.0829, 0.0770,
        0.0416, 0.0817, 0.0850, 0.0462, 0.0865, 0.0361, 0.0621, 0.0627],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,200][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0880, 0.0541, 0.0648, 0.0567, 0.0553, 0.0514, 0.0548, 0.0585, 0.0502,
        0.0589, 0.0496, 0.0513, 0.0664, 0.0567, 0.0602, 0.0653, 0.0578],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,202][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0968, 0.0266, 0.1395, 0.0389, 0.0606, 0.0660, 0.0559, 0.0655, 0.0216,
        0.0575, 0.0621, 0.0367, 0.0492, 0.0149, 0.0608, 0.1035, 0.0438],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,204][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1570, 0.0283, 0.0297, 0.0480, 0.0450, 0.0725, 0.0450, 0.0312, 0.0349,
        0.0562, 0.0700, 0.0528, 0.0542, 0.0862, 0.0605, 0.0563, 0.0721],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,205][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0309, 0.0391, 0.0044, 0.0806, 0.0036, 0.0976, 0.1148, 0.0515, 0.1664,
        0.0177, 0.0760, 0.1233, 0.0198, 0.0694, 0.0020, 0.0316, 0.0712],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,207][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0765, 0.0842, 0.0539, 0.1172, 0.0240, 0.0175, 0.0448, 0.0579, 0.1010,
        0.0601, 0.0634, 0.0790, 0.0840, 0.0745, 0.0051, 0.0241, 0.0329],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,209][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1138, 0.0254, 0.0370, 0.0352, 0.0484, 0.0469, 0.0368, 0.0563, 0.0444,
        0.0621, 0.0455, 0.0608, 0.1482, 0.1134, 0.0203, 0.0581, 0.0474],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,211][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.6611, 0.1318, 0.0464, 0.0241, 0.0093, 0.0123, 0.0295, 0.0070, 0.0088,
        0.0042, 0.0089, 0.0277, 0.0046, 0.0072, 0.0026, 0.0048, 0.0098],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,212][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([9.6239e-06, 2.0591e-01, 1.7149e-03, 1.6875e-04, 1.8632e-01, 3.1506e-04,
        6.4620e-05, 1.2409e-06, 2.6335e-05, 8.7000e-05, 4.9650e-01, 1.3981e-03,
        9.1653e-07, 6.8924e-03, 9.9537e-02, 9.8994e-04, 6.1296e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,213][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0003, 0.0065, 0.0095, 0.0189, 0.0203, 0.0338, 0.0371, 0.0314, 0.0487,
        0.0550, 0.0868, 0.0808, 0.0743, 0.1161, 0.0852, 0.1432, 0.1521],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,214][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([8.5242e-06, 1.6567e-04, 1.7141e-06, 1.0396e-03, 6.2124e-08, 4.1376e-07,
        4.2347e-01, 6.5302e-07, 9.1997e-04, 3.4129e-07, 5.8284e-07, 8.8177e-06,
        7.7858e-09, 5.5801e-03, 3.4397e-08, 1.9801e-07, 5.6880e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,215][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0109, 0.0202, 0.0115, 0.0586, 0.0244, 0.0693, 0.1013, 0.0581, 0.1531,
        0.0251, 0.0745, 0.1299, 0.0477, 0.0674, 0.0201, 0.0253, 0.0821, 0.0204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,217][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0041, 0.0422, 0.0429, 0.0534, 0.0298, 0.0838, 0.0958, 0.0862, 0.0724,
        0.0294, 0.0786, 0.0776, 0.0378, 0.0789, 0.0271, 0.0557, 0.0857, 0.0186],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,219][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0894, 0.0539, 0.0607, 0.0541, 0.0514, 0.0497, 0.0498, 0.0518, 0.0471,
        0.0535, 0.0466, 0.0470, 0.0607, 0.0535, 0.0552, 0.0619, 0.0520, 0.0619],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,220][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0983, 0.0310, 0.1496, 0.0441, 0.0584, 0.0716, 0.0472, 0.0515, 0.0204,
        0.0507, 0.0612, 0.0299, 0.0390, 0.0143, 0.0550, 0.0981, 0.0344, 0.0453],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,222][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.1795, 0.0244, 0.0280, 0.0348, 0.0493, 0.0799, 0.0343, 0.0294, 0.0267,
        0.0556, 0.0570, 0.0391, 0.0474, 0.0596, 0.0557, 0.0543, 0.0454, 0.0997],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,224][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0260, 0.0432, 0.0021, 0.0970, 0.0013, 0.0979, 0.1199, 0.0460, 0.1952,
        0.0087, 0.0754, 0.1285, 0.0113, 0.0587, 0.0006, 0.0237, 0.0637, 0.0008],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,226][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0333, 0.0543, 0.0012, 0.0139, 0.0059, 0.0331, 0.0583, 0.0042, 0.0110,
        0.0928, 0.0066, 0.0685, 0.0014, 0.0978, 0.0011, 0.0063, 0.0467, 0.4638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,227][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0649, 0.0239, 0.0624, 0.0272, 0.0521, 0.0463, 0.0611, 0.0627, 0.0310,
        0.0377, 0.0585, 0.0466, 0.1501, 0.0865, 0.0284, 0.0756, 0.0827, 0.0023],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,229][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.7054, 0.1141, 0.0395, 0.0217, 0.0078, 0.0099, 0.0255, 0.0055, 0.0071,
        0.0033, 0.0072, 0.0258, 0.0037, 0.0065, 0.0024, 0.0042, 0.0090, 0.0013],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,230][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([3.5493e-05, 2.5924e-01, 1.0609e-03, 1.2716e-03, 1.9704e-01, 8.6125e-04,
        7.0784e-05, 1.0292e-05, 8.1569e-05, 4.9411e-04, 1.2799e-01, 1.0049e-03,
        1.4590e-05, 2.0913e-02, 1.3655e-01, 1.8881e-03, 6.6017e-05, 2.5141e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,232][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0002, 0.0055, 0.0078, 0.0158, 0.0166, 0.0280, 0.0308, 0.0266, 0.0417,
        0.0474, 0.0751, 0.0706, 0.0647, 0.1020, 0.0762, 0.1300, 0.1376, 0.1234],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,234][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([5.3755e-07, 1.4658e-05, 1.1300e-08, 2.8469e-05, 2.8538e-07, 1.0988e-08,
        9.1272e-08, 2.1873e-07, 4.3032e-06, 1.1506e-05, 2.2183e-08, 1.0797e-08,
        2.5856e-08, 2.6204e-04, 1.5733e-07, 6.0952e-09, 7.7550e-08, 9.9968e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,235][circuit_model.py][line:2332][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0177, 0.0253, 0.0189, 0.0534, 0.0354, 0.0627, 0.0797, 0.0574, 0.1042,
        0.0348, 0.0623, 0.0972, 0.0548, 0.0620, 0.0323, 0.0331, 0.0727, 0.0366,
        0.0595], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,237][circuit_model.py][line:2335][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0076, 0.0402, 0.0465, 0.0488, 0.0376, 0.0752, 0.0843, 0.0865, 0.0548,
        0.0373, 0.0655, 0.0709, 0.0433, 0.0651, 0.0350, 0.0495, 0.0787, 0.0308,
        0.0424], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,239][circuit_model.py][line:2338][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0773, 0.0474, 0.0585, 0.0490, 0.0507, 0.0468, 0.0481, 0.0511, 0.0444,
        0.0510, 0.0445, 0.0453, 0.0578, 0.0502, 0.0550, 0.0608, 0.0510, 0.0592,
        0.0518], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,241][circuit_model.py][line:2341][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0957, 0.0236, 0.1501, 0.0345, 0.0565, 0.0669, 0.0432, 0.0545, 0.0181,
        0.0530, 0.0601, 0.0266, 0.0402, 0.0116, 0.0551, 0.1045, 0.0322, 0.0496,
        0.0239], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,243][circuit_model.py][line:2344][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.1067, 0.0226, 0.0259, 0.0409, 0.0437, 0.0681, 0.0385, 0.0210, 0.0303,
        0.0416, 0.0657, 0.0451, 0.0366, 0.0691, 0.0544, 0.0471, 0.0611, 0.0946,
        0.0871], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,245][circuit_model.py][line:2347][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0302, 0.0416, 0.0043, 0.0813, 0.0033, 0.0901, 0.1081, 0.0515, 0.1596,
        0.0160, 0.0709, 0.1185, 0.0179, 0.0636, 0.0017, 0.0283, 0.0655, 0.0023,
        0.0453], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,247][circuit_model.py][line:2350][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0747, 0.0383, 0.0777, 0.1111, 0.0037, 0.0206, 0.0967, 0.0220, 0.0748,
        0.0242, 0.0641, 0.0731, 0.0131, 0.0714, 0.0011, 0.0806, 0.0835, 0.0335,
        0.0358], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,248][circuit_model.py][line:2353][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1218, 0.0302, 0.0328, 0.0343, 0.0452, 0.0413, 0.0352, 0.0339, 0.0437,
        0.0511, 0.0447, 0.0500, 0.0842, 0.0988, 0.0188, 0.0569, 0.0426, 0.0746,
        0.0598], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,250][circuit_model.py][line:2356][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.6797, 0.1245, 0.0456, 0.0222, 0.0082, 0.0111, 0.0271, 0.0063, 0.0079,
        0.0039, 0.0082, 0.0251, 0.0042, 0.0062, 0.0022, 0.0043, 0.0087, 0.0013,
        0.0033], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,252][circuit_model.py][line:2359][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.6490e-06, 1.5633e-01, 1.6281e-03, 2.2475e-04, 1.3277e-01, 2.2342e-04,
        3.5075e-05, 1.1159e-06, 1.3594e-05, 8.2287e-05, 5.0982e-01, 5.6030e-04,
        1.2013e-06, 3.1436e-03, 6.5172e-02, 8.4273e-04, 2.8697e-05, 1.2899e-01,
        1.2165e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,253][circuit_model.py][line:2362][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0003, 0.0050, 0.0074, 0.0146, 0.0157, 0.0260, 0.0280, 0.0235, 0.0369,
        0.0413, 0.0658, 0.0603, 0.0550, 0.0868, 0.0632, 0.1065, 0.1121, 0.0984,
        0.1533], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,254][circuit_model.py][line:2365][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([7.3010e-06, 2.3813e-04, 1.6676e-08, 5.3730e-03, 1.9519e-09, 2.7573e-07,
        5.8638e-07, 7.0367e-09, 7.3422e-04, 8.3783e-09, 7.1684e-06, 3.8225e-08,
        7.6265e-11, 8.9438e-03, 1.0194e-09, 7.0353e-09, 4.6274e-07, 1.4702e-07,
        9.8469e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,258][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:16,260][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 5495],
        [ 2941],
        [   44],
        [ 3691],
        [  880],
        [ 3122],
        [ 8719],
        [ 2379],
        [ 7118],
        [10192],
        [ 3817],
        [13647],
        [11449],
        [ 2005],
        [ 1718],
        [  953],
        [ 7653],
        [ 2221],
        [ 3462]], device='cuda:0')
[2024-07-24 10:25:16,262][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 7384],
        [ 6683],
        [    2],
        [15000],
        [ 1411],
        [10610],
        [17720],
        [12411],
        [17909],
        [24568],
        [12248],
        [30818],
        [32836],
        [ 2452],
        [ 3093],
        [ 2787],
        [14054],
        [ 7473],
        [ 6617]], device='cuda:0')
[2024-07-24 10:25:16,264][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[14441],
        [14497],
        [19517],
        [19919],
        [27631],
        [23035],
        [22093],
        [28985],
        [34338],
        [30751],
        [31134],
        [27586],
        [26011],
        [26765],
        [25301],
        [26108],
        [26654],
        [24835],
        [25422]], device='cuda:0')
[2024-07-24 10:25:16,265][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[43756],
        [12918],
        [12837],
        [12775],
        [12895],
        [12834],
        [12961],
        [14427],
        [16125],
        [15168],
        [17004],
        [17089],
        [17087],
        [17185],
        [15157],
        [16811],
        [17865],
        [18231],
        [18590]], device='cuda:0')
[2024-07-24 10:25:16,267][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 9332],
        [13519],
        [27592],
        [31260],
        [32860],
        [34070],
        [35778],
        [35606],
        [36267],
        [35828],
        [34442],
        [35396],
        [33076],
        [32301],
        [32531],
        [31374],
        [32683],
        [31791],
        [32142]], device='cuda:0')
[2024-07-24 10:25:16,269][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 5115],
        [13460],
        [12466],
        [ 9453],
        [10799],
        [12438],
        [14512],
        [14160],
        [16098],
        [16025],
        [15870],
        [16980],
        [17824],
        [16554],
        [16523],
        [16225],
        [17024],
        [16671],
        [16305]], device='cuda:0')
[2024-07-24 10:25:16,271][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 5172],
        [12917],
        [11428],
        [ 6525],
        [10602],
        [ 9135],
        [10613],
        [11319],
        [11040],
        [11002],
        [11022],
        [12061],
        [11947],
        [13522],
        [14770],
        [14841],
        [15298],
        [14214],
        [13830]], device='cuda:0')
[2024-07-24 10:25:16,273][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[14649],
        [11625],
        [14431],
        [ 8916],
        [28116],
        [13237],
        [15842],
        [ 8707],
        [11424],
        [ 5129],
        [ 2961],
        [ 8785],
        [ 6522],
        [16304],
        [19467],
        [14416],
        [15202],
        [15613],
        [11785]], device='cuda:0')
[2024-07-24 10:25:16,275][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[4122],
        [4099],
        [4083],
        [4064],
        [3920],
        [3952],
        [3735],
        [3747],
        [3739],
        [3954],
        [4484],
        [5823],
        [8993],
        [4376],
        [7034],
        [3276],
        [4889],
        [7602],
        [8841]], device='cuda:0')
[2024-07-24 10:25:16,277][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[34017],
        [34369],
        [28730],
        [ 4475],
        [  549],
        [ 2830],
        [ 5663],
        [ 4108],
        [ 4513],
        [12147],
        [ 8679],
        [11115],
        [ 5232],
        [ 8573],
        [27405],
        [24285],
        [22278],
        [14261],
        [21228]], device='cuda:0')
[2024-07-24 10:25:16,279][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 7600],
        [13463],
        [29205],
        [30526],
        [32055],
        [38147],
        [25790],
        [21839],
        [28077],
        [36346],
        [38691],
        [32506],
        [33758],
        [31736],
        [18865],
        [26078],
        [26554],
        [18868],
        [30369]], device='cuda:0')
[2024-07-24 10:25:16,281][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[32823],
        [16759],
        [18683],
        [32024],
        [28269],
        [31203],
        [30535],
        [32716],
        [30202],
        [32019],
        [32094],
        [30166],
        [31107],
        [30481],
        [26479],
        [33130],
        [29505],
        [17010],
        [13729]], device='cuda:0')
[2024-07-24 10:25:16,283][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[1591],
        [6858],
        [2392],
        [2333],
        [2188],
        [1800],
        [1495],
        [ 926],
        [ 989],
        [ 979],
        [ 872],
        [ 978],
        [ 819],
        [ 879],
        [ 957],
        [ 957],
        [ 862],
        [ 944],
        [1006]], device='cuda:0')
[2024-07-24 10:25:16,284][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[3267],
        [ 860],
        [1300],
        [1053],
        [1039],
        [1113],
        [1779],
        [1269],
        [1666],
        [1465],
        [1721],
        [1698],
        [1445],
        [1708],
        [1675],
        [1615],
        [2132],
        [1692],
        [1734]], device='cuda:0')
[2024-07-24 10:25:16,286][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 9969],
        [15513],
        [ 9351],
        [13739],
        [ 1731],
        [14184],
        [16173],
        [11870],
        [14430],
        [ 8433],
        [11657],
        [18031],
        [27476],
        [21208],
        [ 4231],
        [13799],
        [24723],
        [16532],
        [20133]], device='cuda:0')
[2024-07-24 10:25:16,288][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 6553],
        [ 8873],
        [ 9192],
        [11370],
        [11953],
        [11627],
        [12266],
        [12986],
        [12608],
        [13302],
        [12517],
        [12701],
        [13251],
        [13435],
        [13534],
        [13469],
        [13549],
        [13786],
        [13616]], device='cuda:0')
[2024-07-24 10:25:16,290][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[1568],
        [2297],
        [3600],
        [3736],
        [4591],
        [4078],
        [2627],
        [2095],
        [2102],
        [1903],
        [2064],
        [1930],
        [1822],
        [1961],
        [1934],
        [2003],
        [1929],
        [1822],
        [1883]], device='cuda:0')
[2024-07-24 10:25:16,292][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[28650],
        [26935],
        [27327],
        [26180],
        [26778],
        [26385],
        [25966],
        [25813],
        [25702],
        [25886],
        [25959],
        [25438],
        [25946],
        [25906],
        [25893],
        [25637],
        [25349],
        [25343],
        [25064]], device='cuda:0')
[2024-07-24 10:25:16,294][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 3357],
        [ 3193],
        [14542],
        [15413],
        [15224],
        [18225],
        [17349],
        [15718],
        [15954],
        [16581],
        [15386],
        [15194],
        [16040],
        [15516],
        [15372],
        [15949],
        [15621],
        [16112],
        [16271]], device='cuda:0')
[2024-07-24 10:25:16,296][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[ 9309],
        [ 9026],
        [ 8459],
        [ 7265],
        [ 7606],
        [ 7200],
        [ 7519],
        [ 8054],
        [ 8196],
        [ 8846],
        [ 9086],
        [ 9472],
        [ 9427],
        [ 9666],
        [ 9660],
        [ 9736],
        [10062],
        [10126],
        [10109]], device='cuda:0')
[2024-07-24 10:25:16,298][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[31779],
        [30716],
        [30940],
        [32809],
        [32906],
        [34262],
        [35185],
        [35688],
        [35989],
        [35980],
        [36049],
        [36064],
        [36082],
        [35986],
        [35920],
        [35872],
        [35872],
        [35870],
        [35782]], device='cuda:0')
[2024-07-24 10:25:16,300][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[20438],
        [19593],
        [17751],
        [17987],
        [13737],
        [17902],
        [18223],
        [20431],
        [18342],
        [23367],
        [29109],
        [22621],
        [21970],
        [14361],
        [10361],
        [22492],
        [22386],
        [42904],
        [24103]], device='cuda:0')
[2024-07-24 10:25:16,301][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[28234],
        [28898],
        [30599],
        [35134],
        [36720],
        [40898],
        [41099],
        [39984],
        [37819],
        [37962],
        [37355],
        [37090],
        [34874],
        [32734],
        [31597],
        [30931],
        [30064],
        [32907],
        [34484]], device='cuda:0')
[2024-07-24 10:25:16,303][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[26654],
        [24316],
        [23018],
        [22671],
        [23179],
        [22622],
        [22290],
        [22379],
        [22364],
        [22200],
        [21952],
        [21330],
        [21924],
        [21773],
        [21994],
        [21048],
        [21114],
        [21769],
        [21414]], device='cuda:0')
[2024-07-24 10:25:16,305][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[12512],
        [44747],
        [44706],
        [44704],
        [45279],
        [45392],
        [45988],
        [44561],
        [45925],
        [45746],
        [40934],
        [37100],
        [36693],
        [39179],
        [44478],
        [35672],
        [37531],
        [43546],
        [35001]], device='cuda:0')
[2024-07-24 10:25:16,307][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[10926],
        [ 5686],
        [ 5803],
        [ 6011],
        [ 6695],
        [ 6082],
        [ 6805],
        [ 8001],
        [ 8344],
        [ 8697],
        [ 8448],
        [ 8975],
        [ 9805],
        [10559],
        [10931],
        [11184],
        [11905],
        [12785],
        [12662]], device='cuda:0')
[2024-07-24 10:25:16,308][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[22740],
        [41821],
        [ 5275],
        [42159],
        [ 4184],
        [ 8403],
        [ 6023],
        [ 6707],
        [43349],
        [ 8924],
        [ 5456],
        [17869],
        [ 4627],
        [42258],
        [ 4104],
        [32097],
        [ 6045],
        [ 7636],
        [40224]], device='cuda:0')
[2024-07-24 10:25:16,310][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[22486],
        [17064],
        [24685],
        [11695],
        [21525],
        [18330],
        [19242],
        [18215],
        [15680],
        [18475],
        [18030],
        [20211],
        [23120],
        [11864],
        [24366],
        [19863],
        [21286],
        [14607],
        [16418]], device='cuda:0')
[2024-07-24 10:25:16,312][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[10005],
        [ 9604],
        [32946],
        [11881],
        [43305],
        [17338],
        [17710],
        [17040],
        [ 9285],
        [27420],
        [22614],
        [15687],
        [ 9873],
        [ 6028],
        [40694],
        [17264],
        [14272],
        [22178],
        [13919]], device='cuda:0')
[2024-07-24 10:25:16,314][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705],
        [15705]], device='cuda:0')
[2024-07-24 10:25:16,353][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:16,355][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,355][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,356][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,357][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,357][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,358][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,359][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,360][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,361][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,361][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,362][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,363][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,363][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.4268, 0.5732], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,364][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.9856, 0.0144], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,365][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0030, 0.9970], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,366][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0949, 0.9051], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,366][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.4734, 0.5266], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,368][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1056, 0.8944], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,369][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.9219, 0.0781], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,371][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.8957, 0.1043], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,372][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.6925, 0.3075], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,374][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.5136, 0.4864], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,375][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.1203, 0.8797], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,377][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.5391, 0.4609], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,379][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.2379, 0.4243, 0.3378], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,380][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.8038, 0.0717, 0.1244], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,381][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([8.6515e-06, 1.2620e-01, 8.7379e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,383][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.0122, 0.4993, 0.4884], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,384][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.2822, 0.6255, 0.0923], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,386][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.0567, 0.7356, 0.2076], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,388][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.5467, 0.1812, 0.2721], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,389][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.4723, 0.2040, 0.3237], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,391][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.7229, 0.1759, 0.1012], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,393][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.2266, 0.6195, 0.1540], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,394][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.0085, 0.3481, 0.6434], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,396][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.2891, 0.2502, 0.4607], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,397][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.1620, 0.2609, 0.3702, 0.2069], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,399][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.8438, 0.0191, 0.0437, 0.0934], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,400][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ and] are: tensor([8.4147e-07, 6.7814e-03, 4.7203e-01, 5.2118e-01], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,402][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0072, 0.1565, 0.6699, 0.1663], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,403][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1355, 0.4481, 0.0625, 0.3539], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,405][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0250, 0.3153, 0.5436, 0.1160], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,405][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5731, 0.1073, 0.2884, 0.0312], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,406][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.4433, 0.0750, 0.4132, 0.0685], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,407][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.4637, 0.1991, 0.1428, 0.1945], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,408][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0172, 0.1124, 0.8534, 0.0170], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,409][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.0221, 0.2438, 0.5127, 0.2213], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,411][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2251, 0.1984, 0.3786, 0.1980], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,412][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.1156, 0.2010, 0.3334, 0.2283, 0.1217], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,414][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.6662, 0.0461, 0.0756, 0.1583, 0.0539], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,415][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([8.9557e-08, 3.8995e-04, 2.4739e-02, 1.4182e-01, 8.3305e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,417][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0037, 0.1523, 0.5233, 0.2240, 0.0967], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,418][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0982, 0.2735, 0.0769, 0.3238, 0.2276], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,420][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0142, 0.2678, 0.3157, 0.3404, 0.0618], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,421][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.1084, 0.1789, 0.5203, 0.0741, 0.1183], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,423][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.2909, 0.2253, 0.2569, 0.0734, 0.1536], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,425][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.5959, 0.1313, 0.0763, 0.1286, 0.0679], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,426][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0305, 0.0981, 0.7198, 0.0691, 0.0825], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,428][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0084, 0.1718, 0.4136, 0.1318, 0.2744], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,430][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.1641, 0.1480, 0.2942, 0.1533, 0.2404], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,431][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.1038, 0.1926, 0.2511, 0.1767, 0.2000, 0.0757], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,433][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.6682, 0.0350, 0.0609, 0.1265, 0.0405, 0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,434][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ had] are: tensor([8.2787e-09, 5.1842e-05, 1.5026e-03, 3.6543e-02, 8.3202e-01, 1.2989e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,436][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.0102, 0.1294, 0.3548, 0.2209, 0.2193, 0.0655], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,437][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.1196, 0.1750, 0.0477, 0.3393, 0.2198, 0.0986], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,439][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0236, 0.2242, 0.3348, 0.1974, 0.1566, 0.0634], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,441][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.4833, 0.1581, 0.2141, 0.0255, 0.0795, 0.0395], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,442][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.3099, 0.1210, 0.2445, 0.0529, 0.1562, 0.1155], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,444][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.6072, 0.0998, 0.0543, 0.0985, 0.0547, 0.0856], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,445][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0246, 0.0910, 0.4231, 0.0502, 0.4005, 0.0106], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,447][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.0120, 0.1513, 0.2977, 0.0933, 0.2987, 0.1470], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,448][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.1454, 0.1328, 0.2494, 0.1241, 0.2180, 0.1303], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,448][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0916, 0.1481, 0.2053, 0.1836, 0.1733, 0.0876, 0.1105],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,449][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.5602, 0.0382, 0.0628, 0.1177, 0.0454, 0.0746, 0.1012],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,450][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([6.4583e-09, 1.2491e-05, 1.1359e-03, 1.2956e-02, 2.7386e-01, 5.5276e-01,
        1.5928e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,452][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0076, 0.1040, 0.2522, 0.2054, 0.1516, 0.1018, 0.1774],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,453][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1037, 0.1422, 0.0582, 0.2416, 0.2306, 0.0960, 0.1278],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,455][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0177, 0.1843, 0.1918, 0.1447, 0.1819, 0.2650, 0.0145],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,456][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.5834, 0.1385, 0.1197, 0.0410, 0.0286, 0.0685, 0.0204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,458][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2465, 0.0925, 0.2525, 0.0505, 0.1605, 0.1215, 0.0760],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,459][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.5560, 0.1038, 0.0551, 0.1037, 0.0556, 0.0917, 0.0341],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,461][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0301, 0.1035, 0.3290, 0.0432, 0.4021, 0.0732, 0.0188],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,463][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0111, 0.1301, 0.2660, 0.1121, 0.1950, 0.1398, 0.1459],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,465][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1339, 0.1153, 0.2167, 0.1236, 0.1792, 0.1267, 0.1046],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,466][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0796, 0.1228, 0.1622, 0.1467, 0.1189, 0.0803, 0.0920, 0.1975],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,468][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.5276, 0.0340, 0.0573, 0.1175, 0.0400, 0.0712, 0.1033, 0.0491],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,469][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([9.5400e-09, 1.1219e-05, 3.1026e-04, 4.3076e-03, 1.3719e-01, 7.6920e-02,
        6.2311e-01, 1.5815e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,471][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.0011, 0.0461, 0.2226, 0.0982, 0.1153, 0.1224, 0.2729, 0.1214],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,473][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.0676, 0.1339, 0.0355, 0.1910, 0.1208, 0.0882, 0.1945, 0.1685],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,474][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0107, 0.2129, 0.1971, 0.1990, 0.1236, 0.1970, 0.0332, 0.0265],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,476][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.4660, 0.0592, 0.1828, 0.0551, 0.0471, 0.0885, 0.0439, 0.0574],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,478][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.2051, 0.1220, 0.1724, 0.0433, 0.1065, 0.0945, 0.0653, 0.1909],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,479][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.4669, 0.1091, 0.0637, 0.1095, 0.0613, 0.0988, 0.0420, 0.0488],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,481][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0240, 0.1640, 0.1069, 0.0934, 0.1888, 0.2744, 0.1360, 0.0126],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,483][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.0026, 0.1187, 0.2034, 0.0758, 0.1455, 0.0963, 0.1047, 0.2530],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,484][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.1189, 0.0949, 0.1940, 0.0941, 0.1596, 0.1025, 0.0860, 0.1500],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,486][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0718, 0.1098, 0.1615, 0.0987, 0.1333, 0.0503, 0.0790, 0.1946, 0.1010],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,488][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.6194, 0.0200, 0.0433, 0.0832, 0.0274, 0.0452, 0.0680, 0.0324, 0.0611],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,488][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ of] are: tensor([2.7297e-09, 1.8061e-07, 5.6791e-05, 1.7456e-04, 9.4834e-03, 7.0063e-03,
        4.6857e-02, 4.8593e-01, 4.5049e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,489][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0028, 0.0518, 0.1480, 0.0953, 0.0774, 0.0841, 0.2576, 0.2221, 0.0609],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,490][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0251, 0.0594, 0.0804, 0.0979, 0.1824, 0.0326, 0.0967, 0.3760, 0.0495],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,491][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0180, 0.1511, 0.1637, 0.1453, 0.2242, 0.1896, 0.0350, 0.0560, 0.0171],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,492][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.4385, 0.0970, 0.1101, 0.0762, 0.0289, 0.0783, 0.0699, 0.0935, 0.0075],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,493][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.2167, 0.0604, 0.1499, 0.0481, 0.0929, 0.1428, 0.0775, 0.1730, 0.0386],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,495][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.4427, 0.1028, 0.0580, 0.1034, 0.0588, 0.0997, 0.0405, 0.0470, 0.0471],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,497][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0073, 0.0414, 0.1850, 0.0239, 0.5280, 0.0804, 0.0624, 0.0657, 0.0058],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,498][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0065, 0.1074, 0.1560, 0.0982, 0.1491, 0.0907, 0.1252, 0.1449, 0.1222],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,500][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1012, 0.0874, 0.1658, 0.0925, 0.1453, 0.0968, 0.0813, 0.1283, 0.1015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,501][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.0622, 0.1015, 0.1175, 0.0993, 0.0934, 0.0624, 0.0667, 0.1563, 0.1353,
        0.1053], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,503][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.3961, 0.0330, 0.0592, 0.1101, 0.0431, 0.0686, 0.0950, 0.0495, 0.0909,
        0.0545], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,505][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([7.7468e-11, 1.7858e-08, 2.0331e-06, 8.0992e-06, 4.2010e-04, 1.9750e-03,
        5.3044e-03, 9.7591e-03, 5.4580e-01, 4.3673e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,506][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.0015, 0.0334, 0.1022, 0.0528, 0.0670, 0.0527, 0.1922, 0.3371, 0.1013,
        0.0598], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,508][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0312, 0.0843, 0.0314, 0.0941, 0.1301, 0.1407, 0.1270, 0.2457, 0.0705,
        0.0450], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,510][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0131, 0.1836, 0.1717, 0.1354, 0.0945, 0.0895, 0.0400, 0.1121, 0.0771,
        0.0830], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,511][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.1959, 0.2053, 0.1465, 0.0463, 0.0342, 0.0762, 0.0812, 0.1417, 0.0216,
        0.0511], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,513][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.1497, 0.0989, 0.1226, 0.0563, 0.0862, 0.1276, 0.0604, 0.1460, 0.0323,
        0.1201], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,515][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.5052, 0.0924, 0.0494, 0.0935, 0.0470, 0.0833, 0.0297, 0.0368, 0.0342,
        0.0284], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,516][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0184, 0.0466, 0.1013, 0.0185, 0.6545, 0.0377, 0.0321, 0.0599, 0.0258,
        0.0052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,518][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.0022, 0.0732, 0.1736, 0.0513, 0.1165, 0.0776, 0.0764, 0.2276, 0.0500,
        0.1517], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,520][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0890, 0.0767, 0.1481, 0.0776, 0.1234, 0.0831, 0.0687, 0.1194, 0.0877,
        0.1264], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,521][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0501, 0.0913, 0.1261, 0.0865, 0.0952, 0.0503, 0.0669, 0.1387, 0.0973,
        0.1024, 0.0951], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,523][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.4896, 0.0249, 0.0434, 0.0735, 0.0327, 0.0489, 0.0651, 0.0352, 0.0607,
        0.0412, 0.0850], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,524][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ at] are: tensor([8.6310e-11, 1.9631e-09, 1.2201e-07, 1.1994e-06, 2.9463e-05, 4.6719e-05,
        3.1850e-04, 2.6247e-04, 5.0433e-02, 3.8352e-01, 5.6539e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,526][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0019, 0.0314, 0.1707, 0.0595, 0.0628, 0.0659, 0.1705, 0.1938, 0.1319,
        0.0925, 0.0193], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,528][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0367, 0.1031, 0.0211, 0.1465, 0.1092, 0.0445, 0.1191, 0.1700, 0.0966,
        0.0757, 0.0774], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,530][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0132, 0.1261, 0.1070, 0.1124, 0.1165, 0.1545, 0.0438, 0.1312, 0.0558,
        0.1194, 0.0199], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,531][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.6608, 0.0589, 0.0710, 0.0251, 0.0209, 0.0578, 0.0136, 0.0818, 0.0019,
        0.0041, 0.0042], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,532][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1841, 0.0632, 0.1456, 0.0386, 0.0882, 0.1017, 0.0597, 0.1684, 0.0237,
        0.1001, 0.0268], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,533][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.5827, 0.0752, 0.0390, 0.0766, 0.0386, 0.0676, 0.0227, 0.0276, 0.0263,
        0.0223, 0.0214], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,533][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0108, 0.0454, 0.2015, 0.0187, 0.2439, 0.0504, 0.0739, 0.0979, 0.0119,
        0.2417, 0.0039], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,535][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.0028, 0.0629, 0.1482, 0.0487, 0.1176, 0.0704, 0.0745, 0.1927, 0.0550,
        0.1262, 0.1009], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,537][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0822, 0.0676, 0.1338, 0.0734, 0.1164, 0.0748, 0.0646, 0.1083, 0.0808,
        0.1199, 0.0781], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,538][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0443, 0.0717, 0.1034, 0.0803, 0.0958, 0.0429, 0.0539, 0.1350, 0.0983,
        0.1008, 0.1283, 0.0451], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,540][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4501, 0.0213, 0.0386, 0.0655, 0.0288, 0.0434, 0.0573, 0.0313, 0.0534,
        0.0362, 0.0759, 0.0982], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,541][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.0914e-12, 5.5424e-11, 8.8205e-09, 5.2368e-08, 2.0729e-06, 2.0034e-06,
        3.8812e-06, 1.8514e-05, 3.2804e-03, 2.5227e-02, 8.9378e-01, 7.7690e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,543][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0019, 0.0322, 0.0891, 0.0637, 0.0455, 0.0413, 0.1474, 0.2198, 0.0882,
        0.1302, 0.0699, 0.0707], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,545][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0512, 0.0819, 0.0542, 0.1306, 0.0917, 0.0440, 0.1010, 0.1117, 0.0617,
        0.0707, 0.0920, 0.1093], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,546][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0112, 0.1186, 0.0961, 0.1332, 0.0883, 0.1734, 0.0260, 0.1182, 0.0319,
        0.1417, 0.0450, 0.0162], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,548][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5386, 0.0671, 0.0524, 0.0365, 0.0250, 0.0984, 0.0143, 0.1265, 0.0092,
        0.0129, 0.0158, 0.0034], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,550][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1858, 0.0416, 0.1323, 0.0408, 0.0765, 0.1044, 0.0588, 0.1516, 0.0319,
        0.0967, 0.0338, 0.0459], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,552][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5411, 0.0769, 0.0394, 0.0786, 0.0406, 0.0713, 0.0242, 0.0295, 0.0285,
        0.0240, 0.0232, 0.0228], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,553][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0368, 0.0947, 0.1493, 0.0311, 0.3289, 0.0594, 0.0140, 0.0890, 0.0188,
        0.1138, 0.0491, 0.0152], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,555][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0039, 0.0678, 0.1178, 0.0622, 0.0891, 0.0780, 0.0807, 0.1519, 0.0674,
        0.1135, 0.1090, 0.0587], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,557][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0742, 0.0658, 0.1192, 0.0714, 0.1058, 0.0757, 0.0605, 0.0970, 0.0775,
        0.1095, 0.0749, 0.0685], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,558][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0456, 0.0698, 0.0955, 0.0705, 0.0729, 0.0391, 0.0513, 0.1162, 0.0921,
        0.0904, 0.1039, 0.0520, 0.1007], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,560][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.3361, 0.0216, 0.0364, 0.0692, 0.0289, 0.0488, 0.0653, 0.0329, 0.0619,
        0.0383, 0.0937, 0.1177, 0.0490], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,562][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ house] are: tensor([4.0290e-13, 3.3145e-11, 1.1059e-08, 1.9249e-08, 2.0766e-06, 4.6594e-07,
        8.3006e-06, 1.9749e-05, 1.9032e-03, 2.6785e-02, 3.1541e-01, 4.8738e-01,
        1.6849e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,563][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0006, 0.0226, 0.1267, 0.0518, 0.0571, 0.0374, 0.1116, 0.1592, 0.1047,
        0.1452, 0.0422, 0.0958, 0.0452], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,565][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0286, 0.0939, 0.0335, 0.1044, 0.0904, 0.0579, 0.0925, 0.1090, 0.0710,
        0.0579, 0.0726, 0.1508, 0.0375], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,567][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0054, 0.0822, 0.2457, 0.0955, 0.0941, 0.1219, 0.0325, 0.0617, 0.0464,
        0.0998, 0.0579, 0.0396, 0.0172], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,569][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.1990, 0.1718, 0.0934, 0.1009, 0.0221, 0.0795, 0.1416, 0.0824, 0.0152,
        0.0166, 0.0293, 0.0401, 0.0081], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,570][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.1536, 0.0554, 0.1161, 0.0395, 0.0683, 0.0944, 0.0499, 0.1401, 0.0250,
        0.1151, 0.0293, 0.0294, 0.0840], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,572][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.5847, 0.0719, 0.0363, 0.0734, 0.0342, 0.0589, 0.0202, 0.0261, 0.0236,
        0.0191, 0.0191, 0.0186, 0.0141], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,573][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0049, 0.0202, 0.3777, 0.0159, 0.3189, 0.0248, 0.0178, 0.0644, 0.0147,
        0.0750, 0.0154, 0.0330, 0.0172], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,574][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.0021, 0.0618, 0.1304, 0.0482, 0.0848, 0.0498, 0.0694, 0.1784, 0.0454,
        0.1465, 0.0547, 0.0429, 0.0855], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,575][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0649, 0.0549, 0.1148, 0.0566, 0.0947, 0.0638, 0.0509, 0.0911, 0.0631,
        0.1016, 0.0602, 0.0606, 0.1228], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,576][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.0399, 0.0603, 0.0886, 0.0618, 0.0645, 0.0352, 0.0567, 0.1117, 0.0757,
        0.0891, 0.1033, 0.0495, 0.0950, 0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,577][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.6528, 0.0078, 0.0213, 0.0438, 0.0122, 0.0192, 0.0309, 0.0138, 0.0279,
        0.0163, 0.0463, 0.0712, 0.0213, 0.0154], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,578][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [.] are: tensor([3.4306e-12, 1.8154e-11, 8.7125e-09, 1.4542e-08, 1.1080e-06, 7.3554e-07,
        6.1071e-06, 1.1250e-05, 1.9077e-04, 8.6069e-03, 2.1928e-02, 9.9304e-02,
        2.4647e-01, 6.2348e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,580][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0013, 0.0293, 0.1010, 0.0520, 0.0417, 0.0641, 0.0893, 0.1345, 0.0951,
        0.1033, 0.0472, 0.0885, 0.1384, 0.0143], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,582][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0236, 0.0554, 0.0362, 0.0794, 0.0851, 0.0357, 0.1068, 0.1092, 0.0564,
        0.0482, 0.0704, 0.1432, 0.0762, 0.0742], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,583][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0085, 0.0701, 0.1033, 0.0793, 0.0842, 0.1558, 0.0296, 0.0921, 0.0411,
        0.0830, 0.0661, 0.0387, 0.0924, 0.0558], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,585][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.1432, 0.0703, 0.3127, 0.0534, 0.0450, 0.0373, 0.0804, 0.0787, 0.0251,
        0.0275, 0.0345, 0.0184, 0.0260, 0.0476], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,587][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.1597, 0.0308, 0.1279, 0.0277, 0.0824, 0.0782, 0.0437, 0.1394, 0.0199,
        0.1155, 0.0227, 0.0277, 0.1028, 0.0216], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,589][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.2637, 0.0721, 0.0477, 0.0748, 0.0495, 0.0961, 0.0414, 0.0425, 0.0436,
        0.0407, 0.0427, 0.0427, 0.0374, 0.1052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,590][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0121, 0.0465, 0.1321, 0.0395, 0.1482, 0.0590, 0.0430, 0.0932, 0.0245,
        0.0432, 0.0487, 0.0867, 0.1832, 0.0401], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,592][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.0048, 0.0443, 0.1005, 0.0545, 0.0923, 0.0511, 0.0817, 0.1141, 0.0756,
        0.0956, 0.1009, 0.0743, 0.0777, 0.0325], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,594][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0619, 0.0524, 0.1012, 0.0566, 0.0895, 0.0591, 0.0514, 0.0777, 0.0645,
        0.0912, 0.0608, 0.0589, 0.1125, 0.0624], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,596][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0324, 0.0602, 0.0946, 0.0657, 0.0350, 0.0363, 0.0447, 0.0881, 0.0787,
        0.0885, 0.1040, 0.0514, 0.1093, 0.0813, 0.0297], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,598][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.2917, 0.0236, 0.0396, 0.0677, 0.0303, 0.0463, 0.0603, 0.0340, 0.0589,
        0.0393, 0.0820, 0.1026, 0.0495, 0.0391, 0.0350], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,599][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([4.4242e-13, 6.4235e-12, 6.8390e-10, 1.3760e-09, 1.5220e-08, 7.8363e-08,
        1.3546e-07, 2.9501e-07, 8.5395e-06, 9.3954e-05, 3.2100e-04, 2.8409e-03,
        9.9944e-03, 9.0232e-02, 8.9651e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,601][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0005, 0.0240, 0.0865, 0.0343, 0.0150, 0.0332, 0.1270, 0.1574, 0.1072,
        0.1147, 0.0598, 0.1149, 0.0997, 0.0133, 0.0124], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,603][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0219, 0.0682, 0.0175, 0.0817, 0.0521, 0.0478, 0.0880, 0.0667, 0.0620,
        0.0512, 0.0614, 0.1208, 0.0948, 0.1068, 0.0592], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,604][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0040, 0.0795, 0.0951, 0.1091, 0.0171, 0.0754, 0.0357, 0.0902, 0.0404,
        0.1369, 0.0451, 0.0341, 0.1054, 0.1156, 0.0164], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,606][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0594, 0.1122, 0.2472, 0.0426, 0.0611, 0.1604, 0.0373, 0.0646, 0.0247,
        0.0290, 0.0398, 0.0125, 0.0188, 0.0438, 0.0465], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,608][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.1208, 0.1362, 0.1029, 0.0379, 0.0578, 0.0985, 0.0436, 0.0803, 0.0224,
        0.0892, 0.0258, 0.0263, 0.0872, 0.0235, 0.0477], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,610][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.5345, 0.0647, 0.0308, 0.0627, 0.0274, 0.0537, 0.0167, 0.0214, 0.0199,
        0.0172, 0.0157, 0.0156, 0.0129, 0.0681, 0.0388], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,611][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0077, 0.0215, 0.1477, 0.0150, 0.0157, 0.0313, 0.0343, 0.0229, 0.0138,
        0.1817, 0.0277, 0.0568, 0.3782, 0.0294, 0.0165], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,613][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0027, 0.0527, 0.1291, 0.0436, 0.0873, 0.0790, 0.0573, 0.1108, 0.0328,
        0.1121, 0.0571, 0.0397, 0.1006, 0.0232, 0.0720], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,615][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0522, 0.0469, 0.0945, 0.0496, 0.0779, 0.0584, 0.0431, 0.0762, 0.0535,
        0.0867, 0.0519, 0.0514, 0.1065, 0.0594, 0.0917], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,616][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0324, 0.0610, 0.0609, 0.0571, 0.0593, 0.0346, 0.0439, 0.0955, 0.0763,
        0.0858, 0.0949, 0.0452, 0.1005, 0.0766, 0.0497, 0.0261],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,616][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.3568, 0.0174, 0.0364, 0.0648, 0.0243, 0.0357, 0.0493, 0.0264, 0.0469,
        0.0311, 0.0706, 0.0931, 0.0401, 0.0307, 0.0285, 0.0481],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,617][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([7.4134e-14, 3.4106e-13, 2.0809e-11, 1.0481e-10, 3.0608e-09, 1.1751e-09,
        8.6191e-09, 1.6116e-08, 8.5159e-07, 1.4302e-05, 1.2140e-04, 3.9872e-04,
        2.0292e-03, 2.7858e-02, 4.3036e-01, 5.3922e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,619][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0010, 0.0191, 0.0685, 0.0288, 0.0212, 0.0262, 0.0963, 0.2312, 0.0990,
        0.1262, 0.0424, 0.1084, 0.0871, 0.0158, 0.0185, 0.0104],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,621][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0280, 0.0645, 0.0139, 0.0888, 0.0838, 0.0339, 0.0654, 0.0434, 0.0619,
        0.0430, 0.0645, 0.1037, 0.0712, 0.1202, 0.1003, 0.0134],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,622][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0057, 0.0914, 0.0643, 0.0769, 0.0607, 0.0395, 0.0322, 0.0587, 0.0435,
        0.0884, 0.0565, 0.0680, 0.1038, 0.1250, 0.0700, 0.0152],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,624][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.5661, 0.0979, 0.0994, 0.0216, 0.0340, 0.0470, 0.0130, 0.0553, 0.0024,
        0.0089, 0.0082, 0.0055, 0.0059, 0.0056, 0.0207, 0.0084],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,626][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.1341, 0.0831, 0.1091, 0.0357, 0.0616, 0.0796, 0.0459, 0.0836, 0.0194,
        0.0894, 0.0251, 0.0305, 0.0758, 0.0223, 0.0545, 0.0503],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,627][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.4764, 0.0578, 0.0303, 0.0591, 0.0307, 0.0567, 0.0186, 0.0217, 0.0216,
        0.0177, 0.0191, 0.0187, 0.0155, 0.0762, 0.0513, 0.0286],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,629][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0045, 0.0118, 0.0233, 0.0053, 0.1698, 0.0101, 0.0286, 0.0142, 0.0081,
        0.0125, 0.0607, 0.0293, 0.3605, 0.0403, 0.2199, 0.0011],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,631][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0021, 0.0564, 0.0963, 0.0425, 0.0652, 0.0480, 0.0563, 0.1137, 0.0392,
        0.1259, 0.0654, 0.0399, 0.0796, 0.0213, 0.0561, 0.0923],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,633][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0511, 0.0450, 0.0874, 0.0441, 0.0749, 0.0464, 0.0410, 0.0709, 0.0501,
        0.0803, 0.0499, 0.0500, 0.0977, 0.0526, 0.0885, 0.0700],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,635][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0291, 0.0496, 0.0684, 0.0622, 0.0570, 0.0288, 0.0369, 0.1045, 0.0782,
        0.0734, 0.0945, 0.0407, 0.0918, 0.0728, 0.0487, 0.0315, 0.0318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,636][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.2952, 0.0198, 0.0359, 0.0601, 0.0268, 0.0380, 0.0501, 0.0282, 0.0471,
        0.0324, 0.0641, 0.0811, 0.0396, 0.0294, 0.0290, 0.0462, 0.0771],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,638][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([1.1261e-14, 3.2658e-14, 3.6330e-12, 1.1117e-11, 4.0134e-10, 2.7635e-10,
        7.8979e-11, 3.7105e-09, 2.6395e-07, 2.6777e-06, 4.7320e-05, 3.2168e-05,
        1.0927e-04, 5.6669e-04, 2.9680e-02, 5.5928e-01, 4.1028e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,640][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0016, 0.0242, 0.0611, 0.0457, 0.0367, 0.0241, 0.0402, 0.1257, 0.0721,
        0.1238, 0.0471, 0.1163, 0.1160, 0.0198, 0.0381, 0.0631, 0.0445],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,641][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0377, 0.0562, 0.0221, 0.0955, 0.0737, 0.0358, 0.0502, 0.0926, 0.0411,
        0.0345, 0.0550, 0.0959, 0.0552, 0.0884, 0.0770, 0.0234, 0.0658],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,643][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0079, 0.0814, 0.0855, 0.0639, 0.0771, 0.1163, 0.0062, 0.0458, 0.0275,
        0.0750, 0.0318, 0.0335, 0.0637, 0.1216, 0.0911, 0.0644, 0.0073],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,645][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.4788, 0.1238, 0.0657, 0.0379, 0.0197, 0.0558, 0.0175, 0.0954, 0.0062,
        0.0122, 0.0169, 0.0074, 0.0095, 0.0071, 0.0125, 0.0241, 0.0097],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,647][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1088, 0.0455, 0.1046, 0.0252, 0.0686, 0.0600, 0.0372, 0.1242, 0.0184,
        0.0979, 0.0204, 0.0302, 0.1012, 0.0214, 0.0610, 0.0465, 0.0289],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,648][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.5214, 0.0570, 0.0268, 0.0576, 0.0264, 0.0513, 0.0150, 0.0189, 0.0186,
        0.0153, 0.0145, 0.0143, 0.0119, 0.0678, 0.0423, 0.0246, 0.0164],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,650][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0145, 0.0431, 0.1138, 0.0164, 0.1336, 0.0263, 0.0062, 0.0552, 0.0111,
        0.0464, 0.0318, 0.0146, 0.2072, 0.0735, 0.1635, 0.0335, 0.0094],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,652][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0041, 0.0413, 0.0803, 0.0393, 0.0610, 0.0432, 0.0483, 0.1473, 0.0491,
        0.0795, 0.0821, 0.0398, 0.0961, 0.0241, 0.0546, 0.0746, 0.0352],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,654][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0501, 0.0435, 0.0823, 0.0463, 0.0678, 0.0476, 0.0385, 0.0685, 0.0512,
        0.0724, 0.0494, 0.0458, 0.0918, 0.0504, 0.0787, 0.0708, 0.0450],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:16,656][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0296, 0.0550, 0.0496, 0.0559, 0.0650, 0.0333, 0.0325, 0.0787, 0.0647,
        0.0717, 0.0809, 0.0363, 0.1019, 0.0713, 0.0552, 0.0425, 0.0289, 0.0467],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,657][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.2192, 0.0217, 0.0358, 0.0624, 0.0255, 0.0391, 0.0528, 0.0286, 0.0516,
        0.0325, 0.0719, 0.0920, 0.0419, 0.0343, 0.0288, 0.0510, 0.0854, 0.0254],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,658][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([2.1876e-14, 2.8207e-14, 8.1672e-13, 4.4776e-12, 6.8976e-11, 5.9260e-11,
        1.6374e-10, 7.9553e-10, 1.3902e-08, 3.7176e-07, 1.7460e-06, 4.9918e-06,
        3.9831e-05, 3.8019e-04, 1.8156e-03, 6.0442e-02, 2.3370e-01, 7.0361e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,659][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0004, 0.0211, 0.1085, 0.0284, 0.0300, 0.0368, 0.1150, 0.1040, 0.0590,
        0.0829, 0.0259, 0.0659, 0.0479, 0.0077, 0.0274, 0.0834, 0.1132, 0.0426],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,660][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0262, 0.0570, 0.0111, 0.0716, 0.0488, 0.0448, 0.0548, 0.0389, 0.0494,
        0.0276, 0.0585, 0.1111, 0.0598, 0.0923, 0.0521, 0.0449, 0.0679, 0.0832],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,662][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0067, 0.0893, 0.1042, 0.0741, 0.0406, 0.0889, 0.0276, 0.0646, 0.0296,
        0.0608, 0.0286, 0.0384, 0.0441, 0.0733, 0.0448, 0.1294, 0.0351, 0.0196],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,664][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1667, 0.1095, 0.0704, 0.0343, 0.0326, 0.1136, 0.0548, 0.1040, 0.0248,
        0.0340, 0.0339, 0.0146, 0.0272, 0.0272, 0.0266, 0.0433, 0.0388, 0.0438],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,665][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0995, 0.1403, 0.0680, 0.0350, 0.0605, 0.0633, 0.0312, 0.0792, 0.0165,
        0.0887, 0.0222, 0.0192, 0.0691, 0.0244, 0.0499, 0.0432, 0.0248, 0.0648],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,667][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.5948, 0.0506, 0.0224, 0.0524, 0.0206, 0.0449, 0.0121, 0.0145, 0.0143,
        0.0109, 0.0106, 0.0109, 0.0079, 0.0561, 0.0329, 0.0188, 0.0134, 0.0117],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,669][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0120, 0.0115, 0.0519, 0.0074, 0.0898, 0.0199, 0.0102, 0.0400, 0.0084,
        0.0484, 0.0318, 0.0246, 0.4413, 0.0114, 0.1095, 0.0596, 0.0148, 0.0074],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,671][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0036, 0.0444, 0.0763, 0.0318, 0.0767, 0.0555, 0.0393, 0.0972, 0.0281,
        0.1061, 0.0533, 0.0392, 0.0793, 0.0222, 0.0705, 0.0917, 0.0287, 0.0560],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,672][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0445, 0.0379, 0.0805, 0.0370, 0.0653, 0.0422, 0.0308, 0.0602, 0.0392,
        0.0709, 0.0372, 0.0385, 0.0866, 0.0438, 0.0771, 0.0656, 0.0365, 0.1061],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:16,674][circuit_model.py][line:2294][INFO] ##2-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0307, 0.0481, 0.0486, 0.0515, 0.0588, 0.0279, 0.0364, 0.0840, 0.0583,
        0.0648, 0.0816, 0.0312, 0.0833, 0.0579, 0.0507, 0.0302, 0.0327, 0.0759,
        0.0476], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,676][circuit_model.py][line:2297][INFO] ##2-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.3504, 0.0141, 0.0308, 0.0516, 0.0209, 0.0283, 0.0398, 0.0215, 0.0362,
        0.0249, 0.0520, 0.0695, 0.0312, 0.0214, 0.0219, 0.0358, 0.0660, 0.0178,
        0.0658], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,677][circuit_model.py][line:2300][INFO] ##2-th layer ##Weight##: The head3 weight for token [ to] are: tensor([1.6690e-15, 3.7922e-16, 3.1315e-14, 7.9225e-14, 1.6378e-12, 3.6152e-12,
        7.4842e-12, 3.4601e-11, 7.4469e-10, 5.7346e-08, 5.3019e-08, 3.0122e-07,
        8.6992e-07, 2.7058e-06, 1.2145e-04, 5.4124e-03, 3.3479e-02, 5.7374e-01,
        3.8725e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,679][circuit_model.py][line:2303][INFO] ##2-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0009, 0.0180, 0.0516, 0.0306, 0.0226, 0.0240, 0.0862, 0.1259, 0.0815,
        0.0624, 0.0268, 0.0747, 0.0670, 0.0145, 0.0226, 0.0738, 0.1009, 0.1057,
        0.0100], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,681][circuit_model.py][line:2306][INFO] ##2-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0200, 0.0393, 0.0237, 0.0642, 0.0626, 0.0334, 0.0483, 0.0348, 0.0510,
        0.0313, 0.0502, 0.0621, 0.0408, 0.0699, 0.0690, 0.0414, 0.0601, 0.1443,
        0.0537], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,683][circuit_model.py][line:2309][INFO] ##2-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0041, 0.0441, 0.0709, 0.0632, 0.0421, 0.1224, 0.0156, 0.0742, 0.0394,
        0.0529, 0.0387, 0.0301, 0.0638, 0.0545, 0.0526, 0.1208, 0.0209, 0.0708,
        0.0190], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,684][circuit_model.py][line:2312][INFO] ##2-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.7036, 0.0709, 0.0193, 0.0489, 0.0061, 0.0363, 0.0259, 0.0253, 0.0033,
        0.0039, 0.0115, 0.0075, 0.0023, 0.0044, 0.0038, 0.0076, 0.0137, 0.0042,
        0.0014], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,686][circuit_model.py][line:2315][INFO] ##2-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1060, 0.0279, 0.0781, 0.0262, 0.0500, 0.0994, 0.0380, 0.0964, 0.0213,
        0.0618, 0.0264, 0.0339, 0.0706, 0.0257, 0.0474, 0.0635, 0.0294, 0.0749,
        0.0233], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,688][circuit_model.py][line:2318][INFO] ##2-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.4654, 0.0554, 0.0270, 0.0566, 0.0272, 0.0548, 0.0164, 0.0193, 0.0195,
        0.0156, 0.0154, 0.0155, 0.0123, 0.0707, 0.0447, 0.0272, 0.0185, 0.0189,
        0.0195], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,690][circuit_model.py][line:2321][INFO] ##2-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0049, 0.0151, 0.0606, 0.0050, 0.1447, 0.0101, 0.0254, 0.0451, 0.0036,
        0.0362, 0.0041, 0.0332, 0.2060, 0.0235, 0.1827, 0.0158, 0.0459, 0.1374,
        0.0007], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,692][circuit_model.py][line:2324][INFO] ##2-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0061, 0.0434, 0.0642, 0.0496, 0.0588, 0.0533, 0.0504, 0.0798, 0.0511,
        0.0596, 0.0877, 0.0496, 0.0497, 0.0264, 0.0564, 0.0831, 0.0412, 0.0498,
        0.0399], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,693][circuit_model.py][line:2327][INFO] ##2-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0414, 0.0380, 0.0686, 0.0401, 0.0598, 0.0414, 0.0341, 0.0558, 0.0449,
        0.0624, 0.0434, 0.0412, 0.0757, 0.0445, 0.0695, 0.0598, 0.0403, 0.0943,
        0.0448], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:16,740][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:16,741][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,742][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,743][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,743][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,744][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,745][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,745][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,746][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,747][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,747][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,748][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,749][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:16,749][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.4920, 0.5080], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,750][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.6573, 0.3427], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,751][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.3810, 0.6190], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,752][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.6780, 0.3220], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,753][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([1.0000e+00, 1.9747e-06], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,754][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.1239, 0.8761], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,754][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.5683, 0.4317], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,755][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.4790, 0.5210], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,756][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.9964, 0.0036], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,757][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([9.9916e-01, 8.3993e-04], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,757][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.5301, 0.4699], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,758][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.6825, 0.3175], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:16,759][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.1995, 0.2651, 0.5353], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,760][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.8428, 0.1386, 0.0186], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,762][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.1345, 0.3586, 0.5068], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,764][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.2502, 0.4331, 0.3167], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,765][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.9838, 0.0141, 0.0021], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,767][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.0464, 0.6927, 0.2609], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,769][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.3007, 0.2545, 0.4447], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,769][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.3301, 0.3844, 0.2856], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,770][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.9975, 0.0013, 0.0012], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,771][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.7727, 0.1464, 0.0810], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,771][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.2556, 0.2621, 0.4823], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,773][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.3634, 0.1795, 0.4571], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:16,774][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0270, 0.1937, 0.6123, 0.1670], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,776][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.5173, 0.2230, 0.0960, 0.1637], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,778][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0431, 0.0969, 0.6893, 0.1706], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,779][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.1687, 0.1485, 0.6217, 0.0611], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,780][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([9.9535e-01, 4.5114e-03, 3.0049e-05, 1.0550e-04], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,782][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0338, 0.3419, 0.4174, 0.2069], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,784][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2723, 0.1875, 0.3436, 0.1966], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,785][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.2789, 0.2947, 0.2557, 0.1708], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,786][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([9.9763e-01, 4.9865e-04, 6.1588e-04, 1.2566e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,788][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.9923, 0.0013, 0.0055, 0.0010], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,789][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.2444, 0.1920, 0.4761, 0.0876], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,791][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1975, 0.1697, 0.4598, 0.1730], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:16,793][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0347, 0.1998, 0.4771, 0.2401, 0.0483], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,794][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.7313, 0.1348, 0.0213, 0.0829, 0.0297], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,796][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0324, 0.0669, 0.5108, 0.2797, 0.1102], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,797][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.1198, 0.1464, 0.5636, 0.1045, 0.0656], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,799][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.9557, 0.0025, 0.0221, 0.0136, 0.0062], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,801][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.0166, 0.3134, 0.2687, 0.3226, 0.0786], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,802][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.1864, 0.1475, 0.2538, 0.1679, 0.2443], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,804][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.2254, 0.2684, 0.2054, 0.1569, 0.1438], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,805][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([9.9942e-01, 1.2546e-04, 2.1125e-04, 1.9249e-04, 4.7367e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,807][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.3215, 0.2031, 0.0871, 0.3349, 0.0534], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,808][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.1966, 0.1837, 0.3724, 0.0823, 0.1649], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,810][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.2030, 0.1184, 0.3468, 0.1669, 0.1649], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:16,811][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.0498, 0.0814, 0.4128, 0.1566, 0.2346, 0.0648], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,812][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.8101, 0.0929, 0.0126, 0.0555, 0.0183, 0.0105], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,812][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0269, 0.1174, 0.1776, 0.3454, 0.2340, 0.0987], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,813][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.2886, 0.1112, 0.2856, 0.0842, 0.2005, 0.0301], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,814][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([9.7147e-01, 4.4393e-04, 1.1230e-03, 8.6812e-03, 6.2546e-04, 1.7658e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,816][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.0234, 0.2291, 0.2725, 0.2077, 0.1683, 0.0989], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,818][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.1946, 0.1227, 0.2231, 0.1283, 0.1885, 0.1429], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,819][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1991, 0.2237, 0.1834, 0.1303, 0.1355, 0.1279], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,820][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([9.9974e-01, 2.9476e-05, 7.0429e-05, 7.2116e-05, 2.0062e-05, 6.8163e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,822][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.0966, 0.1270, 0.0592, 0.2572, 0.0552, 0.4047], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,824][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.1273, 0.1657, 0.3198, 0.0684, 0.1374, 0.1814], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,825][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.1959, 0.1355, 0.2835, 0.1181, 0.1897, 0.0772], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:16,827][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0212, 0.0846, 0.2729, 0.1502, 0.2106, 0.1672, 0.0932],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,829][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.8112, 0.0879, 0.0123, 0.0539, 0.0188, 0.0109, 0.0049],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,830][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0321, 0.0506, 0.1337, 0.2438, 0.1673, 0.3554, 0.0171],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,832][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1525, 0.1368, 0.2610, 0.1343, 0.1472, 0.0744, 0.0940],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,833][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([6.6605e-01, 4.0086e-04, 2.0967e-03, 9.7304e-03, 6.4314e-03, 3.1324e-01,
        2.0562e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,835][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0181, 0.2026, 0.1865, 0.1735, 0.1662, 0.2208, 0.0323],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,836][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1651, 0.1134, 0.1959, 0.1157, 0.1681, 0.1277, 0.1140],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,838][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1785, 0.1937, 0.1657, 0.1159, 0.1237, 0.1087, 0.1137],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,839][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([9.9924e-01, 3.4454e-05, 6.1516e-05, 1.5072e-04, 3.2181e-05, 1.3078e-04,
        3.4908e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,841][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.3407, 0.0776, 0.0518, 0.1236, 0.0374, 0.3263, 0.0426],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,843][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0885, 0.1319, 0.2511, 0.0517, 0.1034, 0.1436, 0.2299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,844][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1795, 0.1122, 0.2290, 0.1836, 0.0796, 0.1294, 0.0868],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:16,846][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.0104, 0.0694, 0.3144, 0.1081, 0.2098, 0.1340, 0.1007, 0.0532],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,848][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.7870, 0.0957, 0.0139, 0.0567, 0.0211, 0.0131, 0.0063, 0.0062],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,850][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.0118, 0.0408, 0.1704, 0.1358, 0.1639, 0.2155, 0.1256, 0.1362],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,851][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.0771, 0.0600, 0.3004, 0.0600, 0.1476, 0.1104, 0.2076, 0.0369],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,852][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.1144, 0.0012, 0.0065, 0.0100, 0.0047, 0.4012, 0.1441, 0.3180],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,853][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.0114, 0.1986, 0.1952, 0.1894, 0.1280, 0.1610, 0.0503, 0.0661],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,854][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.1327, 0.1016, 0.1747, 0.1093, 0.1551, 0.1204, 0.1081, 0.0981],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,855][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.1580, 0.1745, 0.1420, 0.1069, 0.1046, 0.1011, 0.1044, 0.1086],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,855][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([9.9758e-01, 2.0287e-04, 3.5287e-04, 3.3764e-04, 1.5230e-04, 3.6978e-04,
        5.5241e-04, 4.4834e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,857][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.4754, 0.0895, 0.0533, 0.1215, 0.0344, 0.1756, 0.0367, 0.0137],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,859][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.0896, 0.1134, 0.2204, 0.0472, 0.0944, 0.1228, 0.1911, 0.1212],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,860][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.1541, 0.0887, 0.2797, 0.0828, 0.1088, 0.0775, 0.0631, 0.1453],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:16,862][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0129, 0.0371, 0.2898, 0.0792, 0.3230, 0.0905, 0.0766, 0.0597, 0.0311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,864][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.6616, 0.1218, 0.0261, 0.0796, 0.0364, 0.0248, 0.0142, 0.0126, 0.0229],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,865][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0073, 0.0115, 0.1063, 0.0571, 0.0932, 0.1365, 0.0368, 0.5457, 0.0056],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,867][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1056, 0.0783, 0.2026, 0.0692, 0.1033, 0.0882, 0.2395, 0.0881, 0.0252],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,868][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([2.0656e-03, 4.0962e-10, 1.1112e-04, 4.3695e-08, 4.3691e-06, 3.5856e-07,
        3.0706e-07, 9.9782e-01, 3.3926e-07], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,870][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0193, 0.1685, 0.1512, 0.1652, 0.1593, 0.1513, 0.0487, 0.1015, 0.0350],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,872][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.1386, 0.0916, 0.1584, 0.0947, 0.1352, 0.1047, 0.0948, 0.0803, 0.1018],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,873][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1481, 0.1585, 0.1321, 0.0966, 0.0984, 0.0929, 0.0953, 0.0972, 0.0809],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,874][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([9.9903e-01, 4.5011e-05, 6.8987e-05, 1.1476e-04, 2.8718e-05, 1.1882e-04,
        2.5312e-04, 1.6003e-04, 1.7751e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,876][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.8020, 0.0182, 0.0232, 0.0218, 0.0112, 0.0996, 0.0124, 0.0073, 0.0042],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,878][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0894, 0.0900, 0.2154, 0.0371, 0.0889, 0.1136, 0.1812, 0.1092, 0.0751],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,880][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0981, 0.0811, 0.1153, 0.1469, 0.0554, 0.0884, 0.0853, 0.0864, 0.2432],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:16,881][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.0120, 0.0622, 0.2463, 0.1106, 0.1755, 0.0843, 0.1191, 0.1259, 0.0535,
        0.0106], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,883][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.7115, 0.1034, 0.0188, 0.0666, 0.0289, 0.0198, 0.0101, 0.0091, 0.0168,
        0.0151], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,885][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.0134, 0.0237, 0.0974, 0.0946, 0.0887, 0.2313, 0.0566, 0.2187, 0.1319,
        0.0438], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,886][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.0747, 0.0551, 0.1343, 0.0444, 0.1269, 0.0573, 0.1988, 0.2315, 0.0575,
        0.0194], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,888][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([1.3633e-03, 3.0779e-06, 1.0978e-03, 8.4736e-06, 8.9576e-04, 2.6218e-01,
        9.4849e-04, 7.3238e-01, 1.0545e-03, 6.5895e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,889][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.0130, 0.1695, 0.1494, 0.1327, 0.0866, 0.0923, 0.0488, 0.1235, 0.0713,
        0.1128], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,891][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0971, 0.0822, 0.1333, 0.0919, 0.1220, 0.1024, 0.0898, 0.0880, 0.0997,
        0.0934], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,893][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.1294, 0.1463, 0.1166, 0.0920, 0.0884, 0.0903, 0.0879, 0.0901, 0.0752,
        0.0839], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,894][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([9.9780e-01, 1.3472e-04, 2.3128e-04, 1.7991e-04, 7.7616e-05, 2.1522e-04,
        2.9979e-04, 3.6627e-04, 1.7886e-04, 5.1535e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,895][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.3157, 0.1263, 0.0629, 0.1561, 0.0476, 0.1750, 0.0406, 0.0205, 0.0230,
        0.0323], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,896][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.0764, 0.0902, 0.1815, 0.0373, 0.0776, 0.1007, 0.1557, 0.0992, 0.0690,
        0.1125], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,897][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.1109, 0.0744, 0.1884, 0.0853, 0.0935, 0.0693, 0.0568, 0.1258, 0.0983,
        0.0973], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:16,898][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0332, 0.0382, 0.2416, 0.0810, 0.2689, 0.0897, 0.0950, 0.0509, 0.0418,
        0.0357, 0.0239], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,899][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.7876, 0.0767, 0.0131, 0.0487, 0.0213, 0.0125, 0.0061, 0.0057, 0.0106,
        0.0097, 0.0081], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,901][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0112, 0.0407, 0.1227, 0.1041, 0.0813, 0.1736, 0.0606, 0.2351, 0.0660,
        0.0964, 0.0084], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,903][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0909, 0.0506, 0.2135, 0.0511, 0.0891, 0.0715, 0.1754, 0.1183, 0.0896,
        0.0362, 0.0137], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,904][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([1.8184e-01, 6.8906e-05, 1.1383e-04, 9.0134e-04, 9.2229e-04, 3.2452e-03,
        3.5117e-03, 7.1365e-01, 6.6208e-02, 1.4119e-02, 1.5418e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,905][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0153, 0.1386, 0.1044, 0.1309, 0.0966, 0.1216, 0.0494, 0.1288, 0.0530,
        0.1339, 0.0276], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,907][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1182, 0.0784, 0.1320, 0.0798, 0.1120, 0.0890, 0.0806, 0.0695, 0.0856,
        0.0703, 0.0846], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,909][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1284, 0.1293, 0.1163, 0.0787, 0.0882, 0.0783, 0.0802, 0.0869, 0.0647,
        0.0817, 0.0674], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,910][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([9.9880e-01, 4.2430e-05, 7.4362e-05, 7.7788e-05, 2.8689e-05, 9.6653e-05,
        1.7367e-04, 1.5921e-04, 1.1412e-04, 2.8499e-04, 1.4865e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,912][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.7103, 0.0335, 0.0282, 0.0399, 0.0157, 0.1145, 0.0175, 0.0102, 0.0068,
        0.0210, 0.0023], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,914][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.0531, 0.0799, 0.1604, 0.0297, 0.0628, 0.0895, 0.1476, 0.0865, 0.0598,
        0.1004, 0.1304], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,915][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0991, 0.0511, 0.1062, 0.0969, 0.0469, 0.0570, 0.0604, 0.1153, 0.1549,
        0.0665, 0.1459], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:16,917][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0277, 0.0347, 0.1805, 0.0602, 0.2462, 0.0628, 0.0645, 0.0547, 0.0438,
        0.0310, 0.0657, 0.1282], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,919][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.7805, 0.0827, 0.0124, 0.0526, 0.0202, 0.0114, 0.0054, 0.0052, 0.0098,
        0.0082, 0.0067, 0.0049], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,921][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0100, 0.0208, 0.0883, 0.1041, 0.0837, 0.1990, 0.0240, 0.2414, 0.0916,
        0.0899, 0.0425, 0.0048], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,922][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1125, 0.0702, 0.1317, 0.0656, 0.0682, 0.0477, 0.1565, 0.1226, 0.0578,
        0.0530, 0.0617, 0.0525], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,924][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.3805e-01, 4.0644e-05, 5.5764e-03, 1.8328e-03, 1.2588e-04, 2.3856e-02,
        2.5556e-03, 1.2305e-01, 2.1082e-02, 2.4861e-02, 5.4575e-01, 1.3225e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,925][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0126, 0.1209, 0.0993, 0.1278, 0.0887, 0.1290, 0.0373, 0.1199, 0.0407,
        0.1547, 0.0449, 0.0243], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,927][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1045, 0.0727, 0.1203, 0.0742, 0.1063, 0.0828, 0.0741, 0.0639, 0.0791,
        0.0648, 0.0778, 0.0794], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,929][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1160, 0.1220, 0.1066, 0.0778, 0.0801, 0.0740, 0.0767, 0.0795, 0.0641,
        0.0742, 0.0642, 0.0647], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,930][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([9.9873e-01, 1.4651e-05, 2.1162e-05, 5.3684e-05, 9.7774e-06, 5.2898e-05,
        1.5041e-04, 7.3184e-05, 1.1173e-04, 1.9195e-04, 1.6151e-04, 4.2840e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,932][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.6883, 0.0322, 0.0281, 0.0380, 0.0139, 0.1334, 0.0160, 0.0103, 0.0074,
        0.0236, 0.0028, 0.0060], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,933][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0480, 0.0706, 0.1376, 0.0258, 0.0527, 0.0755, 0.1239, 0.0742, 0.0500,
        0.0848, 0.1078, 0.1490], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,935][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0866, 0.0628, 0.0537, 0.1109, 0.0296, 0.0726, 0.0549, 0.0654, 0.1825,
        0.0405, 0.1866, 0.0540], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:16,936][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.0133, 0.0352, 0.0730, 0.0739, 0.1216, 0.0911, 0.0832, 0.0598, 0.0586,
        0.0366, 0.1126, 0.2155, 0.0256], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,937][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.7924, 0.0757, 0.0112, 0.0487, 0.0180, 0.0105, 0.0049, 0.0050, 0.0091,
        0.0076, 0.0063, 0.0044, 0.0061], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,938][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.0077, 0.0143, 0.1277, 0.0580, 0.0827, 0.0754, 0.1037, 0.2696, 0.0948,
        0.0879, 0.0294, 0.0299, 0.0190], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,939][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0355, 0.0424, 0.2144, 0.0500, 0.0971, 0.0479, 0.1059, 0.0903, 0.0669,
        0.0852, 0.0379, 0.0992, 0.0273], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,940][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([1.1036e-02, 2.0419e-05, 3.6405e-02, 2.2319e-04, 6.3900e-03, 7.4347e-02,
        9.5763e-03, 2.0692e-01, 1.1719e-01, 1.9121e-01, 1.3114e-01, 2.1049e-01,
        5.0497e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,942][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.0067, 0.1046, 0.1681, 0.1057, 0.0842, 0.1019, 0.0407, 0.0837, 0.0504,
        0.1192, 0.0491, 0.0401, 0.0456], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,944][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0811, 0.0667, 0.1033, 0.0718, 0.0956, 0.0807, 0.0695, 0.0661, 0.0750,
        0.0664, 0.0740, 0.0732, 0.0766], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,945][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.1083, 0.1101, 0.0989, 0.0747, 0.0723, 0.0725, 0.0723, 0.0774, 0.0612,
        0.0705, 0.0615, 0.0592, 0.0611], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,947][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([9.9906e-01, 3.3076e-05, 6.4741e-05, 4.7563e-05, 2.1594e-05, 5.0345e-05,
        8.9612e-05, 1.2455e-04, 6.0632e-05, 2.1177e-04, 1.0059e-04, 1.0786e-04,
        3.1346e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,948][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.4458, 0.0791, 0.0444, 0.1069, 0.0323, 0.1692, 0.0311, 0.0147, 0.0152,
        0.0349, 0.0054, 0.0115, 0.0095], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,950][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.0443, 0.0650, 0.1243, 0.0242, 0.0488, 0.0694, 0.1130, 0.0683, 0.0465,
        0.0775, 0.0984, 0.1351, 0.0853], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,952][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0899, 0.0467, 0.1504, 0.0598, 0.0607, 0.0697, 0.0521, 0.1055, 0.0796,
        0.0945, 0.0838, 0.0561, 0.0512], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:16,954][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.0394, 0.0263, 0.1226, 0.0675, 0.1822, 0.0812, 0.0657, 0.0382, 0.0624,
        0.0242, 0.0911, 0.1386, 0.0373, 0.0233], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,955][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.1031, 0.0592, 0.0508, 0.0510, 0.0638, 0.0617, 0.0649, 0.0450, 0.0620,
        0.0736, 0.0688, 0.0790, 0.0827, 0.1344], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,957][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0046, 0.0070, 0.1016, 0.0374, 0.0646, 0.1424, 0.0541, 0.3015, 0.0302,
        0.1153, 0.0262, 0.0144, 0.0954, 0.0053], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,959][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0863, 0.0667, 0.1818, 0.0556, 0.0781, 0.0895, 0.0872, 0.0668, 0.0600,
        0.0389, 0.0411, 0.0644, 0.0800, 0.0036], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,960][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([6.6392e-02, 1.4012e-07, 7.1284e-04, 1.0163e-05, 2.6089e-04, 5.0234e-03,
        3.7720e-03, 5.5903e-01, 6.0024e-03, 3.8435e-03, 2.7285e-02, 2.2688e-01,
        1.0037e-01, 4.1724e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,962][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.0124, 0.0918, 0.0914, 0.0943, 0.0796, 0.1061, 0.0374, 0.0982, 0.0439,
        0.1030, 0.0517, 0.0362, 0.1002, 0.0539], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,963][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0882, 0.0629, 0.1098, 0.0657, 0.0952, 0.0742, 0.0654, 0.0573, 0.0693,
        0.0612, 0.0672, 0.0677, 0.0680, 0.0479], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,965][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.1039, 0.1062, 0.0972, 0.0674, 0.0732, 0.0651, 0.0684, 0.0685, 0.0564,
        0.0689, 0.0578, 0.0577, 0.0599, 0.0494], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,967][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([8.8827e-01, 4.6591e-06, 5.7637e-06, 2.7469e-05, 3.3431e-06, 5.0149e-05,
        8.5947e-05, 3.4025e-05, 6.4173e-05, 1.1187e-04, 1.1209e-04, 3.7450e-04,
        1.1777e-05, 1.1085e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,968][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([9.9050e-01, 3.7698e-04, 1.3988e-03, 3.3029e-04, 2.9849e-04, 5.3219e-03,
        3.7798e-04, 3.1570e-04, 8.9957e-05, 7.8626e-04, 4.2485e-05, 8.2884e-05,
        7.2131e-05, 9.7148e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,970][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.0470, 0.0467, 0.1252, 0.0184, 0.0482, 0.0647, 0.1097, 0.0599, 0.0402,
        0.0730, 0.1017, 0.1490, 0.0907, 0.0257], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,971][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0611, 0.0293, 0.0420, 0.0791, 0.0284, 0.0442, 0.0743, 0.0440, 0.2337,
        0.0349, 0.1566, 0.0755, 0.0355, 0.0613], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:16,973][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0193, 0.0465, 0.1368, 0.0752, 0.0315, 0.1395, 0.0895, 0.0442, 0.0480,
        0.0213, 0.0611, 0.2114, 0.0315, 0.0316, 0.0127], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,975][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.4496, 0.0981, 0.0180, 0.0637, 0.0280, 0.0221, 0.0127, 0.0126, 0.0216,
        0.0198, 0.0183, 0.0138, 0.0166, 0.1696, 0.0355], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,977][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0052, 0.0118, 0.0909, 0.0552, 0.0194, 0.2473, 0.0576, 0.2218, 0.0587,
        0.0920, 0.0171, 0.0209, 0.0832, 0.0097, 0.0093], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,978][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0370, 0.0419, 0.1624, 0.0312, 0.0197, 0.0400, 0.1320, 0.1146, 0.0763,
        0.0645, 0.0610, 0.1393, 0.0606, 0.0055, 0.0139], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,979][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([1.7820e-02, 2.6967e-05, 4.4005e-04, 2.2852e-04, 1.0606e-04, 1.5935e-02,
        3.4864e-03, 5.0583e-02, 2.7114e-02, 1.0290e-01, 2.8822e-02, 5.0763e-02,
        6.3616e-01, 5.8051e-02, 7.5646e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,980][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.0052, 0.1006, 0.0922, 0.1059, 0.0247, 0.0731, 0.0402, 0.0980, 0.0462,
        0.1397, 0.0426, 0.0355, 0.1111, 0.0704, 0.0144], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,981][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0629, 0.0519, 0.0852, 0.0597, 0.0860, 0.0656, 0.0575, 0.0587, 0.0642,
        0.0671, 0.0629, 0.0649, 0.0727, 0.0542, 0.0865], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,982][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0952, 0.1093, 0.0871, 0.0672, 0.0622, 0.0683, 0.0648, 0.0604, 0.0557,
        0.0604, 0.0551, 0.0536, 0.0571, 0.0491, 0.0543], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,983][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([9.9908e-01, 1.9126e-05, 3.8997e-05, 2.4631e-05, 5.9046e-06, 2.7541e-05,
        3.8084e-05, 6.1926e-05, 2.6908e-05, 9.9330e-05, 4.1941e-05, 4.4995e-05,
        1.4974e-05, 4.6468e-04, 1.1867e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,985][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.5548, 0.0547, 0.0320, 0.0814, 0.0177, 0.1449, 0.0263, 0.0120, 0.0130,
        0.0288, 0.0045, 0.0105, 0.0082, 0.0043, 0.0070], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,986][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.0527, 0.0567, 0.1118, 0.0231, 0.0454, 0.0588, 0.0908, 0.0594, 0.0404,
        0.0660, 0.0828, 0.1156, 0.0754, 0.0300, 0.0910], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,988][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0672, 0.0457, 0.0952, 0.0594, 0.0505, 0.0693, 0.0500, 0.0792, 0.0706,
        0.0837, 0.0830, 0.0530, 0.0683, 0.0553, 0.0697], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:16,990][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0328, 0.0457, 0.1518, 0.0607, 0.1784, 0.0421, 0.0783, 0.0207, 0.0329,
        0.0190, 0.0465, 0.1567, 0.0209, 0.0245, 0.0885, 0.0006],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,992][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.5271, 0.0889, 0.0159, 0.0601, 0.0254, 0.0174, 0.0094, 0.0091, 0.0164,
        0.0148, 0.0123, 0.0091, 0.0119, 0.1421, 0.0280, 0.0120],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,993][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0083, 0.0303, 0.0663, 0.0993, 0.0739, 0.1006, 0.0332, 0.2067, 0.0674,
        0.0773, 0.0291, 0.0275, 0.0902, 0.0253, 0.0382, 0.0262],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,995][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0708, 0.0429, 0.1016, 0.0311, 0.0374, 0.0264, 0.0972, 0.1845, 0.0694,
        0.0737, 0.0378, 0.1182, 0.0649, 0.0081, 0.0287, 0.0069],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,996][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([4.2256e-02, 3.1874e-05, 2.0479e-04, 6.0656e-04, 1.1487e-03, 2.6625e-03,
        1.6625e-03, 6.6915e-03, 5.7107e-02, 4.9860e-03, 5.7708e-02, 5.1814e-02,
        1.7457e-02, 7.0873e-01, 4.5886e-02, 1.0501e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:16,998][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0069, 0.1016, 0.0742, 0.0878, 0.0617, 0.0503, 0.0393, 0.0742, 0.0456,
        0.1099, 0.0483, 0.0521, 0.1082, 0.0791, 0.0383, 0.0222],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,000][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0736, 0.0519, 0.0899, 0.0558, 0.0813, 0.0643, 0.0573, 0.0506, 0.0605,
        0.0546, 0.0595, 0.0610, 0.0615, 0.0437, 0.0787, 0.0558],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,002][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0930, 0.1004, 0.0860, 0.0618, 0.0618, 0.0612, 0.0613, 0.0598, 0.0507,
        0.0603, 0.0535, 0.0506, 0.0512, 0.0448, 0.0537, 0.0500],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,003][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([9.9685e-01, 5.0597e-05, 9.5366e-05, 6.5203e-05, 2.8532e-05, 9.9612e-05,
        1.3928e-04, 1.5431e-04, 8.1811e-05, 2.6689e-04, 1.3402e-04, 1.6701e-04,
        5.3832e-05, 1.6600e-03, 6.6173e-05, 8.4003e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,005][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.7199, 0.0329, 0.0315, 0.0359, 0.0159, 0.0876, 0.0129, 0.0091, 0.0062,
        0.0174, 0.0029, 0.0044, 0.0049, 0.0019, 0.0057, 0.0109],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,007][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0396, 0.0495, 0.0999, 0.0183, 0.0380, 0.0530, 0.0857, 0.0523, 0.0344,
        0.0595, 0.0754, 0.1054, 0.0672, 0.0243, 0.0788, 0.1188],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,008][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0662, 0.0511, 0.0846, 0.0495, 0.0534, 0.0301, 0.0477, 0.0825, 0.0707,
        0.0702, 0.0905, 0.0759, 0.0545, 0.0380, 0.0776, 0.0576],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,010][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0250, 0.0298, 0.1437, 0.0478, 0.1738, 0.0467, 0.0428, 0.0391, 0.0371,
        0.0259, 0.0650, 0.1230, 0.0346, 0.0256, 0.0883, 0.0179, 0.0337],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,012][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.5848, 0.0842, 0.0124, 0.0535, 0.0203, 0.0130, 0.0066, 0.0066, 0.0125,
        0.0108, 0.0089, 0.0064, 0.0083, 0.1332, 0.0204, 0.0084, 0.0097],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,014][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0125, 0.0202, 0.0509, 0.1101, 0.0724, 0.1292, 0.0065, 0.1956, 0.0743,
        0.0655, 0.0302, 0.0128, 0.0887, 0.0117, 0.0348, 0.0818, 0.0026],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,016][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0600, 0.0545, 0.0979, 0.0552, 0.0632, 0.0289, 0.0349, 0.0756, 0.0582,
        0.0684, 0.0481, 0.1386, 0.0874, 0.0093, 0.0474, 0.0469, 0.0255],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,017][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([8.4191e-02, 3.9784e-05, 3.5617e-04, 1.1871e-03, 6.4477e-04, 2.9086e-02,
        1.6893e-04, 2.2542e-01, 9.4361e-03, 1.1379e-03, 7.2039e-02, 3.9403e-02,
        3.0224e-02, 1.6525e-01, 3.7933e-02, 2.9031e-01, 1.3180e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,019][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0086, 0.0915, 0.0840, 0.0786, 0.0746, 0.0998, 0.0148, 0.0612, 0.0370,
        0.0983, 0.0376, 0.0350, 0.0844, 0.0779, 0.0507, 0.0557, 0.0101],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,021][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0706, 0.0513, 0.0869, 0.0534, 0.0785, 0.0605, 0.0545, 0.0476, 0.0572,
        0.0494, 0.0563, 0.0570, 0.0563, 0.0396, 0.0736, 0.0528, 0.0544],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,022][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0862, 0.0924, 0.0818, 0.0582, 0.0618, 0.0544, 0.0574, 0.0604, 0.0491,
        0.0574, 0.0491, 0.0495, 0.0521, 0.0431, 0.0542, 0.0454, 0.0476],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,023][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([9.8282e-01, 7.7401e-06, 1.0869e-05, 2.5184e-05, 4.5004e-06, 2.9657e-05,
        5.3475e-05, 3.5592e-05, 4.1416e-05, 8.9695e-05, 6.7423e-05, 1.6159e-04,
        1.1360e-05, 1.5865e-02, 2.4759e-05, 4.4896e-05, 7.1036e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,023][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([8.2576e-01, 1.4852e-02, 1.5940e-02, 1.6735e-02, 8.4513e-03, 6.9834e-02,
        6.6556e-03, 5.7983e-03, 3.1940e-03, 1.1720e-02, 1.3300e-03, 2.1165e-03,
        2.5400e-03, 7.3309e-04, 3.0420e-03, 1.0121e-02, 1.1748e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,025][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0388, 0.0470, 0.0882, 0.0178, 0.0343, 0.0469, 0.0741, 0.0467, 0.0311,
        0.0523, 0.0650, 0.0901, 0.0587, 0.0230, 0.0690, 0.1031, 0.1139],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,026][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0622, 0.0394, 0.0543, 0.0614, 0.0226, 0.0366, 0.0333, 0.0856, 0.1296,
        0.0357, 0.1230, 0.0522, 0.0454, 0.0464, 0.0365, 0.0860, 0.0499],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,028][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0039, 0.0228, 0.0555, 0.0372, 0.1637, 0.0623, 0.0509, 0.0363, 0.0482,
        0.0244, 0.0632, 0.1738, 0.0204, 0.0284, 0.0995, 0.0202, 0.0758, 0.0136],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,030][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.4989, 0.0871, 0.0133, 0.0538, 0.0213, 0.0153, 0.0079, 0.0081, 0.0142,
        0.0135, 0.0110, 0.0084, 0.0108, 0.1530, 0.0275, 0.0127, 0.0142, 0.0289],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,032][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0055, 0.0111, 0.0557, 0.0465, 0.0310, 0.1816, 0.0296, 0.1585, 0.0351,
        0.1039, 0.0131, 0.0151, 0.0938, 0.0099, 0.0150, 0.1571, 0.0151, 0.0226],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,033][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0194, 0.0315, 0.1731, 0.0262, 0.0448, 0.0486, 0.1322, 0.0661, 0.0387,
        0.0452, 0.0218, 0.0670, 0.0337, 0.0038, 0.0340, 0.0826, 0.1156, 0.0158],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,035][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([7.9133e-04, 1.1745e-05, 1.4932e-05, 1.0218e-04, 2.2898e-05, 7.8435e-03,
        1.1920e-04, 8.7235e-04, 8.1139e-03, 7.6961e-04, 1.6732e-02, 5.9601e-02,
        2.6567e-02, 4.5246e-02, 1.0379e-03, 8.1824e-01, 1.1091e-02, 2.8191e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,037][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.0067, 0.0991, 0.1005, 0.0850, 0.0450, 0.0788, 0.0360, 0.0780, 0.0400,
        0.0825, 0.0324, 0.0394, 0.0668, 0.0563, 0.0285, 0.0780, 0.0257, 0.0214],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,038][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0467, 0.0424, 0.0675, 0.0494, 0.0682, 0.0549, 0.0492, 0.0499, 0.0547,
        0.0548, 0.0543, 0.0547, 0.0616, 0.0449, 0.0686, 0.0551, 0.0543, 0.0690],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,040][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0797, 0.0954, 0.0689, 0.0607, 0.0531, 0.0563, 0.0561, 0.0549, 0.0494,
        0.0534, 0.0500, 0.0462, 0.0465, 0.0437, 0.0459, 0.0432, 0.0449, 0.0518],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,042][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([9.9914e-01, 1.8598e-05, 3.9773e-05, 2.1507e-05, 8.5636e-06, 2.5556e-05,
        3.4219e-05, 6.7024e-05, 2.1458e-05, 9.9564e-05, 3.8652e-05, 3.4140e-05,
        1.4284e-05, 2.7519e-04, 1.5140e-05, 2.8212e-05, 9.5179e-05, 2.7354e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,043][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.7378, 0.0292, 0.0220, 0.0355, 0.0131, 0.0889, 0.0134, 0.0069, 0.0058,
        0.0152, 0.0022, 0.0042, 0.0038, 0.0014, 0.0038, 0.0120, 0.0023, 0.0023],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,045][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0424, 0.0431, 0.0837, 0.0177, 0.0348, 0.0440, 0.0670, 0.0435, 0.0300,
        0.0481, 0.0612, 0.0857, 0.0560, 0.0224, 0.0668, 0.0945, 0.1063, 0.0528],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,047][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0620, 0.0458, 0.1087, 0.0329, 0.0515, 0.0388, 0.0238, 0.0855, 0.0335,
        0.0968, 0.0329, 0.0372, 0.0621, 0.0285, 0.0712, 0.0936, 0.0293, 0.0657],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,049][circuit_model.py][line:2332][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0092, 0.0319, 0.0444, 0.0588, 0.0518, 0.0881, 0.0561, 0.0317, 0.0436,
        0.0171, 0.0632, 0.1545, 0.0345, 0.0281, 0.0281, 0.0078, 0.0497, 0.1522,
        0.0491], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,051][circuit_model.py][line:2335][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.5508, 0.0889, 0.0139, 0.0528, 0.0215, 0.0131, 0.0069, 0.0071, 0.0124,
        0.0118, 0.0092, 0.0067, 0.0091, 0.1223, 0.0217, 0.0088, 0.0104, 0.0210,
        0.0116], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,052][circuit_model.py][line:2338][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0048, 0.0109, 0.0507, 0.0472, 0.0420, 0.1066, 0.0287, 0.2070, 0.0304,
        0.1178, 0.0133, 0.0128, 0.0935, 0.0066, 0.0224, 0.0913, 0.0176, 0.0948,
        0.0015], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,054][circuit_model.py][line:2341][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0579, 0.0396, 0.0914, 0.0339, 0.0464, 0.0278, 0.1192, 0.0920, 0.0660,
        0.0255, 0.0231, 0.0827, 0.0423, 0.0066, 0.0319, 0.0721, 0.0952, 0.0450,
        0.0015], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,056][circuit_model.py][line:2344][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([2.8771e-02, 8.9511e-09, 1.9876e-05, 2.2948e-07, 3.0191e-06, 1.6480e-03,
        1.9912e-06, 2.9694e-05, 3.8537e-04, 5.9865e-05, 6.7886e-04, 7.5810e-05,
        1.4789e-04, 4.4934e-04, 3.2853e-04, 9.3590e-01, 5.4485e-04, 2.9239e-02,
        1.7117e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,057][circuit_model.py][line:2347][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0073, 0.0658, 0.0732, 0.0771, 0.0500, 0.0972, 0.0246, 0.0837, 0.0434,
        0.0764, 0.0394, 0.0307, 0.0798, 0.0495, 0.0356, 0.0740, 0.0187, 0.0556,
        0.0181], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,059][circuit_model.py][line:2350][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0646, 0.0463, 0.0769, 0.0477, 0.0680, 0.0541, 0.0488, 0.0425, 0.0523,
        0.0444, 0.0519, 0.0527, 0.0506, 0.0347, 0.0623, 0.0459, 0.0480, 0.0614,
        0.0468], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,061][circuit_model.py][line:2353][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0793, 0.0834, 0.0734, 0.0528, 0.0550, 0.0520, 0.0521, 0.0529, 0.0446,
        0.0510, 0.0456, 0.0447, 0.0462, 0.0387, 0.0480, 0.0434, 0.0434, 0.0566,
        0.0369], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,062][circuit_model.py][line:2356][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([9.6924e-01, 9.1885e-06, 1.1040e-05, 2.6067e-05, 3.8411e-06, 3.4044e-05,
        6.5506e-05, 3.8276e-05, 4.4079e-05, 9.3757e-05, 7.5518e-05, 1.8987e-04,
        1.0587e-05, 2.6786e-02, 2.3810e-05, 5.6818e-05, 1.1843e-03, 5.3974e-05,
        2.0535e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,064][circuit_model.py][line:2359][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([9.5307e-01, 3.4173e-03, 5.4555e-03, 3.1370e-03, 1.9298e-03, 2.0151e-02,
        1.8859e-03, 1.5990e-03, 6.3505e-04, 3.2547e-03, 2.9467e-04, 4.8746e-04,
        4.9545e-04, 1.1648e-04, 6.4332e-04, 2.7203e-03, 2.6550e-04, 3.2916e-04,
        1.1061e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,066][circuit_model.py][line:2362][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0417, 0.0375, 0.0800, 0.0140, 0.0298, 0.0392, 0.0617, 0.0378, 0.0247,
        0.0425, 0.0560, 0.0823, 0.0510, 0.0179, 0.0606, 0.0911, 0.1063, 0.0477,
        0.0784], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,067][circuit_model.py][line:2365][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0458, 0.0316, 0.0343, 0.0508, 0.0180, 0.0333, 0.0402, 0.0478, 0.1312,
        0.0223, 0.1092, 0.0647, 0.0211, 0.0450, 0.0282, 0.0560, 0.0630, 0.0287,
        0.1287], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,071][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:17,073][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 7834],
        [ 2369],
        [ 2500],
        [ 3964],
        [ 2931],
        [ 4739],
        [ 7654],
        [ 6664],
        [ 6730],
        [ 7807],
        [ 3288],
        [19092],
        [22779],
        [ 5255],
        [ 8612],
        [ 6989],
        [10901],
        [12954],
        [ 8212]], device='cuda:0')
[2024-07-24 10:25:17,074][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 6283],
        [ 3269],
        [ 1897],
        [ 4993],
        [ 1015],
        [ 4720],
        [ 9919],
        [ 2243],
        [ 7810],
        [16114],
        [ 6124],
        [18183],
        [14313],
        [ 2345],
        [ 2588],
        [ 1892],
        [ 8655],
        [ 3481],
        [ 3362]], device='cuda:0')
[2024-07-24 10:25:17,076][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 7619],
        [11092],
        [ 7830],
        [ 8209],
        [ 9151],
        [ 9706],
        [10900],
        [ 7540],
        [ 7151],
        [ 6230],
        [ 6342],
        [ 6570],
        [ 6102],
        [ 6240],
        [ 6303],
        [ 6705],
        [ 6902],
        [ 7108],
        [ 7196]], device='cuda:0')
[2024-07-24 10:25:17,078][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[18396],
        [18616],
        [20359],
        [19242],
        [19469],
        [18724],
        [18269],
        [18063],
        [18121],
        [17797],
        [17279],
        [17113],
        [16891],
        [17436],
        [16770],
        [17061],
        [16912],
        [16597],
        [16609]], device='cuda:0')
[2024-07-24 10:25:17,080][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[39298],
        [41329],
        [24672],
        [25511],
        [10373],
        [10156],
        [22673],
        [23932],
        [10075],
        [18316],
        [11789],
        [12345],
        [25947],
        [33054],
        [ 9162],
        [19090],
        [33448],
        [24758],
        [33406]], device='cuda:0')
[2024-07-24 10:25:17,081][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[30689],
        [23803],
        [30610],
        [31798],
        [29742],
        [29135],
        [29740],
        [32042],
        [29727],
        [28748],
        [30299],
        [27758],
        [26933],
        [24882],
        [24544],
        [24610],
        [24407],
        [30344],
        [29734]], device='cuda:0')
[2024-07-24 10:25:17,083][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[33457],
        [38441],
        [36795],
        [39367],
        [37742],
        [39413],
        [38032],
        [39381],
        [37557],
        [39620],
        [39041],
        [37868],
        [38709],
        [38441],
        [39220],
        [39338],
        [39081],
        [38569],
        [37232]], device='cuda:0')
[2024-07-24 10:25:17,085][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[41835],
        [36823],
        [37984],
        [38864],
        [35952],
        [35111],
        [36821],
        [38534],
        [37086],
        [42092],
        [43383],
        [43821],
        [41336],
        [41939],
        [42370],
        [37810],
        [37282],
        [40841],
        [41162]], device='cuda:0')
[2024-07-24 10:25:17,087][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[23957],
        [25721],
        [37093],
        [35657],
        [40100],
        [38549],
        [35204],
        [40340],
        [41545],
        [46027],
        [37657],
        [42896],
        [42867],
        [43836],
        [44344],
        [40101],
        [43513],
        [46534],
        [32459]], device='cuda:0')
[2024-07-24 10:25:17,089][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[12693],
        [17382],
        [42065],
        [42656],
        [38126],
        [36360],
        [37842],
        [38684],
        [36779],
        [37453],
        [38775],
        [37038],
        [37553],
        [38848],
        [36392],
        [36950],
        [38004],
        [33687],
        [32533]], device='cuda:0')
[2024-07-24 10:25:17,091][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[4077],
        [3789],
        [4103],
        [4259],
        [4305],
        [4227],
        [4309],
        [4435],
        [4467],
        [4351],
        [4232],
        [4288],
        [4208],
        [4685],
        [4191],
        [4181],
        [4139],
        [4103],
        [4232]], device='cuda:0')
[2024-07-24 10:25:17,093][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[38084],
        [35803],
        [30889],
        [ 7667],
        [ 9984],
        [12822],
        [15899],
        [29742],
        [17786],
        [18664],
        [13751],
        [20804],
        [11524],
        [23190],
        [17247],
        [22124],
        [21284],
        [23729],
        [20210]], device='cuda:0')
[2024-07-24 10:25:17,095][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[32705],
        [33532],
        [26726],
        [27181],
        [27429],
        [26496],
        [27646],
        [30834],
        [31812],
        [29055],
        [30666],
        [31364],
        [30767],
        [32478],
        [30743],
        [30885],
        [32386],
        [30481],
        [32025]], device='cuda:0')
[2024-07-24 10:25:17,097][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[15104],
        [12407],
        [19812],
        [19690],
        [23239],
        [22071],
        [21518],
        [20363],
        [20604],
        [21200],
        [21442],
        [21554],
        [23417],
        [22863],
        [23636],
        [23464],
        [23191],
        [23334],
        [23107]], device='cuda:0')
[2024-07-24 10:25:17,099][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 8583],
        [ 3430],
        [ 4009],
        [ 2565],
        [ 5524],
        [ 7043],
        [ 3593],
        [11859],
        [ 5463],
        [ 1944],
        [ 2688],
        [ 3851],
        [12857],
        [ 4925],
        [ 5665],
        [ 2426],
        [ 3213],
        [14407],
        [ 4766]], device='cuda:0')
[2024-07-24 10:25:17,101][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[5680],
        [3630],
        [5864],
        [6783],
        [4783],
        [7557],
        [6809],
        [8930],
        [9996],
        [9504],
        [9718],
        [8360],
        [6530],
        [7557],
        [7198],
        [7963],
        [8988],
        [7827],
        [5444]], device='cuda:0')
[2024-07-24 10:25:17,103][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 644],
        [ 549],
        [ 552],
        [ 393],
        [ 452],
        [ 500],
        [ 505],
        [ 504],
        [ 482],
        [ 497],
        [ 515],
        [ 510],
        [ 521],
        [3519],
        [1076],
        [ 928],
        [ 809],
        [1194],
        [1007]], device='cuda:0')
[2024-07-24 10:25:17,105][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[10977],
        [12993],
        [16401],
        [17367],
        [17965],
        [16700],
        [16900],
        [11875],
        [ 9012],
        [11959],
        [11116],
        [12477],
        [ 8484],
        [ 9899],
        [12523],
        [10383],
        [11493],
        [12952],
        [10148]], device='cuda:0')
[2024-07-24 10:25:17,107][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 735],
        [1584],
        [3199],
        [2422],
        [2666],
        [2053],
        [2310],
        [1586],
        [1730],
        [3094],
        [2562],
        [2344],
        [2648],
        [2981],
        [2747],
        [3127],
        [2760],
        [1480],
        [1733]], device='cuda:0')
[2024-07-24 10:25:17,109][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[22688],
        [22688],
        [23126],
        [22860],
        [22908],
        [23454],
        [32610],
        [ 2290],
        [ 1133],
        [  925],
        [ 1576],
        [13361],
        [ 3693],
        [  864],
        [16698],
        [17057],
        [ 6739],
        [ 3579],
        [ 3776]], device='cuda:0')
[2024-07-24 10:25:17,110][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[16537],
        [19153],
        [17940],
        [16524],
        [17399],
        [17761],
        [18560],
        [17671],
        [17540],
        [17373],
        [17511],
        [17533],
        [17094],
        [17141],
        [17063],
        [17259],
        [17481],
        [17214],
        [17468]], device='cuda:0')
[2024-07-24 10:25:17,112][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 223],
        [ 206],
        [ 216],
        [ 302],
        [ 394],
        [ 485],
        [ 581],
        [ 722],
        [ 767],
        [ 988],
        [ 907],
        [ 950],
        [1135],
        [1128],
        [1200],
        [1243],
        [1290],
        [1450],
        [1275]], device='cuda:0')
[2024-07-24 10:25:17,114][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[13329],
        [12418],
        [13927],
        [14358],
        [15075],
        [15049],
        [15101],
        [14617],
        [14601],
        [14551],
        [14554],
        [14674],
        [14639],
        [14673],
        [14902],
        [14789],
        [14859],
        [14966],
        [15044]], device='cuda:0')
[2024-07-24 10:25:17,116][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[13288],
        [13292],
        [13291],
        [13289],
        [13288],
        [13288],
        [13286],
        [13285],
        [13286],
        [13286],
        [13286],
        [13286],
        [13287],
        [13199],
        [13287],
        [13282],
        [13274],
        [13287],
        [13253]], device='cuda:0')
[2024-07-24 10:25:17,117][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[7729],
        [7725],
        [6677],
        [7734],
        [3674],
        [7167],
        [7842],
        [8138],
        [7991],
        [6553],
        [7957],
        [8524],
        [7462],
        [7770],
        [9730],
        [7617],
        [7894],
        [7476],
        [7937]], device='cuda:0')
[2024-07-24 10:25:17,119][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 8168],
        [10489],
        [ 9696],
        [ 9803],
        [ 9855],
        [ 9741],
        [ 9694],
        [ 9383],
        [ 9626],
        [ 9803],
        [ 9633],
        [ 9611],
        [ 9470],
        [ 9500],
        [ 9534],
        [ 9647],
        [ 9643],
        [ 9570],
        [ 9823]], device='cuda:0')
[2024-07-24 10:25:17,121][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 6719],
        [ 9923],
        [12792],
        [10566],
        [ 9369],
        [ 9697],
        [ 9736],
        [16592],
        [14135],
        [18157],
        [17498],
        [16287],
        [18561],
        [16246],
        [16302],
        [16625],
        [16404],
        [17684],
        [16433]], device='cuda:0')
[2024-07-24 10:25:17,123][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[37068],
        [37474],
        [37870],
        [37767],
        [39170],
        [37390],
        [35620],
        [39938],
        [38907],
        [40241],
        [39330],
        [34210],
        [39906],
        [35840],
        [34862],
        [34605],
        [35619],
        [36500],
        [36255]], device='cuda:0')
[2024-07-24 10:25:17,125][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[41158],
        [47402],
        [46428],
        [47172],
        [47566],
        [43608],
        [45991],
        [47584],
        [47281],
        [46773],
        [44099],
        [45264],
        [47076],
        [44242],
        [48198],
        [48598],
        [46145],
        [48126],
        [47763]], device='cuda:0')
[2024-07-24 10:25:17,126][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355],
        [38355]], device='cuda:0')
[2024-07-24 10:25:17,158][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:17,159][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,160][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,160][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,161][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,162][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,162][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,163][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,164][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,165][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,166][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,167][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,167][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,168][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.7168, 0.2832], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,170][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.5306, 0.4694], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,172][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0187, 0.9813], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,173][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0892, 0.9108], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,175][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.9817, 0.0183], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,176][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.5712, 0.4288], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,178][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0797, 0.9203], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,179][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0260, 0.9740], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,180][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [,] are: tensor([1.4687e-08, 1.0000e+00], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,180][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.8002, 0.1998], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,181][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.5626, 0.4374], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,183][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.9915, 0.0085], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,184][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.3048, 0.4654, 0.2298], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,186][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.3077, 0.3488, 0.3435], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,187][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([4.1219e-05, 9.8243e-02, 9.0172e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,188][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.0411, 0.5418, 0.4170], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,190][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.7197, 0.0050, 0.2753], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,192][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.4134, 0.3145, 0.2721], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,193][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.0368, 0.4829, 0.4803], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,194][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([2.2430e-04, 9.9133e-01, 8.4494e-03], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,195][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([1.1554e-08, 8.8984e-02, 9.1102e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,197][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.1054, 0.8793, 0.0153], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,199][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.4144, 0.3114, 0.2742], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,200][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.8854, 0.0407, 0.0740], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,202][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.2007, 0.0728, 0.7072, 0.0193], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,203][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0258, 0.3435, 0.5672, 0.0635], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,205][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0091, 0.0272, 0.8010, 0.1626], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,206][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ and] are: tensor([1.4156e-04, 2.2264e-01, 7.7559e-01, 1.6195e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,208][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9876, 0.0022, 0.0091, 0.0011], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,209][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.2876, 0.2372, 0.2284, 0.2467], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,211][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0224, 0.3419, 0.4811, 0.1546], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,213][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0200, 0.7341, 0.0504, 0.1956], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,214][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ and] are: tensor([1.0253e-11, 2.2937e-02, 9.7258e-01, 4.4869e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,215][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.5937, 0.2480, 0.0242, 0.1341], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,217][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.3179, 0.2425, 0.2179, 0.2217], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,218][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.3833, 0.1996, 0.3963, 0.0208], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,220][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.1440, 0.3569, 0.0893, 0.3171, 0.0927], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,220][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0318, 0.4609, 0.2410, 0.1561, 0.1102], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,221][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([9.0562e-05, 6.4340e-03, 3.1035e-01, 1.3682e-02, 6.6944e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,222][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0048, 0.1586, 0.7888, 0.0022, 0.0456], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,223][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([6.6371e-01, 8.1195e-04, 2.6313e-01, 5.2716e-04, 7.1824e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,224][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.2367, 0.1711, 0.1466, 0.2524, 0.1932], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,226][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0129, 0.2194, 0.2778, 0.1463, 0.3436], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,227][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0165, 0.7838, 0.0136, 0.1196, 0.0665], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,229][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([1.3147e-10, 2.9746e-02, 9.6170e-01, 3.5232e-03, 5.0268e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,230][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0634, 0.5356, 0.0983, 0.2494, 0.0533], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,231][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.2606, 0.1921, 0.1702, 0.1779, 0.1992], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,233][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.7490, 0.0710, 0.1361, 0.0062, 0.0378], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,235][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.1533, 0.3902, 0.0431, 0.3595, 0.0276, 0.0264], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,236][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0136, 0.0536, 0.4842, 0.1330, 0.3015, 0.0142], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,237][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ had] are: tensor([6.2210e-04, 6.0343e-04, 6.4933e-02, 2.3735e-03, 2.2987e-01, 7.0160e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,239][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.0007, 0.2490, 0.5926, 0.0023, 0.1389, 0.0165], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,241][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.9019, 0.0035, 0.0496, 0.0019, 0.0151, 0.0279], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,242][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.1768, 0.1371, 0.1234, 0.1759, 0.1652, 0.2217], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,244][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0118, 0.1732, 0.3136, 0.0873, 0.3605, 0.0536], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,245][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ had] are: tensor([5.8709e-08, 9.7538e-01, 2.0769e-02, 3.7818e-03, 7.3974e-05, 3.4948e-08],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,246][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ had] are: tensor([7.2843e-08, 3.2713e-03, 8.7102e-01, 1.4284e-03, 4.1206e-02, 8.3077e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,248][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.4963, 0.1523, 0.0115, 0.0412, 0.0051, 0.2936], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,250][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.2179, 0.1617, 0.1419, 0.1511, 0.1687, 0.1587], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,251][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.3565, 0.2024, 0.3098, 0.0152, 0.0995, 0.0166], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,253][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2142, 0.1187, 0.2624, 0.1030, 0.1498, 0.0540, 0.0979],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,255][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0066, 0.1368, 0.6113, 0.0843, 0.1453, 0.0067, 0.0090],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,256][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([6.3370e-04, 4.6026e-05, 2.4666e-03, 7.1791e-04, 2.9495e-02, 3.9462e-01,
        5.7202e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,257][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0093, 0.1587, 0.6131, 0.0015, 0.0522, 0.0334, 0.1319],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,259][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.8592, 0.0083, 0.0474, 0.0040, 0.0168, 0.0424, 0.0220],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,261][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1396, 0.1174, 0.1027, 0.1451, 0.1486, 0.1972, 0.1493],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,262][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0154, 0.2205, 0.1962, 0.1313, 0.2834, 0.0801, 0.0731],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,262][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([1.0294e-08, 9.8903e-01, 8.9093e-03, 2.0334e-03, 2.5034e-05, 3.0896e-08,
        5.8787e-10], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,263][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([3.6036e-08, 2.4943e-04, 3.6587e-02, 9.4567e-05, 2.0279e-03, 2.2171e-02,
        9.3887e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,264][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1115, 0.0532, 0.0040, 0.0393, 0.0028, 0.7634, 0.0258],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,266][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1874, 0.1397, 0.1235, 0.1303, 0.1461, 0.1384, 0.1346],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,267][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.2451, 0.2292, 0.3792, 0.0159, 0.1078, 0.0173, 0.0054],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,269][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.1174, 0.2733, 0.0569, 0.2752, 0.0517, 0.0458, 0.0895, 0.0903],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,270][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0216, 0.1175, 0.4799, 0.1582, 0.1389, 0.0330, 0.0447, 0.0061],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,272][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([1.3369e-05, 1.8552e-04, 1.3954e-02, 8.3785e-04, 2.1782e-02, 7.6404e-02,
        4.1109e-01, 4.7574e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,273][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([1.3779e-02, 2.3698e-04, 5.8533e-04, 1.6435e-06, 1.7310e-04, 1.8976e-05,
        5.4128e-04, 9.8466e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,275][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.6840, 0.0041, 0.0997, 0.0028, 0.0430, 0.0963, 0.0282, 0.0419],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,276][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.1371, 0.1001, 0.0815, 0.1260, 0.1150, 0.1599, 0.1320, 0.1484],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,278][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0130, 0.1507, 0.1945, 0.1086, 0.2171, 0.0664, 0.0893, 0.1603],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,279][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([3.5396e-09, 9.7098e-01, 9.9743e-03, 1.8208e-02, 8.3931e-04, 7.1531e-08,
        2.1569e-09, 5.5796e-08], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,280][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([1.4390e-07, 2.5974e-06, 3.4284e-04, 6.5530e-07, 1.3395e-05, 4.1389e-04,
        1.3123e-02, 9.8610e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,282][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0302, 0.1501, 0.0244, 0.0808, 0.1046, 0.0924, 0.4843, 0.0331],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,283][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.1610, 0.1198, 0.1078, 0.1119, 0.1242, 0.1178, 0.1151, 0.1424],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,285][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.5843, 0.1214, 0.1790, 0.0118, 0.0624, 0.0120, 0.0037, 0.0254],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,287][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.3394, 0.1110, 0.2226, 0.0509, 0.0695, 0.0247, 0.0795, 0.0778, 0.0246],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,288][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0015, 0.2493, 0.3652, 0.1743, 0.1858, 0.0080, 0.0136, 0.0013, 0.0011],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,289][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ of] are: tensor([2.6771e-04, 2.1937e-05, 2.2396e-03, 5.2293e-04, 1.5444e-02, 1.4694e-01,
        4.5039e-01, 3.3240e-01, 5.1777e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,291][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ of] are: tensor([1.0075e-05, 3.4777e-06, 2.6082e-05, 6.7496e-08, 3.7838e-06, 8.2400e-07,
        1.0395e-05, 9.9968e-01, 2.6304e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,292][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ of] are: tensor([9.7802e-01, 1.6066e-03, 1.2627e-02, 5.6772e-04, 1.1062e-03, 1.9017e-03,
        1.6501e-03, 2.3242e-03, 1.9577e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,293][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0955, 0.0880, 0.0858, 0.1012, 0.1292, 0.1434, 0.1059, 0.1512, 0.0999],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,295][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0098, 0.1573, 0.1432, 0.0897, 0.2169, 0.0473, 0.0818, 0.1863, 0.0677],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,296][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ of] are: tensor([7.0079e-06, 9.5315e-01, 1.6620e-02, 2.8643e-02, 1.5723e-03, 4.1133e-06,
        1.1785e-07, 4.8348e-06, 6.5227e-07], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,297][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ of] are: tensor([9.4627e-12, 2.1533e-06, 6.0823e-04, 8.7887e-07, 2.8729e-05, 1.2775e-04,
        6.9503e-03, 9.9226e-01, 2.5799e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,299][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0393, 0.1280, 0.0893, 0.0843, 0.0320, 0.1134, 0.3202, 0.0904, 0.1031],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,301][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.1410, 0.1081, 0.0962, 0.0996, 0.1077, 0.1032, 0.1019, 0.1235, 0.1187],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,302][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.2185, 0.2329, 0.3504, 0.0191, 0.1034, 0.0204, 0.0071, 0.0432, 0.0050],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,303][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.2440, 0.1364, 0.0416, 0.2020, 0.0415, 0.0498, 0.0795, 0.0879, 0.0682,
        0.0490], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,304][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0013, 0.3287, 0.2574, 0.2283, 0.1589, 0.0063, 0.0123, 0.0018, 0.0031,
        0.0020], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,305][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([1.5405e-04, 1.2250e-05, 5.1108e-04, 2.4623e-04, 4.0366e-03, 1.2388e-01,
        4.6138e-01, 1.9318e-01, 6.1014e-02, 1.5558e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,306][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([2.8092e-05, 2.1544e-05, 1.0071e-04, 4.2830e-07, 8.4432e-06, 9.5572e-06,
        4.4702e-05, 9.9503e-01, 3.8050e-03, 9.4752e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,307][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.4732, 0.0188, 0.0948, 0.0111, 0.0511, 0.1368, 0.0678, 0.0841, 0.0119,
        0.0504], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,308][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0960, 0.0674, 0.0524, 0.0961, 0.0740, 0.1190, 0.1035, 0.1069, 0.1370,
        0.1477], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,310][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0117, 0.1208, 0.1558, 0.0784, 0.2222, 0.0600, 0.0724, 0.1654, 0.0741,
        0.0392], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,311][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([5.0738e-07, 9.8400e-01, 8.5046e-03, 7.1987e-03, 2.9335e-04, 1.3418e-07,
        3.0871e-09, 1.9331e-07, 2.9033e-08, 8.5863e-09], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,312][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([5.1493e-10, 3.7222e-06, 1.3208e-03, 5.5602e-07, 2.1314e-05, 6.4355e-05,
        3.7054e-03, 9.9332e-01, 1.2767e-05, 1.5506e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,314][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0113, 0.0842, 0.0020, 0.0356, 0.0159, 0.2042, 0.3688, 0.0558, 0.2067,
        0.0155], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,315][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.1273, 0.0949, 0.0846, 0.0889, 0.0983, 0.0929, 0.0906, 0.1129, 0.1075,
        0.1021], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,317][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.3507, 0.2110, 0.2720, 0.0130, 0.0906, 0.0148, 0.0041, 0.0311, 0.0028,
        0.0100], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,319][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0418, 0.0107, 0.2899, 0.0092, 0.1617, 0.0450, 0.1172, 0.1685, 0.0130,
        0.0605, 0.0824], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,321][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0011, 0.0971, 0.3924, 0.2498, 0.2213, 0.0073, 0.0225, 0.0006, 0.0037,
        0.0037, 0.0004], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,322][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ at] are: tensor([2.0803e-07, 7.1126e-05, 1.8738e-02, 3.1518e-04, 1.6890e-02, 5.8789e-02,
        1.8667e-01, 5.3840e-01, 2.3643e-02, 1.5531e-01, 1.1723e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,323][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ at] are: tensor([3.3391e-03, 1.1375e-05, 8.5860e-05, 1.8408e-07, 3.5370e-06, 1.2506e-06,
        2.5321e-05, 3.0971e-01, 4.9110e-03, 1.5057e-03, 6.8041e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,325][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.7865, 0.0060, 0.0474, 0.0022, 0.0110, 0.0329, 0.0194, 0.0283, 0.0025,
        0.0401, 0.0235], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,326][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0731, 0.0577, 0.0541, 0.0731, 0.0802, 0.0936, 0.0802, 0.1037, 0.0900,
        0.1595, 0.1349], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,328][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0105, 0.1275, 0.1315, 0.0767, 0.2105, 0.0463, 0.0666, 0.1617, 0.0658,
        0.0617, 0.0411], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,329][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ at] are: tensor([1.1209e-04, 9.3413e-01, 1.7473e-02, 3.8496e-02, 9.7171e-03, 1.6526e-05,
        9.4401e-07, 3.3097e-05, 1.1320e-05, 3.4231e-06, 3.9386e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,331][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ at] are: tensor([5.4209e-09, 4.9886e-06, 1.0553e-03, 1.3708e-06, 4.8908e-05, 8.1423e-05,
        4.3177e-03, 9.8585e-01, 2.6333e-05, 7.1231e-03, 1.4893e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,332][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0593, 0.1683, 0.0012, 0.0220, 0.0017, 0.2765, 0.2520, 0.0280, 0.0735,
        0.0354, 0.0821], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,334][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.1147, 0.0861, 0.0766, 0.0794, 0.0860, 0.0823, 0.0810, 0.0995, 0.0963,
        0.0912, 0.1069], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,336][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.4755, 0.1272, 0.2263, 0.0135, 0.0804, 0.0154, 0.0048, 0.0300, 0.0035,
        0.0111, 0.0123], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,337][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0386, 0.0054, 0.2760, 0.0046, 0.2059, 0.0370, 0.0865, 0.1122, 0.0071,
        0.0358, 0.0549, 0.1359], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,339][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0034, 0.1540, 0.3580, 0.2211, 0.2209, 0.0098, 0.0209, 0.0017, 0.0037,
        0.0042, 0.0009, 0.0016], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,340][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.7559e-09, 9.1186e-06, 7.3241e-03, 1.0573e-04, 1.4675e-02, 5.1694e-02,
        2.0428e-01, 5.7370e-01, 1.3757e-02, 1.3117e-01, 1.0620e-03, 2.2291e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,341][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([3.3519e-04, 1.3419e-06, 3.2837e-05, 1.9788e-08, 1.9260e-06, 2.3258e-07,
        3.7304e-06, 2.1543e-01, 1.7079e-04, 5.1666e-04, 7.1588e-01, 6.7626e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,343][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.8397, 0.0038, 0.0353, 0.0019, 0.0087, 0.0164, 0.0146, 0.0135, 0.0018,
        0.0224, 0.0155, 0.0264], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,345][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0689, 0.0540, 0.0492, 0.0653, 0.0720, 0.0880, 0.0698, 0.0951, 0.0760,
        0.1386, 0.1135, 0.1095], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,346][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0058, 0.1221, 0.1092, 0.0739, 0.1877, 0.0429, 0.0578, 0.1521, 0.0753,
        0.0478, 0.0500, 0.0754], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,346][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([4.9264e-04, 9.1121e-01, 1.1955e-02, 5.8675e-02, 1.7447e-02, 4.3289e-05,
        2.1688e-06, 6.6383e-05, 3.3101e-05, 8.8816e-06, 1.0798e-05, 5.3848e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,347][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([4.3387e-08, 2.8322e-06, 5.2857e-04, 4.7427e-07, 2.1359e-05, 1.0072e-04,
        3.8924e-03, 6.8234e-01, 1.2989e-05, 4.5933e-03, 1.1140e-03, 3.0739e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,349][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0581, 0.0233, 0.0052, 0.0160, 0.0011, 0.1833, 0.0139, 0.0245, 0.2892,
        0.0615, 0.2965, 0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,351][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1047, 0.0782, 0.0703, 0.0721, 0.0780, 0.0746, 0.0738, 0.0894, 0.0859,
        0.0816, 0.0958, 0.0956], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,352][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5508, 0.1117, 0.2007, 0.0095, 0.0593, 0.0107, 0.0029, 0.0270, 0.0022,
        0.0073, 0.0085, 0.0092], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,354][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0809, 0.0182, 0.0785, 0.0168, 0.0769, 0.0645, 0.1031, 0.1229, 0.0273,
        0.0593, 0.0821, 0.0927, 0.1769], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,355][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0053, 0.1873, 0.1412, 0.4471, 0.1042, 0.0300, 0.0470, 0.0039, 0.0127,
        0.0047, 0.0028, 0.0084, 0.0054], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,357][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ house] are: tensor([9.8688e-08, 3.0456e-05, 1.1238e-02, 2.5422e-04, 1.8242e-02, 7.8662e-02,
        2.3002e-01, 4.5475e-01, 2.8424e-02, 1.6426e-01, 1.4072e-03, 2.3859e-03,
        1.0323e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,358][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ house] are: tensor([2.4782e-05, 7.6371e-07, 7.3008e-06, 1.4914e-08, 8.0372e-07, 1.0636e-07,
        6.9080e-06, 9.6095e-02, 3.2357e-04, 1.8384e-04, 6.8229e-01, 2.1545e-01,
        5.6186e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,360][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.3873, 0.0032, 0.0912, 0.0024, 0.0589, 0.0865, 0.0435, 0.0799, 0.0051,
        0.0599, 0.0600, 0.0502, 0.0721], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,361][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0671, 0.0426, 0.0324, 0.0619, 0.0455, 0.0723, 0.0659, 0.0709, 0.0903,
        0.0985, 0.1135, 0.1121, 0.1270], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,363][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0074, 0.0899, 0.1291, 0.0608, 0.1832, 0.0429, 0.0552, 0.1279, 0.0561,
        0.0441, 0.0476, 0.0942, 0.0616], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,364][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ house] are: tensor([4.5355e-05, 9.1798e-01, 2.0270e-02, 4.8243e-02, 1.3152e-02, 3.8406e-05,
        2.5394e-06, 1.0174e-04, 3.2625e-05, 1.2486e-05, 1.4991e-05, 9.1923e-05,
        1.8357e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,365][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ house] are: tensor([2.5395e-07, 7.0356e-09, 1.0202e-06, 7.1489e-10, 3.4025e-08, 2.7710e-07,
        7.1060e-06, 1.4193e-03, 3.0173e-08, 7.8635e-06, 2.9634e-06, 7.2608e-04,
        9.9784e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,367][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0052, 0.0601, 0.0140, 0.0190, 0.0286, 0.0165, 0.1898, 0.0060, 0.0492,
        0.0088, 0.3845, 0.2056, 0.0126], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,369][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.0956, 0.0706, 0.0631, 0.0657, 0.0723, 0.0686, 0.0670, 0.0834, 0.0796,
        0.0753, 0.0884, 0.0886, 0.0819], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,371][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.5199, 0.1447, 0.2188, 0.0075, 0.0557, 0.0081, 0.0020, 0.0198, 0.0013,
        0.0054, 0.0055, 0.0077, 0.0036], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,372][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.0766, 0.0117, 0.2596, 0.0032, 0.0755, 0.0107, 0.0387, 0.0410, 0.0041,
        0.0072, 0.1384, 0.2422, 0.0873, 0.0038], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,374][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0008, 0.1094, 0.2998, 0.1082, 0.4469, 0.0105, 0.0119, 0.0010, 0.0042,
        0.0039, 0.0008, 0.0010, 0.0008, 0.0009], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,376][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [.] are: tensor([1.2101e-06, 1.0219e-04, 9.2358e-03, 7.2086e-04, 2.1584e-02, 1.1984e-01,
        3.3934e-01, 3.2940e-01, 3.5238e-02, 1.2807e-01, 3.1638e-03, 2.9885e-03,
        9.6694e-03, 6.4376e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,377][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [.] are: tensor([2.0709e-05, 2.2943e-06, 1.3743e-05, 3.7602e-08, 2.5144e-06, 7.1741e-07,
        2.3464e-05, 1.2997e-01, 1.0214e-03, 1.2097e-03, 4.4797e-01, 3.5249e-01,
        5.8538e-02, 8.7356e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,378][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [.] are: tensor([9.7701e-01, 9.4635e-04, 1.4158e-02, 4.4009e-04, 1.0089e-03, 9.8479e-04,
        6.6636e-04, 6.9090e-04, 1.0289e-04, 1.1414e-03, 7.3976e-04, 1.0935e-03,
        8.7780e-04, 1.3847e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,380][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0624, 0.0425, 0.0390, 0.0499, 0.0554, 0.0728, 0.0536, 0.0724, 0.0534,
        0.1007, 0.0831, 0.0831, 0.1365, 0.0951], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,382][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0061, 0.0870, 0.1178, 0.0551, 0.1705, 0.0318, 0.0621, 0.1156, 0.0502,
        0.0394, 0.0424, 0.1025, 0.0676, 0.0518], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,383][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [.] are: tensor([3.3096e-03, 8.4097e-01, 1.7023e-02, 1.0466e-01, 3.2588e-02, 3.7969e-04,
        2.1817e-05, 3.0775e-04, 1.8905e-04, 5.1312e-05, 4.7171e-05, 2.5546e-04,
        7.0508e-05, 1.2762e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,384][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [.] are: tensor([5.3362e-11, 8.1729e-09, 1.4617e-06, 2.0510e-09, 7.3619e-08, 3.6896e-07,
        1.0246e-05, 1.9126e-03, 6.5725e-08, 1.6866e-05, 7.4987e-06, 7.9298e-04,
        9.9725e-01, 4.0371e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,386][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0212, 0.0357, 0.0086, 0.0226, 0.0098, 0.0344, 0.2024, 0.1065, 0.0588,
        0.0343, 0.1547, 0.1875, 0.0570, 0.0666], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,387][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.0879, 0.0670, 0.0602, 0.0613, 0.0663, 0.0636, 0.0631, 0.0755, 0.0724,
        0.0697, 0.0816, 0.0804, 0.0765, 0.0744], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,388][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.5794, 0.0990, 0.1811, 0.0093, 0.0464, 0.0097, 0.0033, 0.0261, 0.0023,
        0.0072, 0.0085, 0.0103, 0.0052, 0.0121], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,389][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0187, 0.0060, 0.1369, 0.0051, 0.0972, 0.0364, 0.0569, 0.0819, 0.0140,
        0.0446, 0.0505, 0.0507, 0.0806, 0.0054, 0.3151], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,390][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0178, 0.2376, 0.0621, 0.4204, 0.1334, 0.0235, 0.0338, 0.0049, 0.0059,
        0.0207, 0.0018, 0.0099, 0.0140, 0.0068, 0.0074], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,391][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([2.7125e-09, 2.6827e-06, 2.6211e-04, 6.8177e-05, 2.1095e-03, 4.7701e-02,
        6.4989e-01, 1.1092e-01, 1.6821e-02, 1.5706e-01, 8.4286e-05, 1.6992e-04,
        1.0933e-03, 4.6667e-05, 1.3770e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,392][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([2.8085e-03, 4.0071e-07, 3.8219e-06, 8.0924e-09, 2.3040e-07, 3.6530e-08,
        1.1632e-06, 1.3083e-02, 1.0869e-04, 3.2894e-05, 3.7586e-02, 1.5499e-02,
        1.9429e-03, 1.8288e-03, 9.2710e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,394][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.6253, 0.0148, 0.1072, 0.0075, 0.0177, 0.0262, 0.0245, 0.0150, 0.0047,
        0.0272, 0.0338, 0.0388, 0.0173, 0.0052, 0.0348], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,396][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0395, 0.0266, 0.0215, 0.0472, 0.0294, 0.0624, 0.0556, 0.0527, 0.0858,
        0.0731, 0.1092, 0.1083, 0.0990, 0.1402, 0.0497], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,397][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0050, 0.0831, 0.0999, 0.0594, 0.1299, 0.0444, 0.0465, 0.1118, 0.0525,
        0.0354, 0.0518, 0.0680, 0.0650, 0.0535, 0.0938], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,398][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([1.4850e-03, 6.2718e-01, 5.0006e-03, 1.7196e-01, 1.8566e-01, 9.6573e-05,
        6.6296e-06, 7.2414e-05, 3.7008e-04, 6.6184e-05, 1.1533e-04, 8.8257e-04,
        1.2494e-04, 1.2964e-03, 5.6833e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,400][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([2.0380e-09, 1.6964e-07, 1.4093e-05, 1.3484e-08, 1.3177e-07, 3.4662e-07,
        1.8696e-05, 3.4525e-03, 4.4647e-08, 1.8168e-05, 2.2926e-06, 5.7193e-04,
        9.9491e-01, 2.8914e-06, 1.0123e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,401][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0018, 0.0222, 0.0036, 0.0146, 0.0019, 0.0888, 0.4033, 0.0073, 0.0569,
        0.0013, 0.2089, 0.1697, 0.0016, 0.0155, 0.0027], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,403][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0831, 0.0599, 0.0538, 0.0555, 0.0619, 0.0585, 0.0573, 0.0721, 0.0686,
        0.0649, 0.0768, 0.0771, 0.0704, 0.0708, 0.0694], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,405][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.6127, 0.0989, 0.1596, 0.0086, 0.0415, 0.0082, 0.0027, 0.0219, 0.0019,
        0.0064, 0.0068, 0.0085, 0.0042, 0.0101, 0.0081], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,407][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0410, 0.0052, 0.2150, 0.0043, 0.0445, 0.0061, 0.0381, 0.0319, 0.0039,
        0.0081, 0.0555, 0.1564, 0.0803, 0.0061, 0.2640, 0.0397],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,408][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0097, 0.1190, 0.1292, 0.4040, 0.1802, 0.0175, 0.0651, 0.0029, 0.0189,
        0.0166, 0.0060, 0.0091, 0.0041, 0.0095, 0.0068, 0.0015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,410][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([7.4808e-09, 2.2920e-05, 1.1726e-03, 2.4729e-04, 4.9359e-03, 5.8176e-02,
        4.3535e-01, 1.9567e-01, 2.5972e-02, 2.2157e-01, 2.4549e-04, 2.9426e-04,
        3.6610e-03, 8.5889e-05, 5.2423e-02, 1.7167e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,411][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([9.4093e-05, 1.8407e-07, 9.7819e-07, 2.6271e-09, 1.1831e-07, 5.0560e-09,
        3.0069e-07, 5.9140e-03, 3.5764e-05, 2.2043e-05, 1.5779e-02, 9.7469e-03,
        4.0024e-03, 8.1144e-04, 9.4240e-01, 2.1196e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,412][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([9.6779e-01, 7.6222e-04, 1.1416e-02, 2.5901e-04, 1.0321e-03, 2.1379e-03,
        1.8003e-03, 1.5764e-03, 1.4597e-04, 1.9616e-03, 1.6759e-03, 2.6887e-03,
        2.3296e-03, 1.1966e-04, 3.9460e-03, 3.5753e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,414][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0470, 0.0334, 0.0283, 0.0447, 0.0386, 0.0588, 0.0500, 0.0567, 0.0587,
        0.0790, 0.0863, 0.0861, 0.1060, 0.1090, 0.0523, 0.0652],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,416][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0050, 0.0908, 0.1067, 0.0472, 0.1471, 0.0278, 0.0483, 0.0815, 0.0474,
        0.0254, 0.0482, 0.0802, 0.0654, 0.0498, 0.1129, 0.0163],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,417][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([5.4992e-04, 9.5075e-01, 1.2958e-02, 2.0166e-02, 1.2400e-02, 2.0324e-05,
        1.2768e-06, 9.6707e-05, 3.0246e-05, 5.9194e-06, 1.3307e-05, 1.0371e-04,
        3.0059e-05, 7.5063e-05, 2.7968e-03, 6.6741e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,418][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([1.0325e-05, 2.1037e-08, 2.3166e-06, 9.1606e-10, 5.1119e-08, 9.4249e-08,
        3.2505e-06, 3.2796e-04, 1.7767e-08, 1.1254e-05, 2.3416e-06, 3.2636e-04,
        4.7554e-01, 4.2253e-06, 2.7520e-03, 5.2102e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,420][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0108, 0.0427, 0.0019, 0.0132, 0.0015, 0.1189, 0.1480, 0.0391, 0.0766,
        0.0041, 0.1100, 0.2776, 0.0075, 0.1142, 0.0023, 0.0315],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,422][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0769, 0.0579, 0.0509, 0.0531, 0.0567, 0.0549, 0.0544, 0.0657, 0.0646,
        0.0604, 0.0717, 0.0722, 0.0656, 0.0662, 0.0632, 0.0655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,423][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.4094, 0.1845, 0.2223, 0.0101, 0.0693, 0.0107, 0.0031, 0.0259, 0.0022,
        0.0084, 0.0087, 0.0101, 0.0064, 0.0151, 0.0100, 0.0037],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,425][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0122, 0.0015, 0.0808, 0.0023, 0.0614, 0.0125, 0.0299, 0.0432, 0.0031,
        0.0146, 0.0379, 0.0716, 0.1698, 0.0031, 0.3989, 0.0228, 0.0343],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,427][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0059, 0.2078, 0.2481, 0.2520, 0.2007, 0.0149, 0.0206, 0.0025, 0.0089,
        0.0130, 0.0020, 0.0051, 0.0034, 0.0064, 0.0060, 0.0012, 0.0014],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,428][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([2.9912e-08, 2.9889e-06, 2.2934e-04, 1.2773e-04, 2.1294e-03, 1.0009e-01,
        5.6569e-01, 1.2577e-01, 3.4328e-02, 1.6030e-01, 1.3312e-04, 1.5079e-04,
        1.3874e-03, 6.1044e-05, 8.7944e-03, 8.2447e-05, 7.2381e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,429][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([5.2872e-04, 6.1301e-08, 5.4387e-07, 7.0857e-10, 3.0441e-08, 7.6834e-09,
        9.8261e-08, 1.6083e-02, 6.4699e-06, 9.2615e-06, 1.1328e-02, 1.8049e-03,
        7.7515e-04, 2.8059e-04, 2.2520e-01, 1.2445e-01, 6.1954e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,430][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.6856, 0.0046, 0.0520, 0.0025, 0.0104, 0.0298, 0.0201, 0.0175, 0.0022,
        0.0319, 0.0238, 0.0322, 0.0312, 0.0012, 0.0196, 0.0117, 0.0236],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,431][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0398, 0.0317, 0.0271, 0.0420, 0.0405, 0.0567, 0.0447, 0.0566, 0.0510,
        0.0812, 0.0776, 0.0750, 0.1084, 0.0995, 0.0493, 0.0606, 0.0581],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,433][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0062, 0.0920, 0.0782, 0.0586, 0.1225, 0.0348, 0.0313, 0.1121, 0.0552,
        0.0313, 0.0321, 0.0581, 0.0623, 0.0658, 0.0975, 0.0309, 0.0311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,434][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([4.2457e-04, 9.2918e-01, 1.0535e-02, 4.0611e-02, 1.6941e-02, 9.1381e-05,
        5.1395e-06, 2.3028e-04, 9.2590e-05, 2.1261e-05, 2.6493e-05, 1.4140e-04,
        3.3817e-05, 1.0948e-04, 1.5357e-03, 1.2784e-05, 9.7896e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,435][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([6.1828e-07, 7.8654e-10, 1.9145e-07, 8.0372e-11, 1.1894e-08, 3.9918e-08,
        5.5884e-07, 1.5845e-04, 4.2745e-09, 1.9321e-06, 1.1282e-06, 7.7520e-05,
        1.2891e-01, 1.0347e-06, 9.3807e-04, 7.9732e-01, 7.2595e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,437][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.1874e-03, 9.3796e-04, 6.6160e-05, 6.8525e-04, 2.9462e-05, 1.2885e-02,
        5.0181e-04, 1.2364e-03, 9.9964e-03, 2.5358e-03, 2.6014e-02, 1.1682e-03,
        1.9349e-03, 3.4692e-03, 7.8595e-05, 9.3564e-01, 1.6294e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,438][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0730, 0.0546, 0.0479, 0.0500, 0.0536, 0.0516, 0.0510, 0.0622, 0.0599,
        0.0568, 0.0662, 0.0667, 0.0615, 0.0617, 0.0598, 0.0605, 0.0631],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,440][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.3155, 0.1866, 0.2719, 0.0129, 0.0793, 0.0135, 0.0043, 0.0323, 0.0030,
        0.0099, 0.0110, 0.0121, 0.0081, 0.0189, 0.0123, 0.0050, 0.0033],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,442][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0190, 0.0257, 0.0211, 0.0407, 0.0211, 0.0228, 0.0570, 0.0442, 0.0421,
        0.0344, 0.0866, 0.1036, 0.0391, 0.0597, 0.0968, 0.1371, 0.0753, 0.0735],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,444][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0075, 0.1542, 0.1754, 0.3015, 0.2441, 0.0152, 0.0279, 0.0031, 0.0082,
        0.0076, 0.0015, 0.0038, 0.0115, 0.0047, 0.0244, 0.0028, 0.0047, 0.0019],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,445][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([3.9595e-07, 4.2360e-05, 1.7544e-03, 3.4432e-04, 3.6097e-03, 2.9008e-02,
        2.3964e-01, 1.6159e-01, 2.8556e-02, 1.8862e-01, 7.0016e-04, 9.2983e-04,
        7.7548e-03, 5.0291e-04, 1.3524e-01, 1.3061e-03, 9.0206e-03, 1.9137e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,446][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([4.2215e-04, 1.4334e-08, 3.6823e-08, 8.3466e-11, 4.8303e-09, 7.7373e-10,
        2.8843e-08, 1.5496e-04, 3.2844e-06, 7.9495e-07, 2.0123e-03, 4.7242e-04,
        4.7661e-05, 1.0015e-04, 4.6026e-02, 1.6060e-02, 1.9899e-01, 7.3571e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,448][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.2724, 0.0016, 0.1101, 0.0015, 0.0579, 0.0807, 0.0327, 0.0731, 0.0040,
        0.0553, 0.0505, 0.0299, 0.0613, 0.0009, 0.0425, 0.0131, 0.0388, 0.0737],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,450][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0322, 0.0211, 0.0155, 0.0407, 0.0214, 0.0495, 0.0462, 0.0447, 0.0759,
        0.0585, 0.1002, 0.0979, 0.0858, 0.1224, 0.0321, 0.0578, 0.0683, 0.0299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,452][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0054, 0.0788, 0.0823, 0.0507, 0.1069, 0.0350, 0.0407, 0.0970, 0.0489,
        0.0275, 0.0469, 0.0635, 0.0659, 0.0495, 0.0789, 0.0259, 0.0382, 0.0580],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,453][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([4.7517e-02, 3.9732e-01, 8.5505e-03, 2.7268e-01, 1.9566e-01, 4.1606e-04,
        2.9464e-05, 1.4338e-04, 7.3742e-04, 9.6967e-05, 2.5654e-04, 1.3374e-03,
        3.9332e-04, 1.8202e-03, 3.7139e-03, 1.4255e-04, 1.3952e-04, 6.9036e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,454][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([8.5379e-08, 1.4162e-09, 5.7394e-07, 1.2163e-10, 1.2403e-08, 1.9791e-08,
        7.3506e-07, 2.7010e-04, 3.1844e-09, 1.4378e-06, 4.8734e-07, 1.0444e-04,
        1.6290e-01, 6.1393e-07, 8.5953e-04, 7.3345e-01, 8.6880e-02, 1.5527e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,456][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([6.7585e-04, 1.0841e-02, 1.3883e-03, 5.0448e-03, 2.7217e-03, 3.6159e-02,
        6.4814e-02, 5.3001e-03, 1.4823e-02, 3.5664e-03, 2.9917e-02, 6.0379e-02,
        7.0896e-03, 2.7698e-02, 4.0046e-03, 6.0287e-01, 1.2217e-01, 5.4063e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,458][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0681, 0.0498, 0.0448, 0.0463, 0.0503, 0.0480, 0.0471, 0.0583, 0.0560,
        0.0531, 0.0626, 0.0628, 0.0580, 0.0576, 0.0568, 0.0570, 0.0597, 0.0638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,459][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([8.2299e-01, 3.2726e-02, 8.7326e-02, 2.3619e-03, 2.5074e-02, 2.5831e-03,
        5.5359e-04, 7.6815e-03, 3.6486e-04, 1.7445e-03, 1.9652e-03, 2.6980e-03,
        1.2119e-03, 3.0603e-03, 3.1304e-03, 8.2520e-04, 4.6860e-04, 3.2402e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,461][circuit_model.py][line:2294][INFO] ##3-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0207, 0.0031, 0.0857, 0.0030, 0.0393, 0.0104, 0.0337, 0.0378, 0.0039,
        0.0111, 0.0778, 0.1277, 0.0950, 0.0048, 0.2806, 0.0306, 0.0650, 0.0508,
        0.0189], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,462][circuit_model.py][line:2297][INFO] ##3-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0065, 0.3163, 0.1648, 0.2890, 0.1326, 0.0158, 0.0385, 0.0035, 0.0049,
        0.0075, 0.0015, 0.0023, 0.0018, 0.0036, 0.0021, 0.0008, 0.0015, 0.0009,
        0.0064], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,464][circuit_model.py][line:2300][INFO] ##3-th layer ##Weight##: The head3 weight for token [ to] are: tensor([6.3263e-09, 3.5575e-06, 1.1755e-03, 8.8123e-05, 4.0771e-03, 4.6512e-02,
        4.3553e-01, 2.6355e-01, 2.3709e-02, 1.4150e-01, 2.2281e-04, 2.4882e-04,
        1.1663e-03, 5.8085e-05, 3.1065e-02, 1.8060e-04, 1.1812e-03, 4.9427e-02,
        3.0253e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,465][circuit_model.py][line:2303][INFO] ##3-th layer ##Weight##: The head4 weight for token [ to] are: tensor([7.8679e-05, 2.4560e-09, 1.4587e-08, 2.0949e-11, 7.4228e-10, 3.0508e-10,
        8.3498e-09, 1.8432e-04, 4.1273e-07, 5.5127e-07, 7.1581e-05, 4.8587e-05,
        2.4986e-05, 6.0068e-06, 4.2525e-03, 2.1316e-03, 3.3562e-02, 9.0848e-01,
        5.1157e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,466][circuit_model.py][line:2306][INFO] ##3-th layer ##Weight##: The head5 weight for token [ to] are: tensor([8.8301e-01, 1.0628e-03, 2.1644e-02, 5.4015e-04, 2.8895e-03, 7.1090e-03,
        5.0457e-03, 4.5268e-03, 4.3569e-04, 6.1133e-03, 5.7044e-03, 8.5281e-03,
        5.6975e-03, 1.9925e-04, 6.1178e-03, 2.7677e-03, 7.6381e-03, 2.9442e-02,
        1.5250e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,468][circuit_model.py][line:2309][INFO] ##3-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0391, 0.0291, 0.0242, 0.0365, 0.0356, 0.0512, 0.0398, 0.0519, 0.0442,
        0.0703, 0.0668, 0.0653, 0.0951, 0.0824, 0.0428, 0.0543, 0.0519, 0.0475,
        0.0719], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,470][circuit_model.py][line:2312][INFO] ##3-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0039, 0.0758, 0.0724, 0.0421, 0.1152, 0.0264, 0.0402, 0.0902, 0.0373,
        0.0311, 0.0298, 0.0638, 0.0665, 0.0468, 0.0875, 0.0201, 0.0392, 0.0860,
        0.0257], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,471][circuit_model.py][line:2315][INFO] ##3-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.6782e-03, 7.2475e-01, 1.7022e-02, 1.5300e-01, 6.7203e-02, 3.1766e-04,
        2.7106e-05, 4.3754e-04, 3.6954e-04, 8.0816e-05, 9.8344e-05, 5.6110e-04,
        1.4702e-04, 4.8085e-04, 2.9917e-03, 5.3423e-05, 4.9848e-05, 3.0389e-02,
        3.4604e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,472][circuit_model.py][line:2318][INFO] ##3-th layer ##Weight##: The head9 weight for token [ to] are: tensor([2.6756e-07, 4.6691e-11, 2.3037e-08, 3.9841e-12, 7.9498e-10, 3.7153e-09,
        9.5772e-08, 3.1233e-05, 3.9429e-10, 2.3217e-07, 1.2560e-07, 2.3141e-05,
        4.4772e-02, 1.7942e-07, 2.9039e-04, 5.0597e-01, 6.2886e-02, 2.6955e-02,
        3.5907e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,473][circuit_model.py][line:2321][INFO] ##3-th layer ##Weight##: The head10 weight for token [ to] are: tensor([8.9222e-04, 1.7071e-03, 2.5235e-04, 7.5751e-04, 9.1076e-05, 4.1533e-03,
        7.1862e-03, 6.6320e-04, 2.2950e-03, 8.5633e-04, 4.8709e-03, 1.4563e-02,
        9.4146e-04, 1.1275e-02, 2.7619e-04, 9.2307e-01, 2.4166e-02, 5.5016e-04,
        1.4292e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,474][circuit_model.py][line:2324][INFO] ##3-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.0642, 0.0485, 0.0428, 0.0444, 0.0476, 0.0457, 0.0452, 0.0548, 0.0525,
        0.0499, 0.0584, 0.0584, 0.0542, 0.0541, 0.0531, 0.0540, 0.0555, 0.0584,
        0.0581], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,476][circuit_model.py][line:2327][INFO] ##3-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.4985, 0.1162, 0.2166, 0.0079, 0.0554, 0.0087, 0.0025, 0.0232, 0.0018,
        0.0061, 0.0071, 0.0080, 0.0044, 0.0123, 0.0082, 0.0029, 0.0020, 0.0157,
        0.0024], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,499][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:17,500][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,501][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,501][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,502][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,503][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,503][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,504][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,505][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,505][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,506][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,507][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,507][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,509][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.8602, 0.1398], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,510][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.2953, 0.7047], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,512][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.4256, 0.5744], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,513][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.5442, 0.4558], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,515][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.6523, 0.3477], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,516][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.6739, 0.3261], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,518][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.4213, 0.5787], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,519][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.7525, 0.2475], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,521][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0519, 0.9481], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,523][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.5073, 0.4927], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,524][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.5685, 0.4315], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,525][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.9626, 0.0374], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,526][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.8883, 0.0780, 0.0337], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,527][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.1406, 0.4160, 0.4434], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,527][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.2486, 0.3509, 0.4005], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,529][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.3528, 0.3614, 0.2858], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,530][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.5075, 0.2585, 0.2340], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,532][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.4691, 0.2327, 0.2982], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,533][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.2548, 0.7262, 0.0190], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,535][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.6229, 0.1799, 0.1972], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,536][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.0356, 0.7894, 0.1751], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,538][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.3466, 0.3088, 0.3445], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,540][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.4151, 0.3063, 0.2786], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,541][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.8624, 0.0725, 0.0652], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,543][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7261, 0.1091, 0.0239, 0.1409], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,544][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.1025, 0.2761, 0.3343, 0.2871], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,546][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.1732, 0.2458, 0.2825, 0.2985], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,548][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2938, 0.2653, 0.2208, 0.2200], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,549][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.4172, 0.2117, 0.1920, 0.1791], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,551][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3573, 0.1831, 0.2299, 0.2297], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,553][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5191, 0.0882, 0.0259, 0.3668], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,554][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.5392, 0.1565, 0.1882, 0.1161], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,556][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0146, 0.4336, 0.3212, 0.2305], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,557][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.2779, 0.2366, 0.2817, 0.2038], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,559][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.3801, 0.2209, 0.2104, 0.1887], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,561][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.8398, 0.0354, 0.0592, 0.0656], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,562][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.7752, 0.0731, 0.0122, 0.0809, 0.0585], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,564][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0558, 0.1729, 0.2495, 0.2070, 0.3147], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,565][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.1214, 0.1851, 0.2088, 0.2302, 0.2545], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,567][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.2190, 0.2159, 0.1919, 0.1911, 0.1820], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,567][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.3476, 0.1776, 0.1593, 0.1494, 0.1661], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,568][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.2700, 0.1500, 0.1746, 0.1827, 0.2227], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,569][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.3687, 0.1197, 0.0070, 0.4681, 0.0365], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,570][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.4548, 0.1320, 0.1616, 0.0952, 0.1564], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,571][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0099, 0.4339, 0.1267, 0.3302, 0.0992], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,573][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.2327, 0.1819, 0.2007, 0.1465, 0.2382], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,574][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2896, 0.1813, 0.1675, 0.1542, 0.2074], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,576][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.4745, 0.0425, 0.1095, 0.1710, 0.2025], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:17,578][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.6319, 0.0853, 0.0134, 0.1071, 0.0389, 0.1233], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,579][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0603, 0.1372, 0.1990, 0.1794, 0.2557, 0.1684], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,581][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0984, 0.1448, 0.1679, 0.1760, 0.2066, 0.2062], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,583][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.1867, 0.1819, 0.1540, 0.1690, 0.1558, 0.1526], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,584][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.2963, 0.1529, 0.1369, 0.1285, 0.1439, 0.1414], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,586][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.2479, 0.1069, 0.1463, 0.1494, 0.1932, 0.1562], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,587][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([9.4188e-01, 5.2843e-04, 3.5904e-05, 2.6069e-04, 1.4213e-04, 5.7154e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,589][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.3600, 0.1183, 0.1547, 0.0897, 0.1564, 0.1210], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,590][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.0125, 0.3549, 0.1161, 0.2701, 0.1263, 0.1201], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,592][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.1803, 0.1470, 0.1700, 0.1241, 0.1967, 0.1820], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,594][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.2656, 0.1562, 0.1360, 0.1328, 0.1929, 0.1166], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,595][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.7933, 0.0199, 0.0287, 0.0804, 0.0516, 0.0261], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:17,597][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.6582, 0.0677, 0.0154, 0.0837, 0.0364, 0.0563, 0.0822],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,599][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0507, 0.1264, 0.1596, 0.1525, 0.2246, 0.1450, 0.1412],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,600][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0828, 0.1243, 0.1418, 0.1509, 0.1726, 0.1740, 0.1536],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,602][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1598, 0.1586, 0.1360, 0.1419, 0.1383, 0.1468, 0.1187],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,604][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2594, 0.1324, 0.1193, 0.1116, 0.1260, 0.1240, 0.1273],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,605][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.2299, 0.0924, 0.1198, 0.1263, 0.1653, 0.1335, 0.1327],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,606][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([3.2217e-01, 1.9828e-04, 3.1265e-05, 3.6149e-04, 2.6137e-04, 6.5353e-01,
        2.3450e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,608][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.3176, 0.1055, 0.1423, 0.0826, 0.1471, 0.1137, 0.0911],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,609][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0116, 0.2714, 0.0967, 0.2225, 0.1643, 0.1927, 0.0408],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,610][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.1517, 0.1269, 0.1485, 0.1103, 0.1738, 0.1561, 0.1327],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,611][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.2190, 0.1433, 0.1292, 0.1203, 0.1842, 0.1064, 0.0976],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,611][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.5469, 0.0427, 0.0727, 0.1266, 0.0972, 0.1065, 0.0075],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:17,613][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.6514, 0.0656, 0.0082, 0.0850, 0.0276, 0.0591, 0.0605, 0.0425],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,615][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.0348, 0.1046, 0.1383, 0.1300, 0.2111, 0.1234, 0.1250, 0.1329],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,616][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.0685, 0.1040, 0.1141, 0.1278, 0.1389, 0.1416, 0.1284, 0.1767],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,618][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.1353, 0.1405, 0.1350, 0.1283, 0.1241, 0.1161, 0.1177, 0.1028],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,619][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.2243, 0.1163, 0.1053, 0.0988, 0.1110, 0.1089, 0.1115, 0.1240],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,621][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.2064, 0.0866, 0.1046, 0.1124, 0.1457, 0.1214, 0.1210, 0.1019],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,622][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([2.2067e-01, 5.7128e-03, 3.2192e-04, 8.5530e-03, 2.7185e-03, 6.4232e-01,
        7.7632e-02, 4.2072e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,624][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.2657, 0.0997, 0.1199, 0.0768, 0.1190, 0.0934, 0.0778, 0.1477],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,626][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.0103, 0.2177, 0.0964, 0.2172, 0.1024, 0.1808, 0.0449, 0.1303],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,627][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.1325, 0.1118, 0.1300, 0.0990, 0.1529, 0.1349, 0.1186, 0.1202],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,629][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.2275, 0.1150, 0.1192, 0.1017, 0.1494, 0.0846, 0.0796, 0.1230],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,630][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.5132, 0.0087, 0.0530, 0.0379, 0.0632, 0.0227, 0.1111, 0.1902],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:17,632][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.5749, 0.0738, 0.0149, 0.0877, 0.0371, 0.0643, 0.0577, 0.0256, 0.0639],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,634][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0402, 0.1015, 0.1166, 0.1194, 0.1797, 0.1149, 0.1174, 0.1237, 0.0865],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,635][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0632, 0.0895, 0.0992, 0.1056, 0.1202, 0.1209, 0.1098, 0.1565, 0.1351],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,637][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1364, 0.1196, 0.1053, 0.1014, 0.1016, 0.1058, 0.0986, 0.1144, 0.1170],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,639][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.1981, 0.1046, 0.0951, 0.0892, 0.1004, 0.0987, 0.1013, 0.1126, 0.0999],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,641][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.1770, 0.0802, 0.1039, 0.1112, 0.1368, 0.1072, 0.1080, 0.0939, 0.0819],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,642][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([6.0974e-01, 1.5146e-04, 1.3748e-05, 2.6543e-04, 1.5471e-04, 2.4224e-01,
        7.9160e-03, 1.5915e-02, 1.2361e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,644][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.2464, 0.0840, 0.1017, 0.0650, 0.1100, 0.0874, 0.0774, 0.1467, 0.0814],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,645][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.0064, 0.2118, 0.0859, 0.1459, 0.1321, 0.1464, 0.0460, 0.0928, 0.1328],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,647][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.1135, 0.0945, 0.1140, 0.0932, 0.1341, 0.1282, 0.1103, 0.1119, 0.1004],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,649][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.2017, 0.1108, 0.1055, 0.0948, 0.1209, 0.0797, 0.0815, 0.1118, 0.0933],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,650][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0804, 0.0066, 0.0889, 0.0294, 0.1406, 0.0273, 0.0303, 0.5945, 0.0022],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:17,651][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.6630, 0.0654, 0.0083, 0.0786, 0.0214, 0.0422, 0.0441, 0.0152, 0.0506,
        0.0112], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,652][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0297, 0.0822, 0.1151, 0.1011, 0.1626, 0.1025, 0.1007, 0.1345, 0.0664,
        0.1051], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,653][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.0500, 0.0778, 0.0854, 0.0975, 0.1072, 0.1080, 0.0981, 0.1350, 0.1186,
        0.1222], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,654][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.1005, 0.1097, 0.1041, 0.1058, 0.1003, 0.1088, 0.0941, 0.0979, 0.1110,
        0.0678], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,656][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.1827, 0.0949, 0.0851, 0.0800, 0.0905, 0.0893, 0.0918, 0.1012, 0.0902,
        0.0943], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,658][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.1629, 0.0715, 0.0884, 0.1009, 0.1240, 0.1015, 0.1034, 0.0893, 0.0739,
        0.0841], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,659][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([2.5171e-02, 5.2737e-05, 6.2790e-06, 2.4043e-04, 8.6177e-05, 4.1234e-01,
        3.1092e-02, 1.2343e-02, 5.1253e-01, 6.1375e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,661][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.2214, 0.0787, 0.1001, 0.0619, 0.0999, 0.0786, 0.0649, 0.1284, 0.0720,
        0.0940], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,662][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.0097, 0.1538, 0.0720, 0.1365, 0.1076, 0.0999, 0.0314, 0.1575, 0.2118,
        0.0199], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,664][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.1145, 0.0944, 0.1059, 0.0765, 0.1213, 0.1094, 0.0929, 0.0938, 0.0833,
        0.1079], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,666][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.1725, 0.1050, 0.0954, 0.0894, 0.1319, 0.0733, 0.0673, 0.1039, 0.0802,
        0.0811], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,667][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.2665, 0.0059, 0.0593, 0.0396, 0.0745, 0.0066, 0.0592, 0.3975, 0.0715,
        0.0193], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:17,669][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.5400, 0.0734, 0.0093, 0.0846, 0.0339, 0.0535, 0.0456, 0.0181, 0.0592,
        0.0088, 0.0735], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,671][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0343, 0.0845, 0.1028, 0.0964, 0.1516, 0.0974, 0.0982, 0.1094, 0.0671,
        0.1088, 0.0497], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,672][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0479, 0.0705, 0.0774, 0.0841, 0.0933, 0.0953, 0.0859, 0.1207, 0.1064,
        0.1073, 0.1112], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,674][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1116, 0.1037, 0.0883, 0.0911, 0.0814, 0.0896, 0.0821, 0.0836, 0.1067,
        0.0715, 0.0903], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,676][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.1677, 0.0863, 0.0781, 0.0732, 0.0829, 0.0815, 0.0834, 0.0932, 0.0824,
        0.0865, 0.0847], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,677][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.1669, 0.0647, 0.0826, 0.0907, 0.1119, 0.0887, 0.0939, 0.0788, 0.0707,
        0.0799, 0.0712], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,679][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([6.0825e-01, 1.0721e-03, 1.9039e-04, 1.6761e-03, 4.0249e-04, 7.4021e-03,
        3.6557e-04, 6.7978e-04, 8.5778e-04, 5.0188e-05, 3.7906e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,680][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.2279, 0.0710, 0.0808, 0.0521, 0.0830, 0.0658, 0.0568, 0.1112, 0.0638,
        0.0852, 0.1024], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,682][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0056, 0.1726, 0.0503, 0.1104, 0.0791, 0.1237, 0.0342, 0.1020, 0.1402,
        0.0334, 0.1484], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,684][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0970, 0.0811, 0.0943, 0.0706, 0.1095, 0.0995, 0.0851, 0.0870, 0.0763,
        0.0986, 0.1011], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,686][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.1754, 0.0981, 0.0925, 0.0796, 0.1067, 0.0645, 0.0638, 0.0907, 0.0735,
        0.0723, 0.0828], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,687][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.1015, 0.0054, 0.0106, 0.0204, 0.0177, 0.0361, 0.0109, 0.5237, 0.0236,
        0.2419, 0.0082], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:17,689][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.5312, 0.0604, 0.0114, 0.0754, 0.0263, 0.0488, 0.0488, 0.0217, 0.0505,
        0.0103, 0.0349, 0.0802], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,691][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0292, 0.0760, 0.0867, 0.0901, 0.1429, 0.0876, 0.0833, 0.1007, 0.0616,
        0.0942, 0.0485, 0.0992], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,692][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0449, 0.0644, 0.0709, 0.0758, 0.0833, 0.0844, 0.0767, 0.1075, 0.0956,
        0.0963, 0.0993, 0.1010], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,693][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0993, 0.0905, 0.0839, 0.0772, 0.0776, 0.0819, 0.0713, 0.0789, 0.0914,
        0.0666, 0.0854, 0.0960], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,694][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1465, 0.0796, 0.0724, 0.0683, 0.0763, 0.0747, 0.0764, 0.0848, 0.0761,
        0.0791, 0.0780, 0.0879], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,695][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1430, 0.0612, 0.0793, 0.0870, 0.1067, 0.0858, 0.0866, 0.0741, 0.0641,
        0.0748, 0.0694, 0.0680], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,696][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([1.1999e-01, 9.2358e-04, 1.0351e-04, 3.5393e-03, 6.3284e-04, 4.3239e-03,
        3.0236e-04, 3.6472e-04, 5.5228e-04, 3.8072e-05, 2.1329e-01, 6.5594e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,698][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1991, 0.0663, 0.0717, 0.0487, 0.0735, 0.0603, 0.0528, 0.0971, 0.0593,
        0.0777, 0.0913, 0.1023], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,699][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0039, 0.1289, 0.0497, 0.1026, 0.0896, 0.1034, 0.0265, 0.1159, 0.1414,
        0.0301, 0.1859, 0.0221], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,701][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0890, 0.0734, 0.0859, 0.0653, 0.0967, 0.0921, 0.0786, 0.0817, 0.0728,
        0.0905, 0.0925, 0.0817], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,703][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1720, 0.0892, 0.0853, 0.0710, 0.0985, 0.0587, 0.0584, 0.0829, 0.0678,
        0.0666, 0.0773, 0.0724], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,704][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1494, 0.0065, 0.0145, 0.0242, 0.0247, 0.0280, 0.0155, 0.1297, 0.0439,
        0.3979, 0.1552, 0.0105], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:17,706][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.6380, 0.0513, 0.0059, 0.0683, 0.0183, 0.0354, 0.0358, 0.0109, 0.0393,
        0.0047, 0.0296, 0.0509, 0.0116], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,708][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0191, 0.0624, 0.0902, 0.0838, 0.1302, 0.0834, 0.0843, 0.0963, 0.0546,
        0.0987, 0.0425, 0.1030, 0.0513], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,710][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.0361, 0.0569, 0.0624, 0.0710, 0.0775, 0.0786, 0.0712, 0.0986, 0.0874,
        0.0886, 0.0928, 0.0915, 0.0873], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,712][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0784, 0.0807, 0.0847, 0.0781, 0.0794, 0.0720, 0.0762, 0.0679, 0.0897,
        0.0552, 0.0860, 0.0969, 0.0548], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,713][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.1479, 0.0729, 0.0656, 0.0618, 0.0695, 0.0680, 0.0696, 0.0778, 0.0697,
        0.0720, 0.0711, 0.0819, 0.0722], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,715][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.1242, 0.0576, 0.0726, 0.0828, 0.1003, 0.0849, 0.0831, 0.0719, 0.0601,
        0.0697, 0.0629, 0.0633, 0.0665], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,716][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([8.4096e-03, 1.5368e-04, 7.4005e-06, 8.1302e-04, 7.4086e-05, 1.5389e-03,
        1.4802e-04, 7.3894e-05, 3.3506e-04, 1.4543e-05, 1.5429e-01, 7.2647e-01,
        1.0767e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,718][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.1762, 0.0580, 0.0687, 0.0437, 0.0680, 0.0542, 0.0448, 0.0887, 0.0510,
        0.0671, 0.0843, 0.0966, 0.0988], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,720][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.0038, 0.1135, 0.0566, 0.1145, 0.0673, 0.0885, 0.0266, 0.0939, 0.1702,
        0.0212, 0.1905, 0.0270, 0.0263], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,721][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.0844, 0.0666, 0.0769, 0.0582, 0.0857, 0.0822, 0.0710, 0.0732, 0.0666,
        0.0816, 0.0846, 0.0737, 0.0954], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,723][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.1474, 0.0845, 0.0785, 0.0664, 0.1020, 0.0554, 0.0519, 0.0791, 0.0586,
        0.0625, 0.0647, 0.0595, 0.0895], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,725][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.2017, 0.0019, 0.0223, 0.0093, 0.0630, 0.0115, 0.0232, 0.0990, 0.0176,
        0.2488, 0.1630, 0.1105, 0.0283], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:17,727][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.4092, 0.0623, 0.0125, 0.0744, 0.0339, 0.0585, 0.0523, 0.0225, 0.0473,
        0.0135, 0.0393, 0.0675, 0.0166, 0.0904], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,728][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0269, 0.0648, 0.0777, 0.0740, 0.1109, 0.0752, 0.0735, 0.0804, 0.0589,
        0.0831, 0.0456, 0.0927, 0.0540, 0.0824], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,730][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0369, 0.0527, 0.0580, 0.0620, 0.0690, 0.0705, 0.0637, 0.0921, 0.0804,
        0.0814, 0.0843, 0.0861, 0.0791, 0.0838], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,732][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0900, 0.0786, 0.0658, 0.0663, 0.0620, 0.0658, 0.0637, 0.0670, 0.0820,
        0.0580, 0.0696, 0.0902, 0.0586, 0.0825], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,734][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.1297, 0.0689, 0.0627, 0.0590, 0.0662, 0.0648, 0.0668, 0.0744, 0.0657,
        0.0688, 0.0671, 0.0763, 0.0683, 0.0613], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,734][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.1362, 0.0536, 0.0698, 0.0782, 0.0967, 0.0739, 0.0767, 0.0635, 0.0568,
        0.0645, 0.0598, 0.0591, 0.0611, 0.0501], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,735][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([1.6126e-01, 2.3888e-04, 4.2085e-05, 2.8358e-04, 4.9739e-05, 2.3225e-04,
        1.4359e-05, 3.2308e-05, 4.0308e-05, 3.0996e-06, 1.6242e-01, 5.4522e-01,
        4.2146e-02, 8.8016e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,736][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.1763, 0.0524, 0.0586, 0.0376, 0.0607, 0.0480, 0.0429, 0.0853, 0.0485,
        0.0647, 0.0779, 0.0894, 0.0929, 0.0648], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,738][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.0034, 0.1069, 0.0551, 0.0751, 0.0713, 0.0687, 0.0369, 0.0966, 0.1032,
        0.0214, 0.1582, 0.0306, 0.0674, 0.1052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,740][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.0798, 0.0636, 0.0715, 0.0538, 0.0792, 0.0768, 0.0643, 0.0674, 0.0608,
        0.0755, 0.0788, 0.0678, 0.0845, 0.0762], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,741][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.1470, 0.0682, 0.0632, 0.0559, 0.0726, 0.0505, 0.0558, 0.0705, 0.0581,
        0.0613, 0.0715, 0.0728, 0.1025, 0.0500], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,743][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0729, 0.0013, 0.0329, 0.0052, 0.0255, 0.0105, 0.0119, 0.2704, 0.0068,
        0.0526, 0.0924, 0.0630, 0.3495, 0.0051], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:17,745][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.6160, 0.0476, 0.0065, 0.0524, 0.0329, 0.0258, 0.0205, 0.0073, 0.0387,
        0.0045, 0.0244, 0.0335, 0.0059, 0.0701, 0.0140], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,746][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0173, 0.0567, 0.0819, 0.0688, 0.1101, 0.0714, 0.0686, 0.0723, 0.0482,
        0.0820, 0.0431, 0.0807, 0.0516, 0.0767, 0.0709], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,748][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0284, 0.0469, 0.0521, 0.0596, 0.0657, 0.0675, 0.0599, 0.0855, 0.0753,
        0.0760, 0.0804, 0.0793, 0.0762, 0.0831, 0.0642], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,750][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0685, 0.0709, 0.0669, 0.0638, 0.0628, 0.0648, 0.0624, 0.0660, 0.0784,
        0.0549, 0.0725, 0.0813, 0.0553, 0.0807, 0.0510], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,752][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.1369, 0.0652, 0.0585, 0.0548, 0.0612, 0.0594, 0.0609, 0.0689, 0.0618,
        0.0632, 0.0629, 0.0734, 0.0634, 0.0578, 0.0517], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,753][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.1166, 0.0519, 0.0603, 0.0706, 0.0836, 0.0719, 0.0749, 0.0613, 0.0548,
        0.0618, 0.0575, 0.0593, 0.0606, 0.0488, 0.0659], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,755][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([1.6776e-02, 4.3091e-05, 1.7102e-06, 1.3852e-04, 1.2236e-05, 1.9931e-03,
        1.6488e-04, 9.8872e-05, 8.1097e-04, 3.9052e-05, 2.8255e-01, 3.7555e-01,
        7.5700e-02, 2.4313e-01, 2.9883e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,756][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.1675, 0.0494, 0.0582, 0.0359, 0.0563, 0.0437, 0.0361, 0.0751, 0.0421,
        0.0551, 0.0722, 0.0844, 0.0852, 0.0623, 0.0765], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,758][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0025, 0.1071, 0.0330, 0.0844, 0.0290, 0.0570, 0.0220, 0.0890, 0.1451,
        0.0146, 0.2009, 0.0218, 0.0347, 0.1448, 0.0142], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,760][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0706, 0.0560, 0.0626, 0.0480, 0.0710, 0.0681, 0.0597, 0.0615, 0.0562,
        0.0725, 0.0730, 0.0640, 0.0815, 0.0710, 0.0842], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,761][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.1553, 0.0699, 0.0709, 0.0579, 0.0808, 0.0457, 0.0438, 0.0642, 0.0528,
        0.0519, 0.0616, 0.0571, 0.0775, 0.0422, 0.0682], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,763][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0612, 0.0026, 0.0103, 0.0130, 0.0197, 0.0190, 0.0191, 0.0489, 0.0178,
        0.2200, 0.1440, 0.0601, 0.2813, 0.0131, 0.0700], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:17,765][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.5561, 0.0529, 0.0074, 0.0652, 0.0151, 0.0421, 0.0360, 0.0115, 0.0412,
        0.0064, 0.0249, 0.0416, 0.0081, 0.0718, 0.0063, 0.0134],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,767][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0181, 0.0522, 0.0644, 0.0641, 0.1047, 0.0681, 0.0703, 0.0808, 0.0456,
        0.0828, 0.0376, 0.0833, 0.0433, 0.0716, 0.0670, 0.0462],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,769][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0282, 0.0447, 0.0503, 0.0549, 0.0622, 0.0622, 0.0560, 0.0808, 0.0696,
        0.0726, 0.0737, 0.0735, 0.0724, 0.0765, 0.0621, 0.0603],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,770][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0725, 0.0697, 0.0605, 0.0642, 0.0628, 0.0581, 0.0574, 0.0548, 0.0777,
        0.0496, 0.0689, 0.0768, 0.0557, 0.0764, 0.0499, 0.0450],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,772][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.1243, 0.0625, 0.0556, 0.0524, 0.0587, 0.0571, 0.0588, 0.0662, 0.0591,
        0.0611, 0.0603, 0.0694, 0.0610, 0.0553, 0.0505, 0.0478],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,774][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.1075, 0.0467, 0.0621, 0.0705, 0.0843, 0.0694, 0.0679, 0.0594, 0.0501,
        0.0587, 0.0536, 0.0530, 0.0564, 0.0453, 0.0620, 0.0532],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,775][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([2.1649e-02, 2.7892e-04, 1.8023e-05, 7.5449e-04, 4.3273e-05, 6.4321e-04,
        4.2182e-05, 2.4896e-05, 1.3105e-04, 1.4905e-05, 1.9185e-01, 5.0294e-01,
        1.1171e-01, 1.2882e-01, 1.9724e-03, 3.9108e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,776][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.1424, 0.0476, 0.0529, 0.0351, 0.0561, 0.0454, 0.0401, 0.0706, 0.0423,
        0.0581, 0.0677, 0.0757, 0.0817, 0.0586, 0.0725, 0.0531],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,777][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0040, 0.1035, 0.0362, 0.0847, 0.0430, 0.0784, 0.0191, 0.0875, 0.1199,
        0.0229, 0.1415, 0.0230, 0.0564, 0.1287, 0.0232, 0.0280],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,778][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0649, 0.0526, 0.0612, 0.0472, 0.0707, 0.0673, 0.0563, 0.0587, 0.0523,
        0.0646, 0.0672, 0.0582, 0.0744, 0.0680, 0.0780, 0.0585],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,780][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.1366, 0.0657, 0.0653, 0.0546, 0.0717, 0.0424, 0.0428, 0.0592, 0.0509,
        0.0484, 0.0580, 0.0550, 0.0734, 0.0399, 0.0661, 0.0701],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,781][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.1372, 0.0049, 0.0057, 0.0207, 0.0099, 0.0130, 0.0115, 0.1627, 0.0664,
        0.0510, 0.2807, 0.0758, 0.0961, 0.0309, 0.0303, 0.0030],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:17,783][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.4624, 0.0476, 0.0108, 0.0572, 0.0244, 0.0412, 0.0520, 0.0156, 0.0420,
        0.0087, 0.0298, 0.0521, 0.0124, 0.0727, 0.0112, 0.0093, 0.0508],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,785][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0212, 0.0537, 0.0603, 0.0615, 0.0964, 0.0610, 0.0588, 0.0712, 0.0431,
        0.0735, 0.0345, 0.0714, 0.0479, 0.0704, 0.0663, 0.0541, 0.0547],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,786][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0294, 0.0432, 0.0478, 0.0514, 0.0573, 0.0581, 0.0526, 0.0756, 0.0659,
        0.0669, 0.0690, 0.0701, 0.0655, 0.0697, 0.0562, 0.0558, 0.0655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,788][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0699, 0.0678, 0.0572, 0.0585, 0.0553, 0.0582, 0.0510, 0.0582, 0.0677,
        0.0468, 0.0622, 0.0714, 0.0524, 0.0729, 0.0457, 0.0504, 0.0542],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,790][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1108, 0.0586, 0.0533, 0.0504, 0.0559, 0.0548, 0.0562, 0.0625, 0.0562,
        0.0580, 0.0573, 0.0654, 0.0580, 0.0528, 0.0489, 0.0465, 0.0544],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,792][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1087, 0.0446, 0.0572, 0.0649, 0.0796, 0.0640, 0.0648, 0.0547, 0.0488,
        0.0546, 0.0515, 0.0519, 0.0539, 0.0431, 0.0591, 0.0513, 0.0474],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,793][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([4.3952e-02, 1.8297e-04, 2.0515e-05, 3.8158e-04, 6.8364e-05, 1.0218e-03,
        4.7388e-05, 6.4751e-05, 2.4593e-04, 1.6099e-05, 1.5451e-01, 4.7488e-01,
        7.8678e-02, 1.5675e-01, 3.1427e-03, 5.9578e-02, 2.6456e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,795][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1359, 0.0440, 0.0486, 0.0328, 0.0502, 0.0410, 0.0365, 0.0680, 0.0405,
        0.0536, 0.0637, 0.0719, 0.0750, 0.0544, 0.0650, 0.0514, 0.0676],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,797][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0035, 0.0915, 0.0322, 0.0793, 0.0603, 0.0671, 0.0136, 0.0937, 0.1157,
        0.0228, 0.1371, 0.0171, 0.0594, 0.1209, 0.0364, 0.0394, 0.0102],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,798][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0614, 0.0496, 0.0580, 0.0443, 0.0666, 0.0630, 0.0537, 0.0554, 0.0496,
        0.0612, 0.0636, 0.0553, 0.0710, 0.0646, 0.0732, 0.0553, 0.0540],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,800][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.1207, 0.0651, 0.0585, 0.0525, 0.0668, 0.0413, 0.0414, 0.0564, 0.0484,
        0.0463, 0.0546, 0.0528, 0.0722, 0.0406, 0.0641, 0.0664, 0.0519],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,802][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1210, 0.0051, 0.0114, 0.0178, 0.0163, 0.0142, 0.0009, 0.2165, 0.0209,
        0.0921, 0.0735, 0.0351, 0.2679, 0.0381, 0.0495, 0.0178, 0.0021],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:17,804][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.5579, 0.0483, 0.0069, 0.0588, 0.0165, 0.0329, 0.0263, 0.0127, 0.0356,
        0.0071, 0.0259, 0.0404, 0.0061, 0.0753, 0.0070, 0.0063, 0.0278, 0.0082],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,806][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0151, 0.0502, 0.0646, 0.0568, 0.0930, 0.0561, 0.0557, 0.0680, 0.0392,
        0.0617, 0.0334, 0.0669, 0.0425, 0.0654, 0.0575, 0.0517, 0.0483, 0.0739],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,807][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0253, 0.0402, 0.0434, 0.0493, 0.0530, 0.0545, 0.0496, 0.0690, 0.0612,
        0.0619, 0.0656, 0.0640, 0.0613, 0.0671, 0.0514, 0.0539, 0.0602, 0.0692],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,809][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0619, 0.0610, 0.0573, 0.0555, 0.0550, 0.0516, 0.0529, 0.0483, 0.0662,
        0.0392, 0.0633, 0.0709, 0.0430, 0.0686, 0.0440, 0.0485, 0.0552, 0.0575],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,811][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.1055, 0.0559, 0.0507, 0.0479, 0.0528, 0.0516, 0.0527, 0.0588, 0.0534,
        0.0547, 0.0542, 0.0620, 0.0547, 0.0505, 0.0463, 0.0442, 0.0519, 0.0522],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,813][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.1017, 0.0439, 0.0531, 0.0624, 0.0748, 0.0670, 0.0634, 0.0558, 0.0451,
        0.0522, 0.0476, 0.0474, 0.0516, 0.0390, 0.0534, 0.0486, 0.0452, 0.0479],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,814][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([2.9508e-05, 2.0905e-07, 5.5258e-09, 2.3650e-07, 1.1469e-08, 1.5726e-06,
        5.5245e-08, 1.4593e-07, 6.8801e-07, 3.1407e-08, 9.0778e-02, 5.0350e-01,
        1.0269e-01, 1.1187e-01, 2.0647e-04, 1.7354e-01, 1.7343e-02, 4.2283e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,816][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.1277, 0.0413, 0.0460, 0.0304, 0.0466, 0.0367, 0.0323, 0.0614, 0.0365,
        0.0485, 0.0594, 0.0672, 0.0695, 0.0511, 0.0622, 0.0497, 0.0631, 0.0706],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,817][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0030, 0.0931, 0.0337, 0.0925, 0.0392, 0.0468, 0.0193, 0.0735, 0.1287,
        0.0113, 0.1697, 0.0206, 0.0360, 0.1214, 0.0208, 0.0604, 0.0135, 0.0163],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,818][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0666, 0.0493, 0.0508, 0.0365, 0.0583, 0.0551, 0.0468, 0.0489, 0.0432,
        0.0549, 0.0571, 0.0498, 0.0675, 0.0577, 0.0681, 0.0529, 0.0512, 0.0852],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,819][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.1343, 0.0581, 0.0564, 0.0487, 0.0603, 0.0367, 0.0362, 0.0481, 0.0441,
        0.0404, 0.0515, 0.0498, 0.0633, 0.0377, 0.0568, 0.0621, 0.0509, 0.0646],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,820][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0506, 0.0019, 0.0223, 0.0211, 0.0130, 0.0107, 0.0051, 0.1233, 0.0281,
        0.1938, 0.0668, 0.0508, 0.0579, 0.0149, 0.0411, 0.2395, 0.0112, 0.0478],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:17,821][circuit_model.py][line:2332][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.3922, 0.0539, 0.0123, 0.0623, 0.0235, 0.0463, 0.0375, 0.0155, 0.0462,
        0.0096, 0.0346, 0.0537, 0.0121, 0.0783, 0.0118, 0.0127, 0.0373, 0.0057,
        0.0544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,823][circuit_model.py][line:2335][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0185, 0.0452, 0.0550, 0.0532, 0.0825, 0.0561, 0.0525, 0.0612, 0.0400,
        0.0611, 0.0323, 0.0633, 0.0418, 0.0609, 0.0571, 0.0471, 0.0501, 0.0794,
        0.0429], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,825][circuit_model.py][line:2338][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0268, 0.0382, 0.0417, 0.0442, 0.0492, 0.0497, 0.0454, 0.0655, 0.0577,
        0.0580, 0.0599, 0.0614, 0.0561, 0.0602, 0.0480, 0.0478, 0.0569, 0.0656,
        0.0678], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,827][circuit_model.py][line:2341][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0664, 0.0599, 0.0508, 0.0500, 0.0482, 0.0521, 0.0495, 0.0491, 0.0617,
        0.0444, 0.0512, 0.0662, 0.0450, 0.0656, 0.0402, 0.0447, 0.0516, 0.0543,
        0.0492], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,828][circuit_model.py][line:2344][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0970, 0.0531, 0.0485, 0.0459, 0.0506, 0.0495, 0.0507, 0.0560, 0.0506,
        0.0521, 0.0514, 0.0586, 0.0522, 0.0479, 0.0446, 0.0425, 0.0494, 0.0498,
        0.0498], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,830][circuit_model.py][line:2347][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0935, 0.0430, 0.0544, 0.0603, 0.0709, 0.0593, 0.0581, 0.0510, 0.0436,
        0.0505, 0.0476, 0.0455, 0.0477, 0.0393, 0.0515, 0.0457, 0.0421, 0.0504,
        0.0455], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,831][circuit_model.py][line:2350][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([7.3022e-03, 4.7513e-05, 3.1403e-06, 1.2453e-04, 1.6518e-05, 1.5459e-04,
        1.0006e-05, 9.1293e-06, 3.2942e-05, 1.7604e-06, 1.3234e-01, 6.3236e-01,
        7.6333e-02, 7.9383e-02, 1.4024e-03, 3.3896e-02, 1.4358e-02, 3.3095e-06,
        2.2221e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,833][circuit_model.py][line:2353][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1285, 0.0388, 0.0417, 0.0278, 0.0420, 0.0344, 0.0310, 0.0593, 0.0355,
        0.0458, 0.0553, 0.0633, 0.0640, 0.0469, 0.0540, 0.0440, 0.0597, 0.0662,
        0.0618], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,835][circuit_model.py][line:2356][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0019, 0.0969, 0.0285, 0.0746, 0.0473, 0.0841, 0.0210, 0.0754, 0.0966,
        0.0180, 0.0918, 0.0237, 0.0672, 0.1103, 0.0263, 0.0423, 0.0151, 0.0472,
        0.0319], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,837][circuit_model.py][line:2359][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0545, 0.0436, 0.0511, 0.0394, 0.0556, 0.0554, 0.0475, 0.0494, 0.0450,
        0.0542, 0.0560, 0.0495, 0.0627, 0.0561, 0.0608, 0.0483, 0.0476, 0.0728,
        0.0505], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,839][circuit_model.py][line:2362][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.1025, 0.0567, 0.0496, 0.0454, 0.0576, 0.0375, 0.0385, 0.0491, 0.0429,
        0.0426, 0.0490, 0.0486, 0.0667, 0.0374, 0.0583, 0.0589, 0.0467, 0.0590,
        0.0529], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,841][circuit_model.py][line:2365][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0365, 0.0010, 0.0047, 0.0079, 0.0051, 0.0032, 0.0032, 0.0890, 0.0046,
        0.2024, 0.0455, 0.0188, 0.0890, 0.0134, 0.0184, 0.0234, 0.0099, 0.4212,
        0.0026], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:17,844][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:17,846][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 7876],
        [11622],
        [ 2811],
        [ 8783],
        [ 1933],
        [ 1238],
        [ 4710],
        [ 4218],
        [ 6527],
        [ 9791],
        [ 4755],
        [14070],
        [27497],
        [ 6733],
        [11733],
        [ 5889],
        [10597],
        [13168],
        [ 2723]], device='cuda:0')
[2024-07-24 10:25:17,848][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 7676],
        [ 1628],
        [ 3385],
        [ 3939],
        [ 2780],
        [ 4730],
        [ 5826],
        [ 4728],
        [ 5056],
        [ 7564],
        [ 2239],
        [19296],
        [22898],
        [ 3563],
        [ 8580],
        [ 6147],
        [ 7940],
        [14203],
        [ 6007]], device='cuda:0')
[2024-07-24 10:25:17,850][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[19906],
        [ 3599],
        [ 3305],
        [16360],
        [ 3955],
        [ 3660],
        [11911],
        [ 5445],
        [11955],
        [ 7514],
        [21246],
        [19176],
        [16903],
        [13386],
        [18032],
        [14124],
        [16022],
        [ 9255],
        [13855]], device='cuda:0')
[2024-07-24 10:25:17,852][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[24333],
        [ 7222],
        [ 2661],
        [ 2122],
        [ 3947],
        [ 2518],
        [ 2000],
        [ 2792],
        [ 3198],
        [ 4064],
        [ 3351],
        [ 3385],
        [ 6760],
        [ 3240],
        [ 7326],
        [ 6529],
        [ 4267],
        [ 5166],
        [ 5353]], device='cuda:0')
[2024-07-24 10:25:17,854][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[24478],
        [37266],
        [33021],
        [32871],
        [31501],
        [33232],
        [34501],
        [36684],
        [36562],
        [36974],
        [38049],
        [37975],
        [37763],
        [37106],
        [36228],
        [36800],
        [36467],
        [32606],
        [36025]], device='cuda:0')
[2024-07-24 10:25:17,856][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[28118],
        [47445],
        [44969],
        [39871],
        [38299],
        [37834],
        [38041],
        [32330],
        [32308],
        [32321],
        [35223],
        [34060],
        [31495],
        [30053],
        [ 5590],
        [ 5370],
        [13010],
        [14040],
        [15446]], device='cuda:0')
[2024-07-24 10:25:17,858][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[12219],
        [14336],
        [22683],
        [12822],
        [21749],
        [14679],
        [17181],
        [22647],
        [13142],
        [29201],
        [24342],
        [21823],
        [30523],
        [13128],
        [30372],
        [13699],
        [28999],
        [30872],
        [17095]], device='cuda:0')
[2024-07-24 10:25:17,860][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[28695],
        [23519],
        [24624],
        [23129],
        [22749],
        [21612],
        [20470],
        [20305],
        [19618],
        [19300],
        [18460],
        [18101],
        [17651],
        [17703],
        [16394],
        [17177],
        [17268],
        [16451],
        [17642]], device='cuda:0')
[2024-07-24 10:25:17,861][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[40868],
        [32818],
        [32958],
        [32861],
        [30151],
        [30137],
        [30707],
        [32423],
        [32198],
        [31776],
        [31593],
        [31074],
        [30981],
        [30614],
        [30494],
        [30009],
        [30739],
        [30917],
        [30562]], device='cuda:0')
[2024-07-24 10:25:17,863][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[42041],
        [ 3628],
        [ 3547],
        [ 3501],
        [ 3442],
        [ 3538],
        [ 3544],
        [ 3527],
        [ 3504],
        [ 3539],
        [ 3495],
        [ 3480],
        [ 3477],
        [ 3434],
        [ 3381],
        [ 3499],
        [ 3475],
        [ 3309],
        [ 3225]], device='cuda:0')
[2024-07-24 10:25:17,865][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[13975],
        [ 3419],
        [10372],
        [11162],
        [11058],
        [12004],
        [ 5084],
        [ 8240],
        [ 8285],
        [ 8304],
        [ 8271],
        [ 7311],
        [17601],
        [17592],
        [17574],
        [23282],
        [24354],
        [23432],
        [16168]], device='cuda:0')
[2024-07-24 10:25:17,866][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[20735],
        [19771],
        [17645],
        [19877],
        [19999],
        [ 9948],
        [ 4345],
        [22994],
        [16635],
        [12375],
        [10898],
        [11196],
        [22334],
        [14733],
        [21404],
        [11807],
        [14586],
        [14884],
        [14759]], device='cuda:0')
[2024-07-24 10:25:17,868][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[14554],
        [15374],
        [15126],
        [15705],
        [16623],
        [16880],
        [17102],
        [16835],
        [16674],
        [16758],
        [16601],
        [16740],
        [16806],
        [16447],
        [16099],
        [16322],
        [16040],
        [15455],
        [15153]], device='cuda:0')
[2024-07-24 10:25:17,870][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[ 9545],
        [ 9525],
        [10301],
        [14108],
        [11182],
        [13719],
        [14466],
        [12353],
        [14813],
        [13892],
        [13278],
        [12807],
        [12977],
        [12796],
        [12432],
        [13737],
        [14510],
        [10815],
        [13373]], device='cuda:0')
[2024-07-24 10:25:17,872][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[20309],
        [48592],
        [37069],
        [47219],
        [39987],
        [34010],
        [44807],
        [25536],
        [37741],
        [40223],
        [35430],
        [28859],
        [33672],
        [42753],
        [34887],
        [35834],
        [36241],
        [13313],
        [ 8994]], device='cuda:0')
[2024-07-24 10:25:17,873][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[15907],
        [17301],
        [17040],
        [19071],
        [18491],
        [19349],
        [19422],
        [19382],
        [20221],
        [19534],
        [20526],
        [20368],
        [19728],
        [21157],
        [20104],
        [20382],
        [20740],
        [20326],
        [21041]], device='cuda:0')
[2024-07-24 10:25:17,876][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[3514],
        [4211],
        [6273],
        [4818],
        [4927],
        [5518],
        [5991],
        [6631],
        [6750],
        [6508],
        [6317],
        [6446],
        [6958],
        [6951],
        [6809],
        [6586],
        [6643],
        [6578],
        [6451]], device='cuda:0')
[2024-07-24 10:25:17,878][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[31622],
        [29850],
        [30862],
        [29894],
        [30974],
        [31577],
        [31893],
        [32228],
        [31918],
        [32157],
        [31685],
        [31459],
        [31713],
        [31704],
        [31745],
        [31548],
        [31425],
        [31717],
        [31712]], device='cuda:0')
[2024-07-24 10:25:17,879][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[40491],
        [36796],
        [36956],
        [36197],
        [34817],
        [35242],
        [35168],
        [34610],
        [34926],
        [34243],
        [34817],
        [34605],
        [34904],
        [34879],
        [34640],
        [34783],
        [34809],
        [34074],
        [34483]], device='cuda:0')
[2024-07-24 10:25:17,881][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[36665],
        [36019],
        [36080],
        [36064],
        [35845],
        [35505],
        [35275],
        [35146],
        [35176],
        [34978],
        [35037],
        [35171],
        [35058],
        [35199],
        [35239],
        [35233],
        [35297],
        [35397],
        [35473]], device='cuda:0')
[2024-07-24 10:25:17,883][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 8669],
        [32886],
        [30765],
        [24869],
        [22563],
        [20523],
        [20286],
        [19379],
        [20498],
        [19837],
        [20444],
        [21326],
        [20762],
        [21679],
        [22773],
        [22829],
        [23558],
        [22687],
        [23352]], device='cuda:0')
[2024-07-24 10:25:17,885][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 1726],
        [37922],
        [41219],
        [18800],
        [23733],
        [ 1747],
        [ 7709],
        [10700],
        [ 5470],
        [22453],
        [ 3639],
        [17015],
        [16731],
        [16020],
        [16915],
        [16865],
        [17381],
        [18135],
        [17856]], device='cuda:0')
[2024-07-24 10:25:17,887][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[14924],
        [14677],
        [14612],
        [14853],
        [14441],
        [14678],
        [14772],
        [14580],
        [14579],
        [14533],
        [14427],
        [14317],
        [14303],
        [14422],
        [14375],
        [14257],
        [14309],
        [14274],
        [14341]], device='cuda:0')
[2024-07-24 10:25:17,889][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25688],
        [22200],
        [24575],
        [26717],
        [25525],
        [25743],
        [25514],
        [25571],
        [25739],
        [26048],
        [26556],
        [26895],
        [27056],
        [26219],
        [25617],
        [26216],
        [26362],
        [26370],
        [25897]], device='cuda:0')
[2024-07-24 10:25:17,891][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[11095],
        [ 9557],
        [12797],
        [14199],
        [15388],
        [16098],
        [16841],
        [17171],
        [17908],
        [18393],
        [19156],
        [19486],
        [19142],
        [19151],
        [19179],
        [18993],
        [18840],
        [18667],
        [18647]], device='cuda:0')
[2024-07-24 10:25:17,893][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[39461],
        [35766],
        [38566],
        [37231],
        [39424],
        [40309],
        [39863],
        [40595],
        [40227],
        [39676],
        [40080],
        [40046],
        [39315],
        [39348],
        [39278],
        [39392],
        [39353],
        [40198],
        [39774]], device='cuda:0')
[2024-07-24 10:25:17,895][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 3305],
        [ 3461],
        [ 3419],
        [ 3372],
        [ 4234],
        [ 3496],
        [ 3704],
        [ 3785],
        [ 1977],
        [ 4808],
        [ 3080],
        [ 8245],
        [ 7593],
        [ 8074],
        [11992],
        [ 5375],
        [ 7649],
        [ 8506],
        [ 8766]], device='cuda:0')
[2024-07-24 10:25:17,897][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[27907],
        [15542],
        [14366],
        [17818],
        [17265],
        [25403],
        [20152],
        [19527],
        [20867],
        [16072],
        [21922],
        [15293],
        [15887],
        [14789],
        [13845],
        [16875],
        [14902],
        [14788],
        [14511]], device='cuda:0')
[2024-07-24 10:25:17,899][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[10675],
        [ 3080],
        [ 7513],
        [ 3179],
        [ 5289],
        [ 9863],
        [ 2513],
        [ 9963],
        [ 3923],
        [ 2339],
        [ 5674],
        [ 3461],
        [ 3107],
        [ 1478],
        [ 2988],
        [ 3185],
        [ 2807],
        [10595],
        [11522]], device='cuda:0')
[2024-07-24 10:25:17,901][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241],
        [35241]], device='cuda:0')
[2024-07-24 10:25:17,948][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:17,950][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,951][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,952][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,953][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,954][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,955][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,955][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,956][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,957][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,957][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,958][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,959][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:17,959][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.6726, 0.3274], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,960][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.7947, 0.2053], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,961][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.5450, 0.4550], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,961][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.7140, 0.2860], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,962][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.4508, 0.5492], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,963][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1904, 0.8096], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,964][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.7208, 0.2792], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,966][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.2078, 0.7922], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,968][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.7104, 0.2896], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,968][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.8393, 0.1607], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,969][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.9864, 0.0136], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,970][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.9573, 0.0427], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:17,971][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.0879, 0.8584, 0.0537], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,971][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.5971, 0.1948, 0.2081], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,973][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.4474, 0.3054, 0.2472], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,974][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.5860, 0.2131, 0.2008], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,976][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.2711, 0.4881, 0.2408], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,978][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.0674, 0.5372, 0.3954], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,979][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.5740, 0.2224, 0.2036], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,981][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.1246, 0.3659, 0.5095], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,982][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.6021, 0.2708, 0.1270], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,984][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.7364, 0.1331, 0.1306], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,986][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.6142, 0.0190, 0.3669], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,987][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.0129, 0.9442, 0.0429], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:17,989][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4957, 0.3947, 0.0307, 0.0789], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,991][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.6757, 0.1312, 0.1690, 0.0241], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,992][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.2288, 0.2445, 0.2443, 0.2824], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,994][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.4478, 0.1989, 0.1948, 0.1585], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,995][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.2480, 0.2580, 0.1749, 0.3191], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,997][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0835, 0.2386, 0.3003, 0.3775], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:17,999][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.4779, 0.1947, 0.1965, 0.1309], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,000][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0130, 0.1244, 0.0180, 0.8446], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,002][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.4498, 0.1867, 0.1448, 0.2187], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,003][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6254, 0.1350, 0.1326, 0.1070], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,005][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.8366, 0.0118, 0.0643, 0.0873], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,006][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ and] are: tensor([9.6297e-13, 3.0309e-04, 9.9970e-01, 4.7643e-10], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,008][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0860, 0.5808, 0.0833, 0.2149, 0.0351], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,010][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.5662, 0.1539, 0.1765, 0.0270, 0.0764], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,011][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.2519, 0.1796, 0.1448, 0.2239, 0.1998], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,013][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.3959, 0.1638, 0.1621, 0.1302, 0.1480], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,014][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.1576, 0.2417, 0.1265, 0.2855, 0.1887], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,014][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0491, 0.1965, 0.1384, 0.2436, 0.3725], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,015][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.4213, 0.1748, 0.1608, 0.1158, 0.1273], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,016][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0382, 0.1691, 0.2167, 0.3022, 0.2738], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,017][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.3461, 0.1489, 0.1597, 0.1780, 0.1674], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,019][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.5916, 0.1144, 0.1129, 0.0903, 0.0909], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,021][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.1525, 0.0058, 0.1140, 0.0043, 0.7234], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,022][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([3.5971e-07, 4.3886e-02, 9.5611e-01, 8.7005e-08, 1.0925e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,023][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.0339, 0.5800, 0.0337, 0.2877, 0.0169, 0.0478], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,025][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.6685, 0.1099, 0.1406, 0.0200, 0.0559, 0.0051], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,026][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.2257, 0.1407, 0.1082, 0.1924, 0.1690, 0.1641], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,028][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.3569, 0.1359, 0.1347, 0.1084, 0.1259, 0.1383], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,030][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.1653, 0.1699, 0.0981, 0.1997, 0.1506, 0.2163], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,031][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0219, 0.1888, 0.2060, 0.2542, 0.3045, 0.0246], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,033][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.3757, 0.1629, 0.1477, 0.1075, 0.1175, 0.0887], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,035][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.0083, 0.1514, 0.0356, 0.4760, 0.0283, 0.3005], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,036][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.3180, 0.1075, 0.1407, 0.1484, 0.1881, 0.0972], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,038][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.4910, 0.1143, 0.1108, 0.0893, 0.0904, 0.1042], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,039][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.2997, 0.0074, 0.0711, 0.0101, 0.0524, 0.5593], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,041][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ had] are: tensor([1.1133e-08, 2.2806e-04, 9.9927e-01, 1.5110e-09, 5.0136e-04, 3.3182e-08],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,042][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1842, 0.4158, 0.0103, 0.1292, 0.0053, 0.2471, 0.0081],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,044][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.5616, 0.1140, 0.1750, 0.0326, 0.0961, 0.0140, 0.0066],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,046][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.2253, 0.1098, 0.0897, 0.1484, 0.1445, 0.1333, 0.1491],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,047][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.3001, 0.1223, 0.1202, 0.0994, 0.1147, 0.1291, 0.1142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,049][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1243, 0.1599, 0.0868, 0.1814, 0.1254, 0.1564, 0.1658],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,051][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0280, 0.1638, 0.1662, 0.2066, 0.3755, 0.0266, 0.0332],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,052][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.3324, 0.1497, 0.1356, 0.1024, 0.1091, 0.0843, 0.0865],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,054][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0152, 0.1061, 0.0270, 0.2475, 0.0489, 0.1100, 0.4452],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,055][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2913, 0.0933, 0.0907, 0.1125, 0.1318, 0.1036, 0.1769],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,057][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.4981, 0.0967, 0.0937, 0.0769, 0.0779, 0.0911, 0.0655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,058][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.3939, 0.0014, 0.0725, 0.0039, 0.3836, 0.1362, 0.0085],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,059][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([4.5352e-05, 2.9945e-03, 9.9647e-01, 3.4761e-08, 3.6301e-04, 3.3145e-06,
        1.2362e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,060][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0527, 0.3371, 0.0333, 0.1837, 0.0075, 0.3513, 0.0171, 0.0173],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,060][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.3348, 0.1923, 0.2382, 0.0432, 0.1240, 0.0139, 0.0104, 0.0431],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,062][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.1660, 0.1046, 0.0856, 0.1342, 0.1295, 0.1213, 0.1358, 0.1231],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,063][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.2694, 0.1069, 0.1040, 0.0862, 0.0991, 0.1129, 0.1009, 0.1206],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,065][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.1061, 0.1324, 0.0732, 0.1485, 0.1057, 0.1318, 0.1386, 0.1637],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,066][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0420, 0.1785, 0.1998, 0.2174, 0.2312, 0.0416, 0.0492, 0.0402],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,068][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.2965, 0.1285, 0.1169, 0.0870, 0.0948, 0.0736, 0.0763, 0.1264],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,070][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.0055, 0.0498, 0.0163, 0.1552, 0.0180, 0.1565, 0.2116, 0.3872],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,071][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.2115, 0.0859, 0.0793, 0.0966, 0.0976, 0.0950, 0.1529, 0.1812],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,073][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.5026, 0.0815, 0.0794, 0.0650, 0.0640, 0.0817, 0.0571, 0.0687],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,075][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.2297, 0.0033, 0.0093, 0.0175, 0.2986, 0.1096, 0.0290, 0.3030],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,076][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([3.4433e-03, 6.9298e-10, 4.2772e-08, 2.5875e-16, 2.9020e-12, 3.1170e-13,
        4.2299e-08, 9.9656e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,078][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.4735, 0.1866, 0.0301, 0.0720, 0.0427, 0.1049, 0.0252, 0.0375, 0.0275],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,079][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.6571, 0.0768, 0.1417, 0.0210, 0.0697, 0.0088, 0.0045, 0.0174, 0.0030],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,081][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1277, 0.0944, 0.0965, 0.1148, 0.1272, 0.1157, 0.1244, 0.1078, 0.0916],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,083][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.2314, 0.0969, 0.0967, 0.0787, 0.0918, 0.1011, 0.0917, 0.1098, 0.1019],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,084][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.1081, 0.0942, 0.0638, 0.1062, 0.0900, 0.1175, 0.1124, 0.1378, 0.1700],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,086][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0250, 0.1032, 0.1988, 0.1538, 0.3609, 0.0407, 0.0549, 0.0205, 0.0422],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,088][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2562, 0.1185, 0.1126, 0.0823, 0.0911, 0.0720, 0.0728, 0.1162, 0.0784],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,090][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0120, 0.0377, 0.0134, 0.0798, 0.0243, 0.0345, 0.0654, 0.1519, 0.5810],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,091][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.1804, 0.0790, 0.0687, 0.0900, 0.1110, 0.0727, 0.1468, 0.1465, 0.1049],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,093][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.4999, 0.0751, 0.0718, 0.0607, 0.0575, 0.0753, 0.0515, 0.0615, 0.0467],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,095][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5255, 0.0036, 0.0162, 0.0186, 0.2694, 0.0696, 0.0077, 0.0656, 0.0238],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,096][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ of] are: tensor([5.0616e-11, 5.4928e-13, 7.0846e-10, 4.3782e-18, 1.3826e-12, 2.1322e-15,
        3.1414e-12, 1.0000e+00, 6.0442e-09], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,098][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.1027, 0.1466, 0.0339, 0.1054, 0.0440, 0.1923, 0.0235, 0.0334, 0.3132,
        0.0050], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,099][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.4356, 0.1198, 0.2052, 0.0408, 0.1226, 0.0210, 0.0106, 0.0326, 0.0075,
        0.0043], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,101][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.1420, 0.0827, 0.0726, 0.1070, 0.1086, 0.1003, 0.1099, 0.1000, 0.0864,
        0.0903], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,102][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.2143, 0.0877, 0.0874, 0.0713, 0.0828, 0.0953, 0.0835, 0.0998, 0.0950,
        0.0830], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,103][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0703, 0.0922, 0.0566, 0.1019, 0.0779, 0.0895, 0.0973, 0.1152, 0.1420,
        0.1571], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,104][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0249, 0.1092, 0.1279, 0.1140, 0.5091, 0.0174, 0.0272, 0.0123, 0.0287,
        0.0292], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,105][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.2362, 0.1113, 0.0999, 0.0774, 0.0823, 0.0651, 0.0671, 0.1072, 0.0750,
        0.0785], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,106][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.0048, 0.0199, 0.0071, 0.0504, 0.0163, 0.0488, 0.0609, 0.1978, 0.5252,
        0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,108][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.1662, 0.0739, 0.0518, 0.0868, 0.0827, 0.0725, 0.1328, 0.1573, 0.0997,
        0.0764], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,109][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.4632, 0.0721, 0.0697, 0.0590, 0.0565, 0.0727, 0.0503, 0.0611, 0.0464,
        0.0490], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,111][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.1101, 0.0058, 0.0762, 0.0403, 0.4151, 0.0148, 0.0092, 0.0293, 0.0829,
        0.2163], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,112][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([4.2952e-10, 7.1877e-11, 1.6423e-08, 1.4572e-15, 5.4694e-12, 1.2053e-13,
        9.8426e-11, 1.0000e+00, 1.3669e-06, 5.1391e-09], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,114][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.1629, 0.1813, 0.0062, 0.0445, 0.0061, 0.1386, 0.0667, 0.1051, 0.1691,
        0.0923, 0.0272], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,115][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.7475, 0.0491, 0.1164, 0.0085, 0.0418, 0.0020, 0.0019, 0.0110, 0.0012,
        0.0009, 0.0197], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,117][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.1249, 0.0738, 0.0771, 0.0921, 0.1052, 0.0937, 0.1007, 0.0899, 0.0751,
        0.0864, 0.0810], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,119][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.2078, 0.0791, 0.0781, 0.0639, 0.0732, 0.0829, 0.0748, 0.0891, 0.0831,
        0.0741, 0.0939], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,121][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0760, 0.0764, 0.0445, 0.0827, 0.0643, 0.0902, 0.0833, 0.1021, 0.1353,
        0.1369, 0.1083], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,122][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0270, 0.0872, 0.1839, 0.1403, 0.3187, 0.0329, 0.0499, 0.0162, 0.0470,
        0.0433, 0.0535], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,124][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.2277, 0.0983, 0.0901, 0.0673, 0.0736, 0.0578, 0.0601, 0.0988, 0.0670,
        0.0723, 0.0867], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,126][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0061, 0.0112, 0.0130, 0.0580, 0.0147, 0.0283, 0.0860, 0.0783, 0.3693,
        0.0512, 0.2839], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,127][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.1505, 0.0667, 0.0409, 0.0705, 0.0700, 0.0665, 0.1292, 0.1377, 0.0976,
        0.0826, 0.0878], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,129][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.4607, 0.0647, 0.0625, 0.0538, 0.0501, 0.0683, 0.0465, 0.0558, 0.0438,
        0.0454, 0.0484], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,131][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.1831, 0.0147, 0.0847, 0.0168, 0.1932, 0.0096, 0.0071, 0.0372, 0.0475,
        0.2038, 0.2023], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,132][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ at] are: tensor([5.1817e-07, 7.2092e-13, 1.6643e-10, 4.7004e-18, 1.0279e-13, 1.3990e-15,
        5.3407e-12, 9.9650e-01, 6.1015e-08, 2.8057e-07, 3.4944e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,134][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2375, 0.1327, 0.0097, 0.0476, 0.0046, 0.1298, 0.0045, 0.0521, 0.1710,
        0.0157, 0.1883, 0.0065], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,136][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.7016, 0.0542, 0.1123, 0.0087, 0.0435, 0.0020, 0.0020, 0.0121, 0.0011,
        0.0009, 0.0252, 0.0364], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,137][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0899, 0.0731, 0.0746, 0.0901, 0.0983, 0.0891, 0.0949, 0.0822, 0.0685,
        0.0757, 0.0729, 0.0907], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,139][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1786, 0.0694, 0.0687, 0.0574, 0.0645, 0.0749, 0.0680, 0.0834, 0.0774,
        0.0692, 0.0891, 0.0995], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,141][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0679, 0.0764, 0.0425, 0.0778, 0.0586, 0.0764, 0.0741, 0.0884, 0.1195,
        0.1201, 0.0964, 0.1019], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,142][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0199, 0.0860, 0.1528, 0.1070, 0.3174, 0.0421, 0.0462, 0.0228, 0.0501,
        0.0494, 0.0510, 0.0553], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,144][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2027, 0.0909, 0.0865, 0.0636, 0.0698, 0.0539, 0.0552, 0.0898, 0.0596,
        0.0644, 0.0785, 0.0851], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,146][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0044, 0.0107, 0.0095, 0.0146, 0.0231, 0.0191, 0.0803, 0.1828, 0.1176,
        0.0365, 0.0983, 0.4031], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,147][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1328, 0.0615, 0.0577, 0.0688, 0.0669, 0.0634, 0.0995, 0.1122, 0.0836,
        0.0859, 0.0784, 0.0893], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,148][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4470, 0.0576, 0.0563, 0.0502, 0.0485, 0.0633, 0.0427, 0.0532, 0.0413,
        0.0433, 0.0462, 0.0504], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,149][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([3.4451e-01, 2.3315e-04, 1.1026e-02, 2.0539e-03, 7.0240e-02, 2.9224e-01,
        4.7135e-03, 8.2788e-02, 6.8934e-03, 1.3653e-01, 4.1883e-02, 6.8867e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,149][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([5.2175e-07, 1.5298e-12, 1.5567e-10, 9.2418e-18, 7.4131e-15, 2.0888e-15,
        2.9839e-12, 5.5615e-01, 8.2428e-07, 6.2165e-09, 4.0526e-01, 3.8592e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,151][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0406, 0.0645, 0.0405, 0.0488, 0.0537, 0.0409, 0.0160, 0.0289, 0.1949,
        0.0097, 0.4145, 0.0176, 0.0295], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,153][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.5083, 0.0873, 0.1565, 0.0148, 0.0645, 0.0041, 0.0029, 0.0159, 0.0017,
        0.0011, 0.0501, 0.0706, 0.0223], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,154][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.1054, 0.0637, 0.0538, 0.0869, 0.0831, 0.0776, 0.0863, 0.0744, 0.0668,
        0.0663, 0.0689, 0.0884, 0.0786], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,156][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.1607, 0.0641, 0.0636, 0.0521, 0.0595, 0.0685, 0.0619, 0.0743, 0.0700,
        0.0626, 0.0807, 0.0913, 0.0907], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,158][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0540, 0.0656, 0.0383, 0.0693, 0.0532, 0.0663, 0.0684, 0.0806, 0.1048,
        0.1099, 0.0838, 0.0906, 0.1151], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,159][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0296, 0.1184, 0.1395, 0.1092, 0.3167, 0.0327, 0.0402, 0.0166, 0.0440,
        0.0338, 0.0372, 0.0491, 0.0331], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,161][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.1917, 0.0840, 0.0770, 0.0570, 0.0623, 0.0483, 0.0503, 0.0827, 0.0557,
        0.0595, 0.0729, 0.0804, 0.0781], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,163][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.0040, 0.0061, 0.0096, 0.0186, 0.0247, 0.0172, 0.0708, 0.0619, 0.2753,
        0.0357, 0.1667, 0.2123, 0.0971], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,165][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.1255, 0.0552, 0.0509, 0.0567, 0.0800, 0.0461, 0.0832, 0.0921, 0.0681,
        0.0691, 0.0705, 0.0885, 0.1139], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,167][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.4147, 0.0572, 0.0558, 0.0471, 0.0463, 0.0583, 0.0403, 0.0505, 0.0399,
        0.0424, 0.0443, 0.0492, 0.0541], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,168][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ house] are: tensor([2.2846e-02, 1.3900e-03, 1.9856e-03, 9.4864e-04, 6.7212e-01, 2.4763e-03,
        3.8313e-03, 1.9064e-02, 5.6793e-04, 9.4563e-02, 2.3129e-03, 2.1882e-03,
        1.7571e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,169][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ house] are: tensor([7.9848e-07, 9.8921e-13, 2.2465e-10, 5.7030e-18, 3.2762e-14, 3.7008e-16,
        1.2145e-11, 2.4024e-01, 8.0223e-08, 2.9800e-09, 9.1238e-02, 6.6844e-01,
        8.0087e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,171][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.2209, 0.0564, 0.0159, 0.0254, 0.0094, 0.0423, 0.0481, 0.0910, 0.1191,
        0.0404, 0.1553, 0.0568, 0.0767, 0.0423], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,172][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.6322, 0.0699, 0.1182, 0.0090, 0.0356, 0.0015, 0.0013, 0.0094, 0.0008,
        0.0007, 0.0275, 0.0478, 0.0170, 0.0291], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,174][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.0673, 0.0606, 0.0711, 0.0691, 0.0852, 0.0781, 0.0818, 0.0732, 0.0559,
        0.0691, 0.0621, 0.0749, 0.0840, 0.0677], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,176][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.1440, 0.0591, 0.0589, 0.0486, 0.0556, 0.0649, 0.0581, 0.0705, 0.0658,
        0.0576, 0.0755, 0.0834, 0.0837, 0.0743], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,178][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0688, 0.0501, 0.0410, 0.0543, 0.0512, 0.0611, 0.0607, 0.0748, 0.0790,
        0.0930, 0.0772, 0.0784, 0.0987, 0.1118], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,179][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0124, 0.0644, 0.1393, 0.1014, 0.3399, 0.0409, 0.0523, 0.0147, 0.0475,
        0.0398, 0.0459, 0.0421, 0.0274, 0.0319], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,181][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.1816, 0.0757, 0.0767, 0.0521, 0.0609, 0.0478, 0.0478, 0.0779, 0.0502,
        0.0556, 0.0674, 0.0727, 0.0731, 0.0605], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,183][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0030, 0.0186, 0.0049, 0.0679, 0.0100, 0.0082, 0.0314, 0.0810, 0.2840,
        0.0439, 0.0955, 0.1563, 0.0832, 0.1119], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,185][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.1157, 0.0486, 0.0410, 0.0555, 0.0643, 0.0447, 0.0907, 0.0921, 0.0657,
        0.0672, 0.0624, 0.0836, 0.1024, 0.0662], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,186][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.4108, 0.0535, 0.0532, 0.0449, 0.0432, 0.0573, 0.0381, 0.0460, 0.0358,
        0.0375, 0.0400, 0.0439, 0.0498, 0.0458], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,188][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.3313, 0.0005, 0.0169, 0.0033, 0.0562, 0.0228, 0.0050, 0.0233, 0.0141,
        0.3653, 0.0612, 0.0068, 0.0883, 0.0051], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,189][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [.] are: tensor([4.3491e-11, 1.0933e-12, 1.4747e-09, 7.2518e-18, 4.7524e-13, 1.9977e-14,
        6.7168e-11, 5.6248e-01, 4.3736e-07, 5.4354e-07, 1.2342e-01, 2.5453e-01,
        5.9562e-02, 5.6095e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,191][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0176, 0.0994, 0.0166, 0.0423, 0.0100, 0.0401, 0.0234, 0.0420, 0.3613,
        0.0083, 0.1425, 0.0372, 0.0661, 0.0668, 0.0264], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,192][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.2400, 0.1477, 0.1910, 0.0253, 0.0730, 0.0057, 0.0050, 0.0220, 0.0030,
        0.0024, 0.0542, 0.0746, 0.0309, 0.0524, 0.0726], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,193][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.1078, 0.0474, 0.0398, 0.0677, 0.0650, 0.0614, 0.0693, 0.0579, 0.0544,
        0.0542, 0.0538, 0.0741, 0.0611, 0.0795, 0.1066], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,194][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.1501, 0.0522, 0.0520, 0.0434, 0.0478, 0.0582, 0.0509, 0.0617, 0.0586,
        0.0507, 0.0684, 0.0755, 0.0747, 0.0680, 0.0877], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,195][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0443, 0.0491, 0.0290, 0.0515, 0.0409, 0.0547, 0.0520, 0.0639, 0.0819,
        0.0836, 0.0666, 0.0707, 0.0883, 0.1302, 0.0932], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,197][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0061, 0.0189, 0.0369, 0.0337, 0.1763, 0.0190, 0.0211, 0.0175, 0.0431,
        0.0181, 0.0398, 0.0461, 0.0505, 0.0506, 0.4222], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,199][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.1665, 0.0708, 0.0645, 0.0484, 0.0528, 0.0415, 0.0438, 0.0722, 0.0491,
        0.0531, 0.0632, 0.0705, 0.0687, 0.0618, 0.0730], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,201][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0022, 0.0041, 0.0095, 0.0071, 0.0142, 0.0137, 0.0663, 0.0352, 0.3341,
        0.0149, 0.1044, 0.2729, 0.0474, 0.0198, 0.0541], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,202][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1303, 0.0462, 0.0426, 0.0497, 0.0439, 0.0644, 0.0686, 0.0711, 0.0673,
        0.0658, 0.0553, 0.0795, 0.1086, 0.0607, 0.0458], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,204][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.3908, 0.0483, 0.0472, 0.0421, 0.0406, 0.0554, 0.0364, 0.0445, 0.0350,
        0.0367, 0.0398, 0.0436, 0.0480, 0.0454, 0.0464], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,206][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.0881, 0.0025, 0.0370, 0.0022, 0.3468, 0.0817, 0.0051, 0.0027, 0.0018,
        0.1672, 0.0058, 0.0032, 0.0452, 0.0006, 0.2099], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,207][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([1.1315e-03, 1.8880e-13, 2.1465e-12, 2.2788e-18, 8.4790e-17, 3.9183e-16,
        1.3970e-12, 2.8158e-03, 4.1388e-08, 6.5586e-10, 4.5140e-03, 4.9684e-03,
        4.2601e-05, 2.6150e-05, 9.8650e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,209][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0082, 0.0898, 0.0050, 0.0544, 0.0052, 0.0350, 0.0113, 0.0113, 0.1935,
        0.0038, 0.4174, 0.0217, 0.0277, 0.0987, 0.0136, 0.0036],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,211][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.3187, 0.1235, 0.1526, 0.0198, 0.0577, 0.0043, 0.0034, 0.0159, 0.0023,
        0.0015, 0.0509, 0.0690, 0.0284, 0.0469, 0.0684, 0.0367],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,212][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0880, 0.0459, 0.0376, 0.0645, 0.0631, 0.0583, 0.0645, 0.0558, 0.0501,
        0.0516, 0.0496, 0.0714, 0.0597, 0.0751, 0.1010, 0.0639],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,214][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.1204, 0.0491, 0.0494, 0.0405, 0.0463, 0.0552, 0.0484, 0.0583, 0.0551,
        0.0482, 0.0637, 0.0716, 0.0715, 0.0633, 0.0846, 0.0742],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,216][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0432, 0.0497, 0.0295, 0.0511, 0.0399, 0.0487, 0.0493, 0.0578, 0.0757,
        0.0775, 0.0612, 0.0644, 0.0800, 0.1169, 0.0840, 0.0711],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,218][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0082, 0.0366, 0.0818, 0.0497, 0.1695, 0.0179, 0.0272, 0.0092, 0.0269,
        0.0219, 0.0343, 0.0298, 0.0267, 0.0266, 0.4273, 0.0066],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,220][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.1599, 0.0656, 0.0611, 0.0449, 0.0500, 0.0390, 0.0405, 0.0673, 0.0453,
        0.0491, 0.0590, 0.0660, 0.0653, 0.0576, 0.0695, 0.0597],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,221][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0020, 0.0106, 0.0028, 0.0379, 0.0026, 0.0220, 0.0387, 0.0555, 0.3740,
        0.0203, 0.0792, 0.1169, 0.0415, 0.0991, 0.0104, 0.0866],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,223][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1082, 0.0350, 0.0373, 0.0477, 0.0532, 0.0352, 0.0750, 0.0849, 0.0607,
        0.0599, 0.0690, 0.0644, 0.0933, 0.0549, 0.0542, 0.0672],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,225][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.3488, 0.0482, 0.0468, 0.0413, 0.0406, 0.0525, 0.0363, 0.0452, 0.0352,
        0.0375, 0.0395, 0.0433, 0.0471, 0.0444, 0.0468, 0.0466],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,227][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.0622, 0.0014, 0.0046, 0.0045, 0.1814, 0.1387, 0.0036, 0.0691, 0.0067,
        0.0184, 0.0165, 0.0105, 0.0894, 0.0046, 0.2857, 0.1027],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,228][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([2.9120e-10, 1.0588e-18, 8.5610e-16, 6.0566e-24, 5.3227e-19, 2.4199e-21,
        1.2546e-17, 4.1676e-06, 2.2006e-13, 1.3436e-13, 1.2602e-07, 6.4948e-08,
        4.2070e-08, 1.1118e-09, 9.9995e-01, 4.5793e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,230][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0749, 0.1317, 0.0030, 0.0385, 0.0022, 0.0521, 0.0023, 0.0247, 0.1845,
        0.0058, 0.1846, 0.0084, 0.0606, 0.0996, 0.0061, 0.1172, 0.0038],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,232][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.4369, 0.0754, 0.1305, 0.0187, 0.0584, 0.0054, 0.0040, 0.0153, 0.0020,
        0.0015, 0.0406, 0.0561, 0.0252, 0.0385, 0.0449, 0.0309, 0.0159],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,234][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0730, 0.0447, 0.0401, 0.0577, 0.0602, 0.0535, 0.0596, 0.0530, 0.0474,
        0.0509, 0.0496, 0.0661, 0.0586, 0.0669, 0.0913, 0.0609, 0.0667],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,236][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1198, 0.0456, 0.0450, 0.0378, 0.0425, 0.0504, 0.0449, 0.0542, 0.0514,
        0.0450, 0.0594, 0.0654, 0.0654, 0.0589, 0.0765, 0.0698, 0.0683],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,236][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0384, 0.0450, 0.0257, 0.0470, 0.0359, 0.0458, 0.0462, 0.0536, 0.0709,
        0.0721, 0.0572, 0.0610, 0.0750, 0.1149, 0.0804, 0.0663, 0.0647],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,237][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0095, 0.0470, 0.0653, 0.0529, 0.1881, 0.0228, 0.0278, 0.0117, 0.0311,
        0.0227, 0.0319, 0.0329, 0.0246, 0.0276, 0.3751, 0.0072, 0.0218],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,238][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1431, 0.0638, 0.0589, 0.0447, 0.0483, 0.0380, 0.0391, 0.0633, 0.0432,
        0.0462, 0.0557, 0.0613, 0.0607, 0.0555, 0.0655, 0.0563, 0.0564],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,240][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0019, 0.0094, 0.0029, 0.0207, 0.0059, 0.0102, 0.0390, 0.0884, 0.1386,
        0.0185, 0.0736, 0.2726, 0.0637, 0.0541, 0.0208, 0.0620, 0.1175],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,242][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.1020, 0.0382, 0.0320, 0.0426, 0.0484, 0.0381, 0.0646, 0.0857, 0.0541,
        0.0594, 0.0548, 0.0611, 0.0861, 0.0526, 0.0496, 0.0614, 0.0691],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,244][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.3593, 0.0454, 0.0441, 0.0388, 0.0374, 0.0487, 0.0329, 0.0402, 0.0320,
        0.0341, 0.0358, 0.0392, 0.0431, 0.0411, 0.0420, 0.0430, 0.0430],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,245][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1611, 0.0006, 0.0219, 0.0015, 0.1669, 0.0534, 0.0034, 0.0404, 0.0029,
        0.0930, 0.0525, 0.0058, 0.1220, 0.0018, 0.2129, 0.0556, 0.0041],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,247][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([6.2848e-07, 1.0312e-16, 1.4679e-14, 9.1204e-22, 8.5841e-18, 1.7338e-19,
        2.1415e-16, 1.1057e-05, 2.6129e-11, 9.4600e-13, 3.9301e-06, 8.7963e-07,
        1.0048e-07, 3.9538e-08, 9.0616e-01, 2.3134e-02, 7.0694e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,248][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0305, 0.0851, 0.0071, 0.0455, 0.0043, 0.0553, 0.0142, 0.0203, 0.1530,
        0.0060, 0.3830, 0.0131, 0.0318, 0.0524, 0.0122, 0.0520, 0.0240, 0.0103],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,250][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.3203, 0.0852, 0.1273, 0.0166, 0.0528, 0.0040, 0.0028, 0.0150, 0.0017,
        0.0016, 0.0547, 0.0787, 0.0360, 0.0465, 0.0612, 0.0463, 0.0215, 0.0278],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,252][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0764, 0.0402, 0.0336, 0.0542, 0.0511, 0.0485, 0.0540, 0.0477, 0.0448,
        0.0438, 0.0464, 0.0601, 0.0512, 0.0624, 0.0828, 0.0554, 0.0616, 0.0858],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,254][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.1163, 0.0427, 0.0419, 0.0347, 0.0392, 0.0464, 0.0411, 0.0498, 0.0469,
        0.0411, 0.0544, 0.0607, 0.0597, 0.0543, 0.0715, 0.0639, 0.0633, 0.0722],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,256][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0377, 0.0437, 0.0250, 0.0451, 0.0348, 0.0427, 0.0428, 0.0516, 0.0666,
        0.0687, 0.0523, 0.0563, 0.0705, 0.1082, 0.0746, 0.0618, 0.0590, 0.0585],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,258][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0106, 0.0304, 0.0892, 0.0354, 0.1910, 0.0188, 0.0203, 0.0154, 0.0278,
        0.0187, 0.0274, 0.0253, 0.0354, 0.0257, 0.3359, 0.0099, 0.0192, 0.0634],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,259][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1363, 0.0598, 0.0560, 0.0409, 0.0454, 0.0349, 0.0361, 0.0599, 0.0398,
        0.0431, 0.0529, 0.0577, 0.0576, 0.0513, 0.0623, 0.0528, 0.0534, 0.0596],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,261][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0006, 0.0076, 0.0025, 0.0530, 0.0043, 0.0184, 0.0284, 0.0196, 0.3549,
        0.0193, 0.0863, 0.1225, 0.0533, 0.0327, 0.0156, 0.0795, 0.0705, 0.0308],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,263][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0978, 0.0398, 0.0347, 0.0442, 0.0487, 0.0398, 0.0640, 0.0710, 0.0567,
        0.0487, 0.0568, 0.0566, 0.0779, 0.0513, 0.0484, 0.0543, 0.0652, 0.0439],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,265][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.3407, 0.0427, 0.0414, 0.0361, 0.0350, 0.0459, 0.0312, 0.0396, 0.0301,
        0.0324, 0.0340, 0.0382, 0.0425, 0.0388, 0.0414, 0.0419, 0.0423, 0.0456],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,267][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.0647, 0.0024, 0.0082, 0.0045, 0.2898, 0.0219, 0.0021, 0.0073, 0.0028,
        0.1726, 0.0189, 0.0029, 0.0189, 0.0004, 0.3301, 0.0379, 0.0022, 0.0124],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,268][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([4.3894e-07, 4.2492e-18, 1.0521e-16, 7.1635e-25, 1.8546e-20, 5.1486e-21,
        2.5590e-16, 3.3208e-07, 3.3067e-13, 7.9003e-15, 1.2740e-07, 4.9029e-07,
        9.0778e-10, 8.2581e-10, 2.8290e-01, 8.4765e-04, 7.0120e-01, 1.5045e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,270][circuit_model.py][line:2294][INFO] ##4-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0843, 0.0183, 0.0029, 0.0115, 0.0021, 0.0286, 0.0101, 0.0631, 0.0304,
        0.0239, 0.0147, 0.0252, 0.0244, 0.0250, 0.0069, 0.1341, 0.0194, 0.4672,
        0.0079], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,272][circuit_model.py][line:2297][INFO] ##4-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.4611, 0.0657, 0.1322, 0.0135, 0.0566, 0.0038, 0.0030, 0.0128, 0.0015,
        0.0015, 0.0338, 0.0476, 0.0209, 0.0273, 0.0454, 0.0275, 0.0142, 0.0177,
        0.0139], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,273][circuit_model.py][line:2300][INFO] ##4-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0557, 0.0409, 0.0386, 0.0517, 0.0553, 0.0496, 0.0533, 0.0472, 0.0409,
        0.0448, 0.0431, 0.0556, 0.0513, 0.0544, 0.0778, 0.0526, 0.0555, 0.0773,
        0.0544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,275][circuit_model.py][line:2303][INFO] ##4-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1001, 0.0399, 0.0394, 0.0330, 0.0366, 0.0431, 0.0388, 0.0475, 0.0445,
        0.0395, 0.0508, 0.0568, 0.0562, 0.0516, 0.0656, 0.0606, 0.0597, 0.0684,
        0.0680], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,277][circuit_model.py][line:2306][INFO] ##4-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0391, 0.0381, 0.0239, 0.0393, 0.0329, 0.0391, 0.0394, 0.0477, 0.0593,
        0.0641, 0.0495, 0.0517, 0.0664, 0.0904, 0.0687, 0.0574, 0.0555, 0.0539,
        0.0833], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,279][circuit_model.py][line:2309][INFO] ##4-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0089, 0.0497, 0.0842, 0.0575, 0.1561, 0.0367, 0.0361, 0.0181, 0.0399,
        0.0297, 0.0386, 0.0337, 0.0276, 0.0330, 0.2407, 0.0094, 0.0247, 0.0485,
        0.0269], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,280][circuit_model.py][line:2312][INFO] ##4-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1274, 0.0566, 0.0540, 0.0394, 0.0437, 0.0341, 0.0348, 0.0569, 0.0380,
        0.0411, 0.0498, 0.0540, 0.0541, 0.0483, 0.0583, 0.0497, 0.0501, 0.0564,
        0.0534], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,281][circuit_model.py][line:2315][INFO] ##4-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0019, 0.0058, 0.0015, 0.0221, 0.0043, 0.0058, 0.0109, 0.0312, 0.1414,
        0.0356, 0.0800, 0.0618, 0.0458, 0.0334, 0.0171, 0.0466, 0.0364, 0.0972,
        0.3209], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,282][circuit_model.py][line:2318][INFO] ##4-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0828, 0.0402, 0.0315, 0.0443, 0.0424, 0.0350, 0.0679, 0.0648, 0.0517,
        0.0458, 0.0536, 0.0550, 0.0613, 0.0512, 0.0437, 0.0482, 0.0666, 0.0522,
        0.0617], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,284][circuit_model.py][line:2321][INFO] ##4-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.3144, 0.0403, 0.0400, 0.0360, 0.0351, 0.0443, 0.0308, 0.0386, 0.0295,
        0.0316, 0.0332, 0.0362, 0.0406, 0.0386, 0.0394, 0.0399, 0.0403, 0.0439,
        0.0473], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,286][circuit_model.py][line:2324][INFO] ##4-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.3172, 0.0020, 0.0170, 0.0210, 0.0617, 0.0560, 0.0079, 0.0207, 0.0147,
        0.1243, 0.0956, 0.0111, 0.0821, 0.0030, 0.0882, 0.0415, 0.0072, 0.0031,
        0.0257], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,287][circuit_model.py][line:2327][INFO] ##4-th layer ##Weight##: The head12 weight for token [ to] are: tensor([4.5388e-10, 4.7063e-20, 1.1664e-17, 9.8754e-27, 3.0589e-21, 1.0519e-22,
        1.7395e-18, 3.1755e-07, 4.0366e-15, 3.5022e-15, 9.0471e-10, 1.9949e-09,
        3.6757e-10, 5.2990e-11, 3.5246e-02, 1.1286e-04, 7.4739e-03, 8.6508e-01,
        9.2083e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,335][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:18,336][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,337][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,338][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,339][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,339][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,340][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,341][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,341][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,342][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,343][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,343][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,344][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,345][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.6320, 0.3680], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,346][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.6458, 0.3542], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,346][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1070, 0.8930], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,347][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1925, 0.8075], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,348][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.8956, 0.1044], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,348][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.2476, 0.7524], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,349][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.6947, 0.3053], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,350][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.2177, 0.7823], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,350][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.7881, 0.2119], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,352][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.7765, 0.2235], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,354][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.9640, 0.0360], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,355][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.9573, 0.0427], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,357][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.0545, 0.3940, 0.5516], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,359][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.3099, 0.4796, 0.2105], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,360][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.2430, 0.1323, 0.6246], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,361][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.0628, 0.3547, 0.5825], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,363][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.9546, 0.0427, 0.0027], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,365][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.0525, 0.1471, 0.8005], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,366][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.2704, 0.5539, 0.1758], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,368][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.1266, 0.3494, 0.5240], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,370][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.6527, 0.2932, 0.0541], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,371][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.8028, 0.0947, 0.1025], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,373][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.8778, 0.1104, 0.0118], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,375][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.0129, 0.9442, 0.0429], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,376][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.1723, 0.1394, 0.5706, 0.1177], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,378][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.1953, 0.2929, 0.2813, 0.2305], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,380][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0502, 0.3048, 0.3736, 0.2713], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,381][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0465, 0.2389, 0.4610, 0.2537], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,383][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.7917, 0.1332, 0.0061, 0.0690], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,384][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.0013, 0.1567, 0.8226, 0.0194], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,386][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2794, 0.2723, 0.2632, 0.1851], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,387][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0138, 0.1208, 0.0196, 0.8458], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,389][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.4331, 0.2366, 0.1664, 0.1639], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,391][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.3268, 0.1251, 0.2077, 0.3404], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,392][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.7723, 0.1444, 0.0156, 0.0677], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,392][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([9.6297e-13, 3.0309e-04, 9.9970e-01, 4.7643e-10], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,393][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0325, 0.1477, 0.4255, 0.1790, 0.2153], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,394][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.1629, 0.3202, 0.1292, 0.2917, 0.0960], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,395][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.2349, 0.0801, 0.1587, 0.2794, 0.2469], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,396][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0400, 0.2028, 0.3341, 0.2365, 0.1866], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,398][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.8288, 0.1014, 0.0074, 0.0411, 0.0213], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,399][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.0067, 0.0835, 0.7957, 0.0231, 0.0910], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,401][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.1446, 0.3263, 0.1713, 0.2459, 0.1120], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,403][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0384, 0.1587, 0.2194, 0.2966, 0.2869], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,404][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.4744, 0.0986, 0.2778, 0.0914, 0.0578], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,406][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.2537, 0.0997, 0.1408, 0.4062, 0.0996], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,408][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.7699, 0.1451, 0.0082, 0.0646, 0.0122], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,409][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([3.5971e-07, 4.3886e-02, 9.5611e-01, 8.7005e-08, 1.0925e-06],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,410][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.0220, 0.1059, 0.3732, 0.1521, 0.2711, 0.0757], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,412][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.1464, 0.1970, 0.1528, 0.2537, 0.0956, 0.1546], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,414][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0553, 0.1153, 0.0987, 0.2797, 0.4246, 0.0264], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,415][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.0309, 0.1701, 0.2978, 0.1875, 0.1970, 0.1166], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,417][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.9056, 0.0495, 0.0019, 0.0315, 0.0044, 0.0072], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,418][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.0175, 0.1619, 0.6223, 0.0394, 0.1008, 0.0582], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,420][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.1695, 0.1757, 0.1569, 0.1511, 0.2543, 0.0924], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,422][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.0086, 0.1421, 0.0371, 0.4674, 0.0309, 0.3140], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,423][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.1836, 0.0714, 0.3810, 0.0825, 0.2057, 0.0759], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,425][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.2751, 0.0904, 0.0976, 0.1786, 0.0791, 0.2794], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,427][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.7879, 0.1098, 0.0103, 0.0377, 0.0040, 0.0503], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,428][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([1.1133e-08, 2.2806e-04, 9.9927e-01, 1.5110e-09, 5.0136e-04, 3.3182e-08],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,429][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0987, 0.1108, 0.2708, 0.1020, 0.2145, 0.1412, 0.0619],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,431][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1162, 0.1932, 0.1054, 0.1848, 0.1030, 0.2166, 0.0809],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,433][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0346, 0.0636, 0.1520, 0.0937, 0.4335, 0.0450, 0.1775],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,434][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0236, 0.1504, 0.2592, 0.1759, 0.1593, 0.1165, 0.1152],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,434][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.9315, 0.0337, 0.0011, 0.0195, 0.0024, 0.0029, 0.0088],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,435][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0911, 0.1312, 0.4483, 0.0267, 0.1021, 0.0501, 0.1506],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,436][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1418, 0.1532, 0.1206, 0.1622, 0.1998, 0.1439, 0.0785],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,437][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0153, 0.0981, 0.0276, 0.2400, 0.0520, 0.1136, 0.4534],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,439][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2435, 0.0753, 0.1103, 0.0589, 0.0702, 0.2814, 0.1604],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,440][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2194, 0.0393, 0.0501, 0.0784, 0.0503, 0.1214, 0.4412],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,442][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.8377, 0.0708, 0.0062, 0.0277, 0.0029, 0.0281, 0.0266],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,443][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([4.5352e-05, 2.9945e-03, 9.9647e-01, 3.4761e-08, 3.6301e-04, 3.3145e-06,
        1.2362e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,444][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.0566, 0.0936, 0.2744, 0.1341, 0.1332, 0.1361, 0.0729, 0.0991],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,446][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.1310, 0.1140, 0.0767, 0.1610, 0.0597, 0.1066, 0.1079, 0.2431],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,448][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.0796, 0.0211, 0.0332, 0.0596, 0.4267, 0.0148, 0.0907, 0.2743],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,449][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.0406, 0.1400, 0.2343, 0.1704, 0.1231, 0.1037, 0.1134, 0.0745],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,451][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([9.4398e-01, 3.1021e-02, 3.4699e-04, 1.5659e-02, 1.3210e-03, 2.7496e-03,
        4.3656e-03, 5.5460e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,452][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([7.8879e-02, 1.1729e-03, 3.4997e-03, 1.8796e-04, 7.0741e-04, 5.9775e-04,
        5.9365e-03, 9.0902e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,453][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.1407, 0.1342, 0.1024, 0.1188, 0.1143, 0.1334, 0.0799, 0.1763],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,455][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.0055, 0.0453, 0.0164, 0.1478, 0.0188, 0.1576, 0.2122, 0.3965],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,457][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.1478, 0.0887, 0.0921, 0.0557, 0.0300, 0.1506, 0.1289, 0.3061],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,459][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.3249, 0.0144, 0.0267, 0.0347, 0.0324, 0.0414, 0.1954, 0.3299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,460][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.9320, 0.0281, 0.0021, 0.0114, 0.0014, 0.0055, 0.0153, 0.0043],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,462][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([3.4433e-03, 6.9298e-10, 4.2772e-08, 2.5875e-16, 2.9020e-12, 3.1170e-13,
        4.2299e-08, 9.9656e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,463][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0692, 0.0594, 0.2650, 0.0703, 0.2075, 0.0856, 0.0628, 0.1018, 0.0782],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,465][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0826, 0.0705, 0.0857, 0.0891, 0.0721, 0.1081, 0.0696, 0.3711, 0.0513],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,467][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0130, 0.0462, 0.0635, 0.0492, 0.1576, 0.0120, 0.1035, 0.2584, 0.2966],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,468][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0187, 0.1164, 0.2088, 0.1317, 0.1306, 0.0886, 0.0956, 0.0811, 0.1284],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,470][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([9.3724e-01, 2.5852e-02, 5.8150e-04, 1.6156e-02, 1.5446e-03, 1.7264e-03,
        4.8078e-03, 5.5454e-04, 1.1539e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,471][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([1.5364e-03, 1.4407e-03, 3.9120e-03, 3.4937e-04, 8.1705e-04, 7.2289e-04,
        4.9013e-03, 9.7927e-01, 7.0547e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,472][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.1116, 0.1099, 0.0948, 0.1129, 0.1354, 0.1121, 0.0794, 0.1681, 0.0758],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,474][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0121, 0.0339, 0.0137, 0.0754, 0.0259, 0.0352, 0.0667, 0.1573, 0.5799],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,475][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.1019, 0.0516, 0.0835, 0.0359, 0.0679, 0.0849, 0.1217, 0.3957, 0.0569],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,476][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.1447, 0.0143, 0.0161, 0.0252, 0.0209, 0.0328, 0.1445, 0.3193, 0.2820],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,477][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.8083, 0.0466, 0.0034, 0.0208, 0.0016, 0.0158, 0.0157, 0.0064, 0.0815],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,478][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([5.0616e-11, 5.4928e-13, 7.0846e-10, 4.3782e-18, 1.3826e-12, 2.1322e-15,
        3.1414e-12, 1.0000e+00, 6.0442e-09], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,479][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.0256, 0.0759, 0.1938, 0.1123, 0.1403, 0.1007, 0.0686, 0.0893, 0.1646,
        0.0289], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,480][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.1060, 0.0830, 0.0688, 0.1012, 0.0496, 0.0816, 0.0827, 0.2939, 0.0984,
        0.0348], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,482][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.0478, 0.0070, 0.0257, 0.0192, 0.2291, 0.0188, 0.0584, 0.0863, 0.0609,
        0.4467], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,483][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.0239, 0.1071, 0.1855, 0.1317, 0.1119, 0.0788, 0.1018, 0.0638, 0.1307,
        0.0649], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,484][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([9.4100e-01, 1.9563e-02, 7.7007e-04, 1.4736e-02, 1.3838e-03, 2.1102e-03,
        3.0226e-03, 7.5641e-04, 7.9882e-03, 8.6713e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,486][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([1.9434e-03, 7.3810e-04, 4.6295e-03, 2.0499e-04, 5.9361e-04, 3.7845e-04,
        2.7765e-03, 9.6274e-01, 1.8152e-02, 7.8424e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,487][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0716, 0.1196, 0.0721, 0.1092, 0.0960, 0.1176, 0.0573, 0.1755, 0.1163,
        0.0648], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,489][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.0047, 0.0181, 0.0071, 0.0481, 0.0168, 0.0488, 0.0611, 0.1999, 0.5237,
        0.0716], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,491][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.1258, 0.1073, 0.0340, 0.0610, 0.0236, 0.1055, 0.0959, 0.3832, 0.0527,
        0.0109], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,492][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.0689, 0.0087, 0.0128, 0.0300, 0.0141, 0.0505, 0.1722, 0.2260, 0.3838,
        0.0330], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,494][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.7858, 0.0521, 0.0050, 0.0244, 0.0027, 0.0157, 0.0224, 0.0065, 0.0754,
        0.0100], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,495][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([4.2952e-10, 7.1877e-11, 1.6423e-08, 1.4572e-15, 5.4694e-12, 1.2053e-13,
        9.8426e-11, 1.0000e+00, 1.3669e-06, 5.1391e-09], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,497][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0443, 0.0648, 0.1296, 0.0711, 0.1061, 0.0827, 0.0760, 0.1131, 0.1051,
        0.0643, 0.1429], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,499][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0472, 0.0820, 0.0468, 0.0893, 0.0442, 0.1132, 0.0597, 0.3122, 0.0752,
        0.0830, 0.0472], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,500][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0245, 0.0140, 0.0729, 0.0316, 0.1652, 0.0170, 0.0296, 0.1117, 0.1431,
        0.3155, 0.0750], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,502][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0245, 0.0948, 0.1817, 0.1116, 0.0983, 0.0714, 0.0784, 0.0663, 0.1082,
        0.0820, 0.0828], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,503][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([9.1948e-01, 2.7812e-02, 4.2292e-04, 1.6895e-02, 1.7637e-03, 1.7631e-03,
        4.2702e-03, 4.8072e-04, 6.7124e-03, 4.6549e-03, 1.5749e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,505][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([1.3304e-02, 6.6281e-04, 2.1107e-03, 1.5076e-04, 4.6174e-04, 4.2595e-04,
        2.5898e-03, 8.4018e-01, 1.1227e-02, 8.6312e-03, 1.2025e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,506][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0966, 0.0907, 0.0860, 0.0892, 0.0973, 0.1001, 0.0686, 0.1425, 0.0920,
        0.1023, 0.0347], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,508][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0061, 0.0100, 0.0130, 0.0539, 0.0153, 0.0283, 0.0855, 0.0797, 0.3633,
        0.0537, 0.2913], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,510][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0911, 0.1043, 0.0176, 0.0483, 0.0207, 0.1119, 0.1466, 0.2920, 0.1200,
        0.0178, 0.0298], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,511][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0605, 0.0060, 0.0088, 0.0129, 0.0119, 0.0125, 0.0799, 0.1917, 0.1444,
        0.0442, 0.4273], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,513][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.7334, 0.0563, 0.0038, 0.0199, 0.0017, 0.0149, 0.0183, 0.0060, 0.0890,
        0.0078, 0.0488], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,514][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([5.1817e-07, 7.2092e-13, 1.6643e-10, 4.7004e-18, 1.0279e-13, 1.3990e-15,
        5.3407e-12, 9.9650e-01, 6.1015e-08, 2.8057e-07, 3.4944e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,516][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0580, 0.0457, 0.1682, 0.0449, 0.1198, 0.0647, 0.0265, 0.0843, 0.0944,
        0.0400, 0.2180, 0.0357], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,517][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0564, 0.0893, 0.0515, 0.0882, 0.0482, 0.1271, 0.0446, 0.2603, 0.0699,
        0.0753, 0.0525, 0.0366], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,518][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0100, 0.0068, 0.0207, 0.0101, 0.0797, 0.0050, 0.0313, 0.1221, 0.0666,
        0.5454, 0.0617, 0.0406], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,519][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0201, 0.0896, 0.1417, 0.1039, 0.0885, 0.0694, 0.0725, 0.0607, 0.1062,
        0.0730, 0.0880, 0.0864], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,520][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([8.8836e-01, 3.2294e-02, 6.4781e-04, 1.9695e-02, 1.3978e-03, 2.2180e-03,
        4.4516e-03, 6.3090e-04, 1.1428e-02, 5.8581e-03, 1.1651e-02, 2.1365e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,521][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([7.7586e-03, 4.0405e-04, 1.7473e-03, 9.6207e-05, 1.9560e-04, 2.1812e-04,
        1.6692e-03, 6.2875e-01, 4.2182e-03, 4.5409e-03, 1.7807e-01, 1.7233e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,522][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0914, 0.0837, 0.0685, 0.0891, 0.1081, 0.0753, 0.0640, 0.1426, 0.0720,
        0.0841, 0.0431, 0.0780], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,524][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0044, 0.0094, 0.0094, 0.0136, 0.0238, 0.0190, 0.0786, 0.1822, 0.1158,
        0.0381, 0.1007, 0.4051], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,526][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0659, 0.0560, 0.0890, 0.0398, 0.0213, 0.1559, 0.0818, 0.3059, 0.0901,
        0.0456, 0.0258, 0.0229], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,527][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0488, 0.0039, 0.0064, 0.0081, 0.0076, 0.0089, 0.0446, 0.1144, 0.0955,
        0.0195, 0.3866, 0.2557], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,529][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.7278, 0.0441, 0.0039, 0.0176, 0.0019, 0.0191, 0.0204, 0.0071, 0.0900,
        0.0070, 0.0398, 0.0213], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,530][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([5.2175e-07, 1.5298e-12, 1.5567e-10, 9.2418e-18, 7.4131e-15, 2.0888e-15,
        2.9839e-12, 5.5615e-01, 8.2428e-07, 6.2165e-09, 4.0526e-01, 3.8592e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,532][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.0161, 0.0412, 0.1602, 0.0598, 0.1074, 0.0363, 0.0400, 0.0522, 0.1069,
        0.0200, 0.2672, 0.0435, 0.0491], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,533][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0418, 0.0748, 0.0357, 0.0985, 0.0324, 0.0847, 0.0731, 0.2561, 0.1003,
        0.0586, 0.0594, 0.0612, 0.0234], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,535][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.0079, 0.0145, 0.0544, 0.0375, 0.1522, 0.0065, 0.0506, 0.0918, 0.1484,
        0.0756, 0.0492, 0.1106, 0.2006], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,537][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0153, 0.0821, 0.1398, 0.1032, 0.0839, 0.0591, 0.0751, 0.0542, 0.0973,
        0.0617, 0.0878, 0.0860, 0.0545], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,538][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([9.3971e-01, 1.4353e-02, 4.6590e-04, 8.3093e-03, 9.6045e-04, 1.3647e-03,
        2.6999e-03, 5.1279e-04, 5.2578e-03, 1.4022e-03, 8.9300e-03, 7.1506e-03,
        8.8795e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,539][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([6.3256e-03, 2.0290e-04, 1.3620e-03, 5.0056e-05, 1.3967e-04, 1.4376e-04,
        1.4754e-03, 4.0470e-01, 5.9164e-03, 3.7069e-03, 1.7156e-01, 3.1198e-01,
        9.2443e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,541][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0692, 0.0874, 0.0658, 0.0830, 0.0889, 0.0940, 0.0500, 0.1293, 0.0801,
        0.0736, 0.0358, 0.0944, 0.0485], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,543][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.0039, 0.0055, 0.0094, 0.0173, 0.0253, 0.0170, 0.0689, 0.0619, 0.2671,
        0.0370, 0.1691, 0.2141, 0.1034], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,545][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.1535, 0.1033, 0.1120, 0.0423, 0.1056, 0.0695, 0.0781, 0.1709, 0.0402,
        0.0160, 0.0350, 0.0503, 0.0234], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,546][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.0288, 0.0024, 0.0035, 0.0083, 0.0041, 0.0129, 0.0543, 0.0749, 0.1087,
        0.0113, 0.3723, 0.2716, 0.0469], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,548][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.7232, 0.0503, 0.0045, 0.0189, 0.0043, 0.0220, 0.0166, 0.0052, 0.0581,
        0.0085, 0.0261, 0.0158, 0.0465], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,549][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([7.9848e-07, 9.8921e-13, 2.2465e-10, 5.7030e-18, 3.2762e-14, 3.7008e-16,
        1.2145e-11, 2.4024e-01, 8.0223e-08, 2.9800e-09, 9.1238e-02, 6.6844e-01,
        8.0087e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:18,551][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.0596, 0.0295, 0.1833, 0.0297, 0.1117, 0.0396, 0.0426, 0.0841, 0.0608,
        0.0452, 0.1540, 0.0522, 0.0833, 0.0244], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,553][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0510, 0.0538, 0.0633, 0.0543, 0.0471, 0.0888, 0.0536, 0.2921, 0.0518,
        0.0748, 0.0471, 0.0483, 0.0478, 0.0263], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,554][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0051, 0.0145, 0.0439, 0.0222, 0.0574, 0.0130, 0.0803, 0.0926, 0.0598,
        0.2668, 0.1112, 0.0371, 0.1402, 0.0558], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,556][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0141, 0.0733, 0.1402, 0.0843, 0.0824, 0.0564, 0.0635, 0.0515, 0.0920,
        0.0637, 0.0711, 0.0835, 0.0628, 0.0614], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,558][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.6286, 0.0409, 0.0012, 0.0282, 0.0027, 0.0033, 0.0080, 0.0009, 0.0132,
        0.0103, 0.0233, 0.0279, 0.0111, 0.2003], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,559][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([1.0177e-03, 6.0753e-04, 1.8795e-03, 1.6631e-04, 3.2148e-04, 4.2457e-04,
        2.3601e-03, 3.4233e-01, 1.0522e-02, 3.7384e-03, 2.1540e-01, 2.8482e-01,
        1.2877e-01, 7.6425e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,560][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0918, 0.0607, 0.0670, 0.0562, 0.0835, 0.0695, 0.0550, 0.1220, 0.0674,
        0.0736, 0.0386, 0.0879, 0.0630, 0.0639], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,561][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0030, 0.0163, 0.0049, 0.0620, 0.0104, 0.0083, 0.0311, 0.0819, 0.2762,
        0.0457, 0.0978, 0.1589, 0.0895, 0.1141], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,562][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.1124, 0.0244, 0.0517, 0.0271, 0.0237, 0.0853, 0.0685, 0.4143, 0.0407,
        0.0395, 0.0197, 0.0240, 0.0491, 0.0195], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,564][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.0329, 0.0032, 0.0051, 0.0076, 0.0061, 0.0097, 0.0428, 0.0823, 0.0906,
        0.0164, 0.3389, 0.2243, 0.0533, 0.0869], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,565][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.6788, 0.0548, 0.0090, 0.0198, 0.0032, 0.0274, 0.0175, 0.0053, 0.0627,
        0.0105, 0.0303, 0.0191, 0.0254, 0.0361], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,567][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([4.3491e-11, 1.0933e-12, 1.4747e-09, 7.2518e-18, 4.7524e-13, 1.9977e-14,
        6.7168e-11, 5.6248e-01, 4.3736e-07, 5.4354e-07, 1.2342e-01, 2.5453e-01,
        5.9562e-02, 5.6095e-06], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:18,568][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0108, 0.0449, 0.1241, 0.0585, 0.0695, 0.0443, 0.0423, 0.0519, 0.1290,
        0.0185, 0.2285, 0.0425, 0.0601, 0.0315, 0.0436], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,570][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0372, 0.0821, 0.0369, 0.0858, 0.0295, 0.0769, 0.0690, 0.2492, 0.0818,
        0.0475, 0.0490, 0.0610, 0.0333, 0.0466, 0.0141], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,572][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0462, 0.0097, 0.0137, 0.0325, 0.0260, 0.0248, 0.0382, 0.0317, 0.0247,
        0.2012, 0.0941, 0.0339, 0.3444, 0.0291, 0.0498], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,574][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0196, 0.0720, 0.1251, 0.0890, 0.0692, 0.0539, 0.0666, 0.0523, 0.0854,
        0.0574, 0.0757, 0.0698, 0.0530, 0.0613, 0.0500], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,575][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.6209, 0.0370, 0.0017, 0.0207, 0.0048, 0.0050, 0.0073, 0.0006, 0.0121,
        0.0053, 0.0247, 0.0133, 0.0113, 0.1551, 0.0803], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,577][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([3.0508e-02, 1.6819e-04, 5.3978e-04, 6.4982e-05, 8.9681e-05, 1.6094e-04,
        2.0190e-03, 1.2349e-01, 5.1258e-03, 3.4794e-03, 1.3417e-01, 1.8004e-01,
        8.0535e-02, 1.4025e-02, 4.2558e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,579][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0421, 0.0901, 0.0522, 0.0742, 0.0333, 0.0968, 0.0503, 0.1055, 0.0780,
        0.0572, 0.0337, 0.0955, 0.0826, 0.0759, 0.0328], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,580][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0022, 0.0037, 0.0094, 0.0067, 0.0145, 0.0136, 0.0648, 0.0354, 0.3243,
        0.0155, 0.1066, 0.2741, 0.0503, 0.0205, 0.0584], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,582][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1845, 0.0417, 0.1180, 0.0365, 0.0240, 0.2963, 0.0507, 0.0522, 0.0340,
        0.0235, 0.0090, 0.0339, 0.0508, 0.0217, 0.0232], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,584][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0144, 0.0013, 0.0016, 0.0055, 0.0014, 0.0121, 0.0389, 0.0467, 0.1143,
        0.0058, 0.3860, 0.2458, 0.0333, 0.0842, 0.0089], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,586][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.6976, 0.0464, 0.0024, 0.0192, 0.0037, 0.0312, 0.0160, 0.0048, 0.0598,
        0.0075, 0.0253, 0.0144, 0.0312, 0.0292, 0.0115], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,587][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([1.1315e-03, 1.8880e-13, 2.1465e-12, 2.2788e-18, 8.4790e-17, 3.9183e-16,
        1.3970e-12, 2.8158e-03, 4.1388e-08, 6.5586e-10, 4.5140e-03, 4.9684e-03,
        4.2601e-05, 2.6150e-05, 9.8650e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:18,589][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0054, 0.0379, 0.0976, 0.0580, 0.0976, 0.0511, 0.0443, 0.0568, 0.0939,
        0.0222, 0.1836, 0.0485, 0.0671, 0.0349, 0.0658, 0.0353],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,591][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0347, 0.0609, 0.0468, 0.0913, 0.0396, 0.0669, 0.0558, 0.2593, 0.0724,
        0.0453, 0.0616, 0.0498, 0.0257, 0.0408, 0.0188, 0.0303],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,592][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0093, 0.0127, 0.0132, 0.0125, 0.0590, 0.0033, 0.0207, 0.0358, 0.0827,
        0.3841, 0.0416, 0.1138, 0.0385, 0.0786, 0.0887, 0.0054],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,594][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0181, 0.0665, 0.1302, 0.0756, 0.0816, 0.0502, 0.0545, 0.0431, 0.0799,
        0.0470, 0.0726, 0.0690, 0.0598, 0.0574, 0.0596, 0.0349],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,595][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([8.1979e-01, 1.9709e-02, 6.9383e-04, 1.2555e-02, 1.4441e-03, 2.5102e-03,
        5.1558e-03, 4.1782e-04, 4.8356e-03, 3.7144e-03, 4.5951e-03, 1.2723e-02,
        5.0924e-03, 8.3222e-02, 2.1625e-02, 1.9159e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,597][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([7.8761e-03, 8.9801e-05, 5.7808e-04, 2.3191e-05, 6.4587e-05, 4.3589e-05,
        5.5106e-04, 1.9132e-01, 2.1478e-03, 1.6404e-03, 4.1312e-02, 6.9133e-02,
        3.4829e-02, 4.0663e-03, 4.9040e-01, 1.5593e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,598][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0536, 0.0694, 0.0479, 0.0615, 0.0620, 0.0670, 0.0418, 0.1125, 0.0691,
        0.0701, 0.0326, 0.0743, 0.0630, 0.0646, 0.0661, 0.0446],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,600][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0020, 0.0094, 0.0028, 0.0353, 0.0027, 0.0218, 0.0382, 0.0562, 0.3640,
        0.0210, 0.0809, 0.1184, 0.0440, 0.1008, 0.0114, 0.0911],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,601][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.0581, 0.0231, 0.0563, 0.0250, 0.0425, 0.0536, 0.1250, 0.3335, 0.0507,
        0.0320, 0.0443, 0.0190, 0.0416, 0.0162, 0.0445, 0.0346],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,602][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0765, 0.0026, 0.0046, 0.0052, 0.0046, 0.0053, 0.0321, 0.0807, 0.0639,
        0.0104, 0.2646, 0.1757, 0.0359, 0.0662, 0.0339, 0.1380],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,603][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.7309, 0.0295, 0.0024, 0.0100, 0.0017, 0.0213, 0.0113, 0.0052, 0.0444,
        0.0046, 0.0297, 0.0112, 0.0155, 0.0274, 0.0076, 0.0474],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,604][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([2.9120e-10, 1.0588e-18, 8.5610e-16, 6.0566e-24, 5.3227e-19, 2.4199e-21,
        1.2546e-17, 4.1676e-06, 2.2006e-13, 1.3436e-13, 1.2602e-07, 6.4948e-08,
        4.2070e-08, 1.1118e-09, 9.9995e-01, 4.5793e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:18,606][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0287, 0.0392, 0.0962, 0.0387, 0.0790, 0.0503, 0.0223, 0.0674, 0.0782,
        0.0256, 0.1717, 0.0381, 0.0786, 0.0318, 0.0587, 0.0772, 0.0182],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,608][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0343, 0.0726, 0.0374, 0.0790, 0.0432, 0.0858, 0.0341, 0.2240, 0.0635,
        0.0607, 0.0513, 0.0377, 0.0343, 0.0385, 0.0222, 0.0591, 0.0224],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,609][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0078, 0.0071, 0.0182, 0.0112, 0.0557, 0.0075, 0.0278, 0.0846, 0.0597,
        0.2090, 0.0387, 0.0444, 0.1540, 0.0836, 0.0986, 0.0336, 0.0586],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,611][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0152, 0.0638, 0.1104, 0.0789, 0.0700, 0.0503, 0.0485, 0.0414, 0.0781,
        0.0523, 0.0705, 0.0651, 0.0537, 0.0555, 0.0538, 0.0382, 0.0542],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,612][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([7.9262e-01, 1.6801e-02, 4.2601e-04, 1.2750e-02, 1.1124e-03, 1.5435e-03,
        3.6901e-03, 3.0939e-04, 4.6594e-03, 3.8802e-03, 6.3318e-03, 1.1659e-02,
        6.8086e-03, 9.6130e-02, 1.9578e-02, 1.2041e-03, 2.0496e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,613][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([3.7927e-02, 1.2248e-04, 3.2214e-04, 2.7286e-05, 6.9366e-05, 6.2048e-05,
        3.1103e-04, 8.9334e-02, 1.6528e-03, 7.2072e-04, 2.6283e-02, 5.0139e-02,
        1.8293e-02, 4.9901e-03, 2.7744e-01, 2.1405e-01, 2.7826e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,615][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0472, 0.0577, 0.0444, 0.0635, 0.0748, 0.0520, 0.0298, 0.0989, 0.0557,
        0.0556, 0.0339, 0.0801, 0.0629, 0.0683, 0.0806, 0.0626, 0.0319],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,617][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0019, 0.0082, 0.0028, 0.0190, 0.0060, 0.0101, 0.0379, 0.0878, 0.1341,
        0.0191, 0.0743, 0.2694, 0.0670, 0.0548, 0.0225, 0.0643, 0.1208],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,619][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0695, 0.0256, 0.0280, 0.0169, 0.0187, 0.0884, 0.0486, 0.3805, 0.0376,
        0.0417, 0.0235, 0.0182, 0.0876, 0.0167, 0.0201, 0.0343, 0.0441],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,621][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0519, 0.0019, 0.0027, 0.0036, 0.0033, 0.0040, 0.0187, 0.0557, 0.0396,
        0.0104, 0.1635, 0.1039, 0.0320, 0.0505, 0.0249, 0.1358, 0.2977],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,623][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.6960, 0.0310, 0.0035, 0.0115, 0.0020, 0.0170, 0.0137, 0.0047, 0.0531,
        0.0053, 0.0264, 0.0126, 0.0271, 0.0323, 0.0098, 0.0302, 0.0236],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,624][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([6.2848e-07, 1.0312e-16, 1.4679e-14, 9.1204e-22, 8.5841e-18, 1.7338e-19,
        2.1415e-16, 1.1057e-05, 2.6129e-11, 9.4600e-13, 3.9301e-06, 8.7963e-07,
        1.0048e-07, 3.9538e-08, 9.0616e-01, 2.3134e-02, 7.0694e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:18,626][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0100, 0.0354, 0.0733, 0.0547, 0.0455, 0.0429, 0.0364, 0.0402, 0.0955,
        0.0159, 0.2943, 0.0317, 0.0418, 0.0306, 0.0308, 0.0441, 0.0274, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,628][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0728, 0.0611, 0.0387, 0.0748, 0.0315, 0.0707, 0.0648, 0.1684, 0.0850,
        0.0332, 0.0581, 0.0501, 0.0226, 0.0335, 0.0149, 0.0409, 0.0396, 0.0394],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,629][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0265, 0.0023, 0.0188, 0.0092, 0.0478, 0.0366, 0.0394, 0.0547, 0.0554,
        0.1578, 0.0790, 0.0356, 0.1100, 0.0107, 0.0930, 0.0975, 0.0861, 0.0396],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,631][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0275, 0.0618, 0.1086, 0.0758, 0.0637, 0.0436, 0.0545, 0.0339, 0.0741,
        0.0427, 0.0655, 0.0635, 0.0444, 0.0519, 0.0463, 0.0344, 0.0575, 0.0503],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,633][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([9.3824e-01, 5.1334e-03, 1.0541e-04, 3.4770e-03, 5.5513e-04, 5.1704e-04,
        1.0974e-03, 1.7929e-04, 1.5439e-03, 1.4920e-03, 1.6180e-03, 2.9488e-03,
        2.4199e-03, 2.4429e-02, 8.3365e-03, 2.1657e-04, 5.5847e-03, 2.1076e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,634][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([1.3253e-02, 2.8018e-05, 4.7468e-05, 2.9726e-06, 1.0534e-05, 1.6273e-05,
        1.6418e-04, 2.9015e-02, 7.5596e-04, 3.8099e-04, 1.3979e-02, 2.1649e-02,
        5.9008e-03, 2.3886e-03, 1.3416e-01, 1.1053e-01, 3.1820e-01, 3.4952e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,636][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0423, 0.0770, 0.0494, 0.0684, 0.0495, 0.0723, 0.0340, 0.0913, 0.0652,
        0.0415, 0.0306, 0.0809, 0.0496, 0.0665, 0.0479, 0.0533, 0.0333, 0.0468],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,637][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0006, 0.0068, 0.0025, 0.0489, 0.0044, 0.0182, 0.0278, 0.0197, 0.3452,
        0.0198, 0.0878, 0.1232, 0.0561, 0.0333, 0.0167, 0.0832, 0.0733, 0.0327],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,639][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.1465, 0.0430, 0.0669, 0.0331, 0.0340, 0.1342, 0.0519, 0.1785, 0.0374,
        0.0162, 0.0251, 0.0167, 0.0432, 0.0395, 0.0352, 0.0362, 0.0467, 0.0156],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,641][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0567, 0.0012, 0.0019, 0.0037, 0.0015, 0.0052, 0.0238, 0.0364, 0.0563,
        0.0051, 0.1979, 0.1173, 0.0179, 0.0361, 0.0081, 0.1248, 0.2920, 0.0142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,643][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.8055, 0.0200, 0.0020, 0.0097, 0.0027, 0.0045, 0.0075, 0.0039, 0.0299,
        0.0064, 0.0240, 0.0091, 0.0078, 0.0178, 0.0067, 0.0134, 0.0113, 0.0180],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,644][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([4.3894e-07, 4.2492e-18, 1.0521e-16, 7.1635e-25, 1.8546e-20, 5.1486e-21,
        2.5590e-16, 3.3208e-07, 3.3067e-13, 7.9003e-15, 1.2740e-07, 4.9029e-07,
        9.0778e-10, 8.2581e-10, 2.8290e-01, 8.4765e-04, 7.0120e-01, 1.5045e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:18,645][circuit_model.py][line:2332][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0284, 0.0213, 0.1129, 0.0243, 0.0841, 0.0328, 0.0354, 0.0650, 0.0454,
        0.0320, 0.0933, 0.0443, 0.0631, 0.0222, 0.0626, 0.0601, 0.0291, 0.1277,
        0.0160], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,646][circuit_model.py][line:2335][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0325, 0.0493, 0.0498, 0.0574, 0.0506, 0.0567, 0.0473, 0.2283, 0.0504,
        0.0713, 0.0475, 0.0396, 0.0313, 0.0323, 0.0237, 0.0317, 0.0295, 0.0448,
        0.0260], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,647][circuit_model.py][line:2338][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0113, 0.0033, 0.0193, 0.0069, 0.0692, 0.0031, 0.0179, 0.0473, 0.0659,
        0.1668, 0.0628, 0.0119, 0.1083, 0.0173, 0.1358, 0.0654, 0.0391, 0.0371,
        0.1113], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,648][circuit_model.py][line:2341][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0155, 0.0569, 0.1001, 0.0660, 0.0589, 0.0438, 0.0472, 0.0357, 0.0698,
        0.0483, 0.0554, 0.0597, 0.0454, 0.0479, 0.0468, 0.0351, 0.0535, 0.0566,
        0.0575], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,650][circuit_model.py][line:2344][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([8.2966e-01, 1.4871e-02, 2.6923e-04, 9.7702e-03, 5.6504e-04, 6.5368e-04,
        1.4625e-03, 1.7550e-04, 3.4941e-03, 1.7589e-03, 4.3947e-03, 6.6017e-03,
        3.6879e-03, 8.6667e-02, 1.3757e-02, 5.1054e-04, 9.7171e-03, 9.9502e-04,
        1.0993e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,651][circuit_model.py][line:2347][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([5.8276e-03, 3.7490e-05, 8.0187e-05, 6.2776e-06, 1.1985e-05, 1.8931e-05,
        2.1601e-04, 5.6391e-02, 5.6618e-04, 4.0048e-04, 9.0718e-03, 1.7127e-02,
        6.2396e-03, 1.1692e-03, 8.2474e-02, 6.9626e-02, 2.3937e-01, 3.7944e-01,
        1.3192e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,653][circuit_model.py][line:2350][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0468, 0.0485, 0.0391, 0.0541, 0.0555, 0.0526, 0.0350, 0.0754, 0.0537,
        0.0528, 0.0308, 0.0619, 0.0553, 0.0474, 0.0561, 0.0530, 0.0366, 0.1080,
        0.0374], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,654][circuit_model.py][line:2353][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0019, 0.0049, 0.0015, 0.0194, 0.0043, 0.0056, 0.0105, 0.0306, 0.1322,
        0.0358, 0.0788, 0.0606, 0.0476, 0.0331, 0.0184, 0.0477, 0.0371, 0.1016,
        0.3285], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,656][circuit_model.py][line:2356][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.0739, 0.0514, 0.0398, 0.0370, 0.0135, 0.0826, 0.0765, 0.2343, 0.0737,
        0.0213, 0.0305, 0.0197, 0.0189, 0.0303, 0.0130, 0.0199, 0.0628, 0.0809,
        0.0201], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,658][circuit_model.py][line:2359][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0720, 0.0016, 0.0025, 0.0028, 0.0030, 0.0021, 0.0152, 0.0401, 0.0278,
        0.0078, 0.1064, 0.0799, 0.0204, 0.0340, 0.0229, 0.0836, 0.2278, 0.0424,
        0.2078], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,660][circuit_model.py][line:2362][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.7838, 0.0215, 0.0026, 0.0079, 0.0014, 0.0072, 0.0090, 0.0044, 0.0303,
        0.0034, 0.0175, 0.0098, 0.0095, 0.0189, 0.0064, 0.0132, 0.0164, 0.0060,
        0.0309], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,661][circuit_model.py][line:2365][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([4.5388e-10, 4.7063e-20, 1.1664e-17, 9.8754e-27, 3.0589e-21, 1.0519e-22,
        1.7395e-18, 3.1755e-07, 4.0366e-15, 3.5022e-15, 9.0471e-10, 1.9949e-09,
        3.6757e-10, 5.2990e-11, 3.5246e-02, 1.1286e-04, 7.4739e-03, 8.6508e-01,
        9.2083e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:18,665][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:18,666][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[7388],
        [ 881],
        [6237],
        [ 888],
        [1872],
        [ 337],
        [ 860],
        [ 538],
        [ 970],
        [ 775],
        [ 441],
        [1059],
        [7924],
        [ 492],
        [4503],
        [ 671],
        [1069],
        [1049],
        [ 178]], device='cuda:0')
[2024-07-24 10:25:18,668][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 7776],
        [10837],
        [15271],
        [ 6575],
        [ 2668],
        [ 1349],
        [ 3214],
        [ 2239],
        [ 4242],
        [ 6152],
        [ 2952],
        [ 9153],
        [19355],
        [ 3966],
        [11073],
        [ 4570],
        [ 6668],
        [10193],
        [ 1457]], device='cuda:0')
[2024-07-24 10:25:18,670][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[ 7777],
        [15203],
        [20988],
        [17363],
        [20446],
        [22038],
        [20841],
        [21138],
        [15821],
        [19898],
        [19585],
        [18049],
        [17699],
        [18463],
        [20225],
        [18813],
        [18747],
        [18039],
        [18937]], device='cuda:0')
[2024-07-24 10:25:18,672][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 954],
        [1322],
        [3311],
        [2798],
        [3589],
        [2966],
        [3752],
        [5015],
        [3337],
        [4686],
        [2766],
        [3430],
        [5112],
        [4111],
        [6282],
        [5925],
        [5598],
        [6662],
        [5838]], device='cuda:0')
[2024-07-24 10:25:18,674][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[11247],
        [ 8374],
        [11507],
        [10197],
        [11152],
        [11479],
        [12002],
        [12678],
        [13303],
        [14136],
        [14757],
        [14527],
        [14853],
        [15410],
        [16521],
        [16877],
        [17184],
        [17490],
        [17467]], device='cuda:0')
[2024-07-24 10:25:18,676][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[2855],
        [2910],
        [3367],
        [3667],
        [3476],
        [3622],
        [3756],
        [3514],
        [3767],
        [3836],
        [4100],
        [4066],
        [3987],
        [3988],
        [3927],
        [3834],
        [3891],
        [3938],
        [4022]], device='cuda:0')
[2024-07-24 10:25:18,678][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[10792],
        [ 5783],
        [ 4401],
        [ 3312],
        [ 2847],
        [ 3713],
        [ 3947],
        [ 4398],
        [ 4864],
        [ 5169],
        [ 5559],
        [ 5642],
        [ 6051],
        [ 5784],
        [ 5618],
        [ 5597],
        [ 5506],
        [ 5586],
        [ 5508]], device='cuda:0')
[2024-07-24 10:25:18,680][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[39871],
        [26644],
        [24704],
        [17374],
        [16483],
        [16323],
        [15858],
        [16331],
        [14921],
        [15561],
        [14854],
        [14463],
        [15007],
        [14096],
        [18049],
        [18668],
        [17809],
        [17997],
        [16329]], device='cuda:0')
[2024-07-24 10:25:18,682][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[19074],
        [18843],
        [17308],
        [18168],
        [17402],
        [17288],
        [17174],
        [17325],
        [18234],
        [18747],
        [19420],
        [19954],
        [20291],
        [21397],
        [22296],
        [23190],
        [23261],
        [22989],
        [22928]], device='cuda:0')
[2024-07-24 10:25:18,684][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[3296],
        [4638],
        [3415],
        [4125],
        [3296],
        [4267],
        [3253],
        [2742],
        [3959],
        [3730],
        [4512],
        [3029],
        [3667],
        [3621],
        [3648],
        [3829],
        [3108],
        [3741],
        [4179]], device='cuda:0')
[2024-07-24 10:25:18,686][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 7745],
        [ 7853],
        [ 6829],
        [ 8602],
        [ 7842],
        [ 8797],
        [11096],
        [14268],
        [13466],
        [14450],
        [14860],
        [14274],
        [14177],
        [14528],
        [13773],
        [15095],
        [14643],
        [13809],
        [13064]], device='cuda:0')
[2024-07-24 10:25:18,688][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[15474],
        [17431],
        [18504],
        [19879],
        [19828],
        [21282],
        [21701],
        [21968],
        [22419],
        [23316],
        [23802],
        [24138],
        [24643],
        [25060],
        [25366],
        [26197],
        [26368],
        [26649],
        [26960]], device='cuda:0')
[2024-07-24 10:25:18,689][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[30843],
        [32577],
        [20563],
        [29984],
        [20740],
        [17421],
        [19477],
        [ 6719],
        [16553],
        [15342],
        [13871],
        [11101],
        [18901],
        [11060],
        [14340],
        [11425],
        [12416],
        [13187],
        [12913]], device='cuda:0')
[2024-07-24 10:25:18,691][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[31686],
        [32564],
        [46210],
        [43421],
        [43696],
        [43427],
        [43449],
        [35976],
        [35970],
        [35970],
        [36018],
        [42730],
        [47874],
        [44849],
        [42934],
        [42829],
        [43786],
        [48194],
        [45077]], device='cuda:0')
[2024-07-24 10:25:18,693][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 5815],
        [ 3967],
        [ 8674],
        [ 4125],
        [10859],
        [11831],
        [11116],
        [ 4938],
        [ 4116],
        [ 6744],
        [ 6379],
        [ 6791],
        [10639],
        [ 4238],
        [10946],
        [ 8669],
        [ 8369],
        [ 2849],
        [ 2679]], device='cuda:0')
[2024-07-24 10:25:18,694][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[7065],
        [5146],
        [4750],
        [4834],
        [5669],
        [6249],
        [6569],
        [6654],
        [6867],
        [6781],
        [7986],
        [8119],
        [8264],
        [7971],
        [8198],
        [8770],
        [9391],
        [9100],
        [8390]], device='cuda:0')
[2024-07-24 10:25:18,696][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[21389],
        [ 9398],
        [ 6931],
        [ 7647],
        [ 6908],
        [ 7270],
        [ 6829],
        [ 6211],
        [ 5764],
        [ 6077],
        [ 5984],
        [ 6054],
        [ 5987],
        [ 5944],
        [ 5922],
        [ 5971],
        [ 5970],
        [ 6024],
        [ 5765]], device='cuda:0')
[2024-07-24 10:25:18,698][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[  41],
        [1208],
        [3652],
        [ 429],
        [1573],
        [3468],
        [4290],
        [7371],
        [4960],
        [5020],
        [5125],
        [3440],
        [4968],
        [2656],
        [3179],
        [3305],
        [3954],
        [3221],
        [4178]], device='cuda:0')
[2024-07-24 10:25:18,700][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[16837],
        [ 7560],
        [ 8002],
        [ 7469],
        [ 6811],
        [ 6655],
        [ 6642],
        [ 7133],
        [ 7024],
        [ 7188],
        [ 7243],
        [ 7202],
        [ 7266],
        [ 7263],
        [ 7079],
        [ 7015],
        [ 7020],
        [ 7141],
        [ 7134]], device='cuda:0')
[2024-07-24 10:25:18,701][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[38457],
        [40965],
        [41269],
        [35660],
        [35839],
        [40554],
        [40341],
        [40695],
        [39341],
        [38591],
        [38195],
        [36079],
        [38040],
        [25329],
        [27928],
        [38249],
        [36661],
        [41823],
        [38119]], device='cuda:0')
[2024-07-24 10:25:18,704][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[19652],
        [36956],
        [29234],
        [29243],
        [28850],
        [29930],
        [24698],
        [38412],
        [38315],
        [38427],
        [37983],
        [33892],
        [24256],
        [22117],
        [23151],
        [26575],
        [15101],
        [11907],
        [14949]], device='cuda:0')
[2024-07-24 10:25:18,705][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[12542],
        [19170],
        [20350],
        [18305],
        [18245],
        [17899],
        [19285],
        [19301],
        [19729],
        [19854],
        [19414],
        [19168],
        [19316],
        [19647],
        [20173],
        [19330],
        [19278],
        [19802],
        [19618]], device='cuda:0')
[2024-07-24 10:25:18,707][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[31030],
        [26620],
        [29995],
        [25457],
        [30012],
        [21410],
        [29313],
        [27458],
        [34021],
        [34477],
        [34024],
        [34954],
        [37071],
        [36458],
        [36479],
        [35826],
        [36773],
        [36521],
        [38142]], device='cuda:0')
[2024-07-24 10:25:18,709][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[15562],
        [18288],
        [19474],
        [22946],
        [21773],
        [15871],
        [13459],
        [17651],
        [18075],
        [18151],
        [19314],
        [17304],
        [19107],
        [17035],
        [12046],
        [18421],
        [17076],
        [16506],
        [17931]], device='cuda:0')
[2024-07-24 10:25:18,711][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[23319],
        [25870],
        [26023],
        [18122],
        [18219],
        [17978],
        [17050],
        [16999],
        [16541],
        [16305],
        [16869],
        [16801],
        [16765],
        [16923],
        [17013],
        [16554],
        [15540],
        [15738],
        [16054]], device='cuda:0')
[2024-07-24 10:25:18,713][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[4827],
        [4672],
        [5012],
        [5652],
        [5900],
        [3438],
        [4287],
        [4614],
        [3498],
        [3774],
        [3219],
        [3479],
        [4192],
        [4053],
        [4033],
        [1875],
        [3228],
        [3519],
        [2988]], device='cuda:0')
[2024-07-24 10:25:18,715][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[32301],
        [32269],
        [33740],
        [16093],
        [16270],
        [16092],
        [16104],
        [17505],
        [17466],
        [17466],
        [17489],
        [27925],
        [39115],
        [27101],
        [32908],
        [32961],
        [31889],
        [32491],
        [12812]], device='cuda:0')
[2024-07-24 10:25:18,717][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[33313],
        [28616],
        [28522],
        [31917],
        [31488],
        [32525],
        [32391],
        [30101],
        [30704],
        [30154],
        [29503],
        [29543],
        [28441],
        [33399],
        [31071],
        [30734],
        [31796],
        [32533],
        [35193]], device='cuda:0')
[2024-07-24 10:25:18,719][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[36653],
        [42675],
        [33667],
        [42780],
        [41016],
        [34827],
        [38106],
        [42297],
        [42342],
        [37595],
        [34641],
        [40428],
        [42652],
        [43782],
        [42883],
        [38475],
        [40663],
        [42343],
        [44453]], device='cuda:0')
[2024-07-24 10:25:18,721][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305],
        [8305]], device='cuda:0')
[2024-07-24 10:25:18,787][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:18,789][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,790][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,791][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,793][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,794][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,796][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,797][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,798][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,800][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,801][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,802][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,804][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:18,805][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0023, 0.9977], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,807][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0366, 0.9634], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,808][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1671, 0.8329], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,810][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.3409, 0.6591], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,812][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.9906, 0.0094], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,813][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.0418, 0.9582], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,815][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2967, 0.7033], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,816][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.9947, 0.0053], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,817][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.1733, 0.8267], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,818][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1264, 0.8736], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,818][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.9893, 0.0107], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,819][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0687, 0.9313], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:18,821][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.0009, 0.4590, 0.5401], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,822][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.0112, 0.5732, 0.4156], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,824][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.1773, 0.1452, 0.6774], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,825][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.5451, 0.2892, 0.1657], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,827][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.9823, 0.0087, 0.0090], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,828][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.0186, 0.5033, 0.4780], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,830][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.1085, 0.4925, 0.3990], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,832][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.9925, 0.0039, 0.0036], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,833][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.1169, 0.7379, 0.1452], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,835][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.0503, 0.5271, 0.4226], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,836][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.9442, 0.0338, 0.0220], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,838][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.0416, 0.4251, 0.5332], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:18,839][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0007, 0.3171, 0.3694, 0.3128], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,841][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0070, 0.4509, 0.2912, 0.2509], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,842][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ and] are: tensor([4.7341e-04, 7.2097e-02, 9.2628e-01, 1.1478e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,844][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.1127, 0.2348, 0.5462, 0.1063], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,846][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.9707, 0.0104, 0.0108, 0.0081], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,847][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0100, 0.3554, 0.3363, 0.2983], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,849][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0955, 0.2952, 0.3698, 0.2395], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,850][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.9884, 0.0045, 0.0041, 0.0029], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,852][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0421, 0.4500, 0.4278, 0.0801], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,854][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0444, 0.3625, 0.3014, 0.2917], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,855][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.4932, 0.3026, 0.1872, 0.0171], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,857][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0167, 0.2898, 0.3447, 0.3487], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:18,858][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0005, 0.2063, 0.2406, 0.2050, 0.3477], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,859][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0060, 0.3690, 0.2455, 0.1834, 0.1961], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,860][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0128, 0.1153, 0.7949, 0.0031, 0.0739], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,860][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.2696, 0.0649, 0.2959, 0.2296, 0.1400], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,861][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.9601, 0.0103, 0.0107, 0.0081, 0.0108], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,863][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0062, 0.2628, 0.2541, 0.2249, 0.2519], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,864][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0438, 0.2504, 0.2241, 0.2351, 0.2466], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,866][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.9848, 0.0043, 0.0040, 0.0027, 0.0042], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,867][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0512, 0.5015, 0.2066, 0.1682, 0.0725], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,869][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0253, 0.2985, 0.2412, 0.2321, 0.2031], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,871][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.6979, 0.2182, 0.0279, 0.0095, 0.0464], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,872][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0150, 0.2204, 0.2850, 0.2141, 0.2654], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:18,874][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.0004, 0.1668, 0.1945, 0.1664, 0.2784, 0.1936], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,876][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0043, 0.2773, 0.1890, 0.1722, 0.1773, 0.1799], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,877][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.0101, 0.1142, 0.8058, 0.0027, 0.0503, 0.0169], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,879][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.0776, 0.0638, 0.2720, 0.1516, 0.3181, 0.1168], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,881][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.9567, 0.0091, 0.0095, 0.0072, 0.0098, 0.0077], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,882][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0051, 0.2176, 0.2059, 0.1835, 0.2007, 0.1872], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,884][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0524, 0.1889, 0.1918, 0.1799, 0.2302, 0.1567], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,885][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.9833, 0.0036, 0.0034, 0.0025, 0.0036, 0.0037], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,887][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.0408, 0.2983, 0.1532, 0.1475, 0.2491, 0.1111], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,889][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0206, 0.2522, 0.2023, 0.1963, 0.1757, 0.1530], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,890][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.7803, 0.1495, 0.0307, 0.0105, 0.0080, 0.0210], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,892][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.0166, 0.1656, 0.2133, 0.1890, 0.2187, 0.1969], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:18,894][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0003, 0.1427, 0.1666, 0.1424, 0.2393, 0.1658, 0.1430],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,895][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0045, 0.2357, 0.1604, 0.1438, 0.1457, 0.1487, 0.1613],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,897][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0537, 0.1262, 0.6583, 0.0045, 0.0806, 0.0241, 0.0527],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,899][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1153, 0.0307, 0.2195, 0.1390, 0.0944, 0.3779, 0.0232],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,900][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.9497, 0.0091, 0.0092, 0.0070, 0.0096, 0.0075, 0.0079],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,901][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0044, 0.1844, 0.1734, 0.1556, 0.1691, 0.1583, 0.1549],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,902][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0405, 0.1642, 0.1750, 0.1540, 0.2153, 0.1455, 0.1055],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,902][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.9775, 0.0042, 0.0038, 0.0027, 0.0040, 0.0040, 0.0038],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,903][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0268, 0.2953, 0.1397, 0.1347, 0.1515, 0.1994, 0.0527],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,905][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0185, 0.2188, 0.1758, 0.1714, 0.1540, 0.1340, 0.1275],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,906][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.7153, 0.1955, 0.0289, 0.0100, 0.0130, 0.0276, 0.0097],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,908][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0091, 0.1347, 0.1762, 0.1508, 0.1776, 0.1389, 0.2128],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:18,910][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0002, 0.1194, 0.1377, 0.1178, 0.1950, 0.1363, 0.1178, 0.1758],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,911][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0049, 0.2014, 0.1338, 0.1093, 0.1131, 0.1143, 0.1219, 0.2013],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,912][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([3.1165e-01, 1.9311e-02, 9.4402e-02, 2.1716e-04, 6.4285e-03, 3.0726e-03,
        1.5854e-02, 5.4906e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,914][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.3443, 0.0216, 0.1126, 0.1311, 0.1107, 0.0835, 0.0437, 0.1525],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,916][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.9418, 0.0090, 0.0091, 0.0071, 0.0096, 0.0077, 0.0084, 0.0074],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,917][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0049, 0.1513, 0.1443, 0.1290, 0.1427, 0.1329, 0.1314, 0.1636],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,919][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0353, 0.1574, 0.1447, 0.1589, 0.1629, 0.1321, 0.1173, 0.0913],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,921][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.9758, 0.0036, 0.0034, 0.0025, 0.0036, 0.0037, 0.0035, 0.0038],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,922][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.0477, 0.1630, 0.1550, 0.0879, 0.1044, 0.1309, 0.1122, 0.1991],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,924][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0198, 0.1815, 0.1490, 0.1464, 0.1323, 0.1155, 0.1101, 0.1455],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,925][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([9.7219e-01, 1.7065e-02, 3.9865e-03, 9.0302e-04, 7.0202e-04, 2.0515e-03,
        1.1547e-03, 1.9434e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,927][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.0137, 0.1169, 0.1445, 0.1258, 0.1286, 0.1158, 0.1618, 0.1929],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:18,928][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0002, 0.1077, 0.1227, 0.1051, 0.1691, 0.1181, 0.1034, 0.1548, 0.1188],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,930][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0030, 0.1642, 0.1074, 0.0995, 0.1011, 0.1009, 0.1109, 0.1831, 0.1299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,932][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0182, 0.0284, 0.1636, 0.0011, 0.0214, 0.0051, 0.0217, 0.7310, 0.0095],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,934][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1161, 0.0065, 0.1383, 0.0431, 0.1924, 0.0732, 0.0355, 0.3390, 0.0560],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,935][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9304, 0.0096, 0.0098, 0.0077, 0.0104, 0.0081, 0.0090, 0.0079, 0.0071],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,937][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0044, 0.1313, 0.1238, 0.1111, 0.1227, 0.1154, 0.1151, 0.1437, 0.1325],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,939][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0326, 0.1249, 0.1418, 0.1079, 0.2014, 0.1079, 0.0879, 0.1117, 0.0839],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,940][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.9695, 0.0043, 0.0039, 0.0029, 0.0043, 0.0042, 0.0040, 0.0043, 0.0025],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,942][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0244, 0.1028, 0.1004, 0.0558, 0.1250, 0.1243, 0.1079, 0.2606, 0.0987],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,943][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0188, 0.1547, 0.1287, 0.1256, 0.1140, 0.1026, 0.0980, 0.1293, 0.1283],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,943][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.9368, 0.0356, 0.0058, 0.0025, 0.0025, 0.0059, 0.0025, 0.0034, 0.0049],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,944][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0095, 0.1014, 0.1321, 0.1177, 0.1188, 0.1041, 0.1499, 0.1571, 0.1094],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:18,945][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.0002, 0.0894, 0.1039, 0.0883, 0.1471, 0.1027, 0.0900, 0.1309, 0.0951,
        0.1524], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,947][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0026, 0.1552, 0.1040, 0.0801, 0.0853, 0.0843, 0.0888, 0.1558, 0.1007,
        0.1432], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,948][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([2.7843e-02, 1.9273e-02, 9.7200e-02, 4.4641e-04, 8.5946e-03, 3.2797e-03,
        1.1955e-02, 8.0954e-01, 1.3548e-02, 8.3185e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,949][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.0540, 0.0084, 0.0867, 0.0331, 0.0874, 0.0380, 0.0507, 0.3312, 0.2857,
        0.0248], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,951][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.9217, 0.0097, 0.0101, 0.0077, 0.0107, 0.0083, 0.0092, 0.0082, 0.0074,
        0.0069], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,953][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0028, 0.1170, 0.1094, 0.0972, 0.1082, 0.1017, 0.1009, 0.1277, 0.1165,
        0.1188], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,954][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0206, 0.1280, 0.1114, 0.1271, 0.1339, 0.1147, 0.1052, 0.0843, 0.0982,
        0.0766], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,956][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.9686, 0.0040, 0.0037, 0.0026, 0.0039, 0.0038, 0.0037, 0.0039, 0.0023,
        0.0034], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,958][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.0246, 0.1185, 0.1184, 0.0429, 0.0526, 0.1024, 0.0769, 0.1610, 0.2319,
        0.0708], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,959][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0129, 0.1406, 0.1141, 0.1112, 0.1007, 0.0895, 0.0855, 0.1179, 0.1155,
        0.1121], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,961][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.8594, 0.0923, 0.0127, 0.0039, 0.0032, 0.0111, 0.0034, 0.0048, 0.0043,
        0.0049], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,963][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0081, 0.0926, 0.1257, 0.0963, 0.1067, 0.0814, 0.1161, 0.1382, 0.0943,
        0.1406], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:18,965][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0002, 0.0829, 0.0956, 0.0810, 0.1332, 0.0919, 0.0802, 0.1210, 0.0911,
        0.1368, 0.0863], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,966][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0020, 0.1221, 0.0824, 0.0770, 0.0782, 0.0784, 0.0863, 0.1442, 0.0995,
        0.1258, 0.1042], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,968][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ at] are: tensor([1.7270e-01, 1.1921e-02, 4.9442e-02, 4.1662e-04, 7.3107e-03, 4.0864e-03,
        1.2689e-02, 5.1659e-01, 1.0916e-02, 1.2780e-02, 2.0115e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,969][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.1342, 0.0093, 0.0643, 0.0344, 0.0613, 0.0838, 0.0271, 0.2731, 0.1064,
        0.1458, 0.0603], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,971][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.9190, 0.0089, 0.0093, 0.0071, 0.0098, 0.0077, 0.0085, 0.0075, 0.0068,
        0.0064, 0.0089], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,973][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0030, 0.1034, 0.0984, 0.0877, 0.0973, 0.0910, 0.0908, 0.1139, 0.1040,
        0.1066, 0.1038], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,974][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0253, 0.1030, 0.1167, 0.0923, 0.1533, 0.0933, 0.0767, 0.0887, 0.0663,
        0.1310, 0.0533], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,976][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.9682, 0.0035, 0.0033, 0.0024, 0.0036, 0.0035, 0.0034, 0.0035, 0.0021,
        0.0031, 0.0033], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,978][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0286, 0.0701, 0.1008, 0.0434, 0.0472, 0.0949, 0.0845, 0.2291, 0.1336,
        0.1310, 0.0368], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,980][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0126, 0.1254, 0.1030, 0.0998, 0.0909, 0.0824, 0.0783, 0.1062, 0.1039,
        0.1021, 0.0955], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,981][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.9649, 0.0148, 0.0020, 0.0013, 0.0010, 0.0030, 0.0011, 0.0012, 0.0022,
        0.0011, 0.0074], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,983][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0065, 0.0812, 0.0975, 0.0943, 0.0947, 0.0841, 0.1286, 0.1226, 0.0843,
        0.1100, 0.0961], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:18,984][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0002, 0.0761, 0.0881, 0.0749, 0.1229, 0.0843, 0.0732, 0.1111, 0.0847,
        0.1255, 0.0800, 0.0790], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,985][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0020, 0.1136, 0.0748, 0.0685, 0.0700, 0.0698, 0.0759, 0.1338, 0.0901,
        0.1135, 0.0953, 0.0928], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,986][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.3387e-01, 7.3116e-03, 3.6242e-02, 2.1533e-04, 3.5423e-03, 1.3710e-03,
        6.7848e-03, 4.1543e-01, 6.3105e-03, 4.8729e-03, 2.4054e-01, 1.4350e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,986][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0827, 0.0048, 0.0377, 0.0219, 0.0361, 0.1053, 0.0099, 0.2922, 0.0665,
        0.1522, 0.1524, 0.0383], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,988][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.9220, 0.0079, 0.0079, 0.0062, 0.0083, 0.0067, 0.0073, 0.0065, 0.0060,
        0.0055, 0.0080, 0.0076], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,990][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0037, 0.0933, 0.0875, 0.0795, 0.0865, 0.0819, 0.0814, 0.1001, 0.0931,
        0.0940, 0.0927, 0.1064], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,992][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0231, 0.0973, 0.1089, 0.0851, 0.1471, 0.0840, 0.0607, 0.0786, 0.0699,
        0.1146, 0.0660, 0.0646], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,993][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.9679, 0.0034, 0.0031, 0.0023, 0.0034, 0.0033, 0.0031, 0.0033, 0.0019,
        0.0028, 0.0031, 0.0023], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,995][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0206, 0.0817, 0.0559, 0.0383, 0.0568, 0.0714, 0.0455, 0.1833, 0.1313,
        0.1193, 0.1242, 0.0719], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,996][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0140, 0.1085, 0.0919, 0.0886, 0.0826, 0.0743, 0.0711, 0.0933, 0.0928,
        0.0916, 0.0859, 0.1054], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:18,998][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.8658, 0.0503, 0.0098, 0.0040, 0.0032, 0.0082, 0.0038, 0.0083, 0.0070,
        0.0059, 0.0282, 0.0056], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,000][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0065, 0.0751, 0.0885, 0.0802, 0.0848, 0.0766, 0.1105, 0.1062, 0.0763,
        0.0959, 0.0791, 0.1202], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,002][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0001, 0.0683, 0.0793, 0.0668, 0.1120, 0.0770, 0.0671, 0.0992, 0.0722,
        0.1146, 0.0696, 0.0686, 0.1051], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,003][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0023, 0.1011, 0.0701, 0.0601, 0.0622, 0.0659, 0.0688, 0.1157, 0.0780,
        0.1055, 0.0832, 0.0797, 0.1076], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,005][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ house] are: tensor([8.9314e-02, 7.2991e-03, 3.5035e-02, 1.6034e-04, 4.3285e-03, 1.4512e-03,
        4.5534e-03, 3.9938e-01, 5.2703e-03, 6.0629e-03, 2.0190e-01, 1.5009e-01,
        9.5158e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,006][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0795, 0.0032, 0.0356, 0.0148, 0.0431, 0.0102, 0.0129, 0.0197, 0.0485,
        0.0196, 0.5662, 0.0879, 0.0589], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,008][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.9124, 0.0077, 0.0080, 0.0061, 0.0083, 0.0066, 0.0072, 0.0066, 0.0059,
        0.0055, 0.0081, 0.0078, 0.0098], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,010][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0021, 0.0864, 0.0803, 0.0721, 0.0787, 0.0745, 0.0736, 0.0932, 0.0858,
        0.0862, 0.0855, 0.1001, 0.0815], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,012][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0133, 0.0925, 0.0859, 0.0916, 0.1109, 0.0829, 0.0776, 0.0672, 0.0744,
        0.0913, 0.0731, 0.0880, 0.0515], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,013][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.9624, 0.0034, 0.0031, 0.0024, 0.0034, 0.0034, 0.0033, 0.0035, 0.0021,
        0.0030, 0.0033, 0.0025, 0.0044], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,015][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.0230, 0.0691, 0.0574, 0.0330, 0.0324, 0.0688, 0.0563, 0.1189, 0.1064,
        0.0847, 0.1283, 0.1705, 0.0511], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,017][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0102, 0.1047, 0.0850, 0.0826, 0.0756, 0.0671, 0.0637, 0.0876, 0.0860,
        0.0837, 0.0788, 0.1000, 0.0750], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,019][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.8701, 0.0711, 0.0114, 0.0028, 0.0021, 0.0053, 0.0028, 0.0047, 0.0041,
        0.0031, 0.0140, 0.0046, 0.0040], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,020][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0054, 0.0690, 0.0819, 0.0737, 0.0760, 0.0664, 0.0939, 0.0926, 0.0714,
        0.0893, 0.0693, 0.1014, 0.1099], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,022][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.0002, 0.0651, 0.0744, 0.0632, 0.0994, 0.0701, 0.0622, 0.0924, 0.0715,
        0.1022, 0.0669, 0.0659, 0.0925, 0.0740], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,024][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0019, 0.0966, 0.0630, 0.0563, 0.0570, 0.0584, 0.0631, 0.1069, 0.0739,
        0.0937, 0.0783, 0.0770, 0.0967, 0.0773], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,026][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.0238, 0.0108, 0.0523, 0.0004, 0.0068, 0.0042, 0.0090, 0.3371, 0.0084,
        0.0082, 0.2018, 0.2301, 0.0881, 0.0192], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,027][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0236, 0.0028, 0.1343, 0.0148, 0.0500, 0.0484, 0.0581, 0.1650, 0.0336,
        0.0528, 0.2221, 0.1217, 0.0696, 0.0032], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,027][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.9034, 0.0079, 0.0080, 0.0061, 0.0084, 0.0067, 0.0073, 0.0065, 0.0060,
        0.0055, 0.0079, 0.0076, 0.0096, 0.0090], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,028][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0025, 0.0795, 0.0745, 0.0670, 0.0729, 0.0689, 0.0679, 0.0848, 0.0793,
        0.0799, 0.0791, 0.0919, 0.0755, 0.0764], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,030][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0222, 0.0734, 0.1068, 0.0663, 0.1375, 0.0756, 0.0602, 0.0725, 0.0547,
        0.1080, 0.0486, 0.0656, 0.0701, 0.0386], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,031][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.9562, 0.0036, 0.0034, 0.0025, 0.0037, 0.0037, 0.0035, 0.0038, 0.0022,
        0.0033, 0.0036, 0.0027, 0.0048, 0.0031], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,033][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0121, 0.0490, 0.0606, 0.0296, 0.0474, 0.0597, 0.0585, 0.1605, 0.0742,
        0.0867, 0.0872, 0.1341, 0.0952, 0.0451], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,035][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0146, 0.0904, 0.0782, 0.0753, 0.0700, 0.0646, 0.0620, 0.0782, 0.0766,
        0.0755, 0.0707, 0.0857, 0.0709, 0.0872], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,037][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.3228, 0.0414, 0.0244, 0.0029, 0.0113, 0.0232, 0.0064, 0.0370, 0.0304,
        0.0571, 0.3328, 0.0246, 0.0283, 0.0573], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,038][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0049, 0.0679, 0.0785, 0.0731, 0.0682, 0.0611, 0.0874, 0.0909, 0.0686,
        0.0822, 0.0629, 0.0936, 0.0919, 0.0689], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,039][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([9.3984e-05, 5.5316e-02, 6.4812e-02, 5.4721e-02, 9.5227e-02, 6.4721e-02,
        5.6356e-02, 8.4127e-02, 6.0366e-02, 9.9892e-02, 5.8987e-02, 5.7622e-02,
        9.0253e-02, 6.4421e-02, 9.3085e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,041][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0014, 0.0987, 0.0662, 0.0498, 0.0523, 0.0517, 0.0561, 0.1078, 0.0662,
        0.0976, 0.0701, 0.0684, 0.0989, 0.0692, 0.0456], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,043][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([2.9421e-01, 2.6169e-03, 6.8405e-03, 9.0817e-05, 1.2935e-03, 7.4647e-04,
        2.7041e-03, 5.5921e-02, 3.4459e-03, 2.9653e-03, 8.0789e-02, 3.6852e-02,
        4.5978e-02, 1.1203e-02, 4.5434e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,044][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0840, 0.0036, 0.0309, 0.0169, 0.0158, 0.0373, 0.0224, 0.0289, 0.0870,
        0.0529, 0.3266, 0.0779, 0.1150, 0.0508, 0.0500], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,046][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.8842, 0.0079, 0.0081, 0.0064, 0.0085, 0.0069, 0.0076, 0.0069, 0.0062,
        0.0059, 0.0083, 0.0081, 0.0102, 0.0098, 0.0150], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,048][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0018, 0.0750, 0.0694, 0.0629, 0.0668, 0.0644, 0.0627, 0.0790, 0.0757,
        0.0752, 0.0748, 0.0875, 0.0700, 0.0725, 0.0625], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,050][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0123, 0.0876, 0.0781, 0.0835, 0.0843, 0.0791, 0.0685, 0.0622, 0.0649,
        0.0744, 0.0594, 0.0717, 0.0578, 0.0485, 0.0676], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,051][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.9592, 0.0031, 0.0028, 0.0021, 0.0030, 0.0030, 0.0029, 0.0030, 0.0019,
        0.0026, 0.0029, 0.0021, 0.0038, 0.0025, 0.0052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,053][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0107, 0.0691, 0.0326, 0.0219, 0.0095, 0.0523, 0.0356, 0.1098, 0.1330,
        0.0556, 0.1032, 0.1076, 0.0717, 0.1725, 0.0149], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,055][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0075, 0.0884, 0.0736, 0.0694, 0.0632, 0.0563, 0.0548, 0.0754, 0.0730,
        0.0710, 0.0667, 0.0867, 0.0648, 0.0854, 0.0638], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,056][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([9.1057e-01, 5.1695e-03, 8.7129e-04, 4.3774e-04, 1.8308e-03, 1.8067e-03,
        8.3325e-04, 1.1484e-03, 9.4188e-04, 1.0824e-03, 4.2587e-03, 9.7078e-04,
        3.8985e-04, 6.6410e-03, 6.3050e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,058][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0044, 0.0596, 0.0735, 0.0561, 0.0669, 0.0541, 0.0800, 0.0783, 0.0652,
        0.0707, 0.0614, 0.0881, 0.0872, 0.0611, 0.0931], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,060][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0001, 0.0540, 0.0628, 0.0523, 0.0859, 0.0593, 0.0519, 0.0783, 0.0583,
        0.0887, 0.0552, 0.0544, 0.0801, 0.0615, 0.0837, 0.0736],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,061][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0012, 0.0827, 0.0561, 0.0499, 0.0512, 0.0523, 0.0574, 0.0993, 0.0671,
        0.0853, 0.0714, 0.0705, 0.0851, 0.0712, 0.0492, 0.0500],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,063][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([1.4664e-01, 1.7904e-03, 1.0619e-02, 6.1886e-05, 7.0686e-04, 4.4139e-04,
        2.2634e-03, 1.0133e-01, 1.9330e-03, 1.6070e-03, 9.6105e-02, 4.6962e-02,
        2.0798e-02, 8.2772e-03, 3.2479e-01, 2.3568e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,064][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0431, 0.0023, 0.0213, 0.0099, 0.0152, 0.0202, 0.0276, 0.2350, 0.0447,
        0.0343, 0.0715, 0.3050, 0.0712, 0.0290, 0.0647, 0.0050],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,066][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.8801, 0.0076, 0.0077, 0.0061, 0.0081, 0.0064, 0.0071, 0.0063, 0.0058,
        0.0054, 0.0077, 0.0075, 0.0095, 0.0092, 0.0143, 0.0113],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,068][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0017, 0.0715, 0.0668, 0.0595, 0.0643, 0.0604, 0.0596, 0.0759, 0.0707,
        0.0703, 0.0700, 0.0824, 0.0660, 0.0673, 0.0608, 0.0528],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,069][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0169, 0.0808, 0.0737, 0.0731, 0.1014, 0.0609, 0.0522, 0.0511, 0.0580,
        0.0703, 0.0488, 0.0631, 0.0577, 0.0466, 0.0881, 0.0572],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,069][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.9591, 0.0028, 0.0026, 0.0019, 0.0027, 0.0027, 0.0026, 0.0027, 0.0017,
        0.0024, 0.0026, 0.0019, 0.0035, 0.0023, 0.0048, 0.0036],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,070][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0144, 0.0658, 0.0484, 0.0256, 0.0502, 0.0288, 0.0359, 0.1030, 0.0815,
        0.0585, 0.1189, 0.0956, 0.0672, 0.1218, 0.0701, 0.0143],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,072][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0083, 0.0841, 0.0685, 0.0667, 0.0602, 0.0536, 0.0509, 0.0686, 0.0683,
        0.0663, 0.0626, 0.0797, 0.0609, 0.0829, 0.0620, 0.0564],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,074][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.8308, 0.0183, 0.0052, 0.0011, 0.0016, 0.0027, 0.0018, 0.0034, 0.0024,
        0.0012, 0.0096, 0.0027, 0.0021, 0.0232, 0.0781, 0.0158],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,076][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0051, 0.0541, 0.0626, 0.0609, 0.0602, 0.0589, 0.0756, 0.0727, 0.0573,
        0.0676, 0.0560, 0.0810, 0.0787, 0.0545, 0.0767, 0.0781],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,077][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0001, 0.0508, 0.0582, 0.0495, 0.0798, 0.0554, 0.0484, 0.0737, 0.0563,
        0.0820, 0.0531, 0.0522, 0.0746, 0.0587, 0.0790, 0.0702, 0.0580],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,079][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0012, 0.0789, 0.0521, 0.0476, 0.0475, 0.0496, 0.0543, 0.0969, 0.0651,
        0.0820, 0.0686, 0.0667, 0.0805, 0.0671, 0.0456, 0.0491, 0.0473],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,080][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([2.2576e-01, 1.8061e-03, 5.9237e-03, 6.7453e-05, 9.4602e-04, 4.5021e-04,
        1.3719e-03, 5.3423e-02, 1.8135e-03, 1.0026e-03, 4.2579e-02, 2.6011e-02,
        1.1168e-02, 6.9492e-03, 2.6850e-01, 1.2176e-01, 2.3047e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,082][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0648, 0.0024, 0.0359, 0.0157, 0.0162, 0.0628, 0.0040, 0.1704, 0.0580,
        0.0990, 0.0910, 0.0448, 0.1716, 0.0202, 0.0505, 0.0796, 0.0131],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,084][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.8668, 0.0076, 0.0077, 0.0061, 0.0082, 0.0065, 0.0070, 0.0063, 0.0058,
        0.0054, 0.0077, 0.0075, 0.0094, 0.0092, 0.0142, 0.0115, 0.0130],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,086][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0021, 0.0660, 0.0613, 0.0554, 0.0593, 0.0561, 0.0555, 0.0697, 0.0660,
        0.0660, 0.0656, 0.0759, 0.0621, 0.0637, 0.0568, 0.0504, 0.0682],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,088][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0167, 0.0665, 0.0766, 0.0602, 0.0997, 0.0584, 0.0386, 0.0523, 0.0531,
        0.0864, 0.0459, 0.0478, 0.0552, 0.0424, 0.0933, 0.0684, 0.0384],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,089][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.9572, 0.0027, 0.0025, 0.0018, 0.0026, 0.0026, 0.0025, 0.0026, 0.0016,
        0.0023, 0.0025, 0.0019, 0.0034, 0.0022, 0.0046, 0.0035, 0.0035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,091][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0086, 0.0571, 0.0299, 0.0259, 0.0285, 0.0400, 0.0109, 0.1348, 0.0871,
        0.0652, 0.0898, 0.0713, 0.0821, 0.1460, 0.0504, 0.0527, 0.0198],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,093][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0092, 0.0757, 0.0637, 0.0614, 0.0572, 0.0518, 0.0495, 0.0649, 0.0640,
        0.0634, 0.0593, 0.0735, 0.0578, 0.0740, 0.0578, 0.0529, 0.0640],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,094][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([9.2481e-01, 2.0291e-02, 2.2277e-03, 1.3948e-03, 7.8444e-04, 1.8122e-03,
        8.6355e-04, 1.0422e-03, 1.2008e-03, 4.6849e-04, 3.3716e-03, 7.4757e-04,
        6.1092e-04, 1.3775e-02, 1.8285e-02, 5.0253e-03, 3.2906e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,096][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0034, 0.0468, 0.0588, 0.0516, 0.0580, 0.0472, 0.0739, 0.0664, 0.0494,
        0.0647, 0.0535, 0.0768, 0.0702, 0.0502, 0.0760, 0.0613, 0.0917],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,098][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0001, 0.0471, 0.0540, 0.0451, 0.0732, 0.0519, 0.0460, 0.0683, 0.0498,
        0.0774, 0.0481, 0.0468, 0.0694, 0.0531, 0.0719, 0.0627, 0.0527, 0.0824],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,099][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0015, 0.0814, 0.0557, 0.0437, 0.0464, 0.0459, 0.0492, 0.0866, 0.0546,
        0.0808, 0.0580, 0.0576, 0.0857, 0.0593, 0.0421, 0.0432, 0.0411, 0.0672],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,101][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([9.4020e-02, 1.0322e-03, 2.8267e-03, 1.2924e-05, 2.8995e-04, 2.3254e-04,
        8.2101e-04, 4.2234e-02, 1.2503e-03, 4.6516e-04, 4.0656e-02, 1.8226e-02,
        9.8561e-03, 2.6971e-03, 2.2961e-01, 1.0643e-01, 2.3907e-01, 2.1027e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,103][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0333, 0.0053, 0.0161, 0.0103, 0.0155, 0.0163, 0.0318, 0.0254, 0.1031,
        0.0217, 0.2060, 0.1414, 0.0951, 0.0407, 0.0554, 0.0233, 0.1373, 0.0221],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,104][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.8439, 0.0081, 0.0082, 0.0064, 0.0086, 0.0070, 0.0076, 0.0070, 0.0063,
        0.0058, 0.0084, 0.0082, 0.0101, 0.0098, 0.0153, 0.0124, 0.0143, 0.0127],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,106][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0021, 0.0618, 0.0582, 0.0518, 0.0559, 0.0523, 0.0517, 0.0650, 0.0614,
        0.0608, 0.0610, 0.0702, 0.0577, 0.0589, 0.0533, 0.0468, 0.0629, 0.0682],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,108][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0141, 0.0717, 0.0626, 0.0679, 0.0763, 0.0596, 0.0526, 0.0469, 0.0537,
        0.0609, 0.0503, 0.0601, 0.0478, 0.0396, 0.0649, 0.0618, 0.0520, 0.0570],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,110][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.9409, 0.0033, 0.0032, 0.0023, 0.0033, 0.0033, 0.0033, 0.0033, 0.0020,
        0.0029, 0.0031, 0.0024, 0.0043, 0.0029, 0.0059, 0.0045, 0.0045, 0.0048],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,111][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0118, 0.0472, 0.0467, 0.0225, 0.0163, 0.0400, 0.0406, 0.0771, 0.1168,
        0.0497, 0.0599, 0.1023, 0.0714, 0.1297, 0.0252, 0.0311, 0.0735, 0.0382],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,112][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0081, 0.0715, 0.0595, 0.0578, 0.0520, 0.0470, 0.0453, 0.0600, 0.0595,
        0.0579, 0.0550, 0.0687, 0.0535, 0.0695, 0.0536, 0.0502, 0.0605, 0.0705],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,112][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([9.0097e-01, 1.4624e-02, 2.4365e-03, 8.2407e-04, 1.0330e-03, 8.7831e-04,
        6.8708e-04, 5.7823e-04, 9.0049e-04, 1.9611e-04, 2.0349e-03, 8.2950e-04,
        6.3038e-04, 4.1828e-03, 4.4067e-02, 8.9061e-03, 3.1362e-03, 1.3089e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,114][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0047, 0.0499, 0.0570, 0.0504, 0.0521, 0.0461, 0.0634, 0.0617, 0.0500,
        0.0594, 0.0505, 0.0663, 0.0664, 0.0489, 0.0663, 0.0562, 0.0733, 0.0774],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,116][circuit_model.py][line:2294][INFO] ##5-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0001, 0.0448, 0.0510, 0.0434, 0.0685, 0.0483, 0.0425, 0.0641, 0.0496,
        0.0706, 0.0466, 0.0458, 0.0640, 0.0513, 0.0682, 0.0611, 0.0508, 0.0790,
        0.0503], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,118][circuit_model.py][line:2297][INFO] ##5-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0011, 0.0755, 0.0503, 0.0432, 0.0446, 0.0436, 0.0485, 0.0822, 0.0548,
        0.0727, 0.0579, 0.0577, 0.0727, 0.0576, 0.0427, 0.0427, 0.0419, 0.0627,
        0.0475], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,119][circuit_model.py][line:2300][INFO] ##5-th layer ##Weight##: The head3 weight for token [ to] are: tensor([7.9113e-02, 1.5253e-03, 2.9054e-03, 1.4688e-05, 2.5651e-04, 2.1454e-04,
        8.7921e-04, 4.1031e-02, 8.2591e-04, 3.2125e-04, 3.3420e-02, 1.6770e-02,
        4.5865e-03, 2.8945e-03, 1.9534e-01, 8.4186e-02, 2.4501e-01, 1.6271e-01,
        1.2800e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,121][circuit_model.py][line:2303][INFO] ##5-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0648, 0.0020, 0.0168, 0.0072, 0.0174, 0.0238, 0.0141, 0.0645, 0.0529,
        0.0275, 0.0778, 0.0491, 0.1612, 0.0200, 0.0710, 0.0656, 0.0682, 0.1613,
        0.0347], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,123][circuit_model.py][line:2306][INFO] ##5-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.8461, 0.0076, 0.0077, 0.0062, 0.0081, 0.0066, 0.0072, 0.0063, 0.0057,
        0.0053, 0.0076, 0.0075, 0.0093, 0.0090, 0.0140, 0.0114, 0.0133, 0.0118,
        0.0090], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,125][circuit_model.py][line:2309][INFO] ##5-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0021, 0.0576, 0.0541, 0.0488, 0.0517, 0.0494, 0.0488, 0.0606, 0.0572,
        0.0569, 0.0566, 0.0656, 0.0539, 0.0555, 0.0492, 0.0444, 0.0596, 0.0641,
        0.0638], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,126][circuit_model.py][line:2312][INFO] ##5-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0156, 0.0564, 0.0705, 0.0496, 0.0925, 0.0530, 0.0421, 0.0506, 0.0390,
        0.0781, 0.0348, 0.0491, 0.0490, 0.0307, 0.0816, 0.0570, 0.0410, 0.0710,
        0.0385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,128][circuit_model.py][line:2315][INFO] ##5-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.9381, 0.0033, 0.0031, 0.0023, 0.0033, 0.0033, 0.0032, 0.0032, 0.0020,
        0.0029, 0.0032, 0.0024, 0.0045, 0.0029, 0.0060, 0.0044, 0.0045, 0.0046,
        0.0029], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,130][circuit_model.py][line:2318][INFO] ##5-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0121, 0.0395, 0.0420, 0.0216, 0.0304, 0.0381, 0.0423, 0.1119, 0.0594,
        0.0624, 0.0503, 0.0912, 0.0609, 0.0646, 0.0442, 0.0400, 0.0670, 0.0763,
        0.0460], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,132][circuit_model.py][line:2321][INFO] ##5-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0093, 0.0661, 0.0561, 0.0540, 0.0499, 0.0453, 0.0435, 0.0558, 0.0552,
        0.0544, 0.0512, 0.0628, 0.0506, 0.0647, 0.0511, 0.0474, 0.0558, 0.0667,
        0.0601], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,133][circuit_model.py][line:2324][INFO] ##5-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.6516e-01, 9.2628e-03, 1.0009e-03, 4.2449e-04, 2.1781e-04, 5.9962e-04,
        2.8543e-04, 3.5138e-04, 3.8279e-04, 1.6308e-04, 1.6672e-03, 3.4042e-04,
        2.0644e-04, 5.5193e-03, 8.7153e-03, 2.5904e-03, 1.5808e-03, 7.0977e-04,
        8.2007e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,135][circuit_model.py][line:2327][INFO] ##5-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0037, 0.0434, 0.0511, 0.0491, 0.0462, 0.0438, 0.0610, 0.0581, 0.0425,
        0.0528, 0.0450, 0.0629, 0.0621, 0.0437, 0.0617, 0.0575, 0.0745, 0.0732,
        0.0679], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,205][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:19,207][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,208][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,209][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,211][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,212][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,214][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,215][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,216][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,217][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,219][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,220][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,220][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,221][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.9479, 0.0521], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,222][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.7740, 0.2260], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,223][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.1671, 0.8329], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,223][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.1053, 0.8947], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,224][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.1769, 0.8231], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,225][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.9733, 0.0267], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,226][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.2004, 0.7996], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,226][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.4349, 0.5651], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,228][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.4731, 0.5269], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,230][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.9850, 0.0150], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,231][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([9.9915e-01, 8.4879e-04], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,232][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.9248, 0.0752], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,234][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.9361, 0.0305, 0.0334], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,235][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.8380, 0.0821, 0.0799], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,237][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.1773, 0.1452, 0.6774], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,239][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.0840, 0.2769, 0.6391], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,240][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.0768, 0.1472, 0.7760], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,242][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.8749, 0.0868, 0.0383], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,243][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.1954, 0.6850, 0.1196], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,244][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.3556, 0.1625, 0.4819], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,245][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.3674, 0.2008, 0.4319], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,246][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.9202, 0.0668, 0.0130], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,246][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.9674, 0.0166, 0.0160], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,248][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.8855, 0.0632, 0.0513], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,249][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.8502, 0.0280, 0.0986, 0.0232], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,251][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.6853, 0.0789, 0.1192, 0.1167], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,252][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([4.7341e-04, 7.2097e-02, 9.2628e-01, 1.1478e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,253][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([5.6815e-04, 1.7775e-01, 8.1116e-01, 1.0522e-02], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,254][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([3.6734e-04, 8.6273e-02, 9.1208e-01, 1.2805e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,256][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.7938, 0.0987, 0.0747, 0.0329], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,258][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0087, 0.2791, 0.6842, 0.0280], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,259][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0145, 0.2095, 0.7645, 0.0115], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,261][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0053, 0.1520, 0.8382, 0.0045], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,262][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.8562, 0.0778, 0.0446, 0.0214], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,264][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.9901, 0.0048, 0.0039, 0.0012], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,266][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.7185, 0.1036, 0.0945, 0.0834], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,267][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.7734, 0.0340, 0.0402, 0.0474, 0.1050], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,269][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.6615, 0.0688, 0.0906, 0.0966, 0.0824], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,271][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0128, 0.1153, 0.7949, 0.0031, 0.0739], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,272][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0077, 0.1448, 0.7668, 0.0116, 0.0690], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,274][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0056, 0.0932, 0.8736, 0.0021, 0.0255], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,275][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.7511, 0.0831, 0.0838, 0.0378, 0.0441], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,277][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0478, 0.4678, 0.3911, 0.0368, 0.0566], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,279][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0348, 0.0988, 0.7977, 0.0074, 0.0614], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,280][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1141, 0.1323, 0.6097, 0.0136, 0.1303], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,282][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.7021, 0.1661, 0.0898, 0.0283, 0.0138], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,284][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.9551, 0.0189, 0.0075, 0.0034, 0.0151], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,285][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.6998, 0.0709, 0.0759, 0.0557, 0.0977], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,287][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.8210, 0.0271, 0.0536, 0.0207, 0.0420, 0.0355], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,288][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.7952, 0.0251, 0.0590, 0.0245, 0.0497, 0.0464], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,289][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0101, 0.1142, 0.8058, 0.0027, 0.0503, 0.0169], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,290][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.0045, 0.1810, 0.6992, 0.0129, 0.0642, 0.0382], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,291][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.0099, 0.1096, 0.7837, 0.0022, 0.0456, 0.0490], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,292][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.8025, 0.0347, 0.0548, 0.0234, 0.0571, 0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,293][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.0666, 0.3120, 0.3740, 0.0394, 0.1055, 0.1025], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,295][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1260, 0.2300, 0.4672, 0.0210, 0.0251, 0.1307], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,296][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.0855, 0.1410, 0.6143, 0.0063, 0.1004, 0.0526], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,298][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.8626, 0.0490, 0.0231, 0.0126, 0.0336, 0.0191], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,300][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.9786, 0.0054, 0.0051, 0.0012, 0.0018, 0.0079], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,301][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.6193, 0.0516, 0.0791, 0.0551, 0.0669, 0.1280], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,303][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.8821, 0.0111, 0.0169, 0.0102, 0.0165, 0.0239, 0.0393],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,305][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.8246, 0.0168, 0.0380, 0.0229, 0.0388, 0.0399, 0.0189],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,306][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0537, 0.1262, 0.6583, 0.0045, 0.0806, 0.0241, 0.0527],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,308][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0124, 0.1881, 0.6174, 0.0158, 0.0749, 0.0578, 0.0336],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,309][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0359, 0.1281, 0.6319, 0.0024, 0.0510, 0.0977, 0.0529],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,311][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.8361, 0.0270, 0.0280, 0.0190, 0.0468, 0.0225, 0.0207],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,313][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0886, 0.2839, 0.2701, 0.0411, 0.0888, 0.1595, 0.0681],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,315][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1854, 0.1876, 0.3072, 0.0162, 0.0281, 0.2160, 0.0595],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,316][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1490, 0.1129, 0.5150, 0.0085, 0.0909, 0.0738, 0.0499],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,318][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.9017, 0.0290, 0.0132, 0.0071, 0.0180, 0.0148, 0.0163],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,319][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([9.8750e-01, 2.1398e-03, 1.9126e-03, 5.4861e-04, 1.1101e-03, 4.4529e-03,
        2.3314e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,321][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.7107, 0.0269, 0.0500, 0.0264, 0.0612, 0.0433, 0.0816],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,322][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.9618, 0.0057, 0.0046, 0.0026, 0.0038, 0.0065, 0.0113, 0.0037],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,324][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.8347, 0.0058, 0.0367, 0.0079, 0.0373, 0.0280, 0.0336, 0.0160],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,325][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([3.1165e-01, 1.9311e-02, 9.4402e-02, 2.1716e-04, 6.4285e-03, 3.0726e-03,
        1.5854e-02, 5.4906e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,327][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.1668, 0.0833, 0.2054, 0.0030, 0.0190, 0.0227, 0.0221, 0.4775],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,328][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([1.6515e-01, 7.8731e-03, 2.5162e-02, 6.0357e-05, 1.6404e-03, 6.2036e-03,
        5.7766e-03, 7.8814e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,329][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.9622, 0.0041, 0.0100, 0.0017, 0.0095, 0.0041, 0.0050, 0.0035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,331][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.2923, 0.0690, 0.0283, 0.0026, 0.0099, 0.0248, 0.0277, 0.5454],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,332][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([6.0227e-01, 1.3310e-02, 4.7084e-02, 5.1500e-04, 3.6190e-03, 1.5773e-02,
        7.7536e-03, 3.0967e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,334][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([6.5413e-01, 1.7121e-02, 8.9776e-02, 4.9934e-04, 1.0561e-02, 9.5597e-03,
        1.4101e-02, 2.0425e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,334][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.9581, 0.0114, 0.0067, 0.0023, 0.0080, 0.0066, 0.0050, 0.0020],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,335][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([9.9448e-01, 1.1120e-03, 1.1072e-03, 2.0169e-04, 2.9182e-04, 1.2494e-03,
        9.8901e-04, 5.7185e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,336][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.8528, 0.0323, 0.0174, 0.0118, 0.0116, 0.0207, 0.0323, 0.0210],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,337][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.8759, 0.0136, 0.0139, 0.0069, 0.0132, 0.0114, 0.0352, 0.0084, 0.0216],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,338][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.5980, 0.0331, 0.0739, 0.0208, 0.0884, 0.0331, 0.0681, 0.0547, 0.0299],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,340][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0182, 0.0284, 0.1636, 0.0011, 0.0214, 0.0051, 0.0217, 0.7310, 0.0095],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,342][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0105, 0.0918, 0.2843, 0.0088, 0.0536, 0.0285, 0.0264, 0.4573, 0.0389],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,343][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([5.4027e-03, 1.0478e-02, 6.1049e-02, 2.7924e-04, 6.9974e-03, 1.1387e-02,
        1.3592e-02, 8.6555e-01, 2.5264e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,345][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.8035, 0.0209, 0.0224, 0.0088, 0.0293, 0.0177, 0.0376, 0.0464, 0.0136],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,346][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0089, 0.0225, 0.0186, 0.0018, 0.0060, 0.0113, 0.0115, 0.8995, 0.0199],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,348][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0562, 0.0265, 0.0764, 0.0024, 0.0099, 0.0288, 0.0130, 0.7586, 0.0283],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,350][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.0899, 0.0291, 0.1664, 0.0018, 0.0384, 0.0162, 0.0314, 0.5995, 0.0273],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,351][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.9330, 0.0129, 0.0071, 0.0029, 0.0100, 0.0063, 0.0094, 0.0064, 0.0120],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,352][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([9.9415e-01, 7.5590e-04, 7.3291e-04, 2.0188e-04, 4.3431e-04, 1.4743e-03,
        9.2246e-04, 3.3555e-04, 9.9329e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,354][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.7829, 0.0201, 0.0192, 0.0224, 0.0187, 0.0232, 0.0340, 0.0134, 0.0660],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,356][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.7923, 0.0089, 0.0144, 0.0039, 0.0155, 0.0078, 0.0276, 0.0061, 0.0608,
        0.0626], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,358][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.6215, 0.0260, 0.0549, 0.0171, 0.0308, 0.0628, 0.0874, 0.0177, 0.0421,
        0.0397], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,359][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([2.7843e-02, 1.9273e-02, 9.7200e-02, 4.4641e-04, 8.5946e-03, 3.2797e-03,
        1.1955e-02, 8.0954e-01, 1.3548e-02, 8.3185e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,361][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.0127, 0.0488, 0.2087, 0.0042, 0.0325, 0.0167, 0.0167, 0.5477, 0.0418,
        0.0700], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,362][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([4.7354e-03, 3.4657e-03, 3.5080e-02, 5.6825e-05, 2.9715e-03, 2.6299e-03,
        4.9198e-03, 9.3012e-01, 6.5178e-03, 9.5005e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,364][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.7807, 0.0313, 0.0215, 0.0074, 0.0158, 0.0237, 0.0319, 0.0190, 0.0517,
        0.0170], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,365][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0078, 0.0154, 0.0160, 0.0014, 0.0079, 0.0113, 0.0143, 0.8161, 0.0852,
        0.0246], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,367][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.0782, 0.0334, 0.1016, 0.0024, 0.0100, 0.0327, 0.0204, 0.6409, 0.0406,
        0.0399], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,369][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.0832, 0.0328, 0.1560, 0.0009, 0.0143, 0.0193, 0.0264, 0.5700, 0.0651,
        0.0320], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,370][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.8568, 0.0310, 0.0085, 0.0054, 0.0086, 0.0102, 0.0171, 0.0235, 0.0327,
        0.0062], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,372][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([9.7715e-01, 3.8845e-03, 3.5440e-03, 5.3564e-04, 1.1211e-03, 5.2433e-03,
        2.2703e-03, 1.0959e-03, 2.3365e-03, 2.8219e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,373][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.7014, 0.0401, 0.0415, 0.0227, 0.0179, 0.0212, 0.0267, 0.0308, 0.0586,
        0.0392], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,375][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.7303, 0.0182, 0.0141, 0.0055, 0.0116, 0.0138, 0.0472, 0.0125, 0.0490,
        0.0566, 0.0412], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,377][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.7371, 0.0250, 0.0510, 0.0057, 0.0286, 0.0173, 0.0295, 0.0210, 0.0189,
        0.0458, 0.0202], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,378][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([1.7270e-01, 1.1921e-02, 4.9442e-02, 4.1662e-04, 7.3107e-03, 4.0864e-03,
        1.2689e-02, 5.1659e-01, 1.0916e-02, 1.2780e-02, 2.0115e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,379][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0593, 0.0454, 0.1104, 0.0039, 0.0223, 0.0178, 0.0173, 0.3227, 0.0315,
        0.0568, 0.3125], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,380][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([4.6081e-02, 3.0090e-03, 2.5962e-02, 7.8161e-05, 2.8269e-03, 3.6334e-03,
        5.3140e-03, 5.3019e-01, 1.2062e-02, 2.2888e-02, 3.4796e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,381][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.7553, 0.0191, 0.0252, 0.0051, 0.0110, 0.0110, 0.0284, 0.0529, 0.0211,
        0.0474, 0.0236], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,382][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0699, 0.0220, 0.0091, 0.0019, 0.0030, 0.0084, 0.0102, 0.4649, 0.0818,
        0.0403, 0.2885], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,383][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.3038, 0.0091, 0.0394, 0.0009, 0.0066, 0.0087, 0.0069, 0.1502, 0.0141,
        0.0298, 0.4307], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,385][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.2366, 0.0166, 0.0846, 0.0006, 0.0101, 0.0100, 0.0198, 0.4277, 0.0264,
        0.0375, 0.1300], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,387][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.8715, 0.0207, 0.0070, 0.0036, 0.0075, 0.0089, 0.0118, 0.0088, 0.0161,
        0.0117, 0.0323], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,388][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([9.8808e-01, 1.2512e-03, 8.4333e-04, 2.7283e-04, 5.2919e-04, 3.0183e-03,
        1.4135e-03, 4.1654e-04, 1.4841e-03, 8.7135e-04, 1.8180e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,389][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.6567, 0.0230, 0.0252, 0.0164, 0.0165, 0.0212, 0.0319, 0.0070, 0.0488,
        0.0275, 0.1257], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,391][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.7709, 0.0103, 0.0061, 0.0034, 0.0067, 0.0085, 0.0236, 0.0089, 0.0339,
        0.0248, 0.0495, 0.0534], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,393][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.8659, 0.0067, 0.0101, 0.0039, 0.0133, 0.0083, 0.0074, 0.0130, 0.0074,
        0.0091, 0.0232, 0.0319], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,394][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([1.3387e-01, 7.3116e-03, 3.6242e-02, 2.1533e-04, 3.5423e-03, 1.3710e-03,
        6.7848e-03, 4.1543e-01, 6.3105e-03, 4.8729e-03, 2.4054e-01, 1.4350e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,396][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0296, 0.0240, 0.0756, 0.0016, 0.0107, 0.0100, 0.0083, 0.2368, 0.0167,
        0.0363, 0.2384, 0.3119], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,397][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.2187e-02, 1.2011e-03, 7.4209e-03, 2.0709e-05, 4.5631e-04, 1.4563e-03,
        2.2289e-03, 4.1127e-01, 4.9999e-03, 7.9703e-03, 3.8532e-01, 1.5547e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,399][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.7825, 0.0158, 0.0149, 0.0053, 0.0211, 0.0079, 0.0125, 0.0271, 0.0134,
        0.0244, 0.0545, 0.0205], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,400][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0337, 0.0070, 0.0046, 0.0006, 0.0008, 0.0043, 0.0048, 0.3521, 0.0458,
        0.0148, 0.3877, 0.1437], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,401][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([9.9543e-02, 4.8230e-03, 1.3067e-02, 2.6778e-04, 1.0083e-03, 4.5367e-03,
        2.5558e-03, 2.3774e-01, 9.8527e-03, 1.4468e-02, 4.9135e-01, 1.2078e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,403][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2092, 0.0134, 0.0644, 0.0005, 0.0085, 0.0047, 0.0087, 0.3190, 0.0110,
        0.0172, 0.2351, 0.1083], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,405][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.8795, 0.0145, 0.0056, 0.0030, 0.0090, 0.0068, 0.0084, 0.0053, 0.0118,
        0.0105, 0.0326, 0.0130], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,406][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([9.8492e-01, 1.4657e-03, 1.3718e-03, 3.5617e-04, 5.4330e-04, 2.8740e-03,
        1.7553e-03, 7.2326e-04, 1.6542e-03, 1.0603e-03, 2.0412e-03, 1.2302e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,408][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.7191, 0.0245, 0.0138, 0.0131, 0.0111, 0.0143, 0.0279, 0.0093, 0.0335,
        0.0152, 0.0576, 0.0606], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,410][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.7107, 0.0068, 0.0157, 0.0039, 0.0126, 0.0080, 0.0245, 0.0068, 0.0423,
        0.0450, 0.0571, 0.0365, 0.0300], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,411][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.6438, 0.0107, 0.0142, 0.0098, 0.0142, 0.0422, 0.0432, 0.0135, 0.0318,
        0.0454, 0.0558, 0.0557, 0.0196], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,412][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([8.9314e-02, 7.2991e-03, 3.5035e-02, 1.6034e-04, 4.3285e-03, 1.4512e-03,
        4.5534e-03, 3.9938e-01, 5.2703e-03, 6.0629e-03, 2.0190e-01, 1.5009e-01,
        9.5158e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,414][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0353, 0.0167, 0.0633, 0.0012, 0.0089, 0.0056, 0.0077, 0.1976, 0.0128,
        0.0272, 0.1719, 0.2177, 0.2342], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,415][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([3.2220e-02, 1.1675e-03, 7.7931e-03, 1.7313e-05, 6.0092e-04, 1.1512e-03,
        2.1848e-03, 3.3519e-01, 4.1708e-03, 5.8107e-03, 2.8638e-01, 2.0167e-01,
        1.2165e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,417][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.7423, 0.0117, 0.0142, 0.0052, 0.0087, 0.0104, 0.0162, 0.0237, 0.0269,
        0.0590, 0.0414, 0.0255, 0.0148], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,419][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0167, 0.0043, 0.0041, 0.0004, 0.0011, 0.0021, 0.0048, 0.2108, 0.0494,
        0.0188, 0.3534, 0.2293, 0.1048], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,420][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([1.1862e-01, 4.7225e-03, 1.1322e-02, 2.4043e-04, 9.9987e-04, 4.3275e-03,
        2.7496e-03, 2.0250e-01, 9.6429e-03, 1.7383e-02, 4.1920e-01, 1.5469e-01,
        5.3601e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,421][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([1.5976e-01, 9.4041e-03, 3.6797e-02, 2.6125e-04, 3.3402e-03, 7.3541e-03,
        8.4867e-03, 2.6171e-01, 1.4673e-02, 2.2879e-02, 2.0968e-01, 1.9690e-01,
        6.8763e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,423][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.8257, 0.0231, 0.0093, 0.0038, 0.0129, 0.0077, 0.0083, 0.0119, 0.0201,
        0.0135, 0.0391, 0.0187, 0.0057], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,424][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([9.7260e-01, 2.9880e-03, 2.4586e-03, 4.2623e-04, 9.3457e-04, 3.1637e-03,
        2.1149e-03, 1.1425e-03, 3.5158e-03, 2.1651e-03, 3.8759e-03, 2.1874e-03,
        2.4284e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,425][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.7281, 0.0209, 0.0169, 0.0147, 0.0158, 0.0179, 0.0179, 0.0170, 0.0334,
        0.0142, 0.0401, 0.0297, 0.0334], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,426][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.5421, 0.0102, 0.0133, 0.0099, 0.0169, 0.0107, 0.0439, 0.0074, 0.0650,
        0.0474, 0.0989, 0.0856, 0.0274, 0.0213], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,427][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.2224, 0.0262, 0.0308, 0.0229, 0.0389, 0.0509, 0.0692, 0.0334, 0.0585,
        0.0440, 0.1373, 0.1903, 0.0434, 0.0318], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,428][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0238, 0.0108, 0.0523, 0.0004, 0.0068, 0.0042, 0.0090, 0.3371, 0.0084,
        0.0082, 0.2018, 0.2301, 0.0881, 0.0192], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,430][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0051, 0.0340, 0.1014, 0.0029, 0.0172, 0.0091, 0.0087, 0.0934, 0.0146,
        0.0219, 0.1917, 0.2205, 0.2324, 0.0471], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,431][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([3.4351e-03, 2.3352e-03, 1.0092e-02, 4.7096e-05, 1.0941e-03, 3.1295e-03,
        3.1458e-03, 2.4483e-01, 9.0216e-03, 1.3480e-02, 2.5734e-01, 2.8387e-01,
        1.5959e-01, 8.5915e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,433][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.4117, 0.0186, 0.0227, 0.0212, 0.0349, 0.0267, 0.0487, 0.0549, 0.0669,
        0.0579, 0.1060, 0.0592, 0.0328, 0.0379], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,435][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0104, 0.0102, 0.0091, 0.0012, 0.0034, 0.0060, 0.0072, 0.2113, 0.0433,
        0.0189, 0.2546, 0.1701, 0.1583, 0.0960], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,436][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0554, 0.0085, 0.0322, 0.0008, 0.0033, 0.0083, 0.0034, 0.1974, 0.0156,
        0.0227, 0.3815, 0.1481, 0.1003, 0.0225], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,438][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.0328, 0.0168, 0.0549, 0.0018, 0.0116, 0.0143, 0.0188, 0.2802, 0.0258,
        0.0234, 0.2438, 0.1711, 0.0912, 0.0135], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,440][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.7791, 0.0181, 0.0087, 0.0054, 0.0176, 0.0096, 0.0120, 0.0089, 0.0179,
        0.0167, 0.0478, 0.0235, 0.0178, 0.0168], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,441][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([9.7930e-01, 1.8695e-03, 1.7876e-03, 5.7226e-04, 7.4060e-04, 3.8873e-03,
        2.0223e-03, 8.3847e-04, 2.0654e-03, 1.4457e-03, 1.8168e-03, 1.3684e-03,
        8.7174e-04, 1.4132e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,443][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.4094, 0.0351, 0.0329, 0.0242, 0.0289, 0.0241, 0.0331, 0.0131, 0.0577,
        0.0305, 0.0803, 0.0482, 0.0462, 0.1362], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,445][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.7059, 0.0079, 0.0067, 0.0067, 0.0148, 0.0131, 0.0259, 0.0069, 0.0337,
        0.0462, 0.0355, 0.0208, 0.0180, 0.0156, 0.0424], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,447][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.2633, 0.0249, 0.0344, 0.0231, 0.0205, 0.0752, 0.1354, 0.0293, 0.0683,
        0.0773, 0.0775, 0.0898, 0.0175, 0.0250, 0.0385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,448][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([2.9421e-01, 2.6169e-03, 6.8405e-03, 9.0817e-05, 1.2935e-03, 7.4647e-04,
        2.7041e-03, 5.5921e-02, 3.4459e-03, 2.9653e-03, 8.0789e-02, 3.6852e-02,
        4.5978e-02, 1.1203e-02, 4.5434e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,450][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0805, 0.0067, 0.0221, 0.0007, 0.0043, 0.0032, 0.0038, 0.0611, 0.0081,
        0.0169, 0.0977, 0.0839, 0.1472, 0.0355, 0.4283], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,451][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([7.1966e-02, 4.1597e-04, 2.5858e-03, 1.1806e-05, 1.9150e-04, 7.1928e-04,
        1.4139e-03, 1.6239e-01, 2.9502e-03, 8.1323e-03, 1.2229e-01, 7.7512e-02,
        1.1329e-01, 8.4829e-03, 4.2764e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,453][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.6480, 0.0151, 0.0155, 0.0112, 0.0088, 0.0400, 0.0393, 0.0090, 0.0363,
        0.0498, 0.0270, 0.0223, 0.0244, 0.0287, 0.0247], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,454][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0622, 0.0067, 0.0024, 0.0004, 0.0009, 0.0018, 0.0036, 0.1099, 0.0211,
        0.0141, 0.1299, 0.1127, 0.0927, 0.1804, 0.2613], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,456][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([3.5867e-01, 3.5535e-03, 1.1569e-02, 3.2678e-04, 1.2169e-03, 2.6221e-03,
        1.9981e-03, 6.0881e-02, 1.2135e-02, 7.2558e-03, 1.8173e-01, 8.7316e-02,
        3.6046e-02, 4.5638e-02, 1.8904e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,458][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.3460, 0.0040, 0.0119, 0.0004, 0.0032, 0.0137, 0.0110, 0.1438, 0.0245,
        0.0168, 0.1074, 0.0688, 0.0733, 0.0106, 0.1647], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,459][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.7378, 0.0204, 0.0106, 0.0046, 0.0031, 0.0092, 0.0169, 0.0120, 0.0210,
        0.0110, 0.0429, 0.0425, 0.0174, 0.0200, 0.0305], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,461][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([9.6153e-01, 1.3351e-03, 9.0931e-04, 3.2495e-04, 1.9218e-03, 3.5914e-03,
        2.0703e-03, 9.5156e-04, 1.3343e-03, 1.4239e-03, 2.1413e-03, 1.4102e-03,
        7.4444e-04, 1.5816e-03, 1.8730e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,463][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.6600, 0.0105, 0.0098, 0.0104, 0.0096, 0.0200, 0.0207, 0.0059, 0.0250,
        0.0074, 0.0274, 0.0294, 0.0209, 0.0401, 0.1030], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,464][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.5294, 0.0095, 0.0128, 0.0041, 0.0082, 0.0075, 0.0575, 0.0093, 0.0326,
        0.0639, 0.0479, 0.0567, 0.0191, 0.0454, 0.0708, 0.0252],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,466][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.4899, 0.0110, 0.0326, 0.0088, 0.0309, 0.0132, 0.0283, 0.0181, 0.0101,
        0.0397, 0.0526, 0.0633, 0.0118, 0.0488, 0.1150, 0.0260],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,467][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([1.4664e-01, 1.7904e-03, 1.0619e-02, 6.1886e-05, 7.0686e-04, 4.4139e-04,
        2.2634e-03, 1.0133e-01, 1.9330e-03, 1.6070e-03, 9.6105e-02, 4.6962e-02,
        2.0798e-02, 8.2772e-03, 3.2479e-01, 2.3568e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,468][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0360, 0.0077, 0.0227, 0.0005, 0.0024, 0.0027, 0.0035, 0.0749, 0.0048,
        0.0105, 0.0698, 0.1021, 0.1056, 0.0243, 0.3121, 0.2207],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,469][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([4.6910e-02, 3.1683e-04, 2.2278e-03, 6.4680e-06, 1.4563e-04, 2.2014e-04,
        5.7636e-04, 6.6745e-02, 9.3105e-04, 1.1797e-03, 5.5270e-02, 3.9116e-02,
        3.8785e-02, 4.5289e-03, 3.5856e-01, 3.8448e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,470][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.5547, 0.0076, 0.0145, 0.0026, 0.0242, 0.0047, 0.0059, 0.0130, 0.0063,
        0.0177, 0.0388, 0.0170, 0.0178, 0.0251, 0.2414, 0.0088],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,471][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([2.8756e-02, 3.0483e-03, 3.6680e-03, 3.0028e-04, 1.2592e-03, 1.5730e-03,
        1.8553e-03, 1.0014e-01, 1.0724e-02, 1.0308e-02, 6.4257e-02, 4.5423e-02,
        1.1045e-01, 5.4209e-02, 3.4570e-01, 2.1834e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,472][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([3.1748e-01, 2.1001e-03, 6.5201e-03, 1.0781e-04, 5.2568e-04, 1.5784e-03,
        1.0088e-03, 1.1232e-01, 3.1324e-03, 8.8098e-03, 1.4353e-01, 2.7622e-02,
        3.8120e-02, 1.2688e-02, 7.8936e-02, 2.4553e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,474][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([1.5033e-01, 4.5681e-03, 1.5489e-02, 1.1661e-04, 2.1859e-03, 1.1274e-03,
        3.0025e-03, 7.9981e-02, 3.7132e-03, 6.3568e-03, 1.2462e-01, 5.7166e-02,
        3.0385e-02, 1.3714e-02, 4.0989e-01, 9.7353e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,475][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.8273, 0.0091, 0.0034, 0.0022, 0.0061, 0.0041, 0.0072, 0.0051, 0.0101,
        0.0058, 0.0335, 0.0150, 0.0120, 0.0121, 0.0387, 0.0082],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,477][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([9.5815e-01, 1.5009e-03, 1.6748e-03, 3.4006e-04, 1.0235e-03, 2.8755e-03,
        2.0185e-03, 1.2852e-03, 2.0678e-03, 6.7144e-04, 2.1512e-03, 1.6396e-03,
        1.5952e-03, 1.9191e-03, 1.3238e-02, 7.8444e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,478][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.4767, 0.0204, 0.0131, 0.0188, 0.0138, 0.0202, 0.0242, 0.0056, 0.0213,
        0.0082, 0.0394, 0.0292, 0.0246, 0.0543, 0.1476, 0.0827],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,480][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.6874, 0.0054, 0.0054, 0.0028, 0.0047, 0.0072, 0.0146, 0.0043, 0.0270,
        0.0313, 0.0400, 0.0337, 0.0115, 0.0221, 0.0383, 0.0255, 0.0389],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,481][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.5165, 0.0107, 0.0187, 0.0108, 0.0190, 0.0182, 0.0130, 0.0489, 0.0277,
        0.0399, 0.0386, 0.0459, 0.0232, 0.0291, 0.0448, 0.0710, 0.0241],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,483][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([2.2576e-01, 1.8061e-03, 5.9237e-03, 6.7453e-05, 9.4602e-04, 4.5021e-04,
        1.3719e-03, 5.3423e-02, 1.8135e-03, 1.0026e-03, 4.2579e-02, 2.6011e-02,
        1.1168e-02, 6.9492e-03, 2.6850e-01, 1.2176e-01, 2.3047e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,485][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0368, 0.0063, 0.0161, 0.0005, 0.0028, 0.0027, 0.0022, 0.0337, 0.0038,
        0.0086, 0.0498, 0.0494, 0.0844, 0.0205, 0.2381, 0.3211, 0.1232],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,486][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([3.9857e-02, 1.6226e-04, 7.1873e-04, 2.4641e-06, 8.3628e-05, 2.1992e-04,
        1.8725e-04, 2.8877e-02, 4.8602e-04, 8.5609e-04, 2.1709e-02, 1.1369e-02,
        1.5359e-02, 1.6300e-03, 1.5576e-01, 4.9086e-01, 2.3185e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,488][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.5700, 0.0078, 0.0081, 0.0056, 0.0136, 0.0057, 0.0081, 0.0262, 0.0229,
        0.0324, 0.0399, 0.0199, 0.0248, 0.0339, 0.1044, 0.0485, 0.0284],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,489][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([3.4905e-02, 2.3018e-03, 1.3425e-03, 2.1560e-04, 5.7098e-04, 1.0938e-03,
        7.7770e-04, 7.5272e-02, 9.1714e-03, 5.8740e-03, 4.3216e-02, 2.0416e-02,
        4.8209e-02, 4.3323e-02, 1.3462e-01, 4.0096e-01, 1.7773e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,490][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([2.0606e-01, 1.4223e-03, 3.0259e-03, 9.8347e-05, 2.2146e-04, 2.1653e-03,
        5.4431e-04, 3.6528e-02, 1.9318e-03, 2.8783e-03, 9.2762e-02, 1.6963e-02,
        1.8963e-02, 6.9829e-03, 2.4056e-02, 5.1188e-01, 7.3518e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,492][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([1.8228e-01, 2.9806e-03, 8.4773e-03, 1.8977e-04, 1.5066e-03, 2.2108e-03,
        2.1103e-03, 8.2542e-02, 6.0677e-03, 5.4538e-03, 7.1882e-02, 4.8267e-02,
        2.9840e-02, 1.0926e-02, 1.7902e-01, 2.0808e-01, 1.5816e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,494][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.8753, 0.0072, 0.0030, 0.0015, 0.0055, 0.0031, 0.0038, 0.0027, 0.0055,
        0.0046, 0.0158, 0.0075, 0.0048, 0.0056, 0.0315, 0.0114, 0.0111],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,495][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([9.8773e-01, 4.9640e-04, 5.2201e-04, 1.1952e-04, 2.8293e-04, 1.1223e-03,
        5.7551e-04, 2.6986e-04, 5.4743e-04, 2.7562e-04, 6.6518e-04, 3.8654e-04,
        4.3250e-04, 3.4768e-04, 2.5796e-03, 1.8268e-03, 1.8217e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,497][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.5366, 0.0106, 0.0145, 0.0100, 0.0140, 0.0115, 0.0205, 0.0046, 0.0163,
        0.0088, 0.0265, 0.0190, 0.0173, 0.0357, 0.1327, 0.0223, 0.0991],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,498][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.6509, 0.0032, 0.0048, 0.0013, 0.0035, 0.0034, 0.0105, 0.0041, 0.0210,
        0.0113, 0.0297, 0.0200, 0.0112, 0.0263, 0.0864, 0.0221, 0.0579, 0.0323],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,500][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.6320, 0.0034, 0.0058, 0.0045, 0.0088, 0.0106, 0.0142, 0.0084, 0.0317,
        0.0107, 0.0624, 0.0388, 0.0135, 0.0243, 0.0373, 0.0493, 0.0336, 0.0108],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,502][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([9.4020e-02, 1.0322e-03, 2.8267e-03, 1.2924e-05, 2.8995e-04, 2.3254e-04,
        8.2101e-04, 4.2234e-02, 1.2503e-03, 4.6516e-04, 4.0656e-02, 1.8226e-02,
        9.8561e-03, 2.6971e-03, 2.2961e-01, 1.0643e-01, 2.3907e-01, 2.1027e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,503][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([1.5910e-02, 1.8652e-03, 4.4237e-03, 9.8303e-05, 7.7321e-04, 1.1852e-03,
        1.9154e-03, 3.4147e-02, 3.1208e-03, 5.0260e-03, 3.4685e-02, 3.8340e-02,
        4.8272e-02, 1.2586e-02, 1.8217e-01, 1.8579e-01, 1.8487e-01, 2.4482e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,504][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([1.3886e-02, 7.6627e-05, 4.4512e-04, 4.6303e-07, 3.0286e-05, 7.3299e-05,
        1.1751e-04, 1.7598e-02, 2.1351e-04, 2.8018e-04, 8.7111e-03, 8.8381e-03,
        5.4159e-03, 1.0096e-03, 1.4027e-01, 1.5574e-01, 2.5230e-01, 3.9499e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,506][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.6084, 0.0038, 0.0041, 0.0012, 0.0044, 0.0035, 0.0074, 0.0068, 0.0285,
        0.0124, 0.0600, 0.0276, 0.0181, 0.0299, 0.0535, 0.0482, 0.0726, 0.0096],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,507][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([6.2447e-02, 2.3859e-03, 6.1463e-04, 5.6806e-05, 2.1363e-04, 7.2796e-04,
        8.3883e-04, 3.3488e-02, 5.8183e-03, 3.0289e-03, 2.7290e-02, 4.3168e-02,
        5.7130e-02, 3.2554e-02, 8.3831e-02, 2.5292e-01, 2.0117e-01, 1.9232e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,509][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([1.6726e-01, 4.2851e-04, 1.2198e-03, 1.1997e-05, 7.6527e-05, 4.4063e-04,
        2.9907e-04, 1.3501e-02, 8.6501e-04, 1.2025e-03, 4.9290e-02, 1.4508e-02,
        4.2898e-03, 2.7842e-03, 2.5059e-02, 2.3491e-01, 7.5629e-02, 4.0823e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,510][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([1.6171e-01, 1.3006e-03, 4.1187e-03, 2.4801e-05, 4.0247e-04, 7.9841e-04,
        1.2520e-03, 2.0374e-02, 4.3012e-03, 2.6519e-03, 5.3902e-02, 3.5289e-02,
        1.9428e-02, 9.3062e-03, 1.5189e-01, 1.0892e-01, 2.0548e-01, 2.1884e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,512][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.7829, 0.0072, 0.0026, 0.0020, 0.0050, 0.0050, 0.0063, 0.0050, 0.0128,
        0.0081, 0.0270, 0.0145, 0.0087, 0.0119, 0.0450, 0.0216, 0.0282, 0.0063],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,513][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([9.3251e-01, 2.1958e-03, 1.6376e-03, 3.2969e-04, 1.0995e-03, 1.9400e-03,
        1.2732e-03, 7.0644e-04, 1.5201e-03, 4.7222e-04, 1.4685e-03, 1.3154e-03,
        1.1598e-03, 8.7511e-04, 1.8609e-02, 7.8076e-03, 5.7113e-03, 1.9373e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,514][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.6876, 0.0098, 0.0087, 0.0053, 0.0035, 0.0063, 0.0077, 0.0035, 0.0172,
        0.0059, 0.0281, 0.0126, 0.0144, 0.0253, 0.0540, 0.0203, 0.0478, 0.0420],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,515][circuit_model.py][line:2332][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.8134, 0.0036, 0.0030, 0.0009, 0.0029, 0.0029, 0.0122, 0.0028, 0.0077,
        0.0067, 0.0101, 0.0182, 0.0044, 0.0122, 0.0245, 0.0144, 0.0377, 0.0112,
        0.0109], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,516][circuit_model.py][line:2335][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.4782, 0.0089, 0.0136, 0.0052, 0.0131, 0.0195, 0.0273, 0.0211, 0.0117,
        0.0122, 0.0264, 0.0427, 0.0108, 0.0151, 0.0369, 0.1218, 0.0635, 0.0139,
        0.0580], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,517][circuit_model.py][line:2338][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([7.9113e-02, 1.5253e-03, 2.9054e-03, 1.4688e-05, 2.5651e-04, 2.1454e-04,
        8.7921e-04, 4.1031e-02, 8.2591e-04, 3.2125e-04, 3.3420e-02, 1.6770e-02,
        4.5865e-03, 2.8945e-03, 1.9534e-01, 8.4186e-02, 2.4501e-01, 1.6271e-01,
        1.2800e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,518][circuit_model.py][line:2341][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([1.1548e-02, 2.4822e-03, 5.4685e-03, 1.3130e-04, 6.8835e-04, 1.0681e-03,
        1.2255e-03, 2.1895e-02, 2.1346e-03, 3.1241e-03, 3.0611e-02, 3.1417e-02,
        4.0915e-02, 9.7906e-03, 1.0847e-01, 1.9562e-01, 1.1296e-01, 2.5843e-01,
        1.6202e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,520][circuit_model.py][line:2344][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([6.2242e-03, 3.1803e-05, 1.8288e-04, 3.1670e-07, 1.1460e-05, 5.3811e-05,
        8.0792e-05, 9.3953e-03, 9.5977e-05, 1.8237e-04, 4.6388e-03, 4.1749e-03,
        4.0076e-03, 3.8064e-04, 5.1672e-02, 1.1204e-01, 1.5403e-01, 5.1065e-01,
        1.4214e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,522][circuit_model.py][line:2347][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.6678, 0.0089, 0.0063, 0.0025, 0.0059, 0.0046, 0.0112, 0.0145, 0.0064,
        0.0091, 0.0188, 0.0165, 0.0101, 0.0117, 0.0511, 0.0483, 0.0550, 0.0183,
        0.0331], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,523][circuit_model.py][line:2350][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([4.4637e-02, 8.9448e-04, 4.6754e-04, 4.1773e-05, 1.1753e-04, 4.0081e-04,
        5.8669e-04, 3.8347e-02, 2.6486e-03, 1.7945e-03, 2.2356e-02, 1.6279e-02,
        2.4884e-02, 2.1449e-02, 5.9013e-02, 2.1325e-01, 1.5980e-01, 1.3435e-01,
        2.5869e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,524][circuit_model.py][line:2353][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([5.2586e-02, 4.2068e-04, 1.2263e-03, 1.1573e-05, 4.6158e-05, 3.0469e-04,
        1.7039e-04, 8.5893e-03, 7.6625e-04, 5.7410e-04, 2.7007e-02, 9.3736e-03,
        2.4569e-03, 2.2669e-03, 1.2711e-02, 2.3126e-01, 4.0442e-02, 3.8324e-01,
        2.2654e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,525][circuit_model.py][line:2356][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.2834e-01, 2.1686e-03, 6.2179e-03, 6.3890e-05, 6.0930e-04, 9.6266e-04,
        1.7432e-03, 4.0711e-02, 2.0215e-03, 1.6381e-03, 2.8158e-02, 2.2128e-02,
        1.0964e-02, 4.1586e-03, 1.0647e-01, 1.3395e-01, 1.7404e-01, 2.3229e-01,
        1.0337e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,527][circuit_model.py][line:2359][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.8623, 0.0050, 0.0019, 0.0009, 0.0037, 0.0020, 0.0029, 0.0024, 0.0038,
        0.0034, 0.0144, 0.0060, 0.0039, 0.0042, 0.0327, 0.0102, 0.0117, 0.0178,
        0.0108], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,529][circuit_model.py][line:2362][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.8960e-01, 4.7842e-04, 4.6231e-04, 8.4670e-05, 1.2751e-04, 4.7323e-04,
        3.1803e-04, 1.6441e-04, 2.8968e-04, 2.1035e-04, 5.9075e-04, 3.0588e-04,
        2.7922e-04, 2.8853e-04, 1.8991e-03, 1.1128e-03, 1.4676e-03, 5.5951e-04,
        1.2871e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,530][circuit_model.py][line:2365][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.5537, 0.0117, 0.0046, 0.0085, 0.0035, 0.0071, 0.0124, 0.0028, 0.0191,
        0.0049, 0.0398, 0.0187, 0.0141, 0.0478, 0.0626, 0.0205, 0.0869, 0.0260,
        0.0551], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,534][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:19,536][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 8013],
        [ 1726],
        [18740],
        [ 1596],
        [ 3535],
        [  628],
        [  855],
        [ 1003],
        [  672],
        [  527],
        [  174],
        [  607],
        [ 7220],
        [  585],
        [ 7766],
        [ 1121],
        [  660],
        [ 1469],
        [  135]], device='cuda:0')
[2024-07-24 10:25:19,538][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 7988],
        [ 2292],
        [17666],
        [ 1737],
        [ 4438],
        [ 1014],
        [ 1273],
        [ 1697],
        [ 1575],
        [ 1451],
        [  617],
        [ 2017],
        [10488],
        [ 1328],
        [11578],
        [ 2897],
        [ 2042],
        [ 2933],
        [  522]], device='cuda:0')
[2024-07-24 10:25:19,540][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[29644],
        [34281],
        [35518],
        [34145],
        [34476],
        [33822],
        [33775],
        [34008],
        [33771],
        [34020],
        [33984],
        [33936],
        [33877],
        [33608],
        [34083],
        [33924],
        [33883],
        [34094],
        [34059]], device='cuda:0')
[2024-07-24 10:25:19,542][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[44043],
        [41821],
        [42997],
        [41511],
        [40281],
        [40151],
        [40050],
        [37620],
        [37308],
        [35579],
        [35406],
        [35630],
        [34867],
        [34919],
        [34988],
        [35460],
        [35663],
        [35502],
        [35789]], device='cuda:0')
[2024-07-24 10:25:19,543][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 331],
        [6310],
        [2203],
        [2769],
        [2596],
        [2562],
        [2182],
        [1335],
        [2061],
        [2218],
        [1140],
        [1019],
        [1124],
        [1116],
        [ 195],
        [ 174],
        [ 241],
        [ 254],
        [ 318]], device='cuda:0')
[2024-07-24 10:25:19,546][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[  924],
        [ 8288],
        [ 7889],
        [23884],
        [17760],
        [18854],
        [13722],
        [ 2350],
        [  958],
        [  524],
        [ 1313],
        [ 1462],
        [10661],
        [ 3804],
        [ 7103],
        [  845],
        [ 2452],
        [ 5661],
        [ 5431]], device='cuda:0')
[2024-07-24 10:25:19,548][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[32967],
        [33146],
        [33332],
        [33490],
        [33631],
        [33636],
        [33643],
        [33713],
        [33806],
        [33863],
        [33897],
        [33843],
        [33884],
        [34023],
        [34236],
        [34324],
        [34352],
        [34443],
        [34474]], device='cuda:0')
[2024-07-24 10:25:19,549][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 4594],
        [ 8193],
        [ 9280],
        [ 8878],
        [ 9431],
        [ 9437],
        [ 9533],
        [ 9602],
        [ 9719],
        [10028],
        [10058],
        [10093],
        [10273],
        [10308],
        [10387],
        [10256],
        [10178],
        [10118],
        [10048]], device='cuda:0')
[2024-07-24 10:25:19,551][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[13882],
        [20279],
        [16871],
        [17868],
        [20198],
        [22461],
        [23093],
        [25895],
        [25629],
        [25991],
        [25749],
        [24663],
        [24409],
        [24337],
        [24107],
        [24087],
        [24226],
        [23707],
        [23643]], device='cuda:0')
[2024-07-24 10:25:19,553][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[16383],
        [16572],
        [16592],
        [16693],
        [16732],
        [16732],
        [16803],
        [16867],
        [16999],
        [17044],
        [17075],
        [17083],
        [17175],
        [17310],
        [17251],
        [17226],
        [17250],
        [17464],
        [17534]], device='cuda:0')
[2024-07-24 10:25:19,555][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[38935],
        [ 2056],
        [ 2232],
        [ 3341],
        [ 2572],
        [ 5035],
        [ 5347],
        [ 5974],
        [ 5777],
        [ 4569],
        [ 4767],
        [ 3835],
        [ 4268],
        [ 4664],
        [ 3909],
        [ 4304],
        [ 4434],
        [ 4782],
        [ 5277]], device='cuda:0')
[2024-07-24 10:25:19,557][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[14718],
        [14736],
        [13324],
        [13150],
        [12458],
        [11981],
        [11756],
        [11584],
        [11780],
        [11564],
        [11482],
        [11566],
        [11568],
        [11727],
        [11645],
        [11670],
        [11733],
        [11758],
        [11813]], device='cuda:0')
[2024-07-24 10:25:19,559][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[  349],
        [  347],
        [  509],
        [ 9097],
        [ 2310],
        [ 1296],
        [ 1934],
        [  365],
        [  391],
        [  586],
        [  359],
        [  551],
        [  531],
        [10998],
        [  453],
        [  662],
        [  370],
        [  376],
        [  332]], device='cuda:0')
[2024-07-24 10:25:19,561][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[26716],
        [28414],
        [46803],
        [42848],
        [44308],
        [42190],
        [40976],
        [40545],
        [39745],
        [41864],
        [40122],
        [39619],
        [37788],
        [37997],
        [39299],
        [40099],
        [39898],
        [39205],
        [38532]], device='cuda:0')
[2024-07-24 10:25:19,562][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[39841],
        [36152],
        [34413],
        [31423],
        [37469],
        [32892],
        [29386],
        [35468],
        [27167],
        [31457],
        [29027],
        [29185],
        [30679],
        [27193],
        [35884],
        [31178],
        [29921],
        [34166],
        [34336]], device='cuda:0')
[2024-07-24 10:25:19,564][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[21221],
        [17851],
        [16785],
        [10082],
        [ 5341],
        [ 9105],
        [14017],
        [19326],
        [14790],
        [12479],
        [10098],
        [10380],
        [ 9839],
        [ 7178],
        [ 9907],
        [ 8128],
        [10365],
        [10393],
        [14537]], device='cuda:0')
[2024-07-24 10:25:19,566][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[12468],
        [10494],
        [ 9850],
        [ 9327],
        [10396],
        [ 9795],
        [ 9884],
        [ 9988],
        [11172],
        [ 9896],
        [ 9526],
        [ 9757],
        [ 9694],
        [13244],
        [12107],
        [10695],
        [10326],
        [ 9762],
        [10402]], device='cuda:0')
[2024-07-24 10:25:19,568][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[31955],
        [23769],
        [20978],
        [20202],
        [19396],
        [19785],
        [20499],
        [23568],
        [21767],
        [22293],
        [22518],
        [24142],
        [23649],
        [24190],
        [10970],
        [ 9790],
        [10687],
        [10616],
        [11555]], device='cuda:0')
[2024-07-24 10:25:19,570][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[26673],
        [11635],
        [12743],
        [14030],
        [15073],
        [15388],
        [16368],
        [12842],
        [13855],
        [12676],
        [12782],
        [13642],
        [12285],
        [11406],
        [ 6697],
        [ 8815],
        [ 9773],
        [ 9520],
        [10160]], device='cuda:0')
[2024-07-24 10:25:19,572][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[14235],
        [ 5122],
        [ 1552],
        [ 1687],
        [ 1775],
        [ 1886],
        [ 2344],
        [ 7259],
        [ 8177],
        [ 8695],
        [ 7326],
        [ 8372],
        [ 7592],
        [ 8456],
        [13376],
        [13798],
        [13733],
        [15093],
        [14783]], device='cuda:0')
[2024-07-24 10:25:19,573][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[38666],
        [38107],
        [34777],
        [30911],
        [28542],
        [30950],
        [32636],
        [37710],
        [31296],
        [30682],
        [29575],
        [30795],
        [29134],
        [17585],
        [24878],
        [20739],
        [22504],
        [24089],
        [26258]], device='cuda:0')
[2024-07-24 10:25:19,575][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[42464],
        [43624],
        [40904],
        [31286],
        [32087],
        [31017],
        [34896],
        [39391],
        [35482],
        [35555],
        [40520],
        [40855],
        [39278],
        [37548],
        [33116],
        [29382],
        [39623],
        [39353],
        [40596]], device='cuda:0')
[2024-07-24 10:25:19,577][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[30857],
        [20971],
        [29506],
        [27550],
        [26586],
        [25216],
        [24043],
        [18641],
        [ 8814],
        [10321],
        [14117],
        [11204],
        [12592],
        [13030],
        [19511],
        [14906],
        [13798],
        [18501],
        [21691]], device='cuda:0')
[2024-07-24 10:25:19,579][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[25180],
        [ 6216],
        [10283],
        [18369],
        [19424],
        [19648],
        [19809],
        [19722],
        [26319],
        [27469],
        [26551],
        [28110],
        [28736],
        [31185],
        [23970],
        [27198],
        [24341],
        [23964],
        [25202]], device='cuda:0')
[2024-07-24 10:25:19,581][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[27900],
        [28015],
        [28112],
        [27258],
        [25756],
        [27396],
        [27398],
        [27801],
        [27828],
        [28141],
        [28145],
        [28047],
        [28001],
        [27720],
        [25959],
        [27161],
        [27309],
        [26672],
        [27863]], device='cuda:0')
[2024-07-24 10:25:19,582][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[45022],
        [45006],
        [43904],
        [44684],
        [42888],
        [44033],
        [44431],
        [44813],
        [44767],
        [44069],
        [44462],
        [44376],
        [44006],
        [44159],
        [42979],
        [43238],
        [44555],
        [42171],
        [44643]], device='cuda:0')
[2024-07-24 10:25:19,584][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[24120],
        [25542],
        [31660],
        [40363],
        [41755],
        [43417],
        [42765],
        [34712],
        [36016],
        [38881],
        [37663],
        [33632],
        [34420],
        [32940],
        [33799],
        [31909],
        [30567],
        [26682],
        [26992]], device='cuda:0')
[2024-07-24 10:25:19,586][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 4191],
        [12020],
        [13085],
        [14432],
        [15828],
        [13717],
        [11742],
        [ 9142],
        [12282],
        [12115],
        [11981],
        [11916],
        [12003],
        [13073],
        [15277],
        [16707],
        [14106],
        [14359],
        [11449]], device='cuda:0')
[2024-07-24 10:25:19,588][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[18054],
        [25219],
        [21607],
        [22175],
        [19926],
        [23149],
        [24822],
        [28171],
        [30226],
        [28745],
        [24603],
        [24448],
        [25791],
        [27638],
        [23487],
        [31752],
        [32343],
        [26620],
        [25035]], device='cuda:0')
[2024-07-24 10:25:19,590][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634],
        [39634]], device='cuda:0')
[2024-07-24 10:25:19,663][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:19,664][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,664][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,665][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,666][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,666][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,667][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,668][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,668][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,669][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,670][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,670][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,671][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:19,672][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.8202, 0.1798], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,673][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.1158, 0.8842], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,674][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.8796, 0.1204], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,674][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.5871, 0.4129], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,675][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.7999, 0.2001], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,677][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.3749, 0.6251], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,678][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.9184, 0.0816], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,680][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.5298, 0.4702], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,681][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.6318, 0.3682], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,683][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0246, 0.9754], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,685][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.8648, 0.1352], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,686][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.2993, 0.7007], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:19,688][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.9269, 0.0628, 0.0103], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,689][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.6943, 0.2346, 0.0711], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,691][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.7159, 0.1100, 0.1741], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,692][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.4368, 0.2775, 0.2857], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,694][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.7711, 0.1364, 0.0925], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,695][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.1637, 0.5831, 0.2531], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,697][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.7190, 0.0778, 0.2032], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,699][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.3826, 0.3602, 0.2572], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,700][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.4128, 0.1444, 0.4428], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,702][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.0058, 0.4726, 0.5216], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,704][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.6525, 0.1682, 0.1794], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,705][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.1896, 0.2467, 0.5637], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:19,707][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0488, 0.4655, 0.4619, 0.0238], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,708][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0526, 0.2738, 0.2917, 0.3819], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,710][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.5313, 0.1327, 0.2030, 0.1330], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,712][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.3444, 0.2306, 0.2328, 0.1922], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,713][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.4982, 0.1715, 0.1481, 0.1823], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,715][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1202, 0.3795, 0.2377, 0.2625], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,716][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.5271, 0.0782, 0.1929, 0.2018], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,716][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.3298, 0.3024, 0.2010, 0.1669], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,717][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0500, 0.2185, 0.7205, 0.0110], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,718][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0051, 0.2988, 0.4354, 0.2608], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,719][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.7021, 0.1012, 0.1109, 0.0858], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,721][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0146, 0.3884, 0.5703, 0.0266], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:19,723][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.3405, 0.3533, 0.2414, 0.0140, 0.0507], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,724][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.2529, 0.1726, 0.1557, 0.3197, 0.0991], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,726][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.6220, 0.0865, 0.1234, 0.0945, 0.0734], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,727][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.2861, 0.1838, 0.1826, 0.1551, 0.1924], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,729][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.6470, 0.1027, 0.0626, 0.1222, 0.0655], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,731][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0879, 0.3190, 0.1826, 0.2302, 0.1803], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,732][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.3989, 0.0534, 0.1358, 0.1430, 0.2689], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,734][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.2777, 0.2607, 0.1782, 0.1477, 0.1358], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,736][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1762, 0.1548, 0.5740, 0.0108, 0.0842], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,737][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0038, 0.2426, 0.3132, 0.2095, 0.2310], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,739][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.6445, 0.0915, 0.1041, 0.0819, 0.0779], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,740][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0643, 0.2411, 0.5601, 0.0145, 0.1200], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:19,742][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.2923, 0.3907, 0.1890, 0.0271, 0.0594, 0.0415], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,744][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0516, 0.1666, 0.0544, 0.3446, 0.0686, 0.3141], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,745][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.5401, 0.0876, 0.1321, 0.1011, 0.0814, 0.0576], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,747][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.2385, 0.1587, 0.1577, 0.1329, 0.1627, 0.1494], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,749][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.5415, 0.1054, 0.0702, 0.1218, 0.0728, 0.0884], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,750][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0763, 0.2389, 0.1562, 0.1963, 0.1508, 0.1816], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,752][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.4176, 0.0370, 0.0983, 0.1062, 0.2083, 0.1326], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,754][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.2581, 0.2296, 0.1535, 0.1255, 0.1146, 0.1187], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,755][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.1642, 0.1767, 0.4613, 0.0116, 0.0956, 0.0907], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,757][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0053, 0.1782, 0.1750, 0.1596, 0.2609, 0.2212], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,758][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.6476, 0.0769, 0.0855, 0.0641, 0.0619, 0.0640], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,760][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.0615, 0.2684, 0.4104, 0.0186, 0.1131, 0.1280], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:19,761][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1016, 0.3013, 0.2865, 0.0554, 0.1333, 0.0689, 0.0530],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,762][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0536, 0.1726, 0.0950, 0.3203, 0.0600, 0.2202, 0.0782],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,762][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.6349, 0.0626, 0.0918, 0.0743, 0.0526, 0.0388, 0.0450],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,763][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.2143, 0.1355, 0.1324, 0.1120, 0.1388, 0.1285, 0.1384],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,765][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.6222, 0.0815, 0.0492, 0.0961, 0.0508, 0.0641, 0.0361],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,766][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0587, 0.2224, 0.1363, 0.1659, 0.1359, 0.1771, 0.1037],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,768][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.4162, 0.0314, 0.0785, 0.0933, 0.1671, 0.1092, 0.1043],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,770][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2072, 0.1948, 0.1357, 0.1138, 0.1025, 0.1062, 0.1399],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,771][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2491, 0.1061, 0.4186, 0.0080, 0.0844, 0.0735, 0.0604],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,773][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0030, 0.1546, 0.1435, 0.1395, 0.2061, 0.2025, 0.1508],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,775][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.6412, 0.0665, 0.0765, 0.0552, 0.0544, 0.0558, 0.0505],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,776][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0664, 0.2096, 0.3482, 0.0211, 0.0933, 0.1944, 0.0671],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:19,778][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.7372, 0.0823, 0.0288, 0.0030, 0.0106, 0.0109, 0.0107, 0.1165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,779][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.1101, 0.0715, 0.0554, 0.1952, 0.0387, 0.2310, 0.0837, 0.2144],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,781][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.3314, 0.1047, 0.1592, 0.1171, 0.0909, 0.0634, 0.0833, 0.0499],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,783][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.2003, 0.1137, 0.1108, 0.0949, 0.1192, 0.1096, 0.1201, 0.1314],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,784][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.6782, 0.0705, 0.0387, 0.0858, 0.0376, 0.0471, 0.0246, 0.0176],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,786][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0821, 0.1786, 0.0901, 0.1310, 0.1006, 0.1320, 0.0812, 0.2045],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,788][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.4114, 0.0246, 0.0669, 0.0897, 0.1535, 0.1002, 0.0935, 0.0602],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,789][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.1615, 0.1527, 0.1133, 0.1014, 0.0920, 0.0949, 0.1255, 0.1586],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,790][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([4.0058e-01, 8.8374e-03, 7.1858e-02, 4.6252e-04, 4.8051e-03, 6.3023e-03,
        9.9151e-03, 4.9724e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,792][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0020, 0.1416, 0.1437, 0.1156, 0.1731, 0.1453, 0.1427, 0.1360],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,794][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.5491, 0.0728, 0.0813, 0.0610, 0.0591, 0.0591, 0.0545, 0.0632],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,796][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.1818, 0.0278, 0.1288, 0.0015, 0.0121, 0.0235, 0.0135, 0.6111],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:19,797][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.1118, 0.2023, 0.1272, 0.0286, 0.0713, 0.0362, 0.0348, 0.3712, 0.0165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,799][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0254, 0.0725, 0.0597, 0.1267, 0.0480, 0.1857, 0.0621, 0.3107, 0.1091],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,800][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.3534, 0.0943, 0.1368, 0.1126, 0.0837, 0.0621, 0.0793, 0.0438, 0.0340],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,802][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1569, 0.1001, 0.1008, 0.0858, 0.1102, 0.1034, 0.1118, 0.1240, 0.1071],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,804][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.5153, 0.0826, 0.0560, 0.1011, 0.0559, 0.0665, 0.0390, 0.0300, 0.0535],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,805][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0491, 0.1529, 0.0924, 0.1204, 0.0956, 0.1239, 0.0758, 0.1801, 0.1099],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,806][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2907, 0.0335, 0.0792, 0.0880, 0.1594, 0.1098, 0.1063, 0.0741, 0.0590],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,807][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.1529, 0.1349, 0.1048, 0.0883, 0.0775, 0.0828, 0.1085, 0.1374, 0.1128],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,807][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0527, 0.0141, 0.0686, 0.0013, 0.0184, 0.0108, 0.0177, 0.8010, 0.0154],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,809][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0016, 0.1042, 0.1212, 0.0973, 0.1756, 0.1486, 0.1243, 0.1477, 0.0794],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,811][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5359, 0.0661, 0.0728, 0.0547, 0.0532, 0.0531, 0.0491, 0.0564, 0.0587],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,812][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0285, 0.0404, 0.1194, 0.0039, 0.0264, 0.0331, 0.0189, 0.7094, 0.0199],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:19,814][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.3050, 0.1788, 0.0979, 0.0169, 0.0473, 0.0326, 0.0213, 0.2592, 0.0143,
        0.0266], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,816][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0763, 0.0455, 0.0261, 0.1323, 0.0366, 0.1394, 0.0707, 0.1814, 0.2280,
        0.0637], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,817][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.3712, 0.0896, 0.1236, 0.1115, 0.0767, 0.0553, 0.0726, 0.0387, 0.0300,
        0.0307], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,819][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.1496, 0.0896, 0.0903, 0.0772, 0.0980, 0.0898, 0.0977, 0.1062, 0.0966,
        0.1050], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,821][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.6221, 0.0685, 0.0387, 0.0836, 0.0380, 0.0484, 0.0253, 0.0185, 0.0344,
        0.0224], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,822][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0371, 0.1489, 0.0834, 0.1211, 0.0986, 0.1020, 0.0699, 0.1738, 0.1031,
        0.0621], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,824][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.3462, 0.0251, 0.0624, 0.0768, 0.1353, 0.0892, 0.0846, 0.0557, 0.0562,
        0.0684], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,826][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.1290, 0.1196, 0.0920, 0.0809, 0.0701, 0.0748, 0.1004, 0.1283, 0.1051,
        0.0998], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,827][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([8.8860e-02, 1.0766e-02, 7.1440e-02, 7.4893e-04, 8.7239e-03, 6.9815e-03,
        1.1973e-02, 7.5564e-01, 2.0616e-02, 2.4254e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,829][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0013, 0.0939, 0.1257, 0.0821, 0.1305, 0.1311, 0.1253, 0.1322, 0.0785,
        0.0992], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,831][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.4878, 0.0665, 0.0720, 0.0522, 0.0512, 0.0514, 0.0468, 0.0556, 0.0576,
        0.0590], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,832][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0347, 0.0285, 0.1144, 0.0021, 0.0247, 0.0213, 0.0145, 0.6964, 0.0209,
        0.0424], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:19,834][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.5332, 0.1243, 0.0430, 0.0122, 0.0286, 0.0190, 0.0178, 0.1516, 0.0093,
        0.0208, 0.0401], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,836][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0344, 0.0475, 0.0169, 0.0988, 0.0200, 0.1018, 0.0475, 0.2534, 0.1578,
        0.0873, 0.1346], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,837][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.2901, 0.0907, 0.1384, 0.1209, 0.0833, 0.0624, 0.0834, 0.0414, 0.0315,
        0.0332, 0.0244], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,839][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.1288, 0.0794, 0.0802, 0.0686, 0.0886, 0.0834, 0.0911, 0.1001, 0.0866,
        0.0988, 0.0943], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,841][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.5785, 0.0675, 0.0401, 0.0877, 0.0391, 0.0491, 0.0262, 0.0188, 0.0373,
        0.0240, 0.0317], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,842][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0394, 0.1223, 0.0779, 0.1084, 0.0848, 0.1101, 0.0607, 0.1577, 0.0934,
        0.0670, 0.0784], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,844][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.2680, 0.0287, 0.0648, 0.0770, 0.1244, 0.0904, 0.0883, 0.0578, 0.0527,
        0.0672, 0.0809], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,846][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1267, 0.1120, 0.0884, 0.0727, 0.0624, 0.0618, 0.0844, 0.1150, 0.0915,
        0.0857, 0.0996], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,848][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.2113, 0.0058, 0.0503, 0.0007, 0.0055, 0.0040, 0.0074, 0.4237, 0.0154,
        0.0196, 0.2563], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,849][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0015, 0.0922, 0.1029, 0.0818, 0.1293, 0.1195, 0.1068, 0.1224, 0.0697,
        0.1153, 0.0584], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,850][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.4516, 0.0625, 0.0684, 0.0513, 0.0496, 0.0500, 0.0460, 0.0527, 0.0549,
        0.0552, 0.0577], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,851][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.1140, 0.0234, 0.1017, 0.0023, 0.0125, 0.0193, 0.0131, 0.4717, 0.0162,
        0.0424, 0.1833], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:19,852][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2964, 0.1376, 0.0483, 0.0162, 0.0383, 0.0227, 0.0244, 0.2330, 0.0143,
        0.0305, 0.0617, 0.0765], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,853][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0217, 0.0364, 0.0209, 0.0769, 0.0175, 0.0956, 0.0437, 0.2062, 0.1311,
        0.0709, 0.2136, 0.0654], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,854][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.3845, 0.0815, 0.1196, 0.1077, 0.0686, 0.0502, 0.0671, 0.0333, 0.0260,
        0.0257, 0.0183, 0.0175], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,856][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1118, 0.0732, 0.0746, 0.0632, 0.0814, 0.0762, 0.0827, 0.0918, 0.0787,
        0.0898, 0.0857, 0.0908], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,858][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.5903, 0.0628, 0.0372, 0.0819, 0.0360, 0.0458, 0.0241, 0.0172, 0.0332,
        0.0212, 0.0280, 0.0224], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,859][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0471, 0.1100, 0.0707, 0.0939, 0.0778, 0.0961, 0.0614, 0.1294, 0.0874,
        0.0624, 0.0781, 0.0858], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,861][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2626, 0.0253, 0.0585, 0.0707, 0.1136, 0.0832, 0.0811, 0.0513, 0.0464,
        0.0586, 0.0700, 0.0788], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,863][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1136, 0.1012, 0.0786, 0.0647, 0.0547, 0.0544, 0.0748, 0.0988, 0.0833,
        0.0776, 0.0917, 0.1066], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,864][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.7462e-01, 3.0986e-03, 3.4112e-02, 2.4371e-04, 2.7895e-03, 1.9017e-03,
        3.2341e-03, 3.6928e-01, 9.9239e-03, 1.1170e-02, 2.6849e-01, 1.2113e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,866][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0013, 0.0856, 0.0918, 0.0788, 0.1216, 0.1145, 0.0903, 0.1190, 0.0695,
        0.0996, 0.0721, 0.0558], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,867][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4609, 0.0560, 0.0622, 0.0453, 0.0438, 0.0438, 0.0403, 0.0473, 0.0491,
        0.0487, 0.0517, 0.0508], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,869][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0635, 0.0101, 0.0602, 0.0009, 0.0058, 0.0116, 0.0050, 0.4153, 0.0108,
        0.0274, 0.2103, 0.1790], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:19,871][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.4512, 0.1141, 0.0476, 0.0095, 0.0276, 0.0207, 0.0171, 0.1241, 0.0099,
        0.0197, 0.0475, 0.0536, 0.0574], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,872][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0289, 0.0349, 0.0385, 0.0736, 0.0202, 0.0965, 0.0360, 0.1018, 0.1208,
        0.0615, 0.2789, 0.0845, 0.0240], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,874][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.3201, 0.0878, 0.1220, 0.1114, 0.0744, 0.0516, 0.0711, 0.0359, 0.0273,
        0.0271, 0.0214, 0.0206, 0.0292], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,876][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.1076, 0.0684, 0.0682, 0.0584, 0.0720, 0.0679, 0.0740, 0.0809, 0.0734,
        0.0790, 0.0791, 0.0845, 0.0865], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,878][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.6142, 0.0597, 0.0331, 0.0768, 0.0324, 0.0419, 0.0215, 0.0150, 0.0286,
        0.0182, 0.0236, 0.0198, 0.0152], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,880][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0374, 0.1059, 0.0662, 0.0933, 0.0682, 0.0933, 0.0504, 0.1405, 0.0790,
        0.0546, 0.0667, 0.0707, 0.0739], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,881][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.2767, 0.0182, 0.0471, 0.0600, 0.1063, 0.0737, 0.0690, 0.0434, 0.0405,
        0.0506, 0.0636, 0.0697, 0.0811], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,883][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.0944, 0.0889, 0.0715, 0.0609, 0.0515, 0.0507, 0.0716, 0.0954, 0.0762,
        0.0700, 0.0822, 0.0977, 0.0891], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,884][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ house] are: tensor([1.8551e-01, 2.2712e-03, 3.1781e-02, 1.6228e-04, 2.2979e-03, 1.7038e-03,
        2.7184e-03, 2.8721e-01, 6.8003e-03, 8.2896e-03, 1.8097e-01, 8.8318e-02,
        2.0197e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,886][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0012, 0.0846, 0.0971, 0.0761, 0.1121, 0.0917, 0.0898, 0.1083, 0.0640,
        0.1006, 0.0667, 0.0621, 0.0457], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,888][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.4187, 0.0559, 0.0595, 0.0454, 0.0425, 0.0444, 0.0399, 0.0462, 0.0486,
        0.0471, 0.0502, 0.0504, 0.0511], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,890][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0839, 0.0069, 0.0462, 0.0005, 0.0031, 0.0068, 0.0043, 0.2837, 0.0094,
        0.0151, 0.1675, 0.1654, 0.2070], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:19,892][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.2849, 0.1151, 0.0748, 0.0127, 0.0486, 0.0266, 0.0174, 0.1506, 0.0127,
        0.0171, 0.0552, 0.0766, 0.0838, 0.0241], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,893][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0080, 0.0300, 0.0293, 0.0514, 0.0134, 0.1110, 0.0420, 0.1566, 0.0853,
        0.0535, 0.2302, 0.1223, 0.0408, 0.0262], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,894][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.1988, 0.0910, 0.1230, 0.1074, 0.0808, 0.0596, 0.0809, 0.0419, 0.0321,
        0.0335, 0.0263, 0.0276, 0.0365, 0.0606], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,895][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0891, 0.0609, 0.0630, 0.0530, 0.0684, 0.0653, 0.0708, 0.0810, 0.0667,
        0.0781, 0.0730, 0.0776, 0.0846, 0.0686], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,896][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.2666, 0.0668, 0.0552, 0.0750, 0.0567, 0.0654, 0.0460, 0.0391, 0.0574,
        0.0492, 0.0598, 0.0501, 0.0430, 0.0696], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,898][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0399, 0.0874, 0.0600, 0.0732, 0.0641, 0.0859, 0.0518, 0.1188, 0.0715,
        0.0542, 0.0650, 0.0719, 0.0716, 0.0846], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,899][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.2024, 0.0244, 0.0544, 0.0628, 0.1031, 0.0770, 0.0747, 0.0503, 0.0412,
        0.0559, 0.0610, 0.0678, 0.0783, 0.0468], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,901][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.1013, 0.0857, 0.0685, 0.0558, 0.0471, 0.0453, 0.0639, 0.0919, 0.0684,
        0.0633, 0.0726, 0.0867, 0.0790, 0.0706], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,903][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0657, 0.0058, 0.0303, 0.0004, 0.0039, 0.0046, 0.0062, 0.2033, 0.0080,
        0.0143, 0.1941, 0.1511, 0.2844, 0.0279], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,905][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0008, 0.0671, 0.0906, 0.0607, 0.1159, 0.1018, 0.0837, 0.1018, 0.0579,
        0.0999, 0.0568, 0.0547, 0.0657, 0.0426], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,906][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.3842, 0.0544, 0.0573, 0.0449, 0.0422, 0.0432, 0.0393, 0.0455, 0.0475,
        0.0470, 0.0489, 0.0489, 0.0490, 0.0478], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,908][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0245, 0.0128, 0.0441, 0.0012, 0.0093, 0.0150, 0.0059, 0.2135, 0.0101,
        0.0201, 0.1693, 0.1822, 0.2538, 0.0384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:19,910][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([9.0203e-01, 2.3877e-02, 5.0450e-03, 8.8889e-04, 3.1204e-03, 2.1659e-03,
        1.2363e-03, 1.6072e-02, 8.5126e-04, 3.0142e-03, 6.1345e-03, 8.5993e-03,
        1.1375e-02, 3.4518e-03, 1.2136e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,911][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0775, 0.0335, 0.0227, 0.0688, 0.0164, 0.1212, 0.0506, 0.1474, 0.0793,
        0.0553, 0.1301, 0.0922, 0.0399, 0.0407, 0.0245], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,913][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.2921, 0.0661, 0.0974, 0.0884, 0.0634, 0.0473, 0.0645, 0.0305, 0.0272,
        0.0282, 0.0216, 0.0200, 0.0291, 0.0520, 0.0722], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,915][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0960, 0.0560, 0.0551, 0.0484, 0.0611, 0.0587, 0.0639, 0.0692, 0.0616,
        0.0691, 0.0669, 0.0719, 0.0763, 0.0633, 0.0826], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,916][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.5621, 0.0543, 0.0305, 0.0707, 0.0306, 0.0397, 0.0207, 0.0151, 0.0288,
        0.0185, 0.0242, 0.0199, 0.0158, 0.0492, 0.0200], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,918][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0255, 0.0927, 0.0576, 0.0722, 0.0595, 0.0791, 0.0408, 0.1395, 0.0698,
        0.0480, 0.0575, 0.0654, 0.0721, 0.0734, 0.0469], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,920][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.2433, 0.0150, 0.0382, 0.0488, 0.0865, 0.0613, 0.0568, 0.0356, 0.0358,
        0.0438, 0.0559, 0.0614, 0.0682, 0.0401, 0.1093], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,922][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0869, 0.0815, 0.0618, 0.0503, 0.0451, 0.0444, 0.0610, 0.0882, 0.0638,
        0.0621, 0.0660, 0.0760, 0.0741, 0.0620, 0.0767], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,923][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([3.6002e-01, 1.7831e-03, 1.8993e-02, 2.0171e-04, 1.1229e-03, 1.4492e-03,
        1.6510e-03, 1.0467e-01, 5.7117e-03, 8.8663e-03, 8.5437e-02, 4.4568e-02,
        1.7053e-01, 3.4706e-02, 1.6029e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,925][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0009, 0.0811, 0.1024, 0.0706, 0.0721, 0.0869, 0.0897, 0.1019, 0.0622,
        0.0706, 0.0592, 0.0520, 0.0494, 0.0457, 0.0553], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,927][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.4134, 0.0451, 0.0514, 0.0392, 0.0379, 0.0384, 0.0358, 0.0401, 0.0422,
        0.0421, 0.0446, 0.0437, 0.0430, 0.0412, 0.0420], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,928][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.1216, 0.0040, 0.0376, 0.0004, 0.0035, 0.0032, 0.0027, 0.1132, 0.0056,
        0.0152, 0.0844, 0.0712, 0.1615, 0.0374, 0.3385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:19,930][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.7360, 0.0447, 0.0110, 0.0021, 0.0053, 0.0045, 0.0037, 0.0421, 0.0021,
        0.0061, 0.0151, 0.0208, 0.0273, 0.0076, 0.0266, 0.0449],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,932][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0218, 0.0299, 0.0130, 0.0691, 0.0139, 0.0585, 0.0324, 0.1288, 0.0802,
        0.0585, 0.2025, 0.0883, 0.0334, 0.0603, 0.0224, 0.0871],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,934][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.3540, 0.0639, 0.0891, 0.0811, 0.0571, 0.0414, 0.0547, 0.0250, 0.0205,
        0.0212, 0.0161, 0.0153, 0.0227, 0.0455, 0.0578, 0.0345],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,936][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0821, 0.0529, 0.0533, 0.0454, 0.0586, 0.0558, 0.0605, 0.0677, 0.0570,
        0.0659, 0.0622, 0.0665, 0.0723, 0.0572, 0.0768, 0.0659],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,937][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.4587, 0.0586, 0.0367, 0.0748, 0.0368, 0.0458, 0.0263, 0.0194, 0.0347,
        0.0243, 0.0304, 0.0247, 0.0199, 0.0532, 0.0247, 0.0310],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,938][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0285, 0.0931, 0.0517, 0.0797, 0.0537, 0.0665, 0.0426, 0.1159, 0.0664,
        0.0451, 0.0579, 0.0649, 0.0624, 0.0768, 0.0425, 0.0522],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,939][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.1908, 0.0161, 0.0406, 0.0490, 0.0841, 0.0628, 0.0591, 0.0395, 0.0322,
        0.0433, 0.0502, 0.0573, 0.0656, 0.0362, 0.1009, 0.0724],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,940][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0803, 0.0781, 0.0579, 0.0467, 0.0412, 0.0422, 0.0586, 0.0837, 0.0601,
        0.0569, 0.0618, 0.0697, 0.0679, 0.0569, 0.0722, 0.0655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,941][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([3.3015e-01, 1.3983e-03, 1.6691e-02, 1.0207e-04, 9.1673e-04, 9.6862e-04,
        1.2561e-03, 9.5953e-02, 2.5877e-03, 3.0275e-03, 7.8207e-02, 3.3112e-02,
        7.8980e-02, 1.7260e-02, 1.7359e-01, 1.6580e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,943][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0009, 0.0584, 0.0703, 0.0522, 0.1060, 0.0792, 0.0720, 0.0811, 0.0432,
        0.0730, 0.0537, 0.0459, 0.0560, 0.0364, 0.0928, 0.0791],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,944][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.3495, 0.0482, 0.0525, 0.0392, 0.0387, 0.0396, 0.0360, 0.0418, 0.0434,
        0.0438, 0.0456, 0.0448, 0.0444, 0.0420, 0.0418, 0.0486],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,946][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([1.2860e-01, 3.3057e-03, 2.8615e-02, 2.9816e-04, 1.8263e-03, 2.4827e-03,
        2.2481e-03, 9.3416e-02, 3.0793e-03, 5.9721e-03, 5.7148e-02, 5.4106e-02,
        6.9709e-02, 1.8730e-02, 1.6553e-01, 3.6494e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:19,947][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3125, 0.0663, 0.0187, 0.0073, 0.0194, 0.0125, 0.0098, 0.0924, 0.0069,
        0.0156, 0.0330, 0.0413, 0.0651, 0.0192, 0.0896, 0.1036, 0.0866],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,949][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0140, 0.0253, 0.0140, 0.0564, 0.0091, 0.0453, 0.0134, 0.1782, 0.1081,
        0.0509, 0.2138, 0.0733, 0.0261, 0.0449, 0.0139, 0.0938, 0.0195],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,951][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.3669, 0.0599, 0.0874, 0.0770, 0.0540, 0.0379, 0.0502, 0.0238, 0.0190,
        0.0198, 0.0144, 0.0138, 0.0210, 0.0412, 0.0543, 0.0300, 0.0295],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,953][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0762, 0.0494, 0.0500, 0.0425, 0.0552, 0.0530, 0.0573, 0.0646, 0.0535,
        0.0628, 0.0586, 0.0625, 0.0685, 0.0537, 0.0715, 0.0615, 0.0590],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,955][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.4370, 0.0574, 0.0358, 0.0742, 0.0357, 0.0440, 0.0251, 0.0187, 0.0339,
        0.0237, 0.0293, 0.0238, 0.0192, 0.0527, 0.0239, 0.0302, 0.0354],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,956][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0256, 0.0830, 0.0535, 0.0683, 0.0561, 0.0678, 0.0428, 0.0974, 0.0617,
        0.0459, 0.0582, 0.0624, 0.0631, 0.0699, 0.0473, 0.0524, 0.0446],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,958][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1798, 0.0182, 0.0405, 0.0489, 0.0789, 0.0599, 0.0586, 0.0368, 0.0324,
        0.0419, 0.0481, 0.0548, 0.0592, 0.0371, 0.0821, 0.0621, 0.0606],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,960][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0759, 0.0736, 0.0572, 0.0446, 0.0384, 0.0378, 0.0536, 0.0775, 0.0578,
        0.0538, 0.0567, 0.0632, 0.0610, 0.0536, 0.0678, 0.0617, 0.0655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,961][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([2.8040e-01, 8.1814e-04, 1.1526e-02, 7.7481e-05, 6.3968e-04, 6.1945e-04,
        7.8918e-04, 5.4996e-02, 2.5506e-03, 1.9142e-03, 5.2946e-02, 2.2401e-02,
        5.9367e-02, 1.5504e-02, 1.2357e-01, 1.6123e-01, 2.1066e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,963][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0011, 0.0616, 0.0590, 0.0549, 0.0862, 0.0798, 0.0606, 0.0851, 0.0452,
        0.0789, 0.0497, 0.0413, 0.0558, 0.0367, 0.0774, 0.0804, 0.0464],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,965][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.3869, 0.0425, 0.0475, 0.0347, 0.0340, 0.0344, 0.0319, 0.0369, 0.0385,
        0.0390, 0.0410, 0.0396, 0.0392, 0.0370, 0.0371, 0.0428, 0.0370],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,966][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([5.3577e-02, 2.0303e-03, 1.2691e-02, 2.0318e-04, 1.0934e-03, 2.8180e-03,
        1.2930e-03, 6.3596e-02, 2.6485e-03, 5.4968e-03, 5.1395e-02, 2.8350e-02,
        6.3633e-02, 1.2645e-02, 1.0241e-01, 4.0583e-01, 1.9028e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:19,968][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.6488, 0.0384, 0.0093, 0.0013, 0.0047, 0.0047, 0.0030, 0.0302, 0.0015,
        0.0034, 0.0110, 0.0150, 0.0136, 0.0048, 0.0217, 0.0394, 0.0353, 0.1139],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,970][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0324, 0.0187, 0.0105, 0.0608, 0.0069, 0.1067, 0.0359, 0.0508, 0.0760,
        0.0379, 0.1793, 0.0554, 0.0134, 0.0249, 0.0130, 0.1961, 0.0664, 0.0148],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,972][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.3395, 0.0594, 0.0876, 0.0681, 0.0532, 0.0355, 0.0476, 0.0245, 0.0187,
        0.0203, 0.0140, 0.0134, 0.0210, 0.0378, 0.0562, 0.0294, 0.0291, 0.0450],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,973][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0698, 0.0457, 0.0464, 0.0395, 0.0515, 0.0489, 0.0537, 0.0592, 0.0498,
        0.0582, 0.0547, 0.0584, 0.0637, 0.0503, 0.0687, 0.0581, 0.0562, 0.0670],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,975][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.4987, 0.0537, 0.0305, 0.0717, 0.0302, 0.0377, 0.0202, 0.0143, 0.0278,
        0.0178, 0.0228, 0.0184, 0.0146, 0.0475, 0.0181, 0.0244, 0.0285, 0.0231],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,977][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0449, 0.0747, 0.0392, 0.0594, 0.0481, 0.0627, 0.0384, 0.0988, 0.0606,
        0.0382, 0.0550, 0.0606, 0.0528, 0.0676, 0.0413, 0.0493, 0.0420, 0.0665],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,979][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.1477, 0.0157, 0.0370, 0.0428, 0.0749, 0.0552, 0.0511, 0.0355, 0.0278,
        0.0399, 0.0438, 0.0499, 0.0590, 0.0312, 0.0887, 0.0622, 0.0530, 0.0848],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,981][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0653, 0.0660, 0.0533, 0.0446, 0.0381, 0.0391, 0.0536, 0.0722, 0.0540,
        0.0510, 0.0529, 0.0618, 0.0590, 0.0510, 0.0633, 0.0581, 0.0630, 0.0539],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,982][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([1.5089e-01, 7.8350e-04, 7.7619e-03, 4.7661e-05, 4.2664e-04, 5.9482e-04,
        8.0623e-04, 4.2666e-02, 2.4961e-03, 1.5892e-03, 3.9661e-02, 2.2636e-02,
        3.3912e-02, 1.3005e-02, 1.1150e-01, 1.2188e-01, 2.1598e-01, 2.3337e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,983][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0008, 0.0600, 0.0833, 0.0533, 0.0673, 0.0676, 0.0670, 0.0769, 0.0487,
        0.0557, 0.0431, 0.0447, 0.0433, 0.0376, 0.0502, 0.0904, 0.0557, 0.0543],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,984][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.2930, 0.0477, 0.0496, 0.0383, 0.0370, 0.0376, 0.0343, 0.0397, 0.0418,
        0.0416, 0.0439, 0.0433, 0.0428, 0.0403, 0.0395, 0.0453, 0.0398, 0.0446],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,985][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([2.9014e-02, 1.8001e-03, 9.0620e-03, 9.7027e-05, 6.9944e-04, 1.3813e-03,
        1.1181e-03, 4.8662e-02, 2.0262e-03, 3.9005e-03, 2.9625e-02, 2.5117e-02,
        3.6540e-02, 1.0389e-02, 9.6775e-02, 1.9909e-01, 2.0700e-01, 2.9770e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:19,987][circuit_model.py][line:2294][INFO] ##6-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.3679, 0.0740, 0.0142, 0.0045, 0.0101, 0.0071, 0.0066, 0.0514, 0.0044,
        0.0080, 0.0177, 0.0237, 0.0299, 0.0138, 0.0398, 0.0547, 0.0531, 0.1594,
        0.0599], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,989][circuit_model.py][line:2297][INFO] ##6-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0100, 0.0202, 0.0088, 0.0393, 0.0080, 0.0658, 0.0277, 0.1347, 0.0731,
        0.0581, 0.1147, 0.0698, 0.0300, 0.0281, 0.0139, 0.1343, 0.0490, 0.0300,
        0.0846], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,990][circuit_model.py][line:2300][INFO] ##6-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.3886, 0.0515, 0.0837, 0.0595, 0.0486, 0.0325, 0.0421, 0.0203, 0.0161,
        0.0179, 0.0118, 0.0114, 0.0185, 0.0316, 0.0517, 0.0243, 0.0239, 0.0388,
        0.0272], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,992][circuit_model.py][line:2303][INFO] ##6-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0620, 0.0435, 0.0450, 0.0378, 0.0498, 0.0477, 0.0517, 0.0585, 0.0470,
        0.0564, 0.0518, 0.0551, 0.0610, 0.0473, 0.0641, 0.0540, 0.0517, 0.0628,
        0.0528], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,994][circuit_model.py][line:2306][INFO] ##6-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.3501, 0.0545, 0.0361, 0.0694, 0.0365, 0.0445, 0.0272, 0.0205, 0.0359,
        0.0261, 0.0320, 0.0259, 0.0215, 0.0518, 0.0261, 0.0312, 0.0368, 0.0312,
        0.0430], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,996][circuit_model.py][line:2309][INFO] ##6-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0310, 0.0703, 0.0456, 0.0577, 0.0493, 0.0600, 0.0404, 0.0810, 0.0551,
        0.0399, 0.0496, 0.0554, 0.0527, 0.0615, 0.0407, 0.0467, 0.0426, 0.0661,
        0.0544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,998][circuit_model.py][line:2312][INFO] ##6-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.1306, 0.0179, 0.0387, 0.0450, 0.0709, 0.0535, 0.0524, 0.0347, 0.0275,
        0.0378, 0.0424, 0.0492, 0.0551, 0.0312, 0.0766, 0.0554, 0.0512, 0.0741,
        0.0560], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:19,999][circuit_model.py][line:2315][INFO] ##6-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0672, 0.0652, 0.0521, 0.0406, 0.0354, 0.0352, 0.0482, 0.0671, 0.0508,
        0.0501, 0.0504, 0.0586, 0.0566, 0.0474, 0.0612, 0.0550, 0.0566, 0.0489,
        0.0533], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,001][circuit_model.py][line:2318][INFO] ##6-th layer ##Weight##: The head9 weight for token [ to] are: tensor([1.5114e-01, 4.5210e-04, 5.5872e-03, 2.0618e-05, 2.6192e-04, 2.9525e-04,
        4.8344e-04, 4.7574e-02, 1.0085e-03, 7.8754e-04, 2.8978e-02, 1.3073e-02,
        3.0738e-02, 7.5751e-03, 1.0102e-01, 8.6911e-02, 1.6681e-01, 2.2885e-01,
        1.2844e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,003][circuit_model.py][line:2321][INFO] ##6-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0009, 0.0489, 0.0610, 0.0452, 0.0795, 0.0691, 0.0604, 0.0726, 0.0397,
        0.0672, 0.0403, 0.0405, 0.0460, 0.0318, 0.0639, 0.0741, 0.0471, 0.0723,
        0.0396], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,004][circuit_model.py][line:2324][INFO] ##6-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.3622, 0.0381, 0.0448, 0.0321, 0.0315, 0.0312, 0.0294, 0.0340, 0.0354,
        0.0358, 0.0379, 0.0366, 0.0366, 0.0339, 0.0335, 0.0383, 0.0334, 0.0390,
        0.0361], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,006][circuit_model.py][line:2327][INFO] ##6-th layer ##Weight##: The head12 weight for token [ to] are: tensor([3.1406e-02, 1.0937e-03, 8.1079e-03, 5.5221e-05, 3.2996e-04, 9.7885e-04,
        5.5682e-04, 3.8397e-02, 9.2080e-04, 2.1218e-03, 1.9198e-02, 1.2504e-02,
        2.4022e-02, 6.5006e-03, 4.1815e-02, 2.1101e-01, 1.3537e-01, 2.9401e-01,
        1.7160e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,071][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:20,072][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,073][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,074][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,074][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,075][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,076][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,076][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,078][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,078][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,079][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,080][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,080][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,081][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.8759, 0.1241], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,082][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0296, 0.9704], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,084][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.4102, 0.5898], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,085][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.8009, 0.1991], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,086][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.4151, 0.5849], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,087][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.6937, 0.3063], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,088][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0867, 0.9133], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,088][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.5467, 0.4533], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,090][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.4738, 0.5262], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,091][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0192, 0.9808], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,092][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.7178, 0.2822], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,094][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.2993, 0.7007], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,095][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.4275, 0.1679, 0.4047], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,097][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.2503, 0.2860, 0.4637], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,099][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.2941, 0.4472, 0.2586], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,100][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.5353, 0.1921, 0.2727], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,102][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.2321, 0.5083, 0.2596], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,103][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.4356, 0.4024, 0.1620], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,105][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.0343, 0.4243, 0.5414], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,107][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.3154, 0.3046, 0.3800], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,108][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.4111, 0.0911, 0.4977], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,110][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.0069, 0.9261, 0.0670], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,112][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.2561, 0.4350, 0.3089], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,113][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.1896, 0.2467, 0.5637], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,115][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0947, 0.2346, 0.6343, 0.0364], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,116][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([3.8716e-04, 1.6323e-01, 8.2226e-01, 1.4129e-02], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,117][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.2251, 0.2711, 0.2043, 0.2996], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,119][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.5704, 0.1480, 0.1471, 0.1345], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,121][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.1796, 0.2778, 0.2112, 0.3314], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,122][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.3009, 0.2891, 0.1968, 0.2133], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,124][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0294, 0.3052, 0.3682, 0.2972], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,126][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0625, 0.4540, 0.3874, 0.0961], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,127][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0022, 0.2979, 0.6991, 0.0007], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,128][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0058, 0.5225, 0.0544, 0.4173], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,129][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.3948, 0.2100, 0.1870, 0.2083], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,130][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0146, 0.3884, 0.5703, 0.0266], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,130][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.2129, 0.1114, 0.4722, 0.0164, 0.1870], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,132][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0180, 0.2359, 0.6458, 0.0192, 0.0812], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,134][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.1930, 0.2555, 0.1621, 0.2185, 0.1709], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,135][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.3147, 0.1243, 0.1846, 0.1824, 0.1940], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,137][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.1893, 0.2360, 0.1283, 0.2320, 0.2145], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,138][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.2711, 0.2395, 0.1538, 0.1898, 0.1458], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,140][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0181, 0.2176, 0.2686, 0.2036, 0.2921], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,142][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.1324, 0.2971, 0.3608, 0.0667, 0.1430], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,143][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0565, 0.1609, 0.7411, 0.0008, 0.0408], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,145][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0052, 0.3299, 0.0320, 0.4504, 0.1824], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,147][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.1777, 0.1936, 0.1546, 0.2542, 0.2199], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,148][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0643, 0.2411, 0.5601, 0.0145, 0.1200], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,150][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.1664, 0.1485, 0.4104, 0.0214, 0.1713, 0.0818], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,152][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0066, 0.3724, 0.3368, 0.0261, 0.0748, 0.1833], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,153][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.1792, 0.1909, 0.1291, 0.1699, 0.1431, 0.1877], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,155][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.2430, 0.1401, 0.1752, 0.1296, 0.2518, 0.0603], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,157][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.1145, 0.1748, 0.1164, 0.1915, 0.1870, 0.2158], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,158][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.1865, 0.1987, 0.1373, 0.1856, 0.1189, 0.1729], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,160][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.0175, 0.1807, 0.2192, 0.1706, 0.2325, 0.1796], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,161][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.0976, 0.2274, 0.3064, 0.0544, 0.1333, 0.1809], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,163][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.0358, 0.2552, 0.6148, 0.0010, 0.0627, 0.0305], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,165][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.0083, 0.2653, 0.0504, 0.3215, 0.2236, 0.1308], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,166][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.1554, 0.1732, 0.1240, 0.1846, 0.2349, 0.1278], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,168][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.0615, 0.2684, 0.4104, 0.0186, 0.1131, 0.1280], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,169][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2029, 0.0804, 0.3525, 0.0230, 0.1972, 0.0907, 0.0533],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,170][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0958, 0.1275, 0.1532, 0.0141, 0.0317, 0.0630, 0.5148],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,171][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1535, 0.1706, 0.1051, 0.1554, 0.1153, 0.1893, 0.1107],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,172][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.3013, 0.0972, 0.1300, 0.1181, 0.1840, 0.0788, 0.0906],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,173][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1107, 0.1547, 0.0918, 0.1666, 0.1534, 0.1898, 0.1331],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,174][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1496, 0.1894, 0.1150, 0.1591, 0.1150, 0.1804, 0.0915],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,176][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0133, 0.1548, 0.1795, 0.1469, 0.1964, 0.1485, 0.1605],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,178][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1671, 0.1582, 0.2649, 0.0547, 0.0905, 0.1692, 0.0954],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,179][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0455, 0.1888, 0.6339, 0.0010, 0.0778, 0.0364, 0.0165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,181][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0119, 0.1829, 0.0349, 0.2735, 0.1552, 0.1499, 0.1917],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,183][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3901, 0.1226, 0.1189, 0.1076, 0.1260, 0.0671, 0.0677],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,185][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0664, 0.2096, 0.3482, 0.0211, 0.0933, 0.1944, 0.0671],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,186][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.4584, 0.0136, 0.1157, 0.0019, 0.0184, 0.0158, 0.0151, 0.3612],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,187][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([2.6491e-01, 4.9672e-03, 5.9268e-03, 2.5845e-04, 8.7896e-04, 3.1767e-03,
        4.8497e-02, 6.7138e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,189][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.2031, 0.1424, 0.0873, 0.1406, 0.1134, 0.1727, 0.0890, 0.0515],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,191][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.2732, 0.0828, 0.1135, 0.0868, 0.1134, 0.0753, 0.1660, 0.0890],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,192][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.0901, 0.1261, 0.0771, 0.1395, 0.1341, 0.1518, 0.1225, 0.1588],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,194][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.2316, 0.1274, 0.0559, 0.1095, 0.0704, 0.1149, 0.0586, 0.2317],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,196][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.0119, 0.1270, 0.1562, 0.1222, 0.1609, 0.1206, 0.1245, 0.1767],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,197][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.2032, 0.0604, 0.1046, 0.0105, 0.0212, 0.0476, 0.0377, 0.5149],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,199][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([4.1537e-01, 1.7350e-03, 4.7224e-02, 4.7845e-06, 5.1654e-04, 3.5999e-04,
        4.0175e-04, 5.3439e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,200][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.0268, 0.1258, 0.0569, 0.1232, 0.1654, 0.1246, 0.2588, 0.1185],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,202][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.2317, 0.1274, 0.0993, 0.1303, 0.1245, 0.0913, 0.0947, 0.1009],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,204][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.1818, 0.0278, 0.1288, 0.0015, 0.0121, 0.0235, 0.0135, 0.6111],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,205][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.1234, 0.0258, 0.1346, 0.0081, 0.0699, 0.0277, 0.0288, 0.5596, 0.0221],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,207][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0128, 0.0072, 0.0135, 0.0010, 0.0041, 0.0024, 0.0410, 0.8973, 0.0208],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,209][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.2100, 0.1271, 0.0811, 0.1464, 0.0975, 0.1712, 0.0851, 0.0528, 0.0289],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,210][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.3122, 0.0809, 0.0881, 0.0875, 0.1249, 0.0635, 0.1022, 0.0606, 0.0802],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,211][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0620, 0.1241, 0.0769, 0.1470, 0.1517, 0.1416, 0.1183, 0.1193, 0.0592],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,212][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.1097, 0.1256, 0.0752, 0.1118, 0.0766, 0.1277, 0.0598, 0.2344, 0.0792],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,213][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0106, 0.1116, 0.1408, 0.1062, 0.1469, 0.1068, 0.1092, 0.1339, 0.1339],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,214][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1059, 0.0751, 0.1386, 0.0175, 0.0456, 0.0552, 0.0411, 0.4633, 0.0576],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,215][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([1.6870e-03, 3.0322e-03, 1.4232e-02, 2.4339e-05, 2.6372e-03, 7.8607e-04,
        9.8850e-04, 9.7634e-01, 2.7614e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,216][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0079, 0.0924, 0.0258, 0.0828, 0.0862, 0.0561, 0.1596, 0.2075, 0.2817],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,218][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.3306, 0.0949, 0.0827, 0.0893, 0.1038, 0.0551, 0.0498, 0.1151, 0.0787],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,219][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0285, 0.0404, 0.1194, 0.0039, 0.0264, 0.0331, 0.0189, 0.7094, 0.0199],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,221][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.1416, 0.0217, 0.1333, 0.0041, 0.0446, 0.0218, 0.0153, 0.5552, 0.0220,
        0.0404], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,222][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0369, 0.0103, 0.0088, 0.0009, 0.0035, 0.0055, 0.0679, 0.6956, 0.0889,
        0.0816], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,224][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.1941, 0.1226, 0.0691, 0.1380, 0.0878, 0.1776, 0.0869, 0.0580, 0.0300,
        0.0358], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,226][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.2494, 0.0677, 0.1208, 0.0665, 0.0888, 0.0381, 0.1038, 0.0897, 0.1078,
        0.0673], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,227][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.1027, 0.1195, 0.0681, 0.1169, 0.1138, 0.1276, 0.0978, 0.1020, 0.0596,
        0.0921], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,229][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.0718, 0.1359, 0.0647, 0.1229, 0.0864, 0.0952, 0.0604, 0.2390, 0.0850,
        0.0387], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,231][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0094, 0.0983, 0.1185, 0.0904, 0.1247, 0.0917, 0.0941, 0.1205, 0.1100,
        0.1426], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,233][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.0865, 0.0742, 0.1167, 0.0127, 0.0299, 0.0393, 0.0331, 0.4710, 0.0458,
        0.0908], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,234][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([4.4253e-03, 2.4266e-03, 1.7812e-02, 1.3186e-05, 1.0524e-03, 5.0522e-04,
        6.7861e-04, 9.7061e-01, 5.3515e-04, 1.9401e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,236][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.0056, 0.0618, 0.0141, 0.0397, 0.0272, 0.0473, 0.0926, 0.0601, 0.5641,
        0.0875], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,237][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.2600, 0.0984, 0.0914, 0.0651, 0.1168, 0.0380, 0.0329, 0.0817, 0.0444,
        0.1713], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,239][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.0347, 0.0285, 0.1144, 0.0021, 0.0247, 0.0213, 0.0145, 0.6964, 0.0209,
        0.0424], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,241][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.3878, 0.0114, 0.1046, 0.0036, 0.0292, 0.0161, 0.0141, 0.3036, 0.0138,
        0.0307, 0.0850], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,242][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([1.7941e-01, 3.1241e-03, 2.7177e-03, 2.2611e-04, 5.8562e-04, 1.9171e-03,
        3.6285e-02, 4.3386e-01, 2.8590e-02, 3.2299e-02, 2.8099e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,243][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.2282, 0.1115, 0.0769, 0.1340, 0.0909, 0.1619, 0.0749, 0.0429, 0.0218,
        0.0297, 0.0273], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,245][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.2269, 0.0669, 0.0747, 0.0689, 0.0970, 0.0497, 0.1273, 0.0564, 0.0895,
        0.0902, 0.0526], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,247][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0631, 0.1060, 0.0727, 0.1236, 0.1363, 0.1172, 0.0976, 0.0946, 0.0504,
        0.0801, 0.0584], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,249][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.0796, 0.1016, 0.0626, 0.1095, 0.0712, 0.1279, 0.0492, 0.2194, 0.0731,
        0.0449, 0.0610], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,250][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0064, 0.0829, 0.1065, 0.0804, 0.1164, 0.0809, 0.0804, 0.1102, 0.0985,
        0.1223, 0.1151], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,252][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1289, 0.0390, 0.0817, 0.0118, 0.0225, 0.0343, 0.0282, 0.3642, 0.0425,
        0.0853, 0.1616], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,253][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([3.6915e-02, 1.9305e-03, 2.4326e-02, 2.2932e-05, 1.1536e-03, 4.3586e-04,
        6.9518e-04, 9.0595e-01, 7.2823e-04, 2.9781e-03, 2.4863e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,254][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0020, 0.0465, 0.0105, 0.0315, 0.0309, 0.0365, 0.1104, 0.1057, 0.3688,
        0.2096, 0.0475], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,255][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.1936, 0.0909, 0.0710, 0.1001, 0.0782, 0.0616, 0.0632, 0.0810, 0.0707,
        0.0965, 0.0932], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,256][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.1140, 0.0234, 0.1017, 0.0023, 0.0125, 0.0193, 0.0131, 0.4717, 0.0162,
        0.0424, 0.1833], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,257][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3425, 0.0082, 0.0675, 0.0022, 0.0176, 0.0086, 0.0098, 0.3234, 0.0098,
        0.0249, 0.0855, 0.1000], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,258][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([9.8402e-02, 1.1870e-03, 1.2937e-03, 1.1766e-04, 2.8060e-04, 6.2127e-04,
        1.9436e-02, 2.2222e-01, 1.4183e-02, 1.5156e-02, 1.8361e-01, 4.4350e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,260][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.2532, 0.0992, 0.0692, 0.1248, 0.0871, 0.1678, 0.0682, 0.0438, 0.0180,
        0.0252, 0.0234, 0.0201], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,262][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2350, 0.0600, 0.0606, 0.0526, 0.0905, 0.0560, 0.0751, 0.0606, 0.0666,
        0.0878, 0.0642, 0.0909], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,263][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0468, 0.0866, 0.0571, 0.1113, 0.1342, 0.1233, 0.0963, 0.0997, 0.0468,
        0.0774, 0.0522, 0.0682], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,265][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0819, 0.1028, 0.0566, 0.1032, 0.0700, 0.1080, 0.0518, 0.1951, 0.0690,
        0.0402, 0.0662, 0.0552], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,267][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0068, 0.0783, 0.0938, 0.0746, 0.1047, 0.0789, 0.0793, 0.0993, 0.0926,
        0.1091, 0.0972, 0.0853], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,268][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1015, 0.0256, 0.0572, 0.0057, 0.0122, 0.0221, 0.0168, 0.3624, 0.0269,
        0.0550, 0.1518, 0.1629], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,270][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([2.6697e-02, 1.1336e-03, 1.7256e-02, 6.5730e-06, 5.8946e-04, 1.9957e-04,
        2.6404e-04, 9.1766e-01, 4.2745e-04, 1.6267e-03, 2.4784e-02, 9.3520e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,271][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0040, 0.0557, 0.0096, 0.0605, 0.0449, 0.0330, 0.0567, 0.0822, 0.3265,
        0.1452, 0.0763, 0.1055], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,273][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2291, 0.0905, 0.0647, 0.0752, 0.0788, 0.0475, 0.0406, 0.0765, 0.0643,
        0.0752, 0.0887, 0.0689], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,275][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0635, 0.0101, 0.0602, 0.0009, 0.0058, 0.0116, 0.0050, 0.4153, 0.0108,
        0.0274, 0.2103, 0.1790], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,277][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.1631, 0.0069, 0.0554, 0.0014, 0.0157, 0.0080, 0.0091, 0.2027, 0.0109,
        0.0210, 0.0955, 0.0963, 0.3138], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,278][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([9.0109e-02, 1.1283e-03, 1.7081e-03, 8.7144e-05, 2.4666e-04, 1.2381e-03,
        1.3274e-02, 7.9947e-02, 1.1105e-02, 1.3911e-02, 1.9789e-01, 5.4626e-01,
        4.3106e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,280][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.2385, 0.1052, 0.0625, 0.1199, 0.0834, 0.1664, 0.0719, 0.0454, 0.0179,
        0.0237, 0.0225, 0.0197, 0.0228], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,281][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.1810, 0.0485, 0.0782, 0.0504, 0.0661, 0.0385, 0.0789, 0.0519, 0.0709,
        0.0766, 0.0631, 0.0964, 0.0997], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,283][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0775, 0.0804, 0.0563, 0.0923, 0.1271, 0.1137, 0.0806, 0.0791, 0.0426,
        0.0678, 0.0517, 0.0590, 0.0720], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,285][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.0874, 0.0896, 0.0556, 0.0981, 0.0590, 0.1067, 0.0422, 0.2096, 0.0592,
        0.0383, 0.0532, 0.0425, 0.0585], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,287][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0058, 0.0725, 0.0900, 0.0688, 0.0941, 0.0647, 0.0685, 0.0846, 0.0845,
        0.0989, 0.0930, 0.0751, 0.0996], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,288][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.0994, 0.0332, 0.0600, 0.0043, 0.0107, 0.0212, 0.0155, 0.2536, 0.0244,
        0.0502, 0.1302, 0.1367, 0.1606], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,289][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([6.2039e-02, 9.4685e-04, 2.4174e-02, 4.8710e-06, 5.9851e-04, 2.2056e-04,
        2.7648e-04, 8.1882e-01, 3.6022e-04, 1.4385e-03, 2.1154e-02, 8.4762e-03,
        6.1487e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,291][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.0055, 0.0245, 0.0076, 0.0259, 0.0163, 0.0393, 0.0818, 0.0577, 0.2643,
        0.1954, 0.0794, 0.0948, 0.1076], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,293][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.2140, 0.0830, 0.0619, 0.0636, 0.0710, 0.0361, 0.0277, 0.0601, 0.0464,
        0.0963, 0.0595, 0.0543, 0.1262], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,294][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0839, 0.0069, 0.0462, 0.0005, 0.0031, 0.0068, 0.0043, 0.2837, 0.0094,
        0.0151, 0.1675, 0.1654, 0.2070], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,295][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.1345, 0.0088, 0.0568, 0.0020, 0.0228, 0.0112, 0.0070, 0.1464, 0.0106,
        0.0159, 0.0786, 0.1066, 0.3778, 0.0209], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,296][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([6.9739e-04, 5.5175e-04, 8.4442e-04, 5.2622e-05, 2.5117e-04, 7.3746e-04,
        9.9283e-03, 6.3876e-02, 5.9656e-03, 8.7542e-03, 1.5771e-01, 6.5161e-01,
        9.2757e-02, 6.2707e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,297][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.2394, 0.0841, 0.0655, 0.1125, 0.0803, 0.1648, 0.0702, 0.0396, 0.0187,
        0.0267, 0.0263, 0.0225, 0.0254, 0.0239], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,299][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.1789, 0.0541, 0.0694, 0.0458, 0.0669, 0.0480, 0.0768, 0.0601, 0.0641,
        0.0676, 0.0595, 0.0851, 0.0725, 0.0512], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,300][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.0501, 0.0764, 0.0598, 0.0986, 0.1240, 0.1044, 0.0877, 0.0684, 0.0428,
        0.0620, 0.0492, 0.0596, 0.0561, 0.0607], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,302][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.1027, 0.0706, 0.0503, 0.0684, 0.0570, 0.0995, 0.0467, 0.1657, 0.0556,
        0.0381, 0.0574, 0.0482, 0.0593, 0.0805], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,304][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0062, 0.0638, 0.0824, 0.0612, 0.0850, 0.0648, 0.0635, 0.0817, 0.0757,
        0.0920, 0.0839, 0.0662, 0.0889, 0.0846], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,305][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0388, 0.0405, 0.0444, 0.0065, 0.0124, 0.0239, 0.0164, 0.2154, 0.0344,
        0.0467, 0.1406, 0.1521, 0.1867, 0.0413], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,306][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([1.4487e-02, 4.8441e-03, 2.8013e-02, 2.3652e-05, 1.6593e-03, 1.3045e-03,
        1.1784e-03, 7.4512e-01, 5.7526e-04, 3.8427e-03, 3.3469e-02, 2.4248e-02,
        1.3783e-01, 3.4041e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,308][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([2.0774e-04, 2.1317e-02, 1.8646e-03, 2.5759e-02, 1.4220e-02, 2.0116e-02,
        8.4743e-02, 3.6460e-02, 3.9425e-01, 1.0624e-01, 5.4797e-02, 1.1937e-01,
        7.4120e-02, 4.6530e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,309][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.1818, 0.0624, 0.0499, 0.0750, 0.0670, 0.0408, 0.0386, 0.0682, 0.0606,
        0.0879, 0.0617, 0.0713, 0.0728, 0.0621], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,311][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0245, 0.0128, 0.0441, 0.0012, 0.0093, 0.0150, 0.0059, 0.2135, 0.0101,
        0.0201, 0.1693, 0.1822, 0.2538, 0.0384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,313][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.3795, 0.0034, 0.0343, 0.0008, 0.0086, 0.0034, 0.0027, 0.0728, 0.0042,
        0.0128, 0.0311, 0.0353, 0.1306, 0.0173, 0.2632], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,314][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([2.4664e-01, 3.3270e-04, 4.5588e-04, 3.3891e-05, 8.1537e-05, 3.2777e-04,
        7.6508e-03, 5.6712e-02, 8.2717e-03, 4.4550e-03, 7.3515e-02, 2.5563e-01,
        3.8363e-02, 4.6574e-03, 3.0288e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,316][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.1213, 0.0936, 0.0617, 0.1061, 0.0791, 0.1582, 0.0963, 0.0504, 0.0225,
        0.0302, 0.0306, 0.0302, 0.0293, 0.0283, 0.0621], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,318][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.1183, 0.0288, 0.0485, 0.0404, 0.0535, 0.0396, 0.0519, 0.0536, 0.0492,
        0.0760, 0.0428, 0.0611, 0.1450, 0.0925, 0.0989], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,319][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0549, 0.0785, 0.0426, 0.0864, 0.0918, 0.1059, 0.0643, 0.0824, 0.0364,
        0.0648, 0.0450, 0.0472, 0.0651, 0.0565, 0.0783], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,321][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.0683, 0.0762, 0.0477, 0.0694, 0.0495, 0.0858, 0.0335, 0.2252, 0.0555,
        0.0358, 0.0475, 0.0446, 0.0666, 0.0629, 0.0315], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,323][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0043, 0.0602, 0.0780, 0.0561, 0.0833, 0.0534, 0.0564, 0.0725, 0.0692,
        0.0813, 0.0711, 0.0632, 0.0799, 0.0767, 0.0945], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,325][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.1552, 0.0173, 0.0470, 0.0042, 0.0084, 0.0140, 0.0111, 0.1668, 0.0191,
        0.0470, 0.1018, 0.0868, 0.1336, 0.0574, 0.1303], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,326][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([4.4251e-01, 8.7459e-04, 2.0044e-02, 1.0810e-05, 3.4327e-04, 2.7841e-04,
        2.3921e-04, 2.6706e-01, 5.9563e-04, 2.7327e-03, 1.7024e-02, 7.3031e-03,
        9.7703e-02, 7.2751e-03, 1.3600e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,328][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0005, 0.0258, 0.0026, 0.0274, 0.0098, 0.0443, 0.0920, 0.0284, 0.3609,
        0.1073, 0.0511, 0.1170, 0.0476, 0.0598, 0.0256], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,330][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2116, 0.0761, 0.0749, 0.0742, 0.0836, 0.0390, 0.0352, 0.0382, 0.0398,
        0.0743, 0.0398, 0.0429, 0.0535, 0.0366, 0.0802], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,331][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.1216, 0.0040, 0.0376, 0.0004, 0.0035, 0.0032, 0.0027, 0.1132, 0.0056,
        0.0152, 0.0844, 0.0712, 0.1615, 0.0374, 0.3385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,333][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.2565, 0.0030, 0.0303, 0.0005, 0.0040, 0.0025, 0.0024, 0.0878, 0.0027,
        0.0061, 0.0309, 0.0343, 0.1523, 0.0134, 0.1853, 0.1880],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,334][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([6.1432e-02, 6.4833e-04, 6.9451e-04, 4.9385e-05, 1.0240e-04, 5.8475e-04,
        1.9324e-02, 1.3568e-01, 7.3789e-03, 5.8265e-03, 7.9009e-02, 3.1918e-01,
        3.9568e-02, 3.3578e-03, 2.6554e-01, 6.1636e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,336][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.1390, 0.1126, 0.0730, 0.1173, 0.0797, 0.1579, 0.0772, 0.0427, 0.0179,
        0.0233, 0.0239, 0.0215, 0.0219, 0.0211, 0.0473, 0.0238],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,337][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.1281, 0.0364, 0.0532, 0.0417, 0.0582, 0.0368, 0.0694, 0.0457, 0.0726,
        0.0700, 0.0533, 0.0783, 0.0742, 0.0546, 0.0988, 0.0288],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,338][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0480, 0.0639, 0.0467, 0.0682, 0.0834, 0.1013, 0.0733, 0.0836, 0.0391,
        0.0582, 0.0450, 0.0527, 0.0577, 0.0542, 0.0689, 0.0559],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,339][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.0713, 0.0902, 0.0410, 0.0900, 0.0448, 0.0636, 0.0385, 0.1908, 0.0544,
        0.0310, 0.0519, 0.0464, 0.0522, 0.0744, 0.0257, 0.0337],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,341][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0051, 0.0548, 0.0670, 0.0511, 0.0698, 0.0531, 0.0531, 0.0700, 0.0637,
        0.0822, 0.0693, 0.0565, 0.0724, 0.0712, 0.0791, 0.0816],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,342][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.1389, 0.0126, 0.0425, 0.0034, 0.0083, 0.0091, 0.0101, 0.2010, 0.0153,
        0.0264, 0.0735, 0.0771, 0.0964, 0.0321, 0.1129, 0.1407],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,344][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([3.3053e-01, 8.1551e-04, 1.9382e-02, 4.6898e-06, 3.0632e-04, 1.9050e-04,
        1.8805e-04, 3.2317e-01, 2.0519e-04, 7.4370e-04, 1.6081e-02, 5.2340e-03,
        3.9678e-02, 3.0608e-03, 1.7561e-01, 8.4801e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,345][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0036, 0.0389, 0.0129, 0.0364, 0.0510, 0.0204, 0.0333, 0.0546, 0.0989,
        0.0772, 0.0521, 0.0688, 0.0461, 0.1550, 0.2108, 0.0401],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,347][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.2145, 0.0580, 0.0675, 0.0505, 0.0703, 0.0362, 0.0328, 0.0559, 0.0362,
        0.0737, 0.0473, 0.0476, 0.0516, 0.0319, 0.0681, 0.0579],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,348][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([1.2860e-01, 3.3057e-03, 2.8615e-02, 2.9816e-04, 1.8263e-03, 2.4827e-03,
        2.2481e-03, 9.3416e-02, 3.0793e-03, 5.9721e-03, 5.7148e-02, 5.4106e-02,
        6.9709e-02, 1.8730e-02, 1.6553e-01, 3.6494e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,350][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2299, 0.0020, 0.0217, 0.0007, 0.0053, 0.0034, 0.0022, 0.0582, 0.0029,
        0.0065, 0.0234, 0.0243, 0.1122, 0.0105, 0.1847, 0.1842, 0.1280],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,351][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([2.1752e-01, 2.5984e-04, 2.1116e-04, 2.6589e-05, 4.1559e-05, 1.8610e-04,
        3.9838e-03, 4.0328e-02, 3.4030e-03, 2.7193e-03, 2.9196e-02, 7.6313e-02,
        1.0108e-02, 1.9521e-03, 1.2101e-01, 3.3417e-02, 4.5932e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,353][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1563, 0.0911, 0.0611, 0.1097, 0.0736, 0.1549, 0.0753, 0.0406, 0.0184,
        0.0248, 0.0256, 0.0233, 0.0241, 0.0226, 0.0448, 0.0239, 0.0300],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,355][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1459, 0.0354, 0.0509, 0.0388, 0.0665, 0.0332, 0.0374, 0.0417, 0.0490,
        0.0640, 0.0432, 0.0632, 0.0888, 0.0471, 0.1116, 0.0401, 0.0432],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,356][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0452, 0.0691, 0.0447, 0.0801, 0.0857, 0.0945, 0.0620, 0.0742, 0.0354,
        0.0548, 0.0396, 0.0458, 0.0519, 0.0509, 0.0690, 0.0491, 0.0479],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,358][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0499, 0.0817, 0.0461, 0.0759, 0.0525, 0.0750, 0.0396, 0.1504, 0.0530,
        0.0349, 0.0562, 0.0490, 0.0578, 0.0705, 0.0349, 0.0396, 0.0329],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,360][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0051, 0.0519, 0.0617, 0.0488, 0.0649, 0.0523, 0.0550, 0.0644, 0.0619,
        0.0703, 0.0658, 0.0565, 0.0715, 0.0671, 0.0724, 0.0686, 0.0620],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,362][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1467, 0.0108, 0.0321, 0.0028, 0.0047, 0.0096, 0.0065, 0.1266, 0.0104,
        0.0230, 0.0562, 0.0487, 0.0815, 0.0235, 0.0801, 0.1834, 0.1534],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,363][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([2.9913e-01, 6.9927e-04, 1.8164e-02, 5.7658e-06, 3.2669e-04, 1.8075e-04,
        1.6753e-04, 2.4838e-01, 3.1822e-04, 6.5276e-04, 1.3184e-02, 4.5279e-03,
        4.0040e-02, 4.0802e-03, 1.6014e-01, 1.1768e-01, 9.2328e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,365][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0010, 0.0191, 0.0031, 0.0252, 0.0157, 0.0184, 0.0283, 0.0500, 0.2341,
        0.1376, 0.0395, 0.0662, 0.0673, 0.0780, 0.0911, 0.0452, 0.0801],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,367][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.2858, 0.0542, 0.0538, 0.0454, 0.0474, 0.0270, 0.0237, 0.0405, 0.0417,
        0.0535, 0.0576, 0.0501, 0.0374, 0.0312, 0.0565, 0.0574, 0.0368],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,368][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([5.3577e-02, 2.0303e-03, 1.2691e-02, 2.0318e-04, 1.0934e-03, 2.8180e-03,
        1.2930e-03, 6.3596e-02, 2.6485e-03, 5.4968e-03, 5.1395e-02, 2.8350e-02,
        6.3633e-02, 1.2645e-02, 1.0241e-01, 4.0583e-01, 1.9028e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,370][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0929, 0.0025, 0.0153, 0.0004, 0.0036, 0.0030, 0.0028, 0.0479, 0.0033,
        0.0045, 0.0230, 0.0261, 0.0580, 0.0096, 0.1722, 0.1558, 0.1433, 0.2358],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,371][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([5.9873e-02, 1.1641e-04, 6.1174e-05, 5.5382e-06, 1.0157e-05, 9.6277e-05,
        1.6055e-03, 1.1211e-02, 1.6946e-03, 1.1474e-03, 2.5799e-02, 5.5079e-02,
        5.4389e-03, 2.0682e-03, 1.3961e-01, 3.0482e-02, 6.1931e-01, 4.6393e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,373][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.1634, 0.0831, 0.0522, 0.1141, 0.0672, 0.1618, 0.0764, 0.0421, 0.0154,
        0.0208, 0.0213, 0.0202, 0.0220, 0.0198, 0.0489, 0.0223, 0.0275, 0.0216],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,375][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0901, 0.0340, 0.0455, 0.0304, 0.0386, 0.0379, 0.0569, 0.0342, 0.0405,
        0.0626, 0.0464, 0.0715, 0.0838, 0.0574, 0.0815, 0.0491, 0.0765, 0.0631],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,377][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0605, 0.0636, 0.0339, 0.0698, 0.0670, 0.0838, 0.0560, 0.0638, 0.0313,
        0.0535, 0.0442, 0.0433, 0.0581, 0.0511, 0.0570, 0.0481, 0.0478, 0.0672],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,378][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.1459, 0.0604, 0.0269, 0.0551, 0.0398, 0.0677, 0.0335, 0.1677, 0.0443,
        0.0255, 0.0467, 0.0404, 0.0417, 0.0590, 0.0274, 0.0358, 0.0287, 0.0535],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,379][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0047, 0.0482, 0.0608, 0.0439, 0.0618, 0.0438, 0.0453, 0.0598, 0.0551,
        0.0663, 0.0619, 0.0496, 0.0648, 0.0625, 0.0681, 0.0641, 0.0517, 0.0878],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,380][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0705, 0.0104, 0.0236, 0.0016, 0.0041, 0.0071, 0.0070, 0.0820, 0.0105,
        0.0174, 0.0478, 0.0437, 0.0616, 0.0196, 0.0879, 0.1425, 0.1484, 0.2142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,380][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([3.1777e-01, 2.6139e-04, 7.3908e-03, 9.7340e-07, 7.6192e-05, 6.5854e-05,
        6.8494e-05, 7.4885e-02, 1.4820e-04, 2.0094e-04, 5.2303e-03, 2.1663e-03,
        8.5522e-03, 1.5044e-03, 8.4007e-02, 4.5558e-02, 5.3884e-02, 3.9823e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,382][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0011, 0.0116, 0.0032, 0.0120, 0.0109, 0.0182, 0.0357, 0.0252, 0.2221,
        0.0578, 0.0679, 0.0964, 0.0687, 0.0724, 0.0707, 0.0776, 0.1259, 0.0227],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,384][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0911, 0.0638, 0.0359, 0.0512, 0.0405, 0.0394, 0.0288, 0.0403, 0.0484,
        0.0640, 0.0573, 0.0493, 0.0496, 0.0492, 0.0651, 0.0805, 0.0589, 0.0867],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,385][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([2.9014e-02, 1.8001e-03, 9.0620e-03, 9.7027e-05, 6.9944e-04, 1.3813e-03,
        1.1181e-03, 4.8662e-02, 2.0262e-03, 3.9005e-03, 2.9625e-02, 2.5117e-02,
        3.6540e-02, 1.0389e-02, 9.6775e-02, 1.9909e-01, 2.0700e-01, 2.9770e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,387][circuit_model.py][line:2332][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.1560, 0.0018, 0.0158, 0.0003, 0.0022, 0.0014, 0.0014, 0.0448, 0.0013,
        0.0020, 0.0141, 0.0152, 0.0524, 0.0065, 0.1099, 0.1235, 0.1089, 0.2418,
        0.1008], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,388][circuit_model.py][line:2335][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([6.7285e-02, 3.3173e-04, 1.5316e-04, 2.9560e-05, 3.4052e-05, 1.5549e-04,
        4.3085e-03, 3.9909e-02, 2.5316e-03, 1.4141e-03, 1.9348e-02, 5.5673e-02,
        6.7958e-03, 1.7484e-03, 8.8542e-02, 1.4636e-02, 4.1985e-01, 2.0598e-02,
        2.5666e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,390][circuit_model.py][line:2338][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1805, 0.0708, 0.0517, 0.1095, 0.0672, 0.1611, 0.0683, 0.0360, 0.0175,
        0.0227, 0.0236, 0.0192, 0.0202, 0.0204, 0.0387, 0.0216, 0.0260, 0.0219,
        0.0231], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,392][circuit_model.py][line:2341][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1707, 0.0282, 0.0429, 0.0252, 0.0435, 0.0269, 0.0443, 0.0261, 0.0399,
        0.0618, 0.0420, 0.0613, 0.0563, 0.0371, 0.0848, 0.0351, 0.0557, 0.0860,
        0.0321], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,394][circuit_model.py][line:2344][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0280, 0.0610, 0.0387, 0.0820, 0.0819, 0.0793, 0.0644, 0.0618, 0.0308,
        0.0469, 0.0338, 0.0441, 0.0380, 0.0434, 0.0596, 0.0383, 0.0443, 0.0699,
        0.0538], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,396][circuit_model.py][line:2347][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0712, 0.0669, 0.0385, 0.0619, 0.0454, 0.0677, 0.0396, 0.1324, 0.0474,
        0.0292, 0.0471, 0.0391, 0.0463, 0.0621, 0.0292, 0.0384, 0.0337, 0.0593,
        0.0446], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,397][circuit_model.py][line:2350][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0041, 0.0446, 0.0558, 0.0425, 0.0598, 0.0440, 0.0443, 0.0544, 0.0533,
        0.0616, 0.0576, 0.0460, 0.0609, 0.0590, 0.0673, 0.0599, 0.0509, 0.0799,
        0.0541], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,399][circuit_model.py][line:2353][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.0741, 0.0072, 0.0186, 0.0010, 0.0025, 0.0047, 0.0033, 0.0834, 0.0061,
        0.0108, 0.0371, 0.0296, 0.0599, 0.0133, 0.0512, 0.1287, 0.0964, 0.2647,
        0.1074], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,400][circuit_model.py][line:2356][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([1.2346e-01, 2.8498e-04, 5.9622e-03, 6.8984e-07, 7.9669e-05, 5.2335e-05,
        6.3579e-05, 1.5745e-01, 6.0080e-05, 1.3988e-04, 3.5040e-03, 1.2412e-03,
        9.9578e-03, 9.6434e-04, 8.2185e-02, 3.0260e-02, 3.7378e-02, 5.1044e-01,
        3.6525e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,402][circuit_model.py][line:2359][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0019, 0.0217, 0.0044, 0.0103, 0.0112, 0.0157, 0.0412, 0.0383, 0.1416,
        0.0597, 0.0356, 0.0597, 0.0412, 0.0593, 0.0749, 0.0990, 0.1364, 0.0257,
        0.1225], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,404][circuit_model.py][line:2362][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.2794, 0.0382, 0.0364, 0.0424, 0.0347, 0.0270, 0.0296, 0.0472, 0.0362,
        0.0327, 0.0605, 0.0529, 0.0259, 0.0282, 0.0395, 0.0579, 0.0442, 0.0536,
        0.0334], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,405][circuit_model.py][line:2365][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([3.1406e-02, 1.0937e-03, 8.1079e-03, 5.5221e-05, 3.2996e-04, 9.7885e-04,
        5.5682e-04, 3.8397e-02, 9.2080e-04, 2.1218e-03, 1.9198e-02, 1.2504e-02,
        2.4022e-02, 6.5006e-03, 4.1815e-02, 2.1101e-01, 1.3537e-01, 2.9401e-01,
        1.7160e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,409][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:20,411][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[ 8400],
        [ 1662],
        [30660],
        [ 7569],
        [ 3988],
        [ 2190],
        [ 4794],
        [ 3008],
        [ 8099],
        [ 5398],
        [ 3596],
        [ 9192],
        [15870],
        [ 9170],
        [10694],
        [ 2788],
        [ 7020],
        [ 3542],
        [ 4175]], device='cuda:0')
[2024-07-24 10:25:20,413][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8347],
        [ 5191],
        [38958],
        [ 4813],
        [14277],
        [ 4119],
        [ 2523],
        [ 3734],
        [ 2060],
        [ 2150],
        [  671],
        [ 1951],
        [ 9271],
        [ 2074],
        [18581],
        [ 3674],
        [ 1144],
        [ 4084],
        [  440]], device='cuda:0')
[2024-07-24 10:25:20,415][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[24416],
        [38993],
        [31378],
        [45830],
        [45715],
        [46103],
        [46117],
        [38832],
        [42948],
        [43001],
        [42515],
        [43062],
        [43339],
        [43860],
        [32942],
        [41347],
        [44957],
        [44642],
        [46280]], device='cuda:0')
[2024-07-24 10:25:20,416][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[ 5773],
        [ 1339],
        [ 5560],
        [14423],
        [ 7846],
        [ 3028],
        [ 4746],
        [ 4223],
        [ 4921],
        [ 5163],
        [ 5174],
        [ 6192],
        [ 7190],
        [ 6769],
        [ 5448],
        [ 5346],
        [ 5718],
        [ 5082],
        [ 6056]], device='cuda:0')
[2024-07-24 10:25:20,418][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[24950],
        [28785],
        [37699],
        [37818],
        [38492],
        [38712],
        [38551],
        [40068],
        [40169],
        [40346],
        [40649],
        [40541],
        [40919],
        [41128],
        [41593],
        [41086],
        [41055],
        [41213],
        [41259]], device='cuda:0')
[2024-07-24 10:25:20,420][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[16222],
        [20595],
        [20690],
        [20563],
        [21437],
        [22047],
        [22392],
        [22269],
        [22067],
        [22395],
        [22416],
        [22469],
        [22416],
        [22395],
        [22488],
        [22334],
        [22181],
        [22165],
        [22144]], device='cuda:0')
[2024-07-24 10:25:20,422][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[4936],
        [4401],
        [4997],
        [3833],
        [3789],
        [4078],
        [3968],
        [4041],
        [4180],
        [4214],
        [4566],
        [4705],
        [4768],
        [6882],
        [4736],
        [5385],
        [5495],
        [5071],
        [6163]], device='cuda:0')
[2024-07-24 10:25:20,424][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[1014],
        [2722],
        [5123],
        [3524],
        [4179],
        [3873],
        [4146],
        [5221],
        [5308],
        [5602],
        [5398],
        [5523],
        [5985],
        [6066],
        [6553],
        [6341],
        [6685],
        [7029],
        [7074]], device='cuda:0')
[2024-07-24 10:25:20,425][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 2155],
        [ 2367],
        [ 2981],
        [ 3700],
        [ 4946],
        [ 5501],
        [ 6176],
        [ 6807],
        [ 7970],
        [ 8422],
        [10089],
        [11054],
        [11503],
        [11799],
        [12117],
        [13033],
        [13117],
        [14264],
        [14020]], device='cuda:0')
[2024-07-24 10:25:20,427][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[37459],
        [32232],
        [33130],
        [32569],
        [32175],
        [31204],
        [29818],
        [28874],
        [28441],
        [27553],
        [26394],
        [24665],
        [23942],
        [23388],
        [23363],
        [23151],
        [22904],
        [22909],
        [22952]], device='cuda:0')
[2024-07-24 10:25:20,429][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[25473],
        [18905],
        [19040],
        [16959],
        [18753],
        [18456],
        [18981],
        [11681],
        [ 9954],
        [ 9815],
        [10141],
        [10454],
        [12752],
        [13422],
        [16114],
        [16144],
        [16237],
        [14295],
        [14872]], device='cuda:0')
[2024-07-24 10:25:20,431][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[44902],
        [39358],
        [21326],
        [22085],
        [26155],
        [32668],
        [31816],
        [32284],
        [32803],
        [33442],
        [34489],
        [34304],
        [33599],
        [33863],
        [33280],
        [34683],
        [34490],
        [33848],
        [34838]], device='cuda:0')
[2024-07-24 10:25:20,432][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[40867],
        [39574],
        [41500],
        [41572],
        [41510],
        [41902],
        [42121],
        [41050],
        [40451],
        [39617],
        [39069],
        [38913],
        [38417],
        [38007],
        [37989],
        [37583],
        [37683],
        [37198],
        [37142]], device='cuda:0')
[2024-07-24 10:25:20,434][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[32683],
        [30590],
        [30848],
        [29620],
        [29988],
        [29944],
        [29748],
        [28262],
        [27450],
        [28164],
        [31476],
        [32126],
        [34570],
        [35191],
        [37427],
        [35979],
        [35815],
        [39481],
        [41507]], device='cuda:0')
[2024-07-24 10:25:20,436][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 7994],
        [20137],
        [12764],
        [14915],
        [ 6896],
        [10779],
        [18415],
        [23297],
        [31500],
        [20464],
        [20053],
        [25582],
        [23687],
        [30857],
        [10078],
        [10136],
        [25342],
        [26472],
        [26104]], device='cuda:0')
[2024-07-24 10:25:20,438][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[26415],
        [25341],
        [18315],
        [12523],
        [15022],
        [11418],
        [11246],
        [24113],
        [10781],
        [10535],
        [18127],
        [15554],
        [ 6503],
        [ 5427],
        [13633],
        [ 8629],
        [ 7643],
        [ 7120],
        [ 6802]], device='cuda:0')
[2024-07-24 10:25:20,440][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[36218],
        [11940],
        [ 5247],
        [ 3692],
        [ 3355],
        [ 3172],
        [ 2460],
        [ 3588],
        [ 3088],
        [ 2286],
        [ 2576],
        [ 1869],
        [ 1946],
        [ 1761],
        [ 1752],
        [ 1339],
        [ 3404],
        [ 3404],
        [ 2843]], device='cuda:0')
[2024-07-24 10:25:20,442][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[24408],
        [29339],
        [30600],
        [32481],
        [33051],
        [33187],
        [32955],
        [33394],
        [33550],
        [33753],
        [33667],
        [33656],
        [33692],
        [33835],
        [34112],
        [34054],
        [34101],
        [34058],
        [33979]], device='cuda:0')
[2024-07-24 10:25:20,444][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[ 6757],
        [11581],
        [13360],
        [13248],
        [15983],
        [16311],
        [14826],
        [14062],
        [15297],
        [16955],
        [17630],
        [18029],
        [18484],
        [18250],
        [20489],
        [20143],
        [20510],
        [19830],
        [19614]], device='cuda:0')
[2024-07-24 10:25:20,446][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[ 9984],
        [20941],
        [30611],
        [31937],
        [35863],
        [37260],
        [36339],
        [38125],
        [37955],
        [38126],
        [38655],
        [38673],
        [38656],
        [38263],
        [39095],
        [39149],
        [38906],
        [39971],
        [39692]], device='cuda:0')
[2024-07-24 10:25:20,448][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[ 8034],
        [29003],
        [28962],
        [26648],
        [23340],
        [24629],
        [23518],
        [19864],
        [20444],
        [20365],
        [20076],
        [19863],
        [19781],
        [19756],
        [19248],
        [19562],
        [19255],
        [18575],
        [18784]], device='cuda:0')
[2024-07-24 10:25:20,450][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[4627],
        [2505],
        [2976],
        [3214],
        [3947],
        [4334],
        [4402],
        [4325],
        [4188],
        [4489],
        [4549],
        [4504],
        [4676],
        [4603],
        [4732],
        [4806],
        [4789],
        [4903],
        [4866]], device='cuda:0')
[2024-07-24 10:25:20,451][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[34231],
        [ 4648],
        [ 3440],
        [ 2996],
        [ 2991],
        [ 3062],
        [ 3309],
        [ 9296],
        [ 7235],
        [ 7385],
        [ 5601],
        [ 5065],
        [ 2758],
        [ 2161],
        [ 2214],
        [ 2557],
        [ 1861],
        [  907],
        [  745]], device='cuda:0')
[2024-07-24 10:25:20,453][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[12933],
        [15549],
        [16843],
        [19128],
        [20063],
        [20753],
        [21382],
        [36044],
        [32434],
        [32419],
        [32547],
        [32489],
        [32378],
        [31516],
        [34782],
        [34988],
        [32991],
        [32325],
        [31467]], device='cuda:0')
[2024-07-24 10:25:20,455][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[25048],
        [11507],
        [11573],
        [14035],
        [14589],
        [15021],
        [18064],
        [16751],
        [14329],
        [13281],
        [13361],
        [14496],
        [14417],
        [14695],
        [15087],
        [14570],
        [14653],
        [15971],
        [15373]], device='cuda:0')
[2024-07-24 10:25:20,457][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[12807],
        [30697],
        [39045],
        [38440],
        [39558],
        [40249],
        [39299],
        [42527],
        [40311],
        [42743],
        [42042],
        [41541],
        [41316],
        [40515],
        [40230],
        [40468],
        [40115],
        [40494],
        [40141]], device='cuda:0')
[2024-07-24 10:25:20,459][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12415],
        [ 9922],
        [10177],
        [13968],
        [14568],
        [14359],
        [14892],
        [ 6956],
        [ 9148],
        [ 9528],
        [11608],
        [11954],
        [ 8257],
        [ 8183],
        [ 7951],
        [ 8094],
        [10388],
        [16926],
        [19706]], device='cuda:0')
[2024-07-24 10:25:20,461][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[26650],
        [34658],
        [32719],
        [32525],
        [30671],
        [29513],
        [29273],
        [21678],
        [26030],
        [25253],
        [23547],
        [24345],
        [29406],
        [30661],
        [28310],
        [29024],
        [28736],
        [28079],
        [27803]], device='cuda:0')
[2024-07-24 10:25:20,463][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[46036],
        [22253],
        [24547],
        [32715],
        [16047],
        [20564],
        [27114],
        [16503],
        [27090],
        [32775],
        [32502],
        [30515],
        [27484],
        [22006],
        [20052],
        [23338],
        [23853],
        [10139],
        [24640]], device='cuda:0')
[2024-07-24 10:25:20,465][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867],
        [36867]], device='cuda:0')
[2024-07-24 10:25:20,547][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:20,548][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,549][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,550][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,552][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,553][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,554][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,555][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,556][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,557][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,558][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,559][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,561][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,562][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.9782, 0.0218], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,562][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0894, 0.9106], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,562][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.1595, 0.8405], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,563][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.1233, 0.8767], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,563][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1875, 0.8125], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,563][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.2514, 0.7486], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,564][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.3578, 0.6422], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,564][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1924, 0.8076], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,564][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.1769, 0.8231], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,565][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.4285, 0.5715], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,566][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [,] are: tensor([2.2430e-06, 1.0000e+00], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,567][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.0694, 0.9306], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,569][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.8309, 0.1021, 0.0670], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,570][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.0509, 0.5338, 0.4153], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,571][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.0069, 0.7736, 0.2195], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,573][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.0599, 0.4984, 0.4417], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,574][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.1221, 0.5158, 0.3621], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,575][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.1450, 0.4415, 0.4135], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,577][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.4156, 0.4203, 0.1641], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,578][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.0757, 0.9092, 0.0151], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,580][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.1414, 0.5023, 0.3563], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,581][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.2796, 0.3326, 0.3878], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,582][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([4.2292e-07, 2.3376e-01, 7.6624e-01], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,583][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.0128, 0.7174, 0.2698], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,585][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4111, 0.1421, 0.4406, 0.0061], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,586][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0342, 0.3096, 0.3125, 0.3437], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,587][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0177, 0.1143, 0.1771, 0.6909], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,589][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0430, 0.3437, 0.3105, 0.3028], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,590][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0084, 0.7597, 0.2153, 0.0166], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,592][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0791, 0.2564, 0.3200, 0.3446], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,593][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.1371, 0.3337, 0.2555, 0.2737], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,594][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0629, 0.4185, 0.0200, 0.4986], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,596][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0105, 0.7644, 0.2168, 0.0083], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,597][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.2103, 0.2700, 0.3220, 0.1977], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,598][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ and] are: tensor([1.5179e-06, 9.7821e-02, 8.9870e-01, 3.4737e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,599][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0131, 0.2831, 0.4021, 0.3017], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,601][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.4282, 0.2106, 0.3538, 0.0047, 0.0028], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,602][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0339, 0.1942, 0.1955, 0.2004, 0.3760], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,604][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0081, 0.2524, 0.1612, 0.4090, 0.1693], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,604][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0335, 0.2549, 0.2351, 0.2248, 0.2517], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,604][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0340, 0.5296, 0.3130, 0.0139, 0.1095], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,605][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0677, 0.1957, 0.1768, 0.2738, 0.2860], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,605][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.1484, 0.2677, 0.1635, 0.2196, 0.2007], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,606][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0527, 0.3179, 0.0120, 0.3781, 0.2392], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,606][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0382, 0.3501, 0.3566, 0.0086, 0.2465], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,606][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.1787, 0.2087, 0.2467, 0.1473, 0.2187], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,607][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([7.9208e-08, 7.0322e-02, 9.1115e-01, 6.7164e-03, 1.1811e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,607][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0122, 0.2943, 0.1463, 0.2129, 0.3342], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,608][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.1185, 0.3760, 0.4866, 0.0071, 0.0071, 0.0047], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,609][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0179, 0.1373, 0.1367, 0.1568, 0.3042, 0.2471], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,610][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.0076, 0.0566, 0.0301, 0.2772, 0.3805, 0.2479], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,611][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.0283, 0.2003, 0.1806, 0.1776, 0.1983, 0.2150], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,613][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.0205, 0.6856, 0.1509, 0.0086, 0.0699, 0.0646], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,614][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0538, 0.1361, 0.1516, 0.2069, 0.2533, 0.1983], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,615][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0780, 0.2150, 0.1574, 0.1741, 0.1715, 0.2041], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,617][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.0273, 0.2107, 0.0097, 0.2534, 0.1625, 0.3365], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,618][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.0244, 0.7010, 0.1528, 0.0051, 0.0832, 0.0335], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,620][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.1387, 0.1770, 0.2113, 0.1277, 0.1908, 0.1545], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,620][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ had] are: tensor([3.1090e-06, 4.0928e-01, 4.1141e-01, 2.2226e-02, 7.2565e-02, 8.4508e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,622][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.0351, 0.1747, 0.0766, 0.2031, 0.2266, 0.2838], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,623][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0197, 0.4824, 0.4537, 0.0149, 0.0143, 0.0132, 0.0018],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,625][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0137, 0.1148, 0.1177, 0.1279, 0.2601, 0.1925, 0.1734],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,626][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0034, 0.0485, 0.0337, 0.1826, 0.2186, 0.3586, 0.1545],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,628][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0212, 0.1686, 0.1517, 0.1483, 0.1682, 0.1807, 0.1612],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,629][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0170, 0.5034, 0.1773, 0.0140, 0.1353, 0.1034, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,630][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0421, 0.1171, 0.1309, 0.1695, 0.2040, 0.1793, 0.1570],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,631][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0574, 0.1808, 0.1308, 0.1443, 0.1399, 0.1694, 0.1773],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,633][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0248, 0.1331, 0.0067, 0.1702, 0.1095, 0.2175, 0.3382],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,634][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0131, 0.6372, 0.1493, 0.0071, 0.1202, 0.0666, 0.0066],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,635][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.1243, 0.1565, 0.1837, 0.1122, 0.1642, 0.1350, 0.1240],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,636][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([7.7959e-07, 3.9685e-01, 5.0382e-01, 1.7704e-02, 3.2408e-02, 4.1294e-02,
        7.9204e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,638][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0027, 0.2068, 0.0968, 0.1891, 0.1950, 0.2168, 0.0927],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,639][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([8.5771e-01, 5.0209e-02, 6.3865e-02, 1.8456e-03, 8.7588e-04, 1.2272e-03,
        3.7203e-04, 2.3895e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,640][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0101, 0.1022, 0.1078, 0.1069, 0.2024, 0.1579, 0.1866, 0.1261],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,642][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.0015, 0.0980, 0.0233, 0.2444, 0.0778, 0.2768, 0.1685, 0.1098],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,643][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.0218, 0.1378, 0.1222, 0.1235, 0.1352, 0.1541, 0.1341, 0.1714],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,644][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.0579, 0.1706, 0.1282, 0.0022, 0.0177, 0.0271, 0.0123, 0.5840],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,646][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0362, 0.1013, 0.1025, 0.1519, 0.1848, 0.1539, 0.1434, 0.1260],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,647][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0842, 0.1542, 0.1058, 0.1226, 0.1117, 0.1442, 0.1463, 0.1309],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,647][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.0286, 0.1206, 0.0037, 0.1523, 0.0895, 0.2000, 0.3344, 0.0709],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,647][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.1045, 0.1181, 0.1162, 0.0008, 0.0125, 0.0105, 0.0015, 0.6360],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,648][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.1060, 0.1321, 0.1499, 0.0935, 0.1365, 0.1161, 0.1064, 0.1595],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,648][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([3.0952e-06, 3.2452e-01, 5.0669e-01, 2.2220e-02, 3.5992e-02, 4.3577e-02,
        3.1187e-02, 3.5814e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,648][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.0041, 0.1679, 0.0723, 0.1343, 0.1846, 0.2360, 0.1127, 0.0882],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,649][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0852, 0.2604, 0.3436, 0.0140, 0.0149, 0.0102, 0.0027, 0.2624, 0.0068],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,649][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0052, 0.0877, 0.0873, 0.1067, 0.2215, 0.1353, 0.1689, 0.1025, 0.0848],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,650][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0007, 0.0100, 0.0074, 0.0854, 0.0953, 0.2586, 0.1151, 0.2603, 0.1674],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,651][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0159, 0.1251, 0.1127, 0.1093, 0.1271, 0.1337, 0.1194, 0.1521, 0.1047],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,652][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0075, 0.1818, 0.0769, 0.0072, 0.0537, 0.0376, 0.0148, 0.6068, 0.0138],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,654][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0329, 0.0858, 0.1047, 0.1241, 0.1768, 0.1316, 0.1223, 0.1136, 0.1082],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,655][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0482, 0.1352, 0.1182, 0.1114, 0.1117, 0.1219, 0.1248, 0.1190, 0.1096],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,656][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0156, 0.1042, 0.0043, 0.1399, 0.0866, 0.1766, 0.2820, 0.0611, 0.1298],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,658][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0060, 0.0997, 0.0598, 0.0015, 0.0342, 0.0160, 0.0022, 0.7788, 0.0017],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,659][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0907, 0.1160, 0.1384, 0.0863, 0.1260, 0.1054, 0.0974, 0.1456, 0.0942],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,660][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ of] are: tensor([2.3332e-07, 1.3958e-01, 6.9077e-01, 1.3709e-02, 4.1050e-02, 4.2763e-02,
        2.6921e-02, 4.0908e-02, 4.2946e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,662][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0117, 0.0855, 0.0654, 0.1000, 0.1855, 0.2011, 0.1144, 0.0856, 0.1507],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,663][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.3277, 0.1964, 0.3145, 0.0094, 0.0070, 0.0108, 0.0030, 0.1230, 0.0067,
        0.0014], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,664][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0140, 0.0866, 0.0778, 0.0908, 0.1582, 0.1293, 0.1433, 0.0878, 0.0840,
        0.1282], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,666][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.0006, 0.0384, 0.0085, 0.0937, 0.0246, 0.1832, 0.0426, 0.1028, 0.2608,
        0.2450], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,667][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.0162, 0.1118, 0.1014, 0.0981, 0.1109, 0.1206, 0.1055, 0.1370, 0.0930,
        0.1055], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,669][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0089, 0.1773, 0.0574, 0.0026, 0.0203, 0.0245, 0.0111, 0.6608, 0.0134,
        0.0237], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,670][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0295, 0.0837, 0.0859, 0.1199, 0.1406, 0.1195, 0.1153, 0.1014, 0.1038,
        0.1004], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,672][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0481, 0.1244, 0.0944, 0.1012, 0.0960, 0.1151, 0.1177, 0.1081, 0.1027,
        0.0924], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,673][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.0149, 0.0945, 0.0038, 0.1111, 0.0747, 0.1456, 0.2281, 0.0536, 0.1215,
        0.1522], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,674][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.0093, 0.0805, 0.0544, 0.0009, 0.0238, 0.0121, 0.0014, 0.8093, 0.0018,
        0.0065], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,676][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0851, 0.1066, 0.1242, 0.0768, 0.1136, 0.0954, 0.0874, 0.1296, 0.0864,
        0.0949], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,677][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([1.9546e-07, 6.7865e-02, 8.3877e-01, 7.8523e-03, 1.7429e-02, 2.7321e-02,
        1.6901e-02, 1.2121e-02, 8.5417e-03, 3.1933e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,678][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0029, 0.1329, 0.0636, 0.1046, 0.1147, 0.1558, 0.0945, 0.0741, 0.1510,
        0.1061], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,680][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.3803, 0.1568, 0.2168, 0.0083, 0.0070, 0.0042, 0.0013, 0.1833, 0.0039,
        0.0023, 0.0358], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,681][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0062, 0.0749, 0.0684, 0.0976, 0.1624, 0.1328, 0.1392, 0.0752, 0.0760,
        0.1161, 0.0514], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,683][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0004, 0.0065, 0.0052, 0.0769, 0.0336, 0.1107, 0.1319, 0.1148, 0.1176,
        0.3732, 0.0293], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,684][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0146, 0.1008, 0.0889, 0.0896, 0.1013, 0.1079, 0.0981, 0.1229, 0.0875,
        0.0980, 0.0906], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,686][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0210, 0.1102, 0.0824, 0.0048, 0.0301, 0.0269, 0.0153, 0.4952, 0.0192,
        0.0294, 0.1654], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,687][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0289, 0.0700, 0.0801, 0.1070, 0.1435, 0.1136, 0.1068, 0.0943, 0.0953,
        0.0903, 0.0703], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,689][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0383, 0.1111, 0.0937, 0.0933, 0.0906, 0.1032, 0.1070, 0.0987, 0.0924,
        0.0823, 0.0892], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,689][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0186, 0.0796, 0.0026, 0.1077, 0.0625, 0.1381, 0.2253, 0.0471, 0.1014,
        0.1199, 0.0973], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,690][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0251, 0.0615, 0.0585, 0.0012, 0.0146, 0.0112, 0.0019, 0.7745, 0.0032,
        0.0099, 0.0384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,690][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0789, 0.0960, 0.1132, 0.0714, 0.1029, 0.0862, 0.0797, 0.1183, 0.0786,
        0.0870, 0.0878], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,690][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ at] are: tensor([1.6959e-07, 3.0153e-01, 4.6103e-01, 3.5530e-02, 4.1400e-02, 3.4994e-02,
        3.1424e-02, 7.1095e-02, 1.1258e-02, 1.0897e-02, 8.4123e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,691][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0031, 0.0848, 0.0472, 0.1075, 0.1190, 0.1363, 0.1097, 0.0625, 0.1554,
        0.1154, 0.0592], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,691][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0419, 0.1988, 0.2447, 0.0137, 0.0110, 0.0098, 0.0018, 0.3454, 0.0081,
        0.0035, 0.0934, 0.0279], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,692][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0061, 0.0728, 0.0634, 0.0823, 0.1433, 0.1180, 0.1208, 0.0856, 0.0673,
        0.1201, 0.0569, 0.0633], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,693][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([3.5613e-04, 5.8221e-03, 4.2224e-03, 5.5969e-02, 2.9824e-02, 1.2270e-01,
        3.3348e-02, 9.5499e-02, 1.7265e-01, 3.8975e-01, 6.1205e-02, 2.8663e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,694][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0128, 0.0932, 0.0813, 0.0829, 0.0931, 0.1005, 0.0898, 0.1138, 0.0793,
        0.0890, 0.0836, 0.0806], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,696][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0254, 0.0809, 0.0727, 0.0015, 0.0213, 0.0136, 0.0054, 0.5508, 0.0066,
        0.0110, 0.1454, 0.0653], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,697][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0256, 0.0675, 0.0700, 0.1008, 0.1299, 0.1056, 0.0973, 0.0826, 0.0894,
        0.0818, 0.0671, 0.0825], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,698][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0287, 0.1042, 0.0850, 0.0846, 0.0824, 0.0935, 0.0994, 0.0921, 0.0833,
        0.0734, 0.0817, 0.0916], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,700][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0175, 0.0720, 0.0022, 0.0924, 0.0536, 0.1224, 0.2032, 0.0434, 0.0941,
        0.1086, 0.0873, 0.1034], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,701][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.5982e-02, 5.2978e-02, 4.0606e-02, 6.1688e-04, 1.1342e-02, 5.6530e-03,
        5.5436e-04, 8.0240e-01, 1.7939e-03, 5.2215e-03, 4.0230e-02, 2.2626e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,702][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0631, 0.0895, 0.1047, 0.0671, 0.0961, 0.0800, 0.0737, 0.1115, 0.0736,
        0.0816, 0.0823, 0.0769], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,703][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([4.7663e-07, 2.3642e-01, 6.0777e-01, 1.8269e-02, 3.1827e-02, 4.0393e-02,
        9.3228e-03, 3.9600e-02, 7.6488e-03, 5.4892e-03, 1.8921e-03, 1.3740e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,705][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0018, 0.1077, 0.0444, 0.1024, 0.1224, 0.1327, 0.0771, 0.0599, 0.1328,
        0.0960, 0.0729, 0.0498], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,706][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.4278, 0.1012, 0.2029, 0.0070, 0.0034, 0.0078, 0.0024, 0.0987, 0.0064,
        0.0011, 0.0951, 0.0412, 0.0050], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,708][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0080, 0.0673, 0.0519, 0.0752, 0.1232, 0.1088, 0.1139, 0.0678, 0.0634,
        0.1088, 0.0580, 0.0698, 0.0840], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,709][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ house] are: tensor([1.9834e-04, 1.7557e-02, 4.0817e-03, 7.2363e-02, 2.9433e-02, 7.8207e-02,
        4.7774e-02, 8.5727e-02, 8.3831e-02, 2.3931e-01, 8.6461e-02, 1.7047e-01,
        8.4587e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,710][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0131, 0.0841, 0.0759, 0.0757, 0.0846, 0.0921, 0.0810, 0.1034, 0.0715,
        0.0795, 0.0770, 0.0741, 0.0881], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,711][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0288, 0.0633, 0.0700, 0.0014, 0.0115, 0.0101, 0.0040, 0.3156, 0.0072,
        0.0097, 0.1466, 0.0672, 0.2647], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,713][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0232, 0.0646, 0.0607, 0.0940, 0.1193, 0.0891, 0.0903, 0.0717, 0.0847,
        0.0793, 0.0647, 0.0777, 0.0807], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,714][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0275, 0.0992, 0.0762, 0.0785, 0.0756, 0.0867, 0.0884, 0.0825, 0.0782,
        0.0687, 0.0771, 0.0830, 0.0783], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,716][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.0173, 0.0677, 0.0021, 0.0834, 0.0511, 0.1079, 0.1709, 0.0377, 0.0842,
        0.0958, 0.0776, 0.0945, 0.1097], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,717][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ house] are: tensor([4.6993e-02, 5.7678e-02, 4.7595e-02, 3.8937e-04, 5.4868e-03, 5.1797e-03,
        6.1697e-04, 6.7033e-01, 1.9517e-03, 4.0574e-03, 4.0968e-02, 1.8897e-02,
        9.9857e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,719][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0685, 0.0811, 0.0936, 0.0576, 0.0839, 0.0718, 0.0659, 0.0992, 0.0664,
        0.0729, 0.0748, 0.0715, 0.0928], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,719][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ house] are: tensor([3.6600e-08, 1.5601e-01, 6.4066e-01, 1.1983e-02, 6.8062e-02, 1.7677e-02,
        1.5796e-02, 4.2403e-02, 1.3304e-02, 2.4769e-02, 3.4312e-03, 3.9262e-03,
        1.9747e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,721][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0037, 0.0856, 0.0448, 0.0848, 0.1052, 0.1325, 0.0785, 0.0664, 0.1023,
        0.0835, 0.0694, 0.0534, 0.0901], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:20,722][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.3589, 0.0930, 0.2494, 0.0070, 0.0080, 0.0091, 0.0031, 0.1248, 0.0066,
        0.0022, 0.0719, 0.0485, 0.0119, 0.0059], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,724][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0042, 0.0576, 0.0585, 0.0710, 0.1595, 0.1016, 0.1081, 0.0613, 0.0538,
        0.0941, 0.0443, 0.0597, 0.0747, 0.0516], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,725][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [.] are: tensor([3.2471e-04, 1.6429e-03, 3.0539e-03, 3.5727e-02, 3.2453e-02, 9.8576e-02,
        6.4174e-02, 5.5260e-02, 5.0379e-02, 2.2720e-01, 1.8355e-02, 7.7032e-02,
        3.2527e-01, 1.0563e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,726][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0116, 0.0793, 0.0694, 0.0697, 0.0795, 0.0855, 0.0766, 0.0956, 0.0671,
        0.0752, 0.0708, 0.0685, 0.0819, 0.0694], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,728][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0118, 0.1040, 0.0625, 0.0032, 0.0208, 0.0179, 0.0087, 0.3417, 0.0090,
        0.0093, 0.1398, 0.0892, 0.1655, 0.0166], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,730][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0185, 0.0563, 0.0656, 0.0851, 0.1168, 0.0912, 0.0882, 0.0744, 0.0739,
        0.0725, 0.0578, 0.0730, 0.0775, 0.0492], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,731][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0354, 0.0813, 0.0760, 0.0693, 0.0735, 0.0812, 0.0873, 0.0785, 0.0684,
        0.0658, 0.0670, 0.0762, 0.0777, 0.0622], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,731][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0148, 0.0602, 0.0017, 0.0772, 0.0472, 0.0997, 0.1684, 0.0339, 0.0748,
        0.0876, 0.0706, 0.0852, 0.0959, 0.0828], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,732][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0211, 0.1171, 0.0546, 0.0007, 0.0163, 0.0152, 0.0009, 0.5785, 0.0030,
        0.0049, 0.0719, 0.0214, 0.0759, 0.0186], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,732][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0615, 0.0746, 0.0870, 0.0553, 0.0789, 0.0686, 0.0629, 0.0941, 0.0618,
        0.0682, 0.0697, 0.0660, 0.0857, 0.0657], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,733][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [.] are: tensor([2.5938e-07, 6.2794e-02, 7.8180e-01, 7.6334e-03, 4.4111e-02, 3.1102e-02,
        2.3699e-02, 2.2175e-02, 5.1581e-03, 4.0884e-03, 1.3811e-03, 2.9993e-03,
        1.2525e-02, 5.3307e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,733][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0032, 0.0376, 0.0435, 0.0438, 0.0954, 0.0987, 0.0639, 0.0562, 0.0812,
        0.1286, 0.0509, 0.0442, 0.1615, 0.0915], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:20,734][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([7.0132e-01, 6.6802e-02, 1.2632e-01, 2.3375e-03, 1.2887e-03, 2.0731e-03,
        7.3195e-04, 4.2299e-02, 1.7933e-03, 3.1045e-04, 2.8966e-02, 1.9637e-02,
        1.9924e-03, 2.6552e-03, 1.4780e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,734][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0086, 0.0547, 0.0522, 0.0576, 0.1046, 0.1001, 0.0866, 0.0596, 0.0536,
        0.0787, 0.0504, 0.0620, 0.0774, 0.0497, 0.1042], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,735][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0004, 0.0212, 0.0084, 0.0413, 0.0164, 0.1183, 0.0987, 0.0854, 0.0996,
        0.1348, 0.0751, 0.0866, 0.1272, 0.0485, 0.0380], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,737][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0106, 0.0746, 0.0657, 0.0661, 0.0729, 0.0804, 0.0704, 0.0908, 0.0616,
        0.0692, 0.0663, 0.0646, 0.0779, 0.0647, 0.0642], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,738][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0760, 0.0436, 0.0939, 0.0016, 0.0125, 0.0103, 0.0039, 0.2025, 0.0053,
        0.0105, 0.0928, 0.0520, 0.1358, 0.0244, 0.2350], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,739][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0226, 0.0546, 0.0474, 0.0822, 0.0883, 0.0914, 0.0745, 0.0677, 0.0721,
        0.0739, 0.0602, 0.0672, 0.0800, 0.0538, 0.0641], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,741][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0623, 0.0774, 0.0445, 0.0629, 0.0526, 0.0826, 0.0787, 0.0624, 0.0699,
        0.0604, 0.0718, 0.0787, 0.0775, 0.0676, 0.0506], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,742][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0138, 0.0567, 0.0015, 0.0752, 0.0425, 0.0927, 0.1482, 0.0272, 0.0718,
        0.0804, 0.0630, 0.0725, 0.0924, 0.0766, 0.0856], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,744][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0558, 0.0199, 0.0722, 0.0007, 0.0195, 0.0042, 0.0006, 0.2675, 0.0012,
        0.0053, 0.0180, 0.0138, 0.0381, 0.0287, 0.4545], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,745][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0660, 0.0684, 0.0796, 0.0477, 0.0705, 0.0594, 0.0543, 0.0837, 0.0560,
        0.0602, 0.0630, 0.0609, 0.0783, 0.0617, 0.0903], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,746][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([3.2500e-08, 6.3303e-02, 7.5796e-01, 1.6253e-02, 2.4456e-02, 4.0320e-02,
        4.7056e-02, 2.5258e-02, 9.7938e-03, 4.7955e-03, 1.2038e-03, 2.8473e-03,
        4.1175e-03, 8.5839e-04, 1.7743e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,748][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0029, 0.0735, 0.0301, 0.0555, 0.0760, 0.1066, 0.0711, 0.0412, 0.0906,
        0.0587, 0.0570, 0.0413, 0.0694, 0.1334, 0.0927], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:20,749][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([9.3570e-01, 1.3094e-02, 3.3398e-02, 4.4802e-04, 3.4012e-04, 2.2512e-04,
        8.6206e-05, 8.1130e-03, 2.3085e-04, 5.3593e-05, 3.6073e-03, 2.2329e-03,
        3.3376e-04, 5.9219e-04, 3.7765e-04, 1.1685e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,750][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0034, 0.0514, 0.0406, 0.0551, 0.1024, 0.0810, 0.0911, 0.0700, 0.0455,
        0.0806, 0.0465, 0.0546, 0.0735, 0.0498, 0.1019, 0.0527],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,751][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([1.7742e-04, 6.1437e-03, 1.9704e-03, 3.0164e-02, 2.6074e-02, 4.2206e-02,
        5.2826e-02, 4.1505e-02, 6.3796e-02, 1.6793e-01, 6.4827e-02, 7.6486e-02,
        2.3890e-01, 3.4912e-02, 8.9649e-02, 6.2439e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,753][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0104, 0.0688, 0.0607, 0.0613, 0.0682, 0.0746, 0.0662, 0.0839, 0.0581,
        0.0648, 0.0619, 0.0595, 0.0715, 0.0603, 0.0599, 0.0698],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,754][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0665, 0.0515, 0.0652, 0.0012, 0.0089, 0.0067, 0.0037, 0.1779, 0.0046,
        0.0076, 0.0736, 0.0476, 0.1092, 0.0155, 0.1838, 0.1767],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,756][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0177, 0.0482, 0.0534, 0.0784, 0.1035, 0.0737, 0.0795, 0.0670, 0.0693,
        0.0625, 0.0506, 0.0657, 0.0641, 0.0442, 0.0789, 0.0431],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,757][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0412, 0.0762, 0.0555, 0.0621, 0.0577, 0.0776, 0.0785, 0.0648, 0.0618,
        0.0557, 0.0608, 0.0686, 0.0697, 0.0582, 0.0494, 0.0621],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,759][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0096, 0.0481, 0.0016, 0.0622, 0.0372, 0.0848, 0.1380, 0.0286, 0.0622,
        0.0709, 0.0582, 0.0690, 0.0825, 0.0710, 0.0834, 0.0926],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,760][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1019, 0.0234, 0.0630, 0.0004, 0.0045, 0.0026, 0.0005, 0.3301, 0.0010,
        0.0022, 0.0192, 0.0092, 0.0467, 0.0164, 0.1209, 0.2580],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,762][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0560, 0.0635, 0.0752, 0.0461, 0.0664, 0.0553, 0.0516, 0.0781, 0.0520,
        0.0564, 0.0588, 0.0561, 0.0722, 0.0573, 0.0844, 0.0707],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,763][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([4.4205e-07, 1.2795e-01, 4.5052e-01, 5.4906e-02, 1.2916e-01, 9.5682e-02,
        2.9685e-02, 2.4657e-02, 1.2750e-02, 1.8543e-02, 4.6992e-03, 1.1727e-02,
        8.0503e-03, 5.6128e-03, 1.3610e-02, 1.2441e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,764][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0027, 0.0672, 0.0222, 0.0612, 0.0751, 0.1106, 0.0602, 0.0434, 0.0784,
        0.0753, 0.0474, 0.0420, 0.0822, 0.0902, 0.0875, 0.0544],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:20,766][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1706, 0.1562, 0.2691, 0.0091, 0.0080, 0.0061, 0.0014, 0.1989, 0.0055,
        0.0017, 0.0660, 0.0310, 0.0106, 0.0092, 0.0094, 0.0398, 0.0074],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,767][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0048, 0.0443, 0.0449, 0.0504, 0.1083, 0.0789, 0.0732, 0.0559, 0.0440,
        0.0769, 0.0380, 0.0463, 0.0715, 0.0419, 0.1045, 0.0549, 0.0613],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,768][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([1.6787e-04, 3.3640e-03, 2.4776e-03, 1.9326e-02, 2.2356e-02, 4.2988e-02,
        1.6742e-02, 4.3927e-02, 9.4636e-02, 2.5480e-01, 3.1921e-02, 3.1390e-02,
        2.3914e-01, 1.3297e-02, 5.6535e-02, 7.0131e-02, 5.6805e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,770][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0086, 0.0656, 0.0564, 0.0580, 0.0648, 0.0704, 0.0632, 0.0799, 0.0552,
        0.0619, 0.0582, 0.0561, 0.0672, 0.0573, 0.0573, 0.0652, 0.0549],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,771][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0241, 0.0236, 0.0356, 0.0008, 0.0075, 0.0058, 0.0025, 0.1822, 0.0030,
        0.0045, 0.0523, 0.0267, 0.0784, 0.0094, 0.1624, 0.2329, 0.1484],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,773][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0178, 0.0483, 0.0504, 0.0742, 0.0918, 0.0761, 0.0710, 0.0619, 0.0652,
        0.0608, 0.0494, 0.0598, 0.0684, 0.0441, 0.0696, 0.0430, 0.0483],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,773][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0305, 0.0676, 0.0550, 0.0563, 0.0548, 0.0685, 0.0719, 0.0633, 0.0580,
        0.0539, 0.0586, 0.0672, 0.0649, 0.0551, 0.0521, 0.0616, 0.0607],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,774][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0104, 0.0443, 0.0014, 0.0606, 0.0342, 0.0764, 0.1233, 0.0271, 0.0573,
        0.0634, 0.0537, 0.0628, 0.0745, 0.0630, 0.0716, 0.0816, 0.0945],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,774][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([3.3244e-02, 1.8271e-02, 3.4284e-02, 3.7108e-04, 4.6679e-03, 3.2814e-03,
        2.3301e-04, 2.9922e-01, 8.3174e-04, 1.4130e-03, 1.3027e-02, 5.7694e-03,
        3.5316e-02, 1.4709e-02, 1.0528e-01, 3.8899e-01, 4.1092e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,775][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0477, 0.0589, 0.0689, 0.0440, 0.0626, 0.0535, 0.0493, 0.0747, 0.0496,
        0.0545, 0.0555, 0.0524, 0.0687, 0.0535, 0.0788, 0.0679, 0.0596],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,775][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([2.1760e-07, 3.4073e-01, 3.6979e-01, 4.2806e-02, 7.8149e-02, 6.1669e-02,
        7.5633e-03, 2.8909e-02, 4.1667e-03, 1.5513e-02, 1.5382e-03, 2.3468e-03,
        1.7766e-02, 2.0260e-03, 1.3029e-02, 1.0180e-02, 3.8136e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,776][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0008, 0.0718, 0.0310, 0.0652, 0.0715, 0.0847, 0.0336, 0.0426, 0.0866,
        0.0706, 0.0520, 0.0363, 0.0959, 0.0805, 0.1128, 0.0344, 0.0298],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:20,777][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([8.6079e-01, 3.1185e-02, 5.8711e-02, 1.1030e-03, 4.6136e-04, 7.4846e-04,
        2.7228e-04, 1.7152e-02, 7.2216e-04, 1.3665e-04, 1.0400e-02, 6.6713e-03,
        7.1961e-04, 1.3426e-03, 7.0700e-04, 4.6617e-03, 1.2348e-03, 2.9845e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,778][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0075, 0.0471, 0.0505, 0.0497, 0.0659, 0.0753, 0.0812, 0.0524, 0.0527,
        0.0717, 0.0482, 0.0516, 0.0578, 0.0498, 0.0633, 0.0475, 0.0705, 0.0572],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,779][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([1.6467e-04, 1.3632e-02, 5.2830e-03, 2.9817e-02, 1.0125e-02, 1.1583e-01,
        5.6870e-02, 2.1759e-02, 6.0979e-02, 5.0236e-02, 2.2464e-02, 6.5669e-02,
        1.0025e-01, 2.3202e-02, 1.9073e-02, 8.2993e-02, 1.4468e-01, 1.7697e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,780][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0084, 0.0612, 0.0534, 0.0538, 0.0584, 0.0665, 0.0591, 0.0767, 0.0516,
        0.0570, 0.0563, 0.0544, 0.0646, 0.0535, 0.0515, 0.0630, 0.0518, 0.0587],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,782][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0420, 0.0405, 0.0460, 0.0006, 0.0058, 0.0062, 0.0025, 0.1026, 0.0038,
        0.0052, 0.0381, 0.0264, 0.0689, 0.0094, 0.1198, 0.1664, 0.1123, 0.2035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,783][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0213, 0.0492, 0.0440, 0.0701, 0.0832, 0.0664, 0.0697, 0.0509, 0.0622,
        0.0581, 0.0491, 0.0589, 0.0604, 0.0434, 0.0634, 0.0397, 0.0482, 0.0620],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,785][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0323, 0.0650, 0.0462, 0.0524, 0.0516, 0.0687, 0.0699, 0.0581, 0.0576,
        0.0507, 0.0577, 0.0654, 0.0635, 0.0523, 0.0472, 0.0573, 0.0579, 0.0461],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,786][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0056, 0.0411, 0.0009, 0.0543, 0.0301, 0.0707, 0.1228, 0.0236, 0.0566,
        0.0605, 0.0540, 0.0620, 0.0726, 0.0592, 0.0752, 0.0832, 0.0952, 0.0322],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,788][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0950, 0.0180, 0.0586, 0.0003, 0.0070, 0.0026, 0.0004, 0.1143, 0.0007,
        0.0019, 0.0100, 0.0060, 0.0224, 0.0125, 0.1867, 0.1609, 0.0502, 0.2523],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,789][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0446, 0.0556, 0.0645, 0.0402, 0.0581, 0.0495, 0.0454, 0.0689, 0.0461,
        0.0506, 0.0516, 0.0486, 0.0640, 0.0503, 0.0742, 0.0629, 0.0560, 0.0688],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,790][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([2.2351e-07, 8.6647e-02, 7.7747e-01, 9.6751e-03, 1.6897e-02, 4.5811e-02,
        2.0607e-02, 1.1569e-02, 1.1326e-02, 2.0926e-03, 8.9396e-04, 2.7978e-03,
        3.6511e-03, 1.1135e-03, 1.5801e-03, 2.3093e-03, 3.7770e-03, 1.7771e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,792][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0055, 0.0615, 0.0473, 0.0469, 0.0860, 0.0885, 0.0488, 0.0452, 0.0631,
        0.0602, 0.0465, 0.0323, 0.0770, 0.0822, 0.0824, 0.0375, 0.0385, 0.0506],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:20,793][circuit_model.py][line:2294][INFO] ##7-th layer ##Weight##: The head1 weight for token [ to] are: tensor([5.6109e-01, 8.9515e-02, 1.6344e-01, 3.0078e-03, 2.3912e-03, 1.3314e-03,
        3.5732e-04, 8.3176e-02, 1.5713e-03, 7.9999e-04, 1.2119e-02, 6.5816e-03,
        5.0756e-03, 4.2358e-03, 3.4184e-03, 1.0821e-02, 1.9175e-03, 2.2596e-02,
        2.6549e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,795][circuit_model.py][line:2297][INFO] ##7-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0032, 0.0448, 0.0393, 0.0544, 0.1033, 0.0676, 0.0750, 0.0444, 0.0406,
        0.0690, 0.0323, 0.0422, 0.0506, 0.0379, 0.0874, 0.0395, 0.0568, 0.0733,
        0.0383], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,795][circuit_model.py][line:2300][INFO] ##7-th layer ##Weight##: The head3 weight for token [ to] are: tensor([9.9218e-05, 1.9498e-03, 1.6537e-03, 1.3740e-02, 1.3952e-02, 4.4794e-02,
        3.2424e-02, 2.7375e-02, 3.2020e-02, 1.1582e-01, 9.8111e-03, 2.6224e-02,
        1.1133e-01, 4.2709e-03, 3.5316e-02, 8.2009e-02, 9.3249e-02, 3.1505e-01,
        3.8914e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,797][circuit_model.py][line:2303][INFO] ##7-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0085, 0.0578, 0.0498, 0.0516, 0.0565, 0.0626, 0.0568, 0.0710, 0.0497,
        0.0548, 0.0527, 0.0509, 0.0600, 0.0513, 0.0503, 0.0587, 0.0496, 0.0540,
        0.0535], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,799][circuit_model.py][line:2306][INFO] ##7-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0307, 0.0415, 0.0479, 0.0006, 0.0049, 0.0048, 0.0018, 0.1398, 0.0024,
        0.0027, 0.0420, 0.0171, 0.0510, 0.0058, 0.1095, 0.1462, 0.1006, 0.1455,
        0.1050], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,800][circuit_model.py][line:2309][INFO] ##7-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0157, 0.0411, 0.0462, 0.0621, 0.0840, 0.0646, 0.0659, 0.0541, 0.0578,
        0.0541, 0.0421, 0.0532, 0.0592, 0.0369, 0.0638, 0.0361, 0.0440, 0.0697,
        0.0494], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,802][circuit_model.py][line:2312][INFO] ##7-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0265, 0.0604, 0.0507, 0.0506, 0.0517, 0.0607, 0.0647, 0.0569, 0.0519,
        0.0487, 0.0522, 0.0591, 0.0586, 0.0493, 0.0490, 0.0542, 0.0545, 0.0472,
        0.0530], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,803][circuit_model.py][line:2315][INFO] ##7-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.0077, 0.0406, 0.0013, 0.0539, 0.0298, 0.0676, 0.1160, 0.0243, 0.0500,
        0.0563, 0.0494, 0.0569, 0.0640, 0.0542, 0.0634, 0.0740, 0.0879, 0.0325,
        0.0703], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,804][circuit_model.py][line:2318][INFO] ##7-th layer ##Weight##: The head9 weight for token [ to] are: tensor([3.8979e-02, 2.1378e-02, 3.4798e-02, 1.2800e-04, 3.1760e-03, 1.8121e-03,
        1.1335e-04, 2.3628e-01, 2.8591e-04, 7.0850e-04, 9.2414e-03, 2.8219e-03,
        2.1025e-02, 1.0908e-02, 9.0557e-02, 2.1475e-01, 3.2954e-02, 2.2263e-01,
        5.7447e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,806][circuit_model.py][line:2321][INFO] ##7-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0401, 0.0521, 0.0615, 0.0390, 0.0556, 0.0477, 0.0441, 0.0670, 0.0439,
        0.0486, 0.0490, 0.0459, 0.0612, 0.0473, 0.0701, 0.0597, 0.0526, 0.0651,
        0.0495], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,807][circuit_model.py][line:2324][INFO] ##7-th layer ##Weight##: The head11 weight for token [ to] are: tensor([1.5361e-07, 1.4964e-01, 5.4912e-01, 3.2898e-02, 7.4656e-02, 4.0739e-02,
        2.6197e-02, 1.2974e-02, 5.5941e-03, 5.7207e-03, 8.0883e-04, 9.3303e-03,
        1.3276e-02, 9.2498e-04, 1.1294e-02, 6.8838e-03, 7.1995e-03, 5.0790e-02,
        1.9513e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,808][circuit_model.py][line:2327][INFO] ##7-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0025, 0.0384, 0.0337, 0.0460, 0.0683, 0.0826, 0.0619, 0.0459, 0.0726,
        0.0775, 0.0382, 0.0413, 0.0808, 0.0563, 0.0731, 0.0403, 0.0491, 0.0538,
        0.0378], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:20,855][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:20,856][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,857][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,857][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,858][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,858][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,858][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,859][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,859][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,859][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,860][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,860][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,860][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:20,861][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.2850, 0.7150], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,862][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.9890, 0.0110], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,864][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0016, 0.9984], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,865][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.5927, 0.4073], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,866][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.1875, 0.8125], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,868][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.9828, 0.0172], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,869][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.8816, 0.1184], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,870][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.3111, 0.6889], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,872][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.1769, 0.8231], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,873][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.2159, 0.7841], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,875][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.0120, 0.9880], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,876][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0262, 0.9738], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:20,878][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.1191, 0.4433, 0.4375], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,879][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.1394, 0.4527, 0.4079], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,880][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([6.3369e-04, 9.8467e-01, 1.4694e-02], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,881][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.3449, 0.2648, 0.3903], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,883][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.1221, 0.5158, 0.3621], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,884][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.5065, 0.1963, 0.2972], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,885][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.2961, 0.4238, 0.2801], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,887][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.0504, 0.8166, 0.1330], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,888][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.1414, 0.5023, 0.3563], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,890][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.1703, 0.3732, 0.4565], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,891][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.0033, 0.9425, 0.0541], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,892][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.0246, 0.8580, 0.1174], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:20,894][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0055, 0.6620, 0.3296, 0.0028], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,895][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.6766, 0.0197, 0.2917, 0.0120], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,897][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0009, 0.2968, 0.0092, 0.6932], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,898][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.0852, 0.5317, 0.3617, 0.0214], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,899][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0084, 0.7597, 0.2153, 0.0166], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,900][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.6868, 0.0636, 0.2291, 0.0205], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,900][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.4858, 0.1632, 0.2125, 0.1385], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,901][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.1556, 0.5299, 0.1702, 0.1442], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,901][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0105, 0.7644, 0.2168, 0.0083], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,901][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0184, 0.5639, 0.3986, 0.0191], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,902][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.0048, 0.4271, 0.0535, 0.5146], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,902][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0223, 0.5108, 0.0723, 0.3947], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:20,903][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0239, 0.4727, 0.4092, 0.0035, 0.0908], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,903][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.1773, 0.0641, 0.2873, 0.0422, 0.4291], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,903][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([3.2181e-04, 3.6318e-01, 6.9510e-03, 4.3489e-01, 1.9466e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,904][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.2119, 0.2590, 0.4288, 0.0144, 0.0858], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,905][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0340, 0.5296, 0.3130, 0.0139, 0.1095], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,907][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.5395, 0.0679, 0.1918, 0.0393, 0.1615], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,908][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.3041, 0.1438, 0.1822, 0.1384, 0.2314], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,909][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0736, 0.5662, 0.0970, 0.1300, 0.1332], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,911][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0382, 0.3501, 0.3566, 0.0086, 0.2465], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,912][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0713, 0.3511, 0.4451, 0.0138, 0.1187], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,914][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.0014, 0.2563, 0.0201, 0.2057, 0.5164], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,915][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0135, 0.3815, 0.0527, 0.3142, 0.2381], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:20,916][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.0465, 0.4607, 0.3264, 0.0017, 0.0826, 0.0821], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,918][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.4606, 0.0267, 0.2633, 0.0132, 0.1989, 0.0373], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,919][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0017, 0.1855, 0.0140, 0.4332, 0.1247, 0.2409], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,921][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.2033, 0.2668, 0.3911, 0.0121, 0.0695, 0.0572], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,922][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.0205, 0.6856, 0.1509, 0.0086, 0.0699, 0.0646], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,923][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.6520, 0.0449, 0.1730, 0.0089, 0.0928, 0.0284], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,925][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.4720, 0.0933, 0.1326, 0.0657, 0.1409, 0.0954], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,926][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1556, 0.3550, 0.1622, 0.0967, 0.1271, 0.1033], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,927][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.0244, 0.7010, 0.1528, 0.0051, 0.0832, 0.0335], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,929][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.0365, 0.2981, 0.4506, 0.0147, 0.1398, 0.0604], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,930][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.0055, 0.1798, 0.0317, 0.2781, 0.2293, 0.2756], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,932][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.0131, 0.2112, 0.0398, 0.1372, 0.1493, 0.4495], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:20,933][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0191, 0.5220, 0.2499, 0.0031, 0.0619, 0.1354, 0.0085],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,934][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.5579, 0.0159, 0.2594, 0.0077, 0.1224, 0.0213, 0.0154],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,936][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0017, 0.0886, 0.0123, 0.3101, 0.0942, 0.1658, 0.3272],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,937][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1415, 0.3014, 0.3534, 0.0147, 0.0894, 0.0723, 0.0273],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,939][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0170, 0.5034, 0.1773, 0.0140, 0.1353, 0.1034, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,940][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.7025, 0.0274, 0.1443, 0.0084, 0.0822, 0.0292, 0.0060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,941][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.4059, 0.0722, 0.1276, 0.0714, 0.1469, 0.1406, 0.0354],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,943][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1923, 0.2920, 0.1687, 0.0927, 0.1109, 0.0716, 0.0719],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,944][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0131, 0.6372, 0.1493, 0.0071, 0.1202, 0.0666, 0.0066],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,946][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0297, 0.3560, 0.3754, 0.0145, 0.1190, 0.0708, 0.0347],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,947][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0036, 0.1042, 0.0210, 0.1807, 0.1484, 0.2338, 0.3084],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,948][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0090, 0.1194, 0.0246, 0.1356, 0.0899, 0.2642, 0.3573],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:20,948][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([1.1955e-01, 5.4575e-02, 1.0493e-01, 1.6090e-04, 4.1825e-03, 1.0271e-02,
        1.1640e-03, 7.0516e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,949][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.4729, 0.0352, 0.2235, 0.0112, 0.1436, 0.0218, 0.0184, 0.0735],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,949][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([2.5949e-04, 8.1677e-02, 2.8937e-03, 8.6897e-02, 5.9587e-02, 1.7636e-01,
        5.4467e-01, 4.7655e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,949][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.3654, 0.0611, 0.2019, 0.0029, 0.0215, 0.0237, 0.0059, 0.3178],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,950][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.0579, 0.1706, 0.1282, 0.0022, 0.0177, 0.0271, 0.0123, 0.5840],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,950][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.7247, 0.0214, 0.1301, 0.0060, 0.0455, 0.0173, 0.0054, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,950][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.3998, 0.0678, 0.1223, 0.0565, 0.1070, 0.1094, 0.0399, 0.0971],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,951][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.1271, 0.3308, 0.1012, 0.0679, 0.0842, 0.0652, 0.1007, 0.1229],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,951][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.1045, 0.1181, 0.1162, 0.0008, 0.0125, 0.0105, 0.0015, 0.6360],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,952][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.0846, 0.0915, 0.1355, 0.0019, 0.0159, 0.0150, 0.0067, 0.6489],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,952][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.0026, 0.1151, 0.0169, 0.0713, 0.2088, 0.0900, 0.4133, 0.0820],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,954][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.0063, 0.1120, 0.0203, 0.0775, 0.0863, 0.1597, 0.3940, 0.1440],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:20,955][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([3.6975e-03, 3.4626e-02, 2.9272e-02, 3.5987e-04, 9.9033e-03, 1.0822e-02,
        2.3884e-03, 9.0754e-01, 1.3951e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,956][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.5664, 0.0097, 0.2334, 0.0052, 0.0887, 0.0172, 0.0140, 0.0423, 0.0230],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,957][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([1.7224e-04, 8.5827e-02, 2.8603e-03, 1.0724e-01, 4.0109e-02, 6.0852e-02,
        3.2922e-01, 3.3953e-02, 3.3976e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,958][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0683, 0.1360, 0.1651, 0.0085, 0.0566, 0.0503, 0.0220, 0.4774, 0.0158],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,959][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0075, 0.1818, 0.0769, 0.0072, 0.0537, 0.0376, 0.0148, 0.6068, 0.0138],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,961][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.6920, 0.0154, 0.1231, 0.0045, 0.0668, 0.0192, 0.0043, 0.0679, 0.0068],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,962][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.4629, 0.0405, 0.1142, 0.0408, 0.1025, 0.0794, 0.0241, 0.1048, 0.0307],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,964][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1403, 0.3191, 0.1213, 0.0810, 0.0946, 0.0437, 0.0551, 0.0958, 0.0491],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,965][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.0060, 0.0997, 0.0598, 0.0015, 0.0342, 0.0160, 0.0022, 0.7788, 0.0017],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,967][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0094, 0.0830, 0.1144, 0.0056, 0.0388, 0.0211, 0.0143, 0.7045, 0.0090],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,968][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0009, 0.0411, 0.0102, 0.0507, 0.1439, 0.0749, 0.3642, 0.0594, 0.2548],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,970][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0147, 0.1129, 0.0292, 0.0556, 0.0740, 0.1530, 0.3547, 0.0935, 0.1123],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:20,970][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([7.8493e-03, 2.5093e-02, 3.8205e-02, 1.7371e-04, 5.0683e-03, 9.1760e-03,
        9.7760e-04, 9.0496e-01, 1.6034e-03, 6.8922e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,972][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.6171, 0.0103, 0.2033, 0.0037, 0.0803, 0.0110, 0.0117, 0.0353, 0.0170,
        0.0104], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,973][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([8.8138e-05, 2.2890e-02, 1.2227e-03, 2.4497e-02, 3.8877e-02, 4.5074e-02,
        1.4104e-01, 1.3315e-02, 3.4793e-01, 3.6506e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,974][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.1095, 0.1377, 0.1527, 0.0045, 0.0260, 0.0323, 0.0093, 0.4846, 0.0097,
        0.0337], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,976][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.0089, 0.1773, 0.0574, 0.0026, 0.0203, 0.0245, 0.0111, 0.6608, 0.0134,
        0.0237], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,977][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.6451, 0.0256, 0.1028, 0.0062, 0.0869, 0.0351, 0.0054, 0.0621, 0.0147,
        0.0161], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,979][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.4912, 0.0386, 0.0976, 0.0310, 0.0613, 0.0602, 0.0190, 0.1299, 0.0310,
        0.0401], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,980][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.1650, 0.3173, 0.1241, 0.0296, 0.1005, 0.0263, 0.0235, 0.0655, 0.0491,
        0.0991], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,982][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.0093, 0.0805, 0.0544, 0.0009, 0.0238, 0.0121, 0.0014, 0.8093, 0.0018,
        0.0065], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,983][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.0106, 0.0689, 0.0813, 0.0021, 0.0229, 0.0165, 0.0074, 0.7603, 0.0092,
        0.0207], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,984][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.0011, 0.0363, 0.0077, 0.0141, 0.0829, 0.0486, 0.2096, 0.0406, 0.5137,
        0.0452], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,986][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.0097, 0.1786, 0.0236, 0.0431, 0.0730, 0.1105, 0.1877, 0.0782, 0.1404,
        0.1552], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:20,987][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([6.0797e-02, 2.6685e-02, 7.2698e-02, 3.0936e-04, 5.7063e-03, 7.7324e-03,
        1.5127e-03, 7.7816e-01, 1.5526e-03, 1.0057e-02, 3.4794e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,988][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.4834, 0.0198, 0.2219, 0.0056, 0.0863, 0.0244, 0.0196, 0.0541, 0.0399,
        0.0169, 0.0282], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,989][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([4.1656e-05, 2.6691e-02, 8.4372e-04, 4.0881e-02, 1.1303e-02, 5.2646e-02,
        2.8151e-01, 1.5081e-02, 2.7628e-01, 2.7161e-01, 2.3101e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,991][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1838, 0.0702, 0.1703, 0.0049, 0.0240, 0.0203, 0.0115, 0.3475, 0.0130,
        0.0399, 0.1147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,991][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0210, 0.1102, 0.0824, 0.0048, 0.0301, 0.0269, 0.0153, 0.4952, 0.0192,
        0.0294, 0.1654], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,992][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.6929, 0.0174, 0.1026, 0.0045, 0.0469, 0.0207, 0.0042, 0.0511, 0.0110,
        0.0135, 0.0354], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,992][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.2786, 0.0581, 0.0827, 0.0528, 0.0960, 0.1093, 0.0368, 0.1140, 0.0517,
        0.0499, 0.0701], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,992][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1001, 0.2074, 0.0993, 0.0857, 0.0667, 0.0557, 0.0682, 0.0719, 0.0556,
        0.0714, 0.1180], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,993][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0251, 0.0615, 0.0585, 0.0012, 0.0146, 0.0112, 0.0019, 0.7745, 0.0032,
        0.0099, 0.0384], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,993][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0481, 0.0587, 0.1746, 0.0038, 0.0335, 0.0151, 0.0100, 0.5674, 0.0112,
        0.0241, 0.0537], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,994][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([3.5062e-04, 2.9182e-02, 4.3946e-03, 3.1579e-02, 4.6459e-02, 5.5623e-02,
        3.4053e-01, 3.4306e-02, 3.7849e-01, 5.3891e-02, 2.5188e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,994][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0031, 0.0557, 0.0120, 0.0547, 0.0448, 0.1689, 0.2994, 0.0519, 0.1462,
        0.0893, 0.0738], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:20,995][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([2.3626e-02, 1.1757e-02, 3.9416e-02, 7.4367e-05, 2.2161e-03, 3.3454e-03,
        3.7372e-04, 8.6886e-01, 6.5853e-04, 4.8080e-03, 3.3034e-02, 1.1830e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,996][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.6376, 0.0082, 0.2032, 0.0034, 0.0667, 0.0096, 0.0068, 0.0322, 0.0114,
        0.0056, 0.0112, 0.0044], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,998][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0003, 0.0314, 0.0023, 0.0671, 0.0250, 0.0730, 0.1706, 0.0198, 0.2490,
        0.2066, 0.0305, 0.1242], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:20,999][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1404, 0.0693, 0.1232, 0.0028, 0.0134, 0.0166, 0.0064, 0.3672, 0.0073,
        0.0208, 0.1245, 0.1082], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,000][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0254, 0.0809, 0.0727, 0.0015, 0.0213, 0.0136, 0.0054, 0.5508, 0.0066,
        0.0110, 0.1454, 0.0653], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,002][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.7610, 0.0138, 0.0911, 0.0029, 0.0381, 0.0165, 0.0023, 0.0371, 0.0041,
        0.0070, 0.0192, 0.0068], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,003][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.3833, 0.0435, 0.0848, 0.0403, 0.0752, 0.0791, 0.0234, 0.1064, 0.0282,
        0.0321, 0.0598, 0.0438], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,005][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1170, 0.1925, 0.0846, 0.0516, 0.0393, 0.0444, 0.0669, 0.0718, 0.0596,
        0.0753, 0.1519, 0.0450], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,006][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([1.5982e-02, 5.2978e-02, 4.0606e-02, 6.1688e-04, 1.1342e-02, 5.6530e-03,
        5.5436e-04, 8.0240e-01, 1.7939e-03, 5.2215e-03, 4.0230e-02, 2.2626e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,007][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0312, 0.0353, 0.1083, 0.0014, 0.0127, 0.0067, 0.0031, 0.6924, 0.0041,
        0.0084, 0.0443, 0.0522], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,009][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0013, 0.0354, 0.0067, 0.0556, 0.0627, 0.0790, 0.2435, 0.0474, 0.2987,
        0.0498, 0.0318, 0.0880], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,010][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0038, 0.0500, 0.0099, 0.0504, 0.0445, 0.1277, 0.1906, 0.0645, 0.1398,
        0.0712, 0.0936, 0.1539], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,011][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([4.9896e-02, 9.3401e-03, 4.5637e-02, 8.5406e-05, 1.5644e-03, 4.4786e-03,
        7.0637e-04, 6.2339e-01, 1.0632e-03, 3.8726e-03, 4.1024e-02, 1.6423e-02,
        2.0252e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,013][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.3551, 0.0249, 0.2118, 0.0055, 0.0972, 0.0274, 0.0300, 0.0759, 0.0332,
        0.0241, 0.0468, 0.0339, 0.0340], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,014][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([9.7653e-05, 1.4170e-02, 9.0607e-04, 2.1841e-02, 1.3404e-02, 4.5701e-02,
        1.0196e-01, 1.0356e-02, 1.9186e-01, 2.7375e-01, 2.2472e-02, 6.0704e-02,
        2.4278e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,015][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.2807, 0.0444, 0.1341, 0.0015, 0.0092, 0.0120, 0.0031, 0.2170, 0.0037,
        0.0102, 0.0780, 0.0426, 0.1635], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,017][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0288, 0.0633, 0.0700, 0.0014, 0.0115, 0.0101, 0.0040, 0.3156, 0.0072,
        0.0097, 0.1466, 0.0672, 0.2647], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,018][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.6925, 0.0167, 0.0863, 0.0027, 0.0327, 0.0123, 0.0032, 0.0306, 0.0062,
        0.0067, 0.0262, 0.0079, 0.0761], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,019][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.2542, 0.0610, 0.0743, 0.0453, 0.0681, 0.0982, 0.0241, 0.0804, 0.0426,
        0.0463, 0.0889, 0.0483, 0.0682], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,021][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.0823, 0.2237, 0.0758, 0.0593, 0.0614, 0.0363, 0.0306, 0.0458, 0.0673,
        0.0649, 0.1090, 0.0472, 0.0964], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,022][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([4.6993e-02, 5.7678e-02, 4.7595e-02, 3.8937e-04, 5.4868e-03, 5.1797e-03,
        6.1697e-04, 6.7033e-01, 1.9517e-03, 4.0574e-03, 4.0968e-02, 1.8897e-02,
        9.9857e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,023][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.0731, 0.0304, 0.0947, 0.0008, 0.0060, 0.0064, 0.0027, 0.4506, 0.0040,
        0.0088, 0.0423, 0.0463, 0.2339], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,025][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.0012, 0.0195, 0.0060, 0.0196, 0.0627, 0.0506, 0.2199, 0.0496, 0.3021,
        0.0657, 0.0498, 0.0936, 0.0598], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,026][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0081, 0.0821, 0.0174, 0.0513, 0.0542, 0.0897, 0.1520, 0.0496, 0.0847,
        0.0891, 0.1033, 0.1322, 0.0863], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,027][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([2.9959e-02, 3.2798e-02, 5.8967e-02, 2.4368e-04, 7.0575e-03, 1.5103e-02,
        2.2872e-03, 5.3083e-01, 2.2777e-03, 6.2156e-03, 5.1703e-02, 3.0877e-02,
        2.2660e-01, 5.0814e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,029][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.7529, 0.0048, 0.1353, 0.0022, 0.0367, 0.0073, 0.0040, 0.0173, 0.0108,
        0.0049, 0.0091, 0.0031, 0.0052, 0.0063], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,030][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([5.0992e-05, 1.8250e-02, 5.2612e-04, 3.0538e-02, 1.1011e-02, 4.8749e-02,
        1.4013e-01, 1.0039e-02, 1.9157e-01, 2.2561e-01, 2.2472e-02, 1.1524e-01,
        1.5707e-01, 2.8744e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,031][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0609, 0.0735, 0.0675, 0.0042, 0.0169, 0.0294, 0.0116, 0.1870, 0.0115,
        0.0225, 0.1339, 0.1380, 0.2036, 0.0394], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,033][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.0118, 0.1040, 0.0625, 0.0032, 0.0208, 0.0179, 0.0087, 0.3417, 0.0090,
        0.0093, 0.1398, 0.0892, 0.1655, 0.0166], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,034][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.5963, 0.0142, 0.0727, 0.0029, 0.0325, 0.0244, 0.0039, 0.0298, 0.0063,
        0.0064, 0.0262, 0.0089, 0.1624, 0.0130], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,034][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.3152, 0.0336, 0.0694, 0.0369, 0.0679, 0.0747, 0.0229, 0.0692, 0.0293,
        0.0294, 0.0603, 0.0478, 0.0683, 0.0752], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,035][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0645, 0.1876, 0.0530, 0.0539, 0.0476, 0.0341, 0.0612, 0.0502, 0.0690,
        0.0629, 0.1553, 0.0634, 0.0599, 0.0374], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,035][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.0211, 0.1171, 0.0546, 0.0007, 0.0163, 0.0152, 0.0009, 0.5785, 0.0030,
        0.0049, 0.0719, 0.0214, 0.0759, 0.0186], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,036][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.0323, 0.0908, 0.1002, 0.0027, 0.0214, 0.0177, 0.0069, 0.3734, 0.0063,
        0.0098, 0.0510, 0.0606, 0.1991, 0.0279], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,036][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([2.7581e-04, 2.5561e-02, 2.5444e-03, 3.7928e-02, 4.8688e-02, 4.4220e-02,
        1.8160e-01, 2.1863e-02, 3.2371e-01, 3.7026e-02, 2.5067e-02, 1.1058e-01,
        2.7072e-02, 1.1387e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,037][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0007, 0.0210, 0.0029, 0.0263, 0.0188, 0.0784, 0.1612, 0.0267, 0.1327,
        0.0802, 0.0947, 0.1754, 0.1115, 0.0695], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,037][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([9.3504e-02, 1.0513e-02, 6.0953e-02, 1.7294e-04, 3.4337e-03, 3.5015e-03,
        7.0646e-04, 3.6055e-01, 1.1817e-03, 6.7697e-03, 3.9040e-02, 1.5110e-02,
        2.3142e-01, 9.6724e-03, 1.6348e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,038][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.2445, 0.0173, 0.1871, 0.0113, 0.1125, 0.0270, 0.0335, 0.0518, 0.0317,
        0.0172, 0.0341, 0.0198, 0.0206, 0.0155, 0.1761], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,039][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([3.0459e-05, 2.2010e-02, 5.5748e-04, 2.3160e-02, 1.2121e-02, 3.9779e-02,
        1.6197e-01, 1.2785e-02, 2.8941e-01, 1.1331e-01, 2.1678e-02, 1.0067e-01,
        1.6037e-01, 2.4408e-02, 1.7745e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,041][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.2167, 0.0310, 0.0919, 0.0025, 0.0094, 0.0121, 0.0038, 0.1414, 0.0042,
        0.0116, 0.0570, 0.0505, 0.1371, 0.0389, 0.1918], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,042][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0760, 0.0436, 0.0939, 0.0016, 0.0125, 0.0103, 0.0039, 0.2025, 0.0053,
        0.0105, 0.0928, 0.0520, 0.1358, 0.0244, 0.2350], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,044][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.6277, 0.0075, 0.0694, 0.0028, 0.0260, 0.0125, 0.0032, 0.0417, 0.0057,
        0.0117, 0.0261, 0.0080, 0.0852, 0.0183, 0.0543], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,045][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.3084, 0.0303, 0.0863, 0.0302, 0.0552, 0.0518, 0.0190, 0.0565, 0.0272,
        0.0317, 0.0469, 0.0367, 0.0616, 0.0596, 0.0984], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,046][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.1225, 0.2012, 0.0652, 0.0574, 0.0497, 0.0286, 0.0313, 0.0403, 0.0470,
        0.0589, 0.0734, 0.0297, 0.0769, 0.0365, 0.0813], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,048][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0558, 0.0199, 0.0722, 0.0007, 0.0195, 0.0042, 0.0006, 0.2675, 0.0012,
        0.0053, 0.0180, 0.0138, 0.0381, 0.0287, 0.4545], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,049][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0822, 0.0256, 0.0891, 0.0013, 0.0140, 0.0087, 0.0041, 0.2509, 0.0055,
        0.0082, 0.0351, 0.0437, 0.2093, 0.0374, 0.1849], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,050][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([2.6815e-04, 2.5453e-02, 2.3771e-03, 1.9413e-02, 3.9202e-02, 5.1017e-02,
        1.7110e-01, 2.6711e-02, 2.9799e-01, 2.2623e-02, 3.1361e-02, 1.4346e-01,
        2.4213e-02, 7.6451e-02, 6.8357e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,052][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0034, 0.0643, 0.0099, 0.0391, 0.0424, 0.0920, 0.1948, 0.0321, 0.0924,
        0.0536, 0.0779, 0.1312, 0.0655, 0.0645, 0.0370], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,053][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([2.4200e-01, 5.9937e-03, 6.5334e-02, 6.9634e-05, 1.8817e-03, 1.8604e-03,
        2.8479e-04, 2.5613e-01, 3.4438e-04, 1.1580e-03, 2.0077e-02, 5.2870e-03,
        3.8690e-02, 3.9633e-03, 9.6335e-02, 2.6060e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,054][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.2401, 0.0203, 0.1391, 0.0068, 0.0599, 0.0264, 0.0274, 0.0607, 0.0378,
        0.0280, 0.0528, 0.0235, 0.0393, 0.0351, 0.1350, 0.0679],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,055][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([1.3857e-04, 2.5096e-02, 1.0890e-03, 3.9010e-02, 1.4689e-02, 3.3436e-02,
        1.6913e-01, 1.1039e-02, 1.4538e-01, 2.2298e-01, 1.9316e-02, 7.9571e-02,
        1.1273e-01, 6.9511e-02, 2.8212e-02, 2.8665e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,057][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.1933, 0.0203, 0.0724, 0.0010, 0.0046, 0.0047, 0.0023, 0.0843, 0.0026,
        0.0067, 0.0361, 0.0234, 0.0656, 0.0203, 0.1104, 0.3519],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,058][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0665, 0.0515, 0.0652, 0.0012, 0.0089, 0.0067, 0.0037, 0.1779, 0.0046,
        0.0076, 0.0736, 0.0476, 0.1092, 0.0155, 0.1838, 0.1767],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,060][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.5295, 0.0108, 0.0847, 0.0027, 0.0370, 0.0110, 0.0031, 0.0353, 0.0043,
        0.0071, 0.0147, 0.0060, 0.1152, 0.0132, 0.0846, 0.0407],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,061][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.2920, 0.0322, 0.0761, 0.0293, 0.0575, 0.0428, 0.0180, 0.0643, 0.0261,
        0.0224, 0.0453, 0.0322, 0.0585, 0.0481, 0.0909, 0.0642],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,063][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0769, 0.1026, 0.0540, 0.0344, 0.0472, 0.0369, 0.0480, 0.0431, 0.0425,
        0.0591, 0.0939, 0.0442, 0.0813, 0.0421, 0.0902, 0.1035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,064][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.1019, 0.0234, 0.0630, 0.0004, 0.0045, 0.0026, 0.0005, 0.3301, 0.0010,
        0.0022, 0.0192, 0.0092, 0.0467, 0.0164, 0.1209, 0.2580],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,066][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0628, 0.0147, 0.0845, 0.0010, 0.0075, 0.0034, 0.0028, 0.1925, 0.0022,
        0.0038, 0.0199, 0.0254, 0.1157, 0.0210, 0.1838, 0.2591],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,067][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.0014, 0.0402, 0.0063, 0.0379, 0.0447, 0.0423, 0.1373, 0.0332, 0.1285,
        0.0537, 0.0315, 0.1216, 0.0431, 0.1610, 0.0855, 0.0319],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,069][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0077, 0.0524, 0.0183, 0.0416, 0.0435, 0.1075, 0.1615, 0.0473, 0.0709,
        0.0439, 0.0406, 0.1065, 0.0635, 0.0900, 0.0389, 0.0658],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,070][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([1.0997e-01, 4.6965e-03, 3.5348e-02, 5.0540e-05, 9.4360e-04, 2.1699e-03,
        2.0448e-04, 1.9427e-01, 3.7932e-04, 1.6329e-03, 1.5903e-02, 4.2625e-03,
        6.0928e-02, 2.4168e-03, 6.3617e-02, 4.2965e-01, 7.3561e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,071][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.6100, 0.0061, 0.1521, 0.0031, 0.0334, 0.0074, 0.0066, 0.0241, 0.0133,
        0.0077, 0.0128, 0.0067, 0.0072, 0.0071, 0.0676, 0.0203, 0.0145],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,072][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([1.5513e-04, 2.3750e-02, 1.4378e-03, 7.4350e-02, 1.6293e-02, 7.1997e-02,
        1.2109e-01, 1.7178e-02, 1.9007e-01, 1.7037e-01, 2.3589e-02, 6.0069e-02,
        1.1830e-01, 3.0498e-02, 1.7496e-02, 3.0142e-02, 3.3224e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,074][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0966, 0.0187, 0.0430, 0.0012, 0.0041, 0.0046, 0.0022, 0.0843, 0.0029,
        0.0075, 0.0427, 0.0307, 0.0710, 0.0237, 0.0961, 0.2983, 0.1724],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,076][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0241, 0.0236, 0.0356, 0.0008, 0.0075, 0.0058, 0.0025, 0.1822, 0.0030,
        0.0045, 0.0523, 0.0267, 0.0784, 0.0094, 0.1624, 0.2329, 0.1484],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,076][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.7076, 0.0057, 0.0568, 0.0012, 0.0180, 0.0082, 0.0013, 0.0194, 0.0019,
        0.0036, 0.0092, 0.0040, 0.0616, 0.0049, 0.0412, 0.0464, 0.0092],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,077][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.4517, 0.0168, 0.0645, 0.0224, 0.0350, 0.0348, 0.0129, 0.0566, 0.0158,
        0.0167, 0.0326, 0.0218, 0.0326, 0.0321, 0.0619, 0.0510, 0.0407],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,077][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0770, 0.1330, 0.0595, 0.0482, 0.0416, 0.0303, 0.0327, 0.0616, 0.0408,
        0.0523, 0.1034, 0.0450, 0.0688, 0.0289, 0.0596, 0.0761, 0.0412],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,078][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([3.3244e-02, 1.8271e-02, 3.4284e-02, 3.7108e-04, 4.6679e-03, 3.2814e-03,
        2.3301e-04, 2.9922e-01, 8.3174e-04, 1.4130e-03, 1.3027e-02, 5.7694e-03,
        3.5316e-02, 1.4709e-02, 1.0528e-01, 3.8899e-01, 4.1092e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,078][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0365, 0.0096, 0.0464, 0.0005, 0.0042, 0.0025, 0.0013, 0.1680, 0.0021,
        0.0034, 0.0184, 0.0197, 0.0912, 0.0122, 0.1023, 0.2695, 0.2120],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,079][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0007, 0.0244, 0.0033, 0.0415, 0.0235, 0.0569, 0.1362, 0.0506, 0.1893,
        0.0439, 0.0213, 0.0894, 0.0556, 0.1208, 0.0538, 0.0292, 0.0595],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,079][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0029, 0.0365, 0.0077, 0.0391, 0.0288, 0.0834, 0.1300, 0.0306, 0.0920,
        0.0490, 0.0650, 0.1281, 0.0712, 0.0920, 0.0278, 0.0487, 0.0671],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,080][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([9.4121e-02, 7.3573e-03, 3.2863e-02, 4.7884e-05, 1.0886e-03, 1.6749e-03,
        2.2635e-04, 1.0552e-01, 3.8240e-04, 1.3444e-03, 1.2289e-02, 3.5330e-03,
        3.9927e-02, 4.0782e-03, 7.1344e-02, 1.9804e-01, 5.8116e-02, 3.6804e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,081][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.2265, 0.0228, 0.1164, 0.0076, 0.0596, 0.0200, 0.0222, 0.0478, 0.0476,
        0.0272, 0.0363, 0.0216, 0.0314, 0.0209, 0.1258, 0.0569, 0.0375, 0.0720],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,082][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([1.3725e-05, 1.2143e-02, 2.2396e-04, 1.5562e-02, 5.5999e-03, 3.3640e-02,
        1.1336e-01, 6.5602e-03, 3.5329e-01, 1.1725e-01, 2.6924e-02, 8.6058e-02,
        1.3118e-01, 3.7342e-02, 1.0074e-02, 1.6041e-02, 2.7893e-02, 6.8512e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,084][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.1130, 0.0174, 0.0395, 0.0007, 0.0036, 0.0048, 0.0017, 0.0682, 0.0026,
        0.0051, 0.0326, 0.0187, 0.0533, 0.0181, 0.0859, 0.1649, 0.0955, 0.2742],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,085][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0420, 0.0405, 0.0460, 0.0006, 0.0058, 0.0062, 0.0025, 0.1026, 0.0038,
        0.0052, 0.0381, 0.0264, 0.0689, 0.0094, 0.1198, 0.1664, 0.1123, 0.2035],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,087][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.4262, 0.0111, 0.0584, 0.0026, 0.0230, 0.0101, 0.0035, 0.0254, 0.0047,
        0.0092, 0.0172, 0.0092, 0.0645, 0.0108, 0.0666, 0.0650, 0.0207, 0.1719],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,088][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.2698, 0.0243, 0.0654, 0.0242, 0.0516, 0.0357, 0.0179, 0.0498, 0.0237,
        0.0170, 0.0395, 0.0308, 0.0373, 0.0415, 0.0820, 0.0529, 0.0458, 0.0910],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,090][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0191, 0.1222, 0.0269, 0.0292, 0.0268, 0.0235, 0.0362, 0.0411, 0.0438,
        0.0436, 0.1100, 0.0579, 0.1074, 0.0329, 0.0908, 0.0807, 0.0462, 0.0617],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,091][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0950, 0.0180, 0.0586, 0.0003, 0.0070, 0.0026, 0.0004, 0.1143, 0.0007,
        0.0019, 0.0100, 0.0060, 0.0224, 0.0125, 0.1867, 0.1609, 0.0502, 0.2523],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,093][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0299, 0.0129, 0.0385, 0.0005, 0.0044, 0.0038, 0.0015, 0.1119, 0.0022,
        0.0035, 0.0149, 0.0154, 0.0672, 0.0129, 0.0971, 0.1570, 0.1495, 0.2770],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,094][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0003, 0.0155, 0.0021, 0.0155, 0.0269, 0.0406, 0.1511, 0.0235, 0.2145,
        0.0440, 0.0454, 0.0841, 0.0603, 0.0661, 0.0712, 0.0380, 0.0898, 0.0112],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,096][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0064, 0.0542, 0.0135, 0.0408, 0.0363, 0.0673, 0.1211, 0.0385, 0.0865,
        0.0521, 0.0819, 0.0938, 0.0544, 0.0778, 0.0366, 0.0531, 0.0616, 0.0241],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,097][circuit_model.py][line:2332][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([5.2699e-02, 3.0147e-03, 1.6162e-02, 1.0400e-05, 3.6004e-04, 6.4152e-04,
        9.2546e-05, 1.0992e-01, 9.7719e-05, 4.6693e-04, 3.8525e-03, 1.7136e-03,
        3.0188e-02, 1.1107e-03, 3.9749e-02, 1.4916e-01, 4.3800e-02, 5.0325e-01,
        4.3708e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,098][circuit_model.py][line:2335][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.7322, 0.0027, 0.1254, 0.0016, 0.0232, 0.0053, 0.0031, 0.0138, 0.0052,
        0.0025, 0.0051, 0.0021, 0.0027, 0.0033, 0.0342, 0.0120, 0.0052, 0.0180,
        0.0025], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,099][circuit_model.py][line:2338][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([1.4883e-04, 3.6997e-02, 1.2887e-03, 4.2550e-02, 1.1285e-02, 5.5093e-02,
        1.6593e-01, 1.5507e-02, 1.4096e-01, 1.0760e-01, 2.6914e-02, 9.0867e-02,
        7.0892e-02, 2.5088e-02, 1.1009e-02, 3.2821e-02, 4.6647e-02, 1.2217e-02,
        1.0618e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,101][circuit_model.py][line:2341][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.0544, 0.0102, 0.0249, 0.0004, 0.0018, 0.0024, 0.0012, 0.0545, 0.0013,
        0.0031, 0.0265, 0.0194, 0.0307, 0.0108, 0.0555, 0.2197, 0.1231, 0.2195,
        0.1408], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,103][circuit_model.py][line:2344][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0307, 0.0415, 0.0479, 0.0006, 0.0049, 0.0048, 0.0018, 0.1398, 0.0024,
        0.0027, 0.0420, 0.0171, 0.0510, 0.0058, 0.1095, 0.1462, 0.1006, 0.1455,
        0.1050], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,104][circuit_model.py][line:2347][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.5266, 0.0054, 0.0434, 0.0007, 0.0121, 0.0051, 0.0008, 0.0201, 0.0012,
        0.0020, 0.0085, 0.0028, 0.0735, 0.0054, 0.0383, 0.0472, 0.0086, 0.1720,
        0.0263], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,105][circuit_model.py][line:2350][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.4301, 0.0142, 0.0578, 0.0125, 0.0264, 0.0227, 0.0076, 0.0375, 0.0094,
        0.0085, 0.0284, 0.0148, 0.0297, 0.0240, 0.0503, 0.0457, 0.0311, 0.1116,
        0.0377], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,107][circuit_model.py][line:2353][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1037, 0.1184, 0.0570, 0.0442, 0.0294, 0.0260, 0.0482, 0.0499, 0.0244,
        0.0342, 0.1085, 0.0353, 0.0354, 0.0191, 0.0475, 0.0796, 0.0419, 0.0680,
        0.0292], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,108][circuit_model.py][line:2356][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([3.8979e-02, 2.1378e-02, 3.4798e-02, 1.2800e-04, 3.1760e-03, 1.8121e-03,
        1.1335e-04, 2.3628e-01, 2.8591e-04, 7.0850e-04, 9.2414e-03, 2.8219e-03,
        2.1025e-02, 1.0908e-02, 9.0557e-02, 2.1475e-01, 3.2954e-02, 2.2263e-01,
        5.7447e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,109][circuit_model.py][line:2359][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([2.9459e-02, 7.7471e-03, 3.6825e-02, 2.9188e-04, 2.0911e-03, 1.9375e-03,
        8.3902e-04, 1.2126e-01, 1.0066e-03, 1.4625e-03, 9.3319e-03, 9.0263e-03,
        4.0308e-02, 7.6964e-03, 5.7827e-02, 1.2555e-01, 1.4048e-01, 3.0727e-01,
        9.9595e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,110][circuit_model.py][line:2362][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.0009, 0.0215, 0.0035, 0.0206, 0.0178, 0.0561, 0.1894, 0.0280, 0.1347,
        0.0206, 0.0243, 0.1214, 0.0342, 0.0822, 0.0326, 0.0540, 0.0767, 0.0091,
        0.0724], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,112][circuit_model.py][line:2365][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0101, 0.0353, 0.0135, 0.0266, 0.0266, 0.0679, 0.1435, 0.0279, 0.0558,
        0.0345, 0.0635, 0.1184, 0.0453, 0.0583, 0.0235, 0.0499, 0.0779, 0.0325,
        0.0891], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,113][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:21,116][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[7133],
        [ 646],
        [6347],
        [ 297],
        [ 191],
        [ 367],
        [ 323],
        [ 160],
        [ 416],
        [ 197],
        [ 152],
        [ 488],
        [ 574],
        [ 260],
        [ 446],
        [ 248],
        [ 308],
        [  83],
        [ 163]], device='cuda:0')
[2024-07-24 10:25:21,117][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[ 8216],
        [ 4606],
        [43055],
        [11107],
        [ 9265],
        [ 5467],
        [ 6656],
        [ 4831],
        [10627],
        [ 8182],
        [ 4162],
        [ 9965],
        [13409],
        [10881],
        [14252],
        [ 5213],
        [ 6004],
        [ 4102],
        [ 4428]], device='cuda:0')
[2024-07-24 10:25:21,119][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[35579],
        [34472],
        [27599],
        [17154],
        [16404],
        [13394],
        [12725],
        [29949],
        [12998],
        [15007],
        [14856],
        [12083],
        [15300],
        [14818],
        [22531],
        [33599],
        [13161],
        [30267],
        [18580]], device='cuda:0')
[2024-07-24 10:25:21,120][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[32983],
        [22578],
        [10974],
        [12168],
        [15560],
        [17376],
        [17199],
        [17567],
        [17703],
        [17237],
        [17326],
        [17338],
        [16577],
        [16353],
        [16322],
        [16209],
        [16101],
        [16397],
        [17143]], device='cuda:0')
[2024-07-24 10:25:21,121][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[25334],
        [11215],
        [ 2017],
        [ 1076],
        [ 2848],
        [11651],
        [ 4913],
        [ 3553],
        [ 6381],
        [ 6696],
        [ 9072],
        [ 9903],
        [ 8379],
        [ 9078],
        [ 6281],
        [ 9339],
        [ 9530],
        [ 6905],
        [14560]], device='cuda:0')
[2024-07-24 10:25:21,122][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[226],
        [223],
        [196],
        [270],
        [287],
        [395],
        [439],
        [449],
        [458],
        [483],
        [526],
        [569],
        [661],
        [706],
        [733],
        [774],
        [793],
        [872],
        [911]], device='cuda:0')
[2024-07-24 10:25:21,123][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 4117],
        [ 7243],
        [ 9030],
        [ 8582],
        [ 8624],
        [ 8325],
        [ 8889],
        [13132],
        [12907],
        [13011],
        [10741],
        [12000],
        [ 9931],
        [10510],
        [10017],
        [ 9384],
        [ 8855],
        [ 8520],
        [ 9766]], device='cuda:0')
[2024-07-24 10:25:21,125][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[40228],
        [40822],
        [34975],
        [35284],
        [38046],
        [36161],
        [35901],
        [36426],
        [36544],
        [37226],
        [37307],
        [37563],
        [37805],
        [37764],
        [38342],
        [38371],
        [38224],
        [39050],
        [39220]], device='cuda:0')
[2024-07-24 10:25:21,126][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 2925],
        [13225],
        [11908],
        [16642],
        [16129],
        [15932],
        [15306],
        [16380],
        [18131],
        [18526],
        [19411],
        [19262],
        [18942],
        [19365],
        [19746],
        [20355],
        [21085],
        [21171],
        [21162]], device='cuda:0')
[2024-07-24 10:25:21,128][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[33919],
        [16667],
        [16142],
        [18356],
        [18650],
        [19216],
        [18753],
        [18136],
        [17920],
        [16185],
        [15478],
        [14435],
        [13153],
        [12677],
        [12798],
        [12719],
        [12828],
        [12603],
        [12589]], device='cuda:0')
[2024-07-24 10:25:21,130][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[17494],
        [12326],
        [16290],
        [14146],
        [17171],
        [13878],
        [13963],
        [ 7300],
        [ 6819],
        [ 6672],
        [ 6482],
        [ 6224],
        [ 5856],
        [ 6074],
        [10397],
        [ 6352],
        [ 5695],
        [ 7803],
        [ 6364]], device='cuda:0')
[2024-07-24 10:25:21,131][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[44205],
        [33777],
        [25361],
        [23239],
        [24385],
        [22345],
        [21209],
        [20971],
        [20135],
        [20127],
        [19839],
        [19617],
        [19386],
        [19475],
        [19413],
        [18551],
        [18292],
        [18017],
        [18020]], device='cuda:0')
[2024-07-24 10:25:21,132][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[11460],
        [20517],
        [  130],
        [   52],
        [   47],
        [ 1704],
        [  896],
        [  741],
        [  173],
        [   66],
        [ 1006],
        [  360],
        [  264],
        [   89],
        [  103],
        [  954],
        [ 2052],
        [   95],
        [  487]], device='cuda:0')
[2024-07-24 10:25:21,134][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[2031],
        [2133],
        [ 639],
        [ 327],
        [ 403],
        [ 531],
        [ 513],
        [ 520],
        [ 507],
        [ 579],
        [ 571],
        [ 569],
        [ 529],
        [ 475],
        [ 594],
        [ 530],
        [ 515],
        [ 489],
        [ 510]], device='cuda:0')
[2024-07-24 10:25:21,136][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[8358],
        [5049],
        [4939],
        [4790],
        [4953],
        [5114],
        [4549],
        [4231],
        [4275],
        [4629],
        [4499],
        [4242],
        [4139],
        [4565],
        [5143],
        [4841],
        [4533],
        [5036],
        [4544]], device='cuda:0')
[2024-07-24 10:25:21,137][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[10873],
        [23843],
        [15837],
        [19468],
        [15810],
        [14847],
        [15295],
        [ 5944],
        [ 6060],
        [ 6018],
        [ 6155],
        [ 6284],
        [ 6816],
        [ 7528],
        [ 8036],
        [ 8092],
        [10524],
        [10887],
        [11021]], device='cuda:0')
[2024-07-24 10:25:21,139][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[11097],
        [11206],
        [11163],
        [ 4631],
        [14975],
        [11290],
        [ 8758],
        [10154],
        [ 8466],
        [ 8003],
        [11238],
        [ 7657],
        [12901],
        [ 7261],
        [13297],
        [12876],
        [ 9956],
        [13274],
        [ 8802]], device='cuda:0')
[2024-07-24 10:25:21,140][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[ 5895],
        [16356],
        [16263],
        [ 9949],
        [10749],
        [ 6631],
        [ 8740],
        [10233],
        [14883],
        [ 9098],
        [10049],
        [ 9954],
        [11968],
        [11654],
        [14304],
        [11336],
        [11457],
        [15053],
        [11290]], device='cuda:0')
[2024-07-24 10:25:21,142][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[7893],
        [3698],
        [7208],
        [6818],
        [8557],
        [7834],
        [7692],
        [6272],
        [5588],
        [5555],
        [6362],
        [6933],
        [4303],
        [4066],
        [4579],
        [1690],
        [2483],
        [3039],
        [2275]], device='cuda:0')
[2024-07-24 10:25:21,143][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[31444],
        [24929],
        [25236],
        [25298],
        [26207],
        [25265],
        [25951],
        [16769],
        [17368],
        [16741],
        [17727],
        [18029],
        [21328],
        [20920],
        [19268],
        [19399],
        [20203],
        [20753],
        [19396]], device='cuda:0')
[2024-07-24 10:25:21,145][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[27702],
        [28034],
        [37897],
        [33572],
        [42649],
        [41022],
        [39839],
        [37256],
        [39109],
        [41178],
        [36896],
        [35441],
        [38059],
        [40111],
        [40756],
        [39837],
        [37228],
        [41820],
        [43119]], device='cuda:0')
[2024-07-24 10:25:21,146][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[27338],
        [13492],
        [14823],
        [ 7086],
        [ 8679],
        [ 9089],
        [ 6184],
        [ 5640],
        [ 6828],
        [ 6013],
        [ 5367],
        [ 6985],
        [ 6530],
        [ 7303],
        [ 8753],
        [ 9242],
        [ 9926],
        [10744],
        [14153]], device='cuda:0')
[2024-07-24 10:25:21,148][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[16918],
        [ 4913],
        [ 4959],
        [ 4722],
        [ 4089],
        [ 3551],
        [ 3524],
        [ 3201],
        [ 3599],
        [ 3677],
        [ 3537],
        [ 3592],
        [ 3736],
        [ 3690],
        [ 4047],
        [ 4573],
        [ 4321],
        [ 4393],
        [ 4588]], device='cuda:0')
[2024-07-24 10:25:21,149][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[43088],
        [47111],
        [48970],
        [48306],
        [48495],
        [48212],
        [48331],
        [47992],
        [47398],
        [47264],
        [47227],
        [46984],
        [46014],
        [46200],
        [36452],
        [44236],
        [43949],
        [41921],
        [43685]], device='cuda:0')
[2024-07-24 10:25:21,151][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[39755],
        [24618],
        [15173],
        [17712],
        [10753],
        [ 8530],
        [10423],
        [25982],
        [24857],
        [25887],
        [20971],
        [24047],
        [25653],
        [22433],
        [13992],
        [10549],
        [ 8820],
        [ 7352],
        [ 7472]], device='cuda:0')
[2024-07-24 10:25:21,152][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[36871],
        [26962],
        [27519],
        [26692],
        [30022],
        [25646],
        [21895],
        [21256],
        [19779],
        [19724],
        [18945],
        [18903],
        [17955],
        [16807],
        [16921],
        [17684],
        [16893],
        [16473],
        [15949]], device='cuda:0')
[2024-07-24 10:25:21,154][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[29159],
        [ 2101],
        [ 1704],
        [ 2919],
        [ 2163],
        [ 4285],
        [ 2923],
        [ 2969],
        [ 2971],
        [ 4552],
        [ 4559],
        [ 4382],
        [ 4987],
        [ 5502],
        [ 4218],
        [ 4027],
        [ 4352],
        [ 4261],
        [ 4127]], device='cuda:0')
[2024-07-24 10:25:21,155][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[13056],
        [34145],
        [28945],
        [36109],
        [27039],
        [33079],
        [36208],
        [38985],
        [37460],
        [36917],
        [38861],
        [39331],
        [36985],
        [38110],
        [38425],
        [38410],
        [39132],
        [35673],
        [37465]], device='cuda:0')
[2024-07-24 10:25:21,157][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[36481],
        [40984],
        [42932],
        [42194],
        [41716],
        [41438],
        [42736],
        [43455],
        [43876],
        [43160],
        [42945],
        [43359],
        [42531],
        [41834],
        [41213],
        [42176],
        [42862],
        [41348],
        [42430]], device='cuda:0')
[2024-07-24 10:25:21,158][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011],
        [18011]], device='cuda:0')
[2024-07-24 10:25:21,210][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:21,210][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,210][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,211][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,211][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,212][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,213][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,213][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,213][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,214][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,214][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,214][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,215][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,215][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.7969, 0.2031], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,216][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.9845, 0.0155], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,218][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.5748, 0.4252], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,219][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.0251, 0.9749], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,220][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.2090, 0.7910], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,222][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.8520, 0.1480], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,223][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.9543, 0.0457], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,224][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.0408, 0.9592], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,226][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.6511, 0.3489], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,227][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.1952, 0.8048], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,228][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.8710, 0.1290], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,230][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.9611, 0.0389], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,231][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.6132, 0.1863, 0.2005], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,233][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.1300, 0.7119, 0.1581], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,234][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.4004, 0.2124, 0.3872], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,235][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.0014, 0.9945, 0.0040], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,237][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.0765, 0.6366, 0.2869], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,238][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.3139, 0.3246, 0.3614], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,239][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.5876, 0.1769, 0.2355], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,240][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([2.9901e-03, 9.9668e-01, 3.2597e-04], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,242][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.1823, 0.4733, 0.3444], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,243][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.1002, 0.6600, 0.2398], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,244][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.8348, 0.0631, 0.1020], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,246][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.4164, 0.3373, 0.2463], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,247][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.4656, 0.2028, 0.2062, 0.1254], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,249][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.4553, 0.0067, 0.5371, 0.0009], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,250][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.2439, 0.2257, 0.3503, 0.1801], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,251][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.0096, 0.6334, 0.0183, 0.3388], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,253][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0015, 0.9139, 0.0831, 0.0015], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,253][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.1345, 0.5078, 0.3525, 0.0052], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,254][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.8020, 0.0526, 0.1083, 0.0371], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,254][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ and] are: tensor([3.5711e-03, 8.6488e-01, 8.2337e-04, 1.3072e-01], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,254][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.1476, 0.1730, 0.5638, 0.1156], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,255][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0823, 0.4068, 0.1778, 0.3332], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,255][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.6203, 0.1796, 0.1355, 0.0646], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,255][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.2561, 0.3257, 0.3596, 0.0586], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,256][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.4159, 0.2093, 0.1816, 0.1146, 0.0786], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,256][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0955, 0.3747, 0.4490, 0.0253, 0.0555], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,257][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.1721, 0.1681, 0.2794, 0.1334, 0.2470], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,258][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0051, 0.5360, 0.0124, 0.2498, 0.1968], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,259][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([2.3748e-03, 8.8927e-01, 1.0058e-01, 6.4187e-04, 7.1412e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,261][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.2131, 0.2986, 0.4029, 0.0062, 0.0793], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,262][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.6989, 0.0649, 0.1266, 0.0465, 0.0631], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,263][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0017, 0.7726, 0.0009, 0.1119, 0.1129], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,264][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1749, 0.0926, 0.3057, 0.0579, 0.3690], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,266][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0666, 0.3140, 0.1282, 0.2543, 0.2369], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,267][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.7931, 0.0694, 0.0849, 0.0236, 0.0290], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,269][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.2241, 0.4173, 0.2321, 0.0565, 0.0701], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,270][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.4007, 0.1902, 0.1871, 0.1108, 0.0671, 0.0442], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,271][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.1627, 0.0784, 0.2436, 0.0389, 0.4321, 0.0443], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,273][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.1222, 0.1518, 0.2242, 0.1314, 0.2188, 0.1515], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,274][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.0072, 0.3964, 0.0116, 0.2049, 0.1585, 0.2213], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,275][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.0166, 0.8059, 0.1389, 0.0009, 0.0029, 0.0348], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,277][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.3371, 0.2788, 0.2614, 0.0025, 0.0203, 0.1000], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,278][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.7610, 0.0418, 0.0999, 0.0292, 0.0394, 0.0286], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,280][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.0056, 0.4527, 0.0007, 0.0674, 0.0656, 0.4080], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,281][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.0852, 0.1603, 0.2601, 0.1773, 0.1590, 0.1581], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,283][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0538, 0.2419, 0.1061, 0.1986, 0.1854, 0.2141], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,284][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.8024, 0.0548, 0.0638, 0.0206, 0.0241, 0.0344], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,286][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.3103, 0.0852, 0.3621, 0.0340, 0.0460, 0.1624], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,287][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3274, 0.1839, 0.1765, 0.1074, 0.0585, 0.0432, 0.1031],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,289][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0090, 0.4045, 0.2813, 0.1266, 0.0967, 0.0771, 0.0048],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,290][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1159, 0.1211, 0.2000, 0.0983, 0.1727, 0.1135, 0.1786],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,291][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0050, 0.3087, 0.0099, 0.1682, 0.1261, 0.1839, 0.1982],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,292][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0026, 0.8523, 0.0684, 0.0019, 0.0057, 0.0647, 0.0044],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,294][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1757, 0.3070, 0.3231, 0.0029, 0.0351, 0.1463, 0.0098],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,295][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.8238, 0.0284, 0.0717, 0.0190, 0.0244, 0.0176, 0.0151],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,296][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0016, 0.3595, 0.0005, 0.0560, 0.0540, 0.3372, 0.1912],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,296][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0193, 0.2371, 0.1253, 0.2417, 0.1100, 0.1670, 0.0995],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,297][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0412, 0.2016, 0.0862, 0.1644, 0.1547, 0.1788, 0.1731],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,297][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.6776, 0.0834, 0.0657, 0.0356, 0.0296, 0.0486, 0.0595],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,297][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.3313, 0.1352, 0.2475, 0.0471, 0.0448, 0.1693, 0.0248],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,298][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.4118, 0.1370, 0.1458, 0.0719, 0.0521, 0.0307, 0.0781, 0.0728],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,298][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0622, 0.0783, 0.2614, 0.0333, 0.2734, 0.1622, 0.0306, 0.0986],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,299][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.1450, 0.1030, 0.1251, 0.0763, 0.1235, 0.0984, 0.1441, 0.1847],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,301][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.0041, 0.2557, 0.0074, 0.1325, 0.1085, 0.1515, 0.1617, 0.1786],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,301][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([9.5488e-02, 8.5996e-02, 4.1262e-02, 1.3222e-05, 7.0434e-05, 2.3269e-03,
        7.4541e-05, 7.7477e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,303][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.4171, 0.0501, 0.1293, 0.0005, 0.0046, 0.0226, 0.0014, 0.3746],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,304][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.7057, 0.0421, 0.0946, 0.0275, 0.0364, 0.0269, 0.0229, 0.0437],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,305][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([4.8150e-03, 2.6441e-01, 1.1276e-04, 3.5438e-02, 2.8607e-02, 2.8793e-01,
        1.5398e-01, 2.2471e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,306][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.0482, 0.1058, 0.1004, 0.0895, 0.0813, 0.0870, 0.0577, 0.4302],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,308][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0346, 0.1716, 0.0705, 0.1376, 0.1286, 0.1536, 0.1460, 0.1574],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,309][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.7021, 0.0410, 0.0350, 0.0131, 0.0113, 0.0202, 0.0223, 0.1548],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,311][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.5430, 0.0782, 0.1430, 0.0086, 0.0323, 0.0919, 0.0099, 0.0931],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,312][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.3316, 0.1472, 0.1592, 0.0895, 0.0549, 0.0361, 0.0777, 0.0666, 0.0372],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,313][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0152, 0.0092, 0.0710, 0.0036, 0.4781, 0.0682, 0.0133, 0.3398, 0.0015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,315][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0970, 0.0926, 0.1240, 0.0730, 0.1171, 0.0816, 0.1264, 0.1764, 0.1119],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,316][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0028, 0.2251, 0.0063, 0.1202, 0.0970, 0.1222, 0.1377, 0.1667, 0.1220],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,317][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ of] are: tensor([2.1572e-04, 4.8383e-02, 6.8249e-03, 1.6933e-04, 1.0040e-03, 4.6260e-03,
        4.8783e-04, 9.3791e-01, 3.7706e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,319][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0316, 0.0617, 0.0817, 0.0010, 0.0150, 0.0426, 0.0052, 0.7569, 0.0043],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,320][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.7792, 0.0262, 0.0744, 0.0178, 0.0245, 0.0172, 0.0144, 0.0290, 0.0173],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,321][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ of] are: tensor([8.4731e-04, 2.1159e-01, 1.5459e-04, 4.1869e-02, 3.7616e-02, 2.4162e-01,
        1.7752e-01, 2.0461e-01, 8.4162e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,322][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0393, 0.0463, 0.1264, 0.0426, 0.1148, 0.0624, 0.0444, 0.4533, 0.0705],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,324][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0288, 0.1453, 0.0635, 0.1205, 0.1129, 0.1303, 0.1278, 0.1392, 0.1318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,325][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.3388, 0.0880, 0.0733, 0.0339, 0.0347, 0.0531, 0.0554, 0.2751, 0.0477],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,327][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1017, 0.1994, 0.2359, 0.0563, 0.0560, 0.1812, 0.0496, 0.0710, 0.0488],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,328][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.3102, 0.1398, 0.1463, 0.0911, 0.0507, 0.0401, 0.0746, 0.0697, 0.0371,
        0.0404], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,329][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0198, 0.3477, 0.1793, 0.0302, 0.1518, 0.0691, 0.0353, 0.1076, 0.0167,
        0.0426], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,331][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.0791, 0.0821, 0.1007, 0.0633, 0.1078, 0.0708, 0.1067, 0.1499, 0.0855,
        0.1542], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,332][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.0027, 0.1729, 0.0069, 0.0955, 0.0821, 0.1142, 0.1163, 0.1271, 0.1139,
        0.1684], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,333][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([2.5516e-04, 5.2928e-02, 4.8317e-03, 1.4034e-04, 5.8596e-04, 4.1836e-03,
        6.1356e-04, 9.3564e-01, 3.5230e-04, 4.6901e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,334][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([4.5387e-02, 6.3774e-02, 5.9312e-02, 4.6819e-04, 5.6312e-03, 2.9787e-02,
        2.7055e-03, 7.8500e-01, 4.4701e-03, 3.4607e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,336][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.7138, 0.0330, 0.0848, 0.0222, 0.0293, 0.0225, 0.0185, 0.0367, 0.0208,
        0.0185], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,337][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([1.1427e-03, 1.4939e-01, 1.4492e-04, 2.8374e-02, 3.0162e-02, 2.0031e-01,
        1.3418e-01, 1.6604e-01, 6.9744e-02, 2.2051e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,337][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.0153, 0.0809, 0.1526, 0.0572, 0.1063, 0.0602, 0.0421, 0.2794, 0.1540,
        0.0521], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,338][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0260, 0.1318, 0.0572, 0.1080, 0.0997, 0.1176, 0.1131, 0.1235, 0.1173,
        0.1059], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,338][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.4369, 0.0637, 0.0879, 0.0322, 0.0313, 0.0374, 0.0465, 0.1993, 0.0389,
        0.0259], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,339][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0444, 0.1043, 0.0743, 0.0573, 0.0539, 0.4078, 0.0440, 0.1251, 0.0737,
        0.0152], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,339][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.2893, 0.1292, 0.1432, 0.0886, 0.0501, 0.0389, 0.0770, 0.0663, 0.0375,
        0.0420, 0.0378], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,339][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0060, 0.0236, 0.0690, 0.0158, 0.1738, 0.0787, 0.0700, 0.4206, 0.0338,
        0.1066, 0.0021], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,340][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0657, 0.0692, 0.0866, 0.0543, 0.0897, 0.0585, 0.0920, 0.1298, 0.0843,
        0.1263, 0.1438], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,340][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0021, 0.1806, 0.0034, 0.0878, 0.0686, 0.0936, 0.1060, 0.1097, 0.0978,
        0.1719, 0.0784], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,341][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ at] are: tensor([7.7044e-03, 6.1865e-02, 2.1057e-02, 2.0736e-04, 6.2211e-04, 8.5959e-03,
        7.2016e-04, 8.7948e-01, 8.2446e-04, 1.5505e-03, 1.7371e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,342][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.2050, 0.0536, 0.1276, 0.0013, 0.0101, 0.0220, 0.0026, 0.4587, 0.0048,
        0.0037, 0.1105], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,344][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.6814, 0.0347, 0.0833, 0.0240, 0.0303, 0.0220, 0.0194, 0.0357, 0.0235,
        0.0198, 0.0260], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,345][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ at] are: tensor([1.8298e-03, 1.1876e-01, 6.5833e-05, 3.2890e-02, 2.1557e-02, 1.8042e-01,
        1.4221e-01, 1.2476e-01, 7.3310e-02, 2.3103e-01, 7.3161e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,346][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0600, 0.0626, 0.0969, 0.0460, 0.0963, 0.0377, 0.0179, 0.3602, 0.1166,
        0.0703, 0.0355], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,347][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0233, 0.1190, 0.0488, 0.0982, 0.0908, 0.1063, 0.1036, 0.1127, 0.1075,
        0.0959, 0.0939], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,349][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.1461, 0.0977, 0.0903, 0.0483, 0.0511, 0.0497, 0.0706, 0.3061, 0.0579,
        0.0320, 0.0502], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,350][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0248, 0.1434, 0.1271, 0.1534, 0.0775, 0.1666, 0.0675, 0.1349, 0.0709,
        0.0269, 0.0071], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,352][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.4157, 0.1092, 0.1349, 0.0594, 0.0396, 0.0172, 0.0557, 0.0536, 0.0229,
        0.0284, 0.0216, 0.0418], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,353][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0065, 0.0593, 0.1012, 0.0474, 0.1002, 0.0649, 0.0053, 0.4364, 0.0208,
        0.1076, 0.0494, 0.0009], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,354][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0792, 0.0589, 0.0836, 0.0452, 0.0780, 0.0495, 0.0776, 0.1072, 0.0712,
        0.1072, 0.1255, 0.1169], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,356][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0023, 0.1497, 0.0039, 0.0795, 0.0602, 0.0855, 0.0951, 0.1051, 0.0884,
        0.1489, 0.0746, 0.1067], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,357][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([6.3162e-03, 5.7853e-02, 2.1375e-02, 3.4957e-04, 9.6978e-04, 6.3773e-03,
        6.8969e-04, 8.7175e-01, 9.4281e-04, 1.5556e-03, 1.9357e-02, 1.2463e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,358][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.3259e-01, 2.5588e-02, 1.0838e-01, 2.5346e-04, 4.2907e-03, 9.4311e-03,
        6.8686e-04, 5.2738e-01, 1.5340e-03, 1.3756e-03, 6.5591e-02, 2.2898e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,359][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.7511, 0.0258, 0.0668, 0.0168, 0.0219, 0.0150, 0.0130, 0.0265, 0.0164,
        0.0132, 0.0190, 0.0147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,360][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.3562e-03, 1.0814e-01, 6.0958e-05, 3.0353e-02, 2.1107e-02, 1.6606e-01,
        1.2232e-01, 1.1097e-01, 6.9597e-02, 2.1727e-01, 6.5917e-02, 8.6840e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,362][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0255, 0.0565, 0.0476, 0.0412, 0.0386, 0.0467, 0.0098, 0.5235, 0.0814,
        0.0786, 0.0375, 0.0131], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,363][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0241, 0.1071, 0.0459, 0.0878, 0.0803, 0.0940, 0.0912, 0.0985, 0.0950,
        0.0849, 0.0842, 0.1068], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,364][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2892, 0.0827, 0.0629, 0.0378, 0.0352, 0.0480, 0.0555, 0.2408, 0.0502,
        0.0260, 0.0351, 0.0366], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,366][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2126, 0.0754, 0.1436, 0.0506, 0.0359, 0.1773, 0.0343, 0.1649, 0.0486,
        0.0242, 0.0042, 0.0286], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,367][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.3884, 0.1186, 0.1272, 0.0686, 0.0391, 0.0236, 0.0587, 0.0505, 0.0234,
        0.0260, 0.0235, 0.0394, 0.0130], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,369][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0338, 0.1718, 0.0709, 0.0210, 0.1435, 0.0270, 0.0150, 0.2580, 0.0186,
        0.1309, 0.0518, 0.0080, 0.0496], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,370][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.0564, 0.0528, 0.0634, 0.0406, 0.0689, 0.0456, 0.0720, 0.0989, 0.0617,
        0.0989, 0.1061, 0.0975, 0.1372], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,372][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0018, 0.1486, 0.0030, 0.0689, 0.0602, 0.0780, 0.0855, 0.0897, 0.0827,
        0.1332, 0.0670, 0.0933, 0.0882], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,373][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ house] are: tensor([9.8227e-03, 6.9927e-02, 1.7515e-02, 3.1952e-04, 8.5799e-04, 1.0643e-02,
        1.2789e-03, 7.7062e-01, 1.4503e-03, 1.8924e-03, 1.6780e-02, 1.7942e-02,
        8.0954e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,374][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ house] are: tensor([4.0174e-01, 2.0425e-02, 8.4392e-02, 2.0191e-04, 1.6130e-03, 9.8349e-03,
        5.8883e-04, 2.7743e-01, 1.4521e-03, 6.8763e-04, 4.8661e-02, 1.6517e-02,
        1.3646e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,375][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.7239, 0.0261, 0.0677, 0.0175, 0.0240, 0.0168, 0.0141, 0.0279, 0.0170,
        0.0144, 0.0192, 0.0147, 0.0166], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,376][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ house] are: tensor([2.4100e-03, 6.0273e-02, 3.0172e-05, 1.9382e-02, 1.2174e-02, 1.3089e-01,
        9.3792e-02, 8.0648e-02, 5.7020e-02, 1.5097e-01, 5.1013e-02, 6.9035e-02,
        2.7237e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,378][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.0528, 0.0477, 0.1053, 0.0404, 0.0997, 0.0507, 0.0220, 0.2568, 0.0765,
        0.0482, 0.0689, 0.0521, 0.0788], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,379][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0210, 0.0993, 0.0390, 0.0803, 0.0728, 0.0878, 0.0841, 0.0924, 0.0875,
        0.0777, 0.0766, 0.0996, 0.0819], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,380][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ house] are: tensor([0.4342, 0.0529, 0.0655, 0.0221, 0.0251, 0.0244, 0.0510, 0.1708, 0.0274,
        0.0179, 0.0273, 0.0304, 0.0510], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,380][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0272, 0.1693, 0.0833, 0.0848, 0.0669, 0.3278, 0.0464, 0.0455, 0.0575,
        0.0057, 0.0030, 0.0446, 0.0381], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,381][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.3597, 0.0987, 0.1421, 0.0667, 0.0416, 0.0268, 0.0527, 0.0467, 0.0300,
        0.0257, 0.0255, 0.0427, 0.0145, 0.0266], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,381][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0273, 0.0021, 0.2184, 0.0019, 0.1244, 0.0408, 0.0194, 0.2752, 0.0061,
        0.0561, 0.0111, 0.0098, 0.2068, 0.0006], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,382][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.0602, 0.0528, 0.0649, 0.0390, 0.0603, 0.0411, 0.0644, 0.0843, 0.0565,
        0.0914, 0.0968, 0.0932, 0.1172, 0.0779], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,382][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0025, 0.1098, 0.0036, 0.0593, 0.0460, 0.0675, 0.0706, 0.0738, 0.0693,
        0.1039, 0.0579, 0.0858, 0.0789, 0.1710], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,382][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0065, 0.1843, 0.0308, 0.0008, 0.0019, 0.0211, 0.0016, 0.6458, 0.0031,
        0.0023, 0.0329, 0.0169, 0.0410, 0.0111], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,383][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [.] are: tensor([3.5722e-01, 5.7858e-02, 1.1690e-01, 3.5172e-04, 4.0288e-03, 1.4661e-02,
        1.1766e-03, 2.6816e-01, 2.4542e-03, 1.5622e-03, 4.7923e-02, 1.9493e-02,
        9.5855e-02, 1.2357e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,384][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.7572, 0.0218, 0.0605, 0.0150, 0.0198, 0.0135, 0.0117, 0.0223, 0.0137,
        0.0111, 0.0162, 0.0122, 0.0130, 0.0120], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,385][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [.] are: tensor([1.2482e-03, 6.9173e-02, 2.2132e-05, 2.1312e-02, 1.5642e-02, 1.3529e-01,
        1.1395e-01, 6.1268e-02, 5.7821e-02, 1.6033e-01, 4.4495e-02, 6.2670e-02,
        2.4280e-01, 1.3984e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,387][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0520, 0.0282, 0.0912, 0.0145, 0.0347, 0.0584, 0.0269, 0.3843, 0.0869,
        0.0401, 0.0393, 0.0424, 0.0686, 0.0325], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,388][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0202, 0.0898, 0.0403, 0.0741, 0.0686, 0.0794, 0.0771, 0.0830, 0.0793,
        0.0720, 0.0711, 0.0881, 0.0738, 0.0832], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,389][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [.] are: tensor([0.2981, 0.0576, 0.0572, 0.0269, 0.0294, 0.0391, 0.0499, 0.2180, 0.0371,
        0.0232, 0.0350, 0.0305, 0.0565, 0.0415], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,391][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0926, 0.1214, 0.1155, 0.0355, 0.0240, 0.1695, 0.0269, 0.1564, 0.0412,
        0.0099, 0.0126, 0.0393, 0.0647, 0.0904], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,392][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.2841, 0.1184, 0.1069, 0.0684, 0.0441, 0.0289, 0.0657, 0.0539, 0.0328,
        0.0351, 0.0292, 0.0497, 0.0186, 0.0288, 0.0354], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,394][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0179, 0.0769, 0.0523, 0.0051, 0.0126, 0.0896, 0.0431, 0.4155, 0.0093,
        0.1556, 0.0169, 0.0150, 0.0435, 0.0091, 0.0376], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,395][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0455, 0.0372, 0.0518, 0.0256, 0.0528, 0.0293, 0.0500, 0.0731, 0.0427,
        0.0737, 0.0804, 0.0714, 0.1052, 0.0605, 0.2007], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,396][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0012, 0.1228, 0.0022, 0.0516, 0.0439, 0.0620, 0.0620, 0.0676, 0.0578,
        0.1003, 0.0479, 0.0713, 0.0707, 0.1703, 0.0685], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,397][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([1.9331e-02, 5.3088e-02, 4.0031e-02, 1.8077e-04, 1.3066e-03, 7.5529e-03,
        4.7265e-04, 6.0537e-01, 7.8041e-04, 1.6726e-03, 1.9946e-02, 2.7850e-02,
        1.1547e-01, 2.0088e-02, 8.6863e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,399][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.1764, 0.0189, 0.0811, 0.0004, 0.0068, 0.0190, 0.0014, 0.2968, 0.0034,
        0.0045, 0.0553, 0.0369, 0.1870, 0.0230, 0.0890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,400][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.6122, 0.0323, 0.0762, 0.0229, 0.0309, 0.0206, 0.0183, 0.0318, 0.0204,
        0.0181, 0.0250, 0.0202, 0.0223, 0.0205, 0.0286], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,401][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([5.7279e-04, 4.5578e-02, 1.9813e-05, 1.1249e-02, 7.6648e-03, 9.4566e-02,
        5.6680e-02, 1.0233e-01, 3.6000e-02, 1.4482e-01, 4.1473e-02, 5.8417e-02,
        3.2930e-01, 1.3692e-02, 5.7634e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,403][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1161, 0.0143, 0.0697, 0.0101, 0.0994, 0.0523, 0.0089, 0.1371, 0.0208,
        0.0560, 0.0112, 0.0119, 0.1121, 0.0680, 0.2121], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,404][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0170, 0.0851, 0.0314, 0.0682, 0.0626, 0.0759, 0.0728, 0.0797, 0.0751,
        0.0672, 0.0656, 0.0867, 0.0706, 0.0799, 0.0623], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,406][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.4394, 0.0443, 0.0630, 0.0167, 0.0199, 0.0237, 0.0365, 0.1837, 0.0248,
        0.0162, 0.0235, 0.0261, 0.0400, 0.0263, 0.0159], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,407][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0743, 0.0677, 0.0448, 0.0159, 0.0177, 0.3445, 0.0134, 0.1215, 0.0188,
        0.0048, 0.0056, 0.0227, 0.1267, 0.0753, 0.0463], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,409][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.3181, 0.1126, 0.1185, 0.0675, 0.0417, 0.0276, 0.0590, 0.0498, 0.0262,
        0.0282, 0.0249, 0.0406, 0.0151, 0.0213, 0.0291, 0.0197],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,410][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0054, 0.0078, 0.0174, 0.0081, 0.0558, 0.0252, 0.0122, 0.1596, 0.0145,
        0.1479, 0.0601, 0.0097, 0.1295, 0.0095, 0.3278, 0.0096],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,412][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0292, 0.0342, 0.0394, 0.0268, 0.0444, 0.0293, 0.0491, 0.0683, 0.0434,
        0.0717, 0.0798, 0.0721, 0.0936, 0.0552, 0.1593, 0.1042],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,413][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0008, 0.1201, 0.0014, 0.0537, 0.0411, 0.0581, 0.0605, 0.0666, 0.0552,
        0.1046, 0.0453, 0.0638, 0.0618, 0.1778, 0.0614, 0.0278],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,414][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([7.1317e-02, 2.2280e-02, 3.2797e-02, 8.3531e-05, 2.3672e-04, 2.7315e-03,
        2.3144e-04, 4.3431e-01, 3.8541e-04, 6.0969e-04, 1.9157e-02, 9.8954e-03,
        5.2364e-02, 7.9379e-03, 1.5496e-02, 3.3017e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,415][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([4.0398e-01, 1.1631e-02, 6.1179e-02, 1.7058e-04, 1.6283e-03, 5.1810e-03,
        5.3624e-04, 1.2489e-01, 1.0379e-03, 4.5271e-04, 2.4045e-02, 1.0700e-02,
        4.7029e-02, 8.8128e-03, 2.7439e-02, 2.7128e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,417][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.6799, 0.0234, 0.0653, 0.0172, 0.0228, 0.0156, 0.0142, 0.0254, 0.0152,
        0.0139, 0.0181, 0.0146, 0.0161, 0.0146, 0.0213, 0.0225],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,418][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([2.6455e-03, 4.0004e-02, 2.9309e-05, 1.2295e-02, 8.9869e-03, 8.4572e-02,
        6.1208e-02, 6.5359e-02, 3.6028e-02, 1.0719e-01, 4.4045e-02, 6.3329e-02,
        2.3102e-01, 1.7243e-02, 6.2823e-02, 1.6323e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,419][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0314, 0.0183, 0.0750, 0.0241, 0.0280, 0.0734, 0.0177, 0.3497, 0.0846,
        0.0417, 0.0262, 0.0217, 0.0928, 0.0419, 0.0372, 0.0362],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,421][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0156, 0.0792, 0.0328, 0.0652, 0.0619, 0.0710, 0.0686, 0.0740, 0.0701,
        0.0635, 0.0612, 0.0794, 0.0656, 0.0751, 0.0610, 0.0558],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,422][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.3893, 0.0467, 0.0685, 0.0210, 0.0233, 0.0257, 0.0350, 0.1589, 0.0276,
        0.0209, 0.0266, 0.0271, 0.0379, 0.0255, 0.0207, 0.0454],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,422][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0772, 0.0617, 0.1485, 0.0267, 0.0357, 0.1804, 0.0195, 0.1390, 0.0274,
        0.0236, 0.0089, 0.0233, 0.0707, 0.0665, 0.0678, 0.0231],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,423][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3512, 0.0980, 0.1156, 0.0542, 0.0355, 0.0199, 0.0519, 0.0456, 0.0235,
        0.0263, 0.0215, 0.0403, 0.0133, 0.0175, 0.0271, 0.0161, 0.0425],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,423][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0011, 0.1040, 0.0534, 0.0412, 0.0296, 0.0229, 0.0011, 0.1268, 0.0252,
        0.0522, 0.0318, 0.0044, 0.3365, 0.0204, 0.1342, 0.0134, 0.0018],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,424][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0414, 0.0322, 0.0425, 0.0255, 0.0425, 0.0285, 0.0445, 0.0605, 0.0414,
        0.0628, 0.0723, 0.0696, 0.0888, 0.0542, 0.1469, 0.0957, 0.0505],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,424][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0013, 0.0875, 0.0021, 0.0483, 0.0349, 0.0537, 0.0565, 0.0640, 0.0550,
        0.0929, 0.0476, 0.0690, 0.0676, 0.1507, 0.0607, 0.0342, 0.0737],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,424][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([1.1703e-02, 4.5375e-02, 1.7535e-02, 4.2826e-04, 4.8303e-04, 7.6669e-03,
        7.3171e-04, 5.2457e-01, 1.1123e-03, 1.3082e-03, 1.9877e-02, 1.3828e-02,
        4.7379e-02, 1.2191e-02, 1.2214e-02, 2.5109e-01, 3.2508e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,425][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([3.2415e-01, 6.8807e-03, 5.3717e-02, 8.5117e-05, 1.1300e-03, 3.0215e-03,
        1.6589e-04, 1.0422e-01, 5.0011e-04, 2.8310e-04, 1.8473e-02, 5.2844e-03,
        3.7600e-02, 4.5266e-03, 2.4565e-02, 3.7094e-01, 4.4461e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,427][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.7176, 0.0215, 0.0590, 0.0152, 0.0183, 0.0128, 0.0117, 0.0225, 0.0129,
        0.0109, 0.0154, 0.0120, 0.0131, 0.0113, 0.0160, 0.0182, 0.0116],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,428][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([1.1863e-03, 4.4497e-02, 1.4803e-05, 1.4373e-02, 8.7067e-03, 9.5240e-02,
        7.0990e-02, 5.6671e-02, 4.0818e-02, 1.2783e-01, 3.8790e-02, 5.1219e-02,
        2.0833e-01, 1.1728e-02, 5.1012e-02, 1.2385e-01, 5.4746e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,429][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0136, 0.0288, 0.0196, 0.0263, 0.0162, 0.0282, 0.0103, 0.5049, 0.0621,
        0.0345, 0.0337, 0.0181, 0.0631, 0.0430, 0.0219, 0.0593, 0.0162],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,431][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0166, 0.0726, 0.0316, 0.0598, 0.0557, 0.0649, 0.0631, 0.0687, 0.0653,
        0.0597, 0.0585, 0.0739, 0.0617, 0.0694, 0.0559, 0.0524, 0.0701],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,432][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.4099, 0.0493, 0.0420, 0.0239, 0.0194, 0.0298, 0.0378, 0.1431, 0.0333,
        0.0180, 0.0243, 0.0247, 0.0383, 0.0306, 0.0146, 0.0240, 0.0371],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,434][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.2545, 0.0498, 0.1297, 0.0316, 0.0270, 0.1210, 0.0138, 0.0548, 0.0217,
        0.0166, 0.0057, 0.0240, 0.0949, 0.0581, 0.0522, 0.0249, 0.0197],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,435][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.4318, 0.0841, 0.0964, 0.0337, 0.0326, 0.0130, 0.0396, 0.0404, 0.0184,
        0.0217, 0.0170, 0.0330, 0.0119, 0.0138, 0.0310, 0.0155, 0.0348, 0.0313],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,437][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.1515, 0.0698, 0.2962, 0.0104, 0.0308, 0.0391, 0.0240, 0.0882, 0.0174,
        0.0347, 0.0355, 0.0213, 0.0116, 0.0133, 0.0410, 0.0800, 0.0277, 0.0073],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,438][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0266, 0.0275, 0.0317, 0.0212, 0.0376, 0.0235, 0.0397, 0.0546, 0.0336,
        0.0550, 0.0600, 0.0634, 0.0831, 0.0446, 0.1489, 0.0735, 0.0409, 0.1344],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,440][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0005, 0.1021, 0.0008, 0.0395, 0.0307, 0.0504, 0.0530, 0.0555, 0.0571,
        0.0886, 0.0448, 0.0694, 0.0603, 0.1686, 0.0618, 0.0295, 0.0687, 0.0186],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,441][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([3.4294e-02, 5.9107e-02, 1.7042e-02, 4.7614e-05, 1.0066e-04, 2.6886e-03,
        1.5372e-04, 2.7968e-01, 1.6601e-04, 1.6957e-04, 1.7598e-02, 7.8851e-03,
        2.2516e-02, 8.3368e-03, 8.7827e-03, 1.4874e-01, 3.8467e-02, 3.5423e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,442][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([2.3723e-01, 1.0173e-02, 4.7455e-02, 1.7831e-04, 1.4617e-03, 5.6231e-03,
        4.4301e-04, 6.8529e-02, 1.2405e-03, 5.4058e-04, 2.0624e-02, 7.5578e-03,
        2.9806e-02, 7.6584e-03, 2.5599e-02, 1.9013e-01, 4.9555e-02, 2.9620e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,443][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.4586, 0.0382, 0.0803, 0.0277, 0.0333, 0.0254, 0.0231, 0.0390, 0.0246,
        0.0228, 0.0280, 0.0244, 0.0264, 0.0231, 0.0330, 0.0330, 0.0233, 0.0357],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,444][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([7.6399e-04, 3.9277e-02, 5.3570e-06, 7.5405e-03, 4.7204e-03, 8.4617e-02,
        4.7026e-02, 5.0883e-02, 2.9664e-02, 1.0643e-01, 3.6923e-02, 4.8241e-02,
        2.4172e-01, 1.2893e-02, 4.5086e-02, 1.7164e-01, 6.3383e-02, 9.1936e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,446][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0212, 0.0258, 0.0461, 0.0132, 0.0543, 0.0286, 0.0113, 0.2192, 0.0488,
        0.0474, 0.0526, 0.0270, 0.0673, 0.0456, 0.1241, 0.0902, 0.0199, 0.0574],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,447][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0148, 0.0707, 0.0270, 0.0572, 0.0514, 0.0619, 0.0603, 0.0654, 0.0628,
        0.0556, 0.0555, 0.0720, 0.0581, 0.0659, 0.0513, 0.0485, 0.0676, 0.0538],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,449][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.7768, 0.0157, 0.0263, 0.0058, 0.0054, 0.0075, 0.0113, 0.0482, 0.0074,
        0.0056, 0.0079, 0.0085, 0.0137, 0.0108, 0.0040, 0.0122, 0.0117, 0.0212],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,450][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.5091, 0.0595, 0.0511, 0.0101, 0.0149, 0.1062, 0.0081, 0.0205, 0.0069,
        0.0024, 0.0029, 0.0154, 0.0385, 0.0371, 0.0345, 0.0175, 0.0110, 0.0542],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,452][circuit_model.py][line:2294][INFO] ##8-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.4157, 0.0841, 0.1098, 0.0392, 0.0307, 0.0140, 0.0396, 0.0381, 0.0179,
        0.0188, 0.0163, 0.0303, 0.0103, 0.0127, 0.0247, 0.0135, 0.0344, 0.0274,
        0.0224], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,454][circuit_model.py][line:2297][INFO] ##8-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0028, 0.0073, 0.0448, 0.0038, 0.1327, 0.0465, 0.0466, 0.1724, 0.0042,
        0.0661, 0.0017, 0.0206, 0.1203, 0.0011, 0.1372, 0.0371, 0.0551, 0.0985,
        0.0014], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,455][circuit_model.py][line:2300][INFO] ##8-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0428, 0.0283, 0.0363, 0.0225, 0.0379, 0.0262, 0.0397, 0.0496, 0.0332,
        0.0519, 0.0560, 0.0572, 0.0739, 0.0440, 0.1247, 0.0772, 0.0410, 0.1179,
        0.0398], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,456][circuit_model.py][line:2303][INFO] ##8-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.0014, 0.0856, 0.0019, 0.0422, 0.0328, 0.0476, 0.0507, 0.0570, 0.0497,
        0.0770, 0.0425, 0.0616, 0.0582, 0.1286, 0.0549, 0.0299, 0.0628, 0.0210,
        0.0946], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,458][circuit_model.py][line:2306][INFO] ##8-th layer ##Weight##: The head5 weight for token [ to] are: tensor([1.5764e-02, 4.1197e-02, 1.9114e-02, 9.9130e-05, 3.1135e-04, 3.0406e-03,
        3.5301e-04, 4.4730e-01, 4.1295e-04, 5.9933e-04, 1.0448e-02, 5.8038e-03,
        1.7884e-02, 8.4565e-03, 6.7915e-03, 1.0291e-01, 2.0726e-02, 2.5179e-01,
        4.6993e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,458][circuit_model.py][line:2309][INFO] ##8-th layer ##Weight##: The head6 weight for token [ to] are: tensor([2.5643e-01, 3.7734e-03, 3.5500e-02, 2.3667e-05, 5.4331e-04, 1.2326e-03,
        5.8777e-05, 4.9321e-02, 1.3868e-04, 9.8778e-05, 7.4622e-03, 1.6567e-03,
        1.7525e-02, 2.3045e-03, 1.4524e-02, 1.9041e-01, 2.1144e-02, 3.6905e-01,
        2.8797e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,460][circuit_model.py][line:2312][INFO] ##8-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.7270, 0.0184, 0.0521, 0.0122, 0.0158, 0.0111, 0.0099, 0.0198, 0.0112,
        0.0092, 0.0137, 0.0101, 0.0108, 0.0096, 0.0147, 0.0162, 0.0100, 0.0177,
        0.0103], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,461][circuit_model.py][line:2315][INFO] ##8-th layer ##Weight##: The head8 weight for token [ to] are: tensor([1.7493e-03, 4.2284e-02, 1.2461e-05, 1.3302e-02, 8.0139e-03, 8.5359e-02,
        6.9273e-02, 5.3401e-02, 3.4775e-02, 1.0849e-01, 3.6053e-02, 4.9401e-02,
        2.0935e-01, 1.4304e-02, 4.4311e-02, 9.9727e-02, 5.5423e-02, 8.4233e-03,
        6.6339e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,462][circuit_model.py][line:2318][INFO] ##8-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0412, 0.0139, 0.0633, 0.0126, 0.0605, 0.0242, 0.0078, 0.2759, 0.0324,
        0.0397, 0.0260, 0.0217, 0.0901, 0.0405, 0.0883, 0.0515, 0.0142, 0.0417,
        0.0545], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,464][circuit_model.py][line:2321][INFO] ##8-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0150, 0.0649, 0.0280, 0.0531, 0.0481, 0.0566, 0.0554, 0.0596, 0.0574,
        0.0515, 0.0516, 0.0651, 0.0541, 0.0617, 0.0485, 0.0471, 0.0624, 0.0512,
        0.0687], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,464][circuit_model.py][line:2324][INFO] ##8-th layer ##Weight##: The head11 weight for token [ to] are: tensor([0.1929, 0.0636, 0.0523, 0.0255, 0.0237, 0.0290, 0.0470, 0.1561, 0.0311,
        0.0168, 0.0302, 0.0315, 0.0429, 0.0357, 0.0173, 0.0401, 0.0417, 0.0521,
        0.0705], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,465][circuit_model.py][line:2327][INFO] ##8-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0401, 0.0888, 0.0896, 0.0327, 0.0300, 0.1420, 0.0231, 0.0291, 0.0359,
        0.0053, 0.0050, 0.0214, 0.0338, 0.0902, 0.0504, 0.0110, 0.0235, 0.2156,
        0.0326], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,511][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:21,512][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,512][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,513][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,514][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,514][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,515][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,516][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,517][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,518][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,519][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,520][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,521][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,523][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.9864, 0.0136], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,524][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0133, 0.9867], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,526][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.9173, 0.0827], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,527][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.7164, 0.2836], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,529][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.1555, 0.8445], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,531][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.8664, 0.1336], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,532][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.4033, 0.5967], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,534][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.2466, 0.7534], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,535][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.9740, 0.0260], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,537][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.2680, 0.7320], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,539][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.7687, 0.2313], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,540][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.6931, 0.3069], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,542][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.6234, 0.2671, 0.1094], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,543][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.0153, 0.7746, 0.2101], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,545][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.6267, 0.1843, 0.1890], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,546][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.2250, 0.4512, 0.3238], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,548][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.1217, 0.5860, 0.2923], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,550][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.3308, 0.3036, 0.3656], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,551][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.1139, 0.6826, 0.2035], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,553][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.1003, 0.6408, 0.2589], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,554][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.4485, 0.2170, 0.3345], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,555][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.1716, 0.4831, 0.3453], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,556][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.3150, 0.3502, 0.3348], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,557][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.1175, 0.5146, 0.3679], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:21,557][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.7933, 0.0937, 0.1114, 0.0017], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,558][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0084, 0.4504, 0.1123, 0.4289], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,560][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.5371, 0.0898, 0.2730, 0.1001], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,561][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.4402, 0.1786, 0.2888, 0.0924], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,563][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0173, 0.8668, 0.1118, 0.0041], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,564][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.1450, 0.4838, 0.3662, 0.0050], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,566][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.2536, 0.4081, 0.2027, 0.1356], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,568][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0074, 0.9034, 0.0871, 0.0020], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,569][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.6985, 0.0389, 0.2588, 0.0038], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,571][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0382, 0.6826, 0.2358, 0.0434], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,572][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.3838, 0.2415, 0.3446, 0.0302], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,574][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1441, 0.2227, 0.5553, 0.0779], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:21,576][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.3649, 0.4584, 0.0686, 0.0014, 0.1067], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,577][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0040, 0.4166, 0.0690, 0.3342, 0.1762], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,579][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.3340, 0.1695, 0.1958, 0.1507, 0.1500], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,581][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.3022, 0.1943, 0.2629, 0.0718, 0.1688], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,582][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0461, 0.6943, 0.2210, 0.0056, 0.0331], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,584][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.2291, 0.2809, 0.4141, 0.0058, 0.0701], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,585][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.1725, 0.3233, 0.1697, 0.1245, 0.2100], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,587][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0268, 0.6700, 0.2217, 0.0046, 0.0769], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,589][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.4717, 0.1619, 0.2713, 0.0130, 0.0820], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,590][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0728, 0.4285, 0.2759, 0.0383, 0.1845], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,592][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.3147, 0.1850, 0.3108, 0.0233, 0.1662], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,594][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0537, 0.3964, 0.2574, 0.1281, 0.1644], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:21,595][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([8.4568e-01, 3.1602e-02, 3.9457e-02, 1.8779e-04, 1.4658e-02, 6.8412e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,596][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0093, 0.2477, 0.0804, 0.2383, 0.1243, 0.3000], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,597][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.7976, 0.0282, 0.0873, 0.0295, 0.0300, 0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,598][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.3037, 0.1459, 0.2555, 0.0482, 0.1190, 0.1278], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,599][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.0878, 0.5876, 0.1500, 0.0026, 0.0078, 0.1642], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,599][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.3708, 0.2638, 0.2703, 0.0023, 0.0174, 0.0754], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,601][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.3546, 0.1576, 0.2497, 0.0476, 0.1456, 0.0449], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,602][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1217, 0.5474, 0.1966, 0.0007, 0.0238, 0.1097], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,603][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.8055, 0.0159, 0.1488, 0.0011, 0.0203, 0.0084], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,605][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.0950, 0.4264, 0.2999, 0.0206, 0.0815, 0.0766], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,607][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.5529, 0.0467, 0.2673, 0.0056, 0.0623, 0.0652], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,608][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.1486, 0.1987, 0.2971, 0.0485, 0.1132, 0.1938], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:21,609][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([6.4925e-01, 7.8658e-02, 8.3149e-02, 9.7197e-04, 3.8139e-02, 1.4983e-01,
        2.5176e-07], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,611][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0112, 0.1959, 0.0564, 0.1778, 0.0969, 0.1647, 0.2972],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,613][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4546, 0.0777, 0.1670, 0.0741, 0.1025, 0.0634, 0.0607],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,614][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.2585, 0.1459, 0.2362, 0.0461, 0.0923, 0.1350, 0.0861],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,616][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0157, 0.6032, 0.0598, 0.0029, 0.0054, 0.2979, 0.0152],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,618][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.2022, 0.2956, 0.3490, 0.0028, 0.0313, 0.1109, 0.0082],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,619][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.5134, 0.1511, 0.1647, 0.0471, 0.0698, 0.0299, 0.0240],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,621][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0115, 0.6056, 0.0900, 0.0012, 0.0261, 0.2561, 0.0094],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,623][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.7199, 0.0322, 0.2003, 0.0017, 0.0325, 0.0123, 0.0011],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,624][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0278, 0.4371, 0.1630, 0.0274, 0.1299, 0.1226, 0.0922],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,626][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.4479, 0.0900, 0.2553, 0.0077, 0.0795, 0.1055, 0.0142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,628][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0680, 0.1829, 0.2191, 0.0385, 0.0892, 0.2277, 0.1745],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:21,629][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([3.6922e-01, 3.6169e-01, 7.3852e-02, 1.7229e-03, 2.5640e-02, 1.5757e-01,
        1.3676e-07, 1.0303e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,630][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.0041, 0.1234, 0.0365, 0.1055, 0.0694, 0.1425, 0.3461, 0.1723],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,632][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.5136, 0.0379, 0.0863, 0.0338, 0.0458, 0.0425, 0.0379, 0.2021],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,634][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.1784, 0.1562, 0.1564, 0.0402, 0.1072, 0.1002, 0.0604, 0.2010],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,635][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([1.1801e-01, 1.0554e-01, 6.3287e-02, 4.4206e-04, 1.7156e-03, 4.4673e-02,
        2.1953e-03, 6.6414e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,636][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([4.5098e-01, 4.4446e-02, 1.3058e-01, 4.0597e-04, 4.0362e-03, 1.7404e-02,
        1.1004e-03, 3.5104e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,638][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.2348, 0.2177, 0.1598, 0.0646, 0.1204, 0.0445, 0.0616, 0.0966],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,639][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([1.0641e-01, 6.3035e-02, 6.6562e-02, 1.1852e-04, 3.0957e-03, 1.9398e-02,
        6.4209e-04, 7.4073e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,639][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([7.3828e-01, 2.4484e-02, 1.2391e-01, 1.4994e-03, 1.2620e-02, 7.7130e-03,
        7.1203e-04, 9.0786e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,640][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.1693, 0.1978, 0.1698, 0.0061, 0.0358, 0.0403, 0.0145, 0.3664],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,641][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.3967, 0.0894, 0.1827, 0.0062, 0.0470, 0.0733, 0.0076, 0.1970],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,642][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.0845, 0.0889, 0.1388, 0.0174, 0.0444, 0.0980, 0.0707, 0.4573],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:21,643][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([6.0716e-01, 8.8047e-02, 1.0681e-01, 6.0971e-04, 7.9502e-02, 1.0990e-01,
        1.8224e-07, 6.9203e-03, 1.0448e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,645][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0054, 0.0954, 0.0355, 0.0917, 0.0606, 0.1139, 0.2398, 0.1418, 0.2159],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,646][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.2558, 0.0615, 0.1439, 0.0642, 0.1030, 0.0704, 0.0511, 0.1973, 0.0528],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,648][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1342, 0.1047, 0.1621, 0.0536, 0.0764, 0.1088, 0.0741, 0.1723, 0.1138],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,649][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([8.0519e-03, 1.0255e-01, 2.6044e-02, 4.7332e-04, 2.7070e-03, 4.3145e-02,
        3.2258e-03, 8.1048e-01, 3.3220e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,651][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0380, 0.0573, 0.0899, 0.0009, 0.0140, 0.0331, 0.0043, 0.7587, 0.0038],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,653][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.3823, 0.1097, 0.1986, 0.0381, 0.1142, 0.0281, 0.0355, 0.0726, 0.0210],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,654][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([9.9785e-04, 4.4924e-02, 7.9547e-03, 1.3988e-04, 3.2549e-03, 1.4325e-02,
        1.7072e-03, 9.2599e-01, 7.0230e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,655][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.5957, 0.0171, 0.1634, 0.0013, 0.0234, 0.0077, 0.0007, 0.1892, 0.0014],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,657][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0101, 0.1100, 0.0571, 0.0158, 0.0537, 0.0379, 0.0344, 0.6595, 0.0217],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,659][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.2619, 0.0742, 0.2053, 0.0096, 0.0883, 0.0811, 0.0125, 0.2527, 0.0144],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,660][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0319, 0.0604, 0.1227, 0.0144, 0.0664, 0.0887, 0.0672, 0.4988, 0.0494],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:21,661][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([2.5975e-01, 4.2470e-01, 8.0323e-02, 2.1672e-03, 6.8230e-02, 1.0182e-01,
        1.4262e-07, 1.1672e-02, 1.3318e-03, 4.9999e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,663][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0034, 0.0888, 0.0249, 0.0791, 0.0493, 0.0895, 0.2074, 0.1088, 0.1934,
        0.1555], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,665][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.3761, 0.0589, 0.1046, 0.0573, 0.0565, 0.0425, 0.0340, 0.1761, 0.0349,
        0.0590], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,667][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.1064, 0.0825, 0.1092, 0.0292, 0.0688, 0.0899, 0.0754, 0.1981, 0.1685,
        0.0720], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,668][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([6.1902e-03, 1.0571e-01, 1.9697e-02, 3.6330e-04, 1.7758e-03, 4.7468e-02,
        4.9932e-03, 8.0858e-01, 3.0999e-03, 2.1238e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,669][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([5.3762e-02, 5.8702e-02, 6.4541e-02, 4.3978e-04, 5.2204e-03, 2.3220e-02,
        2.2531e-03, 7.8501e-01, 3.9117e-03, 2.9435e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,671][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.1476, 0.1661, 0.1315, 0.0543, 0.1155, 0.0597, 0.0580, 0.1374, 0.0395,
        0.0903], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,672][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([1.5648e-03, 2.2709e-02, 8.2885e-03, 5.2954e-05, 1.9320e-03, 1.5147e-02,
        1.0690e-03, 9.4522e-01, 6.0297e-04, 3.4144e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,674][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.5579, 0.0235, 0.1323, 0.0012, 0.0250, 0.0114, 0.0012, 0.2418, 0.0029,
        0.0028], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,675][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.0302, 0.1808, 0.0962, 0.0087, 0.0285, 0.0497, 0.0246, 0.5505, 0.0109,
        0.0200], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,677][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.2772, 0.0810, 0.1553, 0.0063, 0.0542, 0.0803, 0.0103, 0.3078, 0.0118,
        0.0159], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,679][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.0347, 0.0433, 0.0685, 0.0097, 0.0232, 0.0864, 0.0595, 0.5483, 0.0438,
        0.0827], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:21,680][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([6.0909e-01, 5.0580e-02, 1.1252e-01, 6.2099e-04, 8.4536e-02, 7.2820e-02,
        3.6332e-07, 9.8738e-03, 1.0002e-03, 3.0974e-02, 2.7993e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,681][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0029, 0.0639, 0.0224, 0.0648, 0.0326, 0.0727, 0.2175, 0.0952, 0.1691,
        0.1355, 0.1233], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,682][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.2170, 0.0457, 0.1179, 0.0550, 0.0794, 0.0495, 0.0455, 0.1933, 0.0433,
        0.0644, 0.0890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,683][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.1348, 0.0858, 0.1140, 0.0363, 0.0572, 0.0879, 0.0784, 0.1045, 0.1480,
        0.0547, 0.0985], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,683][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([3.6513e-02, 8.2597e-02, 4.0510e-02, 5.8237e-04, 2.4642e-03, 5.9552e-02,
        3.5100e-03, 6.3193e-01, 4.9210e-03, 5.7149e-03, 1.3170e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,685][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.2290, 0.0487, 0.1336, 0.0012, 0.0094, 0.0175, 0.0022, 0.4509, 0.0043,
        0.0033, 0.0998], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,686][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.1700, 0.1639, 0.1331, 0.0649, 0.0877, 0.0375, 0.0707, 0.0817, 0.0630,
        0.0724, 0.0550], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,688][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([2.8095e-02, 2.9689e-02, 2.8655e-02, 1.7889e-04, 2.6271e-03, 1.1292e-02,
        1.1700e-03, 8.5068e-01, 9.8987e-04, 5.0743e-03, 4.1552e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,689][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([6.9552e-01, 1.7893e-02, 1.3934e-01, 1.2714e-03, 1.6908e-02, 7.1821e-03,
        6.3109e-04, 1.1050e-01, 2.0807e-03, 2.7963e-03, 5.8760e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,690][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.0107, 0.1217, 0.0490, 0.0192, 0.0640, 0.0506, 0.0390, 0.5537, 0.0237,
        0.0370, 0.0313], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,692][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.3846, 0.0540, 0.1878, 0.0073, 0.0580, 0.0492, 0.0086, 0.1514, 0.0126,
        0.0127, 0.0738], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,693][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0535, 0.0485, 0.1048, 0.0146, 0.0456, 0.0630, 0.0640, 0.2711, 0.0561,
        0.0886, 0.1902], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:21,695][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([5.4837e-01, 7.6949e-02, 1.0430e-01, 1.5166e-03, 7.3817e-02, 1.1081e-01,
        8.7982e-07, 2.1872e-02, 1.5452e-03, 3.6160e-02, 2.4660e-02, 1.3153e-07],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,696][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0041, 0.0624, 0.0208, 0.0683, 0.0324, 0.0648, 0.1384, 0.0778, 0.1406,
        0.1117, 0.0976, 0.1812], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,698][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1983, 0.0454, 0.1181, 0.0513, 0.0772, 0.0531, 0.0441, 0.1676, 0.0505,
        0.0516, 0.0835, 0.0593], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,700][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1876, 0.0668, 0.1371, 0.0266, 0.0469, 0.0589, 0.0393, 0.1137, 0.0866,
        0.0442, 0.0893, 0.1032], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,701][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([4.1876e-02, 5.4161e-02, 3.6563e-02, 2.4713e-04, 1.0945e-03, 2.3372e-02,
        9.8324e-04, 6.5215e-01, 1.7800e-03, 1.8947e-03, 1.2756e-01, 5.8324e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,702][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([2.6985e-01, 2.2288e-02, 1.1432e-01, 2.2564e-04, 3.8956e-03, 7.0667e-03,
        5.4338e-04, 5.0414e-01, 1.2895e-03, 1.1460e-03, 5.6288e-02, 1.8943e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,704][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.3419, 0.1513, 0.1496, 0.0444, 0.0632, 0.0224, 0.0361, 0.0644, 0.0296,
        0.0252, 0.0426, 0.0291], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,706][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([1.1950e-02, 1.5458e-02, 1.7234e-02, 4.5943e-05, 1.1859e-03, 4.6922e-03,
        2.5194e-04, 9.0448e-01, 4.0338e-04, 2.8135e-03, 2.4492e-02, 1.6990e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,707][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([7.8653e-01, 8.0362e-03, 1.0793e-01, 3.4722e-04, 7.0167e-03, 2.9168e-03,
        1.4754e-04, 8.2708e-02, 5.2319e-04, 8.6219e-04, 2.4314e-03, 5.5629e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,708][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0096, 0.1301, 0.0507, 0.0124, 0.0394, 0.0418, 0.0293, 0.5586, 0.0142,
        0.0304, 0.0281, 0.0555], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,710][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.4876, 0.0394, 0.1795, 0.0034, 0.0413, 0.0355, 0.0037, 0.1341, 0.0050,
        0.0057, 0.0435, 0.0212], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,712][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0874, 0.0472, 0.1491, 0.0070, 0.0436, 0.0457, 0.0287, 0.2796, 0.0232,
        0.0391, 0.1370, 0.1124], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:21,713][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([5.4647e-01, 1.7791e-01, 4.2266e-02, 4.5771e-04, 1.8567e-02, 2.9766e-02,
        7.6736e-08, 2.0836e-03, 2.4991e-04, 1.0885e-02, 7.8580e-03, 4.3264e-09,
        1.6349e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,715][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0013, 0.0491, 0.0108, 0.0433, 0.0250, 0.0526, 0.1324, 0.0607, 0.1182,
        0.0900, 0.0827, 0.1586, 0.1752], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,717][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.2533, 0.0420, 0.0825, 0.0479, 0.0543, 0.0466, 0.0424, 0.1604, 0.0349,
        0.0607, 0.0692, 0.0614, 0.0443], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,718][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0418, 0.1354, 0.0462, 0.0205, 0.0521, 0.0525, 0.0642, 0.1027, 0.1280,
        0.0724, 0.0859, 0.1023, 0.0961], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,720][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([6.0117e-02, 4.1826e-02, 3.3475e-02, 1.5623e-04, 8.1094e-04, 2.6592e-02,
        1.1431e-03, 5.3983e-01, 1.7451e-03, 1.1986e-03, 6.2540e-02, 3.9353e-02,
        1.9121e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,721][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([4.7276e-01, 1.6982e-02, 8.7308e-02, 1.6799e-04, 1.4161e-03, 7.1158e-03,
        4.4795e-04, 2.5896e-01, 1.1993e-03, 5.6010e-04, 4.1219e-02, 1.3189e-02,
        9.8676e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,722][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.1487, 0.1301, 0.0969, 0.0468, 0.0779, 0.0499, 0.0674, 0.0905, 0.0437,
        0.0676, 0.0460, 0.0407, 0.0938], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,723][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([4.5050e-02, 1.5788e-02, 2.5928e-02, 6.8763e-05, 1.0344e-03, 1.0320e-02,
        4.3327e-04, 5.8967e-01, 1.1038e-03, 2.0545e-03, 3.6391e-02, 2.1732e-02,
        2.5042e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,724][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.7171, 0.0237, 0.0955, 0.0010, 0.0080, 0.0108, 0.0007, 0.1118, 0.0020,
        0.0014, 0.0099, 0.0036, 0.0143], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,725][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([0.0626, 0.1830, 0.0928, 0.0124, 0.0296, 0.0509, 0.0226, 0.3650, 0.0151,
        0.0175, 0.0262, 0.0424, 0.0799], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,726][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([0.4200, 0.0562, 0.1446, 0.0034, 0.0307, 0.0504, 0.0045, 0.1121, 0.0058,
        0.0077, 0.0495, 0.0184, 0.0967], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,728][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0774, 0.0449, 0.0840, 0.0051, 0.0160, 0.0488, 0.0217, 0.1547, 0.0214,
        0.0294, 0.1280, 0.1067, 0.2621], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:21,729][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([4.4987e-01, 9.8260e-03, 8.1281e-02, 2.1729e-04, 5.3860e-02, 2.8732e-02,
        1.3056e-07, 9.1410e-03, 3.4779e-04, 1.8529e-02, 1.6110e-02, 2.0301e-08,
        3.3199e-01, 9.8317e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,731][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0004, 0.0300, 0.0068, 0.0283, 0.0140, 0.0508, 0.1548, 0.0531, 0.1360,
        0.0883, 0.0769, 0.1782, 0.1569, 0.0256], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,732][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.1509, 0.0425, 0.1253, 0.0542, 0.0803, 0.0567, 0.0422, 0.1351, 0.0490,
        0.0481, 0.0845, 0.0508, 0.0461, 0.0342], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,734][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.2588, 0.0433, 0.1585, 0.0230, 0.0616, 0.0544, 0.0267, 0.0486, 0.0726,
        0.0216, 0.0528, 0.0774, 0.0339, 0.0668], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,735][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.0639, 0.1361, 0.0558, 0.0004, 0.0018, 0.0473, 0.0014, 0.3652, 0.0046,
        0.0015, 0.1212, 0.0409, 0.0996, 0.0602], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,737][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([4.0701e-01, 5.1549e-02, 1.2259e-01, 3.2273e-04, 3.7301e-03, 1.1303e-02,
        9.6211e-04, 2.5900e-01, 2.1389e-03, 1.3542e-03, 4.1971e-02, 1.6416e-02,
        7.2221e-02, 9.4308e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,738][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.1262, 0.1552, 0.0778, 0.0616, 0.0520, 0.0418, 0.0886, 0.0577, 0.0642,
        0.0355, 0.0710, 0.0536, 0.0494, 0.0654], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,740][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([6.2571e-02, 6.2117e-02, 4.8967e-02, 1.4388e-04, 3.8990e-03, 1.4876e-02,
        1.1349e-03, 5.6732e-01, 1.0754e-03, 2.4632e-03, 2.9082e-02, 2.0823e-02,
        1.7012e-01, 1.5410e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,741][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([8.4060e-01, 7.9935e-03, 9.4942e-02, 3.8762e-04, 5.9506e-03, 3.6741e-03,
        2.1506e-04, 3.4495e-02, 6.8902e-04, 5.7131e-04, 2.5898e-03, 6.8925e-04,
        3.7094e-03, 3.4966e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,743][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.0196, 0.1051, 0.0717, 0.0115, 0.0340, 0.0352, 0.0310, 0.4726, 0.0164,
        0.0309, 0.0238, 0.0503, 0.0474, 0.0504], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,745][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([0.4204, 0.0720, 0.1537, 0.0057, 0.0370, 0.0420, 0.0053, 0.0999, 0.0063,
        0.0068, 0.0445, 0.0180, 0.0426, 0.0459], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,746][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.1212, 0.0293, 0.1142, 0.0042, 0.0228, 0.0386, 0.0205, 0.1203, 0.0182,
        0.0206, 0.1133, 0.1164, 0.1847, 0.0758], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:21,748][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([1.5816e-01, 2.1542e-01, 2.9206e-02, 5.8750e-04, 3.4106e-02, 5.2294e-02,
        8.2726e-08, 1.3536e-03, 2.1148e-04, 1.2491e-02, 4.5424e-03, 5.6293e-09,
        3.3209e-01, 2.9907e-04, 1.5925e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,749][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0003, 0.0259, 0.0051, 0.0234, 0.0121, 0.0384, 0.1359, 0.0466, 0.1024,
        0.0720, 0.0762, 0.1922, 0.1912, 0.0303, 0.0481], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,751][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0921, 0.0566, 0.0689, 0.0458, 0.0502, 0.0433, 0.0541, 0.2155, 0.0396,
        0.0728, 0.0653, 0.0769, 0.0465, 0.0360, 0.0363], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,753][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.1182, 0.0614, 0.0980, 0.0193, 0.0656, 0.0491, 0.0203, 0.0755, 0.0309,
        0.0213, 0.0436, 0.0649, 0.1028, 0.0844, 0.1446], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,755][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0715, 0.0333, 0.0561, 0.0004, 0.0032, 0.0278, 0.0015, 0.2926, 0.0027,
        0.0035, 0.0665, 0.0674, 0.2026, 0.0650, 0.1059], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,756][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.1970, 0.0182, 0.0869, 0.0004, 0.0068, 0.0164, 0.0013, 0.3034, 0.0033,
        0.0043, 0.0540, 0.0350, 0.1658, 0.0205, 0.0867], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,758][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.1576, 0.1032, 0.1089, 0.0348, 0.0820, 0.0188, 0.0292, 0.0469, 0.0297,
        0.0320, 0.0368, 0.0357, 0.0708, 0.0538, 0.1597], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,760][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([2.1594e-02, 1.1500e-02, 2.5993e-02, 1.3597e-04, 2.1545e-03, 8.0947e-03,
        5.4159e-04, 3.5339e-01, 1.0094e-03, 4.9798e-03, 3.2452e-02, 2.7301e-02,
        3.9916e-01, 2.8991e-02, 8.2699e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,761][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.5668, 0.0292, 0.1300, 0.0019, 0.0161, 0.0116, 0.0009, 0.1432, 0.0020,
        0.0035, 0.0096, 0.0042, 0.0258, 0.0192, 0.0360], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,763][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.0259, 0.0794, 0.0651, 0.0084, 0.0313, 0.0392, 0.0183, 0.4115, 0.0097,
        0.0242, 0.0222, 0.0474, 0.0965, 0.0480, 0.0728], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,765][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2109, 0.0481, 0.1047, 0.0054, 0.0415, 0.0506, 0.0068, 0.1230, 0.0078,
        0.0093, 0.0477, 0.0279, 0.0967, 0.0547, 0.1650], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,766][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0236, 0.0237, 0.0402, 0.0063, 0.0121, 0.0336, 0.0231, 0.1144, 0.0217,
        0.0253, 0.0911, 0.1192, 0.2735, 0.1057, 0.0865], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:21,766][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([2.5878e-01, 5.8839e-02, 3.1785e-02, 3.8696e-04, 1.9109e-02, 4.1956e-02,
        3.9890e-08, 1.1983e-03, 1.7684e-04, 8.1831e-03, 6.4299e-03, 3.9954e-09,
        6.0778e-02, 1.1767e-04, 8.6760e-02, 4.2550e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,767][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0020, 0.0368, 0.0130, 0.0359, 0.0201, 0.0427, 0.1069, 0.0516, 0.0921,
        0.0791, 0.0690, 0.1310, 0.1314, 0.0394, 0.0587, 0.0904],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,768][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.2647, 0.0407, 0.0619, 0.0445, 0.0355, 0.0329, 0.0298, 0.1467, 0.0318,
        0.0404, 0.0633, 0.0575, 0.0316, 0.0307, 0.0290, 0.0590],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,770][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0590, 0.0768, 0.0695, 0.0238, 0.0508, 0.0431, 0.0306, 0.0909, 0.0464,
        0.0470, 0.0489, 0.0554, 0.0619, 0.0993, 0.1009, 0.0958],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,771][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([7.0743e-02, 1.3727e-02, 2.1362e-02, 1.1751e-04, 5.0142e-04, 7.5347e-03,
        3.9567e-04, 1.1996e-01, 6.7861e-04, 5.7590e-04, 3.9288e-02, 1.6092e-02,
        6.6983e-02, 2.5340e-02, 2.2338e-02, 5.9437e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,773][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([4.4518e-01, 1.0978e-02, 6.4300e-02, 1.6621e-04, 1.5474e-03, 4.3635e-03,
        4.7927e-04, 1.2658e-01, 9.8397e-04, 4.1852e-04, 2.3163e-02, 1.0011e-02,
        4.0478e-02, 7.6139e-03, 2.5842e-02, 2.3790e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,774][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.1689, 0.0604, 0.0897, 0.0229, 0.0560, 0.0141, 0.0297, 0.0500, 0.0204,
        0.0392, 0.0253, 0.0262, 0.0517, 0.0735, 0.2040, 0.0681],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,775][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([2.0085e-01, 6.8708e-03, 3.6325e-02, 3.0661e-05, 9.8525e-04, 2.0824e-03,
        1.0181e-04, 1.6090e-01, 1.5930e-04, 4.0042e-04, 1.0966e-02, 4.6656e-03,
        4.1228e-02, 1.0735e-02, 4.7714e-02, 4.7598e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,777][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([7.1793e-01, 9.5029e-03, 8.8497e-02, 6.4862e-04, 8.7489e-03, 5.3067e-03,
        4.6820e-04, 7.8916e-02, 1.4881e-03, 1.7599e-03, 5.2568e-03, 1.5519e-03,
        9.8212e-03, 8.3529e-03, 1.5408e-02, 4.6347e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,778][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.0154, 0.0972, 0.0579, 0.0096, 0.0423, 0.0283, 0.0138, 0.3228, 0.0089,
        0.0223, 0.0142, 0.0248, 0.0541, 0.0409, 0.0728, 0.1745],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,780][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([0.3492, 0.0256, 0.1162, 0.0026, 0.0252, 0.0239, 0.0026, 0.0607, 0.0040,
        0.0044, 0.0353, 0.0116, 0.0392, 0.0295, 0.1007, 0.1692],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,782][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0475, 0.0265, 0.0560, 0.0060, 0.0176, 0.0196, 0.0162, 0.1111, 0.0137,
        0.0217, 0.0824, 0.0742, 0.1567, 0.0688, 0.0871, 0.1948],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:21,783][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([3.8305e-01, 5.5532e-02, 5.4434e-02, 7.4684e-04, 2.1590e-02, 6.2782e-02,
        2.6302e-07, 4.8440e-03, 3.3406e-04, 9.3511e-03, 7.4075e-03, 4.1556e-08,
        9.2732e-02, 2.2698e-04, 5.1533e-02, 2.5543e-01, 2.3664e-07],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,785][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0015, 0.0303, 0.0101, 0.0346, 0.0161, 0.0334, 0.0837, 0.0421, 0.0861,
        0.0656, 0.0602, 0.1212, 0.1213, 0.0351, 0.0470, 0.0758, 0.1358],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,787][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2206, 0.0389, 0.0917, 0.0381, 0.0546, 0.0318, 0.0333, 0.1336, 0.0303,
        0.0465, 0.0584, 0.0479, 0.0359, 0.0251, 0.0384, 0.0412, 0.0335],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,789][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.1359, 0.0297, 0.0871, 0.0131, 0.0287, 0.0320, 0.0184, 0.0558, 0.0400,
        0.0294, 0.0491, 0.0630, 0.0537, 0.0733, 0.0706, 0.1025, 0.1176],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,790][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([2.4946e-02, 1.3324e-02, 1.0027e-02, 1.0219e-04, 2.0392e-04, 8.0666e-03,
        2.7825e-04, 1.1145e-01, 6.7822e-04, 4.0095e-04, 3.1948e-02, 1.5042e-02,
        4.9545e-02, 2.7500e-02, 1.0283e-02, 5.6892e-01, 1.2729e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,791][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([3.8313e-01, 6.5475e-03, 5.9124e-02, 8.3459e-05, 1.0881e-03, 2.4753e-03,
        1.4655e-04, 1.0744e-01, 4.7223e-04, 2.6212e-04, 1.7630e-02, 4.8776e-03,
        3.1591e-02, 3.8408e-03, 2.3329e-02, 3.1898e-01, 3.8986e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,793][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2759, 0.0808, 0.1047, 0.0425, 0.0361, 0.0179, 0.0268, 0.0645, 0.0238,
        0.0257, 0.0276, 0.0176, 0.0340, 0.0309, 0.0766, 0.0590, 0.0555],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,794][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([3.7838e-02, 4.5106e-03, 1.4234e-02, 2.1242e-05, 3.6545e-04, 1.9884e-03,
        8.6567e-05, 1.6332e-01, 1.4343e-04, 4.8936e-04, 8.0543e-03, 4.2455e-03,
        5.4304e-02, 6.9226e-03, 2.5558e-02, 5.2183e-01, 1.5610e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,796][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([8.6545e-01, 3.2010e-03, 6.4216e-02, 1.6475e-04, 3.6523e-03, 1.3689e-03,
        6.4377e-05, 2.9246e-02, 1.8244e-04, 3.7840e-04, 1.1618e-03, 2.6076e-04,
        2.7242e-03, 1.4347e-03, 6.4798e-03, 1.8187e-02, 1.8256e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,797][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0104, 0.0591, 0.0412, 0.0070, 0.0204, 0.0228, 0.0187, 0.3575, 0.0089,
        0.0212, 0.0186, 0.0441, 0.0596, 0.0501, 0.0503, 0.1171, 0.0930],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,799][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3962, 0.0215, 0.0979, 0.0022, 0.0171, 0.0176, 0.0024, 0.0698, 0.0031,
        0.0035, 0.0296, 0.0118, 0.0289, 0.0229, 0.0800, 0.1385, 0.0571],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,801][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0797, 0.0137, 0.0577, 0.0024, 0.0093, 0.0167, 0.0095, 0.0810, 0.0074,
        0.0142, 0.0615, 0.0451, 0.1267, 0.0449, 0.0668, 0.1798, 0.1835],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:21,802][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([2.2012e-01, 9.5459e-02, 1.7124e-02, 3.9795e-04, 1.5344e-02, 1.5474e-02,
        2.1906e-08, 4.5972e-04, 1.5151e-04, 8.9761e-03, 3.8007e-03, 1.0571e-09,
        1.9126e-01, 1.4282e-04, 1.0440e-01, 1.3346e-01, 1.6791e-08, 1.9342e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,804][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0005, 0.0191, 0.0061, 0.0206, 0.0126, 0.0338, 0.1030, 0.0381, 0.0838,
        0.0631, 0.0611, 0.1285, 0.1241, 0.0226, 0.0400, 0.0649, 0.1375, 0.0406],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,806][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.4566, 0.0355, 0.0716, 0.0270, 0.0258, 0.0260, 0.0211, 0.0972, 0.0140,
        0.0244, 0.0324, 0.0282, 0.0160, 0.0154, 0.0162, 0.0382, 0.0230, 0.0312],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,808][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0397, 0.0621, 0.0409, 0.0141, 0.0264, 0.0294, 0.0244, 0.0683, 0.0499,
        0.0358, 0.0456, 0.0560, 0.0628, 0.0576, 0.0815, 0.0813, 0.1000, 0.1242],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,809][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([3.7612e-02, 2.2000e-02, 1.6272e-02, 2.0096e-04, 5.4284e-04, 8.9941e-03,
        6.3129e-04, 8.4190e-02, 8.4572e-04, 5.0960e-04, 3.6143e-02, 1.5048e-02,
        3.0597e-02, 2.3939e-02, 1.6980e-02, 2.5747e-01, 1.3834e-01, 3.0968e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,810][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([2.7229e-01, 9.7430e-03, 5.1016e-02, 1.7156e-04, 1.4090e-03, 4.8778e-03,
        4.0631e-04, 7.1392e-02, 1.2089e-03, 5.1766e-04, 2.0511e-02, 7.3148e-03,
        2.7119e-02, 6.9622e-03, 2.5100e-02, 1.7594e-01, 4.6351e-02, 2.7767e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,810][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0427, 0.0688, 0.0533, 0.0427, 0.0485, 0.0275, 0.0393, 0.0501, 0.0402,
        0.0636, 0.0295, 0.0395, 0.0538, 0.0585, 0.1530, 0.0488, 0.0661, 0.0741],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,812][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([5.6716e-02, 8.7220e-03, 1.8283e-02, 3.3954e-05, 6.3709e-04, 2.8460e-03,
        1.2285e-04, 6.3448e-02, 2.2556e-04, 5.0249e-04, 9.1819e-03, 3.4727e-03,
        2.9493e-02, 8.0007e-03, 2.4834e-02, 2.3731e-01, 7.7465e-02, 4.5871e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,814][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.6013, 0.0204, 0.0858, 0.0012, 0.0105, 0.0083, 0.0009, 0.0628, 0.0021,
        0.0020, 0.0098, 0.0030, 0.0111, 0.0094, 0.0227, 0.0521, 0.0124, 0.0840],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,815][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.0394, 0.0591, 0.0734, 0.0041, 0.0231, 0.0128, 0.0093, 0.1430, 0.0041,
        0.0105, 0.0089, 0.0170, 0.0266, 0.0149, 0.0430, 0.1014, 0.0319, 0.3777],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,817][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.1454, 0.0378, 0.0623, 0.0040, 0.0195, 0.0319, 0.0052, 0.0687, 0.0076,
        0.0061, 0.0441, 0.0209, 0.0436, 0.0374, 0.0813, 0.1369, 0.0698, 0.1775],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,819][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0203, 0.0099, 0.0207, 0.0031, 0.0069, 0.0170, 0.0125, 0.0566, 0.0149,
        0.0237, 0.0600, 0.0532, 0.1795, 0.0484, 0.0599, 0.1352, 0.1733, 0.1050],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:21,820][circuit_model.py][line:2332][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([4.2900e-01, 5.1022e-02, 6.8977e-02, 2.7075e-04, 1.2031e-02, 1.9195e-02,
        4.8482e-07, 2.5220e-03, 4.4854e-04, 6.3339e-03, 9.0384e-03, 4.8217e-08,
        5.0136e-02, 2.4496e-04, 3.6576e-02, 2.0356e-01, 4.7062e-07, 1.1001e-01,
        6.2673e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,822][circuit_model.py][line:2335][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0015, 0.0259, 0.0092, 0.0242, 0.0126, 0.0298, 0.0720, 0.0338, 0.0670,
        0.0489, 0.0564, 0.1000, 0.0988, 0.0246, 0.0386, 0.0719, 0.1226, 0.0467,
        0.1156], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,824][circuit_model.py][line:2338][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.1785, 0.0308, 0.0909, 0.0348, 0.0431, 0.0318, 0.0312, 0.1194, 0.0268,
        0.0359, 0.0620, 0.0476, 0.0328, 0.0224, 0.0327, 0.0446, 0.0344, 0.0576,
        0.0429], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,826][circuit_model.py][line:2341][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1260, 0.0311, 0.0669, 0.0074, 0.0255, 0.0182, 0.0136, 0.0465, 0.0329,
        0.0166, 0.0354, 0.0451, 0.0327, 0.0470, 0.0659, 0.0650, 0.0830, 0.0879,
        0.1531], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,827][circuit_model.py][line:2344][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([4.8588e-02, 7.7524e-03, 1.1514e-02, 1.4966e-05, 1.1842e-04, 1.9048e-03,
        7.4035e-05, 7.9766e-02, 1.3603e-04, 1.1681e-04, 1.2811e-02, 3.0664e-03,
        9.7703e-03, 1.1749e-02, 5.4872e-03, 2.1928e-01, 5.5733e-02, 3.7609e-01,
        1.5602e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,828][circuit_model.py][line:2347][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([3.0386e-01, 3.7133e-03, 3.9823e-02, 2.4377e-05, 5.4468e-04, 1.0703e-03,
        5.5465e-05, 5.3353e-02, 1.3894e-04, 9.7890e-05, 7.5050e-03, 1.6307e-03,
        1.5701e-02, 2.0714e-03, 1.4345e-02, 1.7158e-01, 1.9645e-02, 3.3934e-01,
        2.5499e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,830][circuit_model.py][line:2350][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.2727, 0.0634, 0.1065, 0.0242, 0.0305, 0.0112, 0.0283, 0.0392, 0.0173,
        0.0122, 0.0255, 0.0175, 0.0238, 0.0212, 0.0783, 0.0613, 0.0695, 0.0825,
        0.0148], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,832][circuit_model.py][line:2353][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([3.0767e-02, 2.2885e-03, 6.6098e-03, 3.1157e-06, 9.7957e-05, 4.4336e-04,
        2.0804e-05, 7.2811e-02, 2.3500e-05, 8.2742e-05, 2.2934e-03, 9.4681e-04,
        1.4684e-02, 2.9076e-03, 7.3291e-03, 1.1186e-01, 4.8859e-02, 6.1791e-01,
        8.0062e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,833][circuit_model.py][line:2356][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([8.1016e-01, 2.9467e-03, 6.7201e-02, 1.0909e-04, 3.7925e-03, 1.2620e-03,
        5.1800e-05, 3.3976e-02, 1.3236e-04, 2.5547e-04, 1.2729e-03, 2.5195e-04,
        2.6453e-03, 1.9531e-03, 7.0857e-03, 2.0752e-02, 2.0757e-03, 4.1268e-02,
        2.8053e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,834][circuit_model.py][line:2359][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.0122, 0.0737, 0.0454, 0.0070, 0.0205, 0.0168, 0.0180, 0.2475, 0.0067,
        0.0147, 0.0138, 0.0281, 0.0377, 0.0423, 0.0372, 0.0933, 0.0585, 0.1903,
        0.0362], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,836][circuit_model.py][line:2362][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([0.3211, 0.0239, 0.0904, 0.0017, 0.0157, 0.0152, 0.0017, 0.0523, 0.0021,
        0.0021, 0.0216, 0.0086, 0.0192, 0.0175, 0.0673, 0.1140, 0.0437, 0.1380,
        0.0438], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,838][circuit_model.py][line:2365][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0861, 0.0138, 0.0528, 0.0011, 0.0081, 0.0099, 0.0043, 0.0727, 0.0036,
        0.0078, 0.0329, 0.0243, 0.0924, 0.0271, 0.0573, 0.1470, 0.1035, 0.1615,
        0.0938], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:21,842][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:21,844][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[5744],
        [  75],
        [1814],
        [  44],
        [  11],
        [  75],
        [  40],
        [   6],
        [  26],
        [   5],
        [   9],
        [  33],
        [  16],
        [   5],
        [  20],
        [  27],
        [  19],
        [   3],
        [  12]], device='cuda:0')
[2024-07-24 10:25:21,846][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[6001],
        [ 458],
        [9374],
        [ 325],
        [ 252],
        [ 964],
        [ 447],
        [ 175],
        [ 345],
        [ 233],
        [ 258],
        [ 750],
        [ 463],
        [ 248],
        [ 281],
        [ 445],
        [ 341],
        [  85],
        [ 183]], device='cuda:0')
[2024-07-24 10:25:21,848][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[8725],
        [3614],
        [2597],
        [2174],
        [1826],
        [1871],
        [1957],
        [1994],
        [2064],
        [2018],
        [2048],
        [2048],
        [2066],
        [2083],
        [2054],
        [2051],
        [2111],
        [2118],
        [2156]], device='cuda:0')
[2024-07-24 10:25:21,850][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[27544],
        [27944],
        [10788],
        [    4],
        [  273],
        [  611],
        [ 1791],
        [  606],
        [ 7680],
        [ 4139],
        [13071],
        [14183],
        [14274],
        [ 2929],
        [17709],
        [11978],
        [ 9323],
        [  568],
        [ 9643]], device='cuda:0')
[2024-07-24 10:25:21,851][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[1862],
        [2280],
        [4680],
        [4059],
        [3603],
        [3073],
        [2803],
        [2610],
        [2534],
        [2517],
        [2205],
        [2103],
        [2022],
        [2137],
        [2133],
        [1951],
        [1931],
        [2099],
        [2040]], device='cuda:0')
[2024-07-24 10:25:21,853][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[40213],
        [36329],
        [36262],
        [38593],
        [38375],
        [37599],
        [36470],
        [33309],
        [32268],
        [30206],
        [29901],
        [29669],
        [30005],
        [31245],
        [32132],
        [31930],
        [31306],
        [31493],
        [31295]], device='cuda:0')
[2024-07-24 10:25:21,855][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[29853],
        [ 9571],
        [ 9971],
        [ 9246],
        [ 9252],
        [ 9379],
        [ 9231],
        [ 9813],
        [ 9838],
        [ 9826],
        [ 9801],
        [ 9712],
        [ 9961],
        [ 9212],
        [ 9119],
        [13172],
        [11707],
        [15450],
        [13266]], device='cuda:0')
[2024-07-24 10:25:21,856][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[ 5917],
        [ 9869],
        [17490],
        [19708],
        [19403],
        [19541],
        [21940],
        [19389],
        [23914],
        [23690],
        [19465],
        [20179],
        [16134],
        [16938],
        [18485],
        [27839],
        [31698],
        [27905],
        [28725]], device='cuda:0')
[2024-07-24 10:25:21,858][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 5400],
        [ 7007],
        [35424],
        [19828],
        [30321],
        [24697],
        [18461],
        [26643],
        [21258],
        [26131],
        [28776],
        [24223],
        [26029],
        [23849],
        [34120],
        [30620],
        [27842],
        [38105],
        [26671]], device='cuda:0')
[2024-07-24 10:25:21,860][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[41028],
        [38189],
        [38125],
        [38151],
        [37235],
        [35893],
        [34907],
        [33576],
        [32711],
        [30168],
        [29407],
        [29347],
        [27778],
        [28086],
        [26881],
        [27537],
        [27899],
        [27767],
        [28299]], device='cuda:0')
[2024-07-24 10:25:21,862][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[21531],
        [31370],
        [48376],
        [49294],
        [47820],
        [45899],
        [40059],
        [23111],
        [23911],
        [32766],
        [25505],
        [15479],
        [30069],
        [21493],
        [35346],
        [21249],
        [13229],
        [27172],
        [26098]], device='cuda:0')
[2024-07-24 10:25:21,863][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[11643],
        [ 4232],
        [ 3542],
        [ 4202],
        [ 3522],
        [ 3495],
        [ 3509],
        [ 3612],
        [ 3446],
        [ 3209],
        [ 3166],
        [ 2999],
        [ 2875],
        [ 2748],
        [ 2588],
        [ 2513],
        [ 2522],
        [ 2489],
        [ 2425]], device='cuda:0')
[2024-07-24 10:25:21,866][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[14469],
        [ 2468],
        [ 9361],
        [ 3522],
        [ 6476],
        [ 4856],
        [ 2368],
        [ 1817],
        [ 1217],
        [ 1648],
        [ 1244],
        [ 1208],
        [ 1815],
        [ 1441],
        [ 1823],
        [ 1878],
        [ 1513],
        [ 2890],
        [ 1327]], device='cuda:0')
[2024-07-24 10:25:21,868][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[13962],
        [24417],
        [ 6175],
        [ 1704],
        [17500],
        [  324],
        [ 3953],
        [ 8930],
        [12630],
        [28875],
        [23847],
        [16769],
        [29853],
        [25025],
        [33154],
        [20502],
        [18520],
        [25367],
        [22362]], device='cuda:0')
[2024-07-24 10:25:21,869][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 7956],
        [ 2789],
        [ 1094],
        [ 8258],
        [  989],
        [ 4606],
        [ 5457],
        [ 7492],
        [10213],
        [ 3746],
        [ 3861],
        [ 3548],
        [ 4013],
        [ 3026],
        [ 1084],
        [ 3020],
        [ 3211],
        [ 1115],
        [ 3030]], device='cuda:0')
[2024-07-24 10:25:21,871][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[20551],
        [16599],
        [  891],
        [ 1771],
        [ 1171],
        [ 2425],
        [ 1288],
        [ 1326],
        [ 1473],
        [ 1222],
        [ 1496],
        [ 1461],
        [  622],
        [ 2588],
        [ 3217],
        [10480],
        [ 5977],
        [ 8952],
        [ 7429]], device='cuda:0')
[2024-07-24 10:25:21,873][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 8648],
        [12308],
        [ 9292],
        [15240],
        [13297],
        [15790],
        [11311],
        [ 8371],
        [10290],
        [10128],
        [10260],
        [ 9149],
        [ 9055],
        [ 8766],
        [ 8876],
        [10077],
        [10671],
        [10568],
        [12234]], device='cuda:0')
[2024-07-24 10:25:21,875][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[4147],
        [ 461],
        [ 211],
        [ 256],
        [ 192],
        [ 474],
        [ 222],
        [ 668],
        [ 385],
        [ 441],
        [ 436],
        [ 389],
        [ 428],
        [ 369],
        [ 430],
        [ 407],
        [ 394],
        [ 431],
        [ 399]], device='cuda:0')
[2024-07-24 10:25:21,877][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[17343],
        [10238],
        [11706],
        [12770],
        [14455],
        [17721],
        [15874],
        [16400],
        [13937],
        [12794],
        [ 9769],
        [ 6900],
        [ 6123],
        [ 6529],
        [ 5955],
        [ 5821],
        [ 4727],
        [ 6448],
        [ 8806]], device='cuda:0')
[2024-07-24 10:25:21,879][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[33564],
        [ 8825],
        [ 7517],
        [ 8272],
        [ 7436],
        [ 7442],
        [ 7276],
        [13244],
        [14172],
        [14065],
        [15125],
        [15538],
        [12725],
        [ 9074],
        [ 8197],
        [ 6571],
        [ 4963],
        [ 7989],
        [11744]], device='cuda:0')
[2024-07-24 10:25:21,881][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[35308],
        [34883],
        [29171],
        [25375],
        [25456],
        [28023],
        [23844],
        [38349],
        [39534],
        [39904],
        [38984],
        [39683],
        [38538],
        [38010],
        [36921],
        [30778],
        [27088],
        [22085],
        [20582]], device='cuda:0')
[2024-07-24 10:25:21,883][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[26707],
        [18496],
        [17608],
        [16990],
        [14452],
        [ 7557],
        [10669],
        [14953],
        [10738],
        [18918],
        [18869],
        [15097],
        [21985],
        [21384],
        [19133],
        [19576],
        [20093],
        [22660],
        [21050]], device='cuda:0')
[2024-07-24 10:25:21,885][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 1914],
        [ 3603],
        [ 4587],
        [ 4131],
        [ 5678],
        [ 6596],
        [ 8976],
        [ 7046],
        [ 6772],
        [ 6898],
        [ 8049],
        [ 7755],
        [12231],
        [10890],
        [18412],
        [23825],
        [27313],
        [25132],
        [22857]], device='cuda:0')
[2024-07-24 10:25:21,887][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 4956],
        [ 5172],
        [25863],
        [13761],
        [25130],
        [ 7696],
        [11717],
        [14607],
        [23386],
        [25182],
        [17035],
        [11557],
        [16380],
        [ 7991],
        [23407],
        [14352],
        [ 6373],
        [13127],
        [ 4876]], device='cuda:0')
[2024-07-24 10:25:21,889][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[27591],
        [18996],
        [17615],
        [17823],
        [17791],
        [17224],
        [16890],
        [13281],
        [11172],
        [11813],
        [11430],
        [11360],
        [12541],
        [11719],
        [11666],
        [13046],
        [12469],
        [ 8820],
        [10532]], device='cuda:0')
[2024-07-24 10:25:21,891][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 9378],
        [13675],
        [14434],
        [13097],
        [ 9356],
        [ 6885],
        [ 9205],
        [ 9568],
        [10430],
        [10581],
        [ 7980],
        [ 6834],
        [ 8112],
        [ 9470],
        [ 9977],
        [10169],
        [ 9672],
        [14772],
        [13195]], device='cuda:0')
[2024-07-24 10:25:21,892][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[12497],
        [  142],
        [  486],
        [  986],
        [ 1389],
        [ 6601],
        [13177],
        [12524],
        [14114],
        [16539],
        [17889],
        [15681],
        [26624],
        [23693],
        [27342],
        [29270],
        [31970],
        [33066],
        [28033]], device='cuda:0')
[2024-07-24 10:25:21,894][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[32809],
        [48628],
        [45977],
        [47060],
        [45403],
        [46983],
        [45683],
        [40995],
        [39370],
        [38559],
        [39678],
        [42680],
        [41264],
        [42628],
        [39270],
        [39723],
        [42392],
        [40595],
        [41462]], device='cuda:0')
[2024-07-24 10:25:21,896][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[41767],
        [37270],
        [48209],
        [44284],
        [49118],
        [45524],
        [46579],
        [46532],
        [46464],
        [46108],
        [47600],
        [48156],
        [45375],
        [46313],
        [47463],
        [44677],
        [45911],
        [45387],
        [45365]], device='cuda:0')
[2024-07-24 10:25:21,898][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093],
        [32093]], device='cuda:0')
[2024-07-24 10:25:21,989][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:21,989][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,990][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,991][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,991][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,992][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,993][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,993][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,994][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,995][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,995][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,996][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,997][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:21,997][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [,] are: tensor([9.9976e-01, 2.4258e-04], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,998][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.1311, 0.8689], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,999][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.9376, 0.0624], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:21,999][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.9939, 0.0061], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,001][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.0081, 0.9919], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,003][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.1147, 0.8853], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,004][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0235, 0.9765], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,006][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.6929, 0.3071], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,007][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.9980, 0.0020], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,009][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.2202, 0.7798], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,010][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.7009, 0.2991], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,012][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.1877, 0.8123], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,014][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.4790, 0.3500, 0.1710], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,015][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.0296, 0.7509, 0.2195], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,017][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.2200, 0.3687, 0.4112], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,018][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.5450, 0.0962, 0.3588], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,020][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.0087, 0.8655, 0.1258], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,022][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.0422, 0.5769, 0.3809], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,023][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([8.8285e-03, 9.9068e-01, 4.9641e-04], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,024][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.0947, 0.6293, 0.2760], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,026][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.6528, 0.0584, 0.2888], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,028][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.1396, 0.7328, 0.1276], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,029][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.3251, 0.3503, 0.3246], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,031][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.0645, 0.7396, 0.1959], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,032][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ and] are: tensor([9.6086e-01, 7.0066e-04, 3.7924e-02, 5.1867e-04], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,033][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0677, 0.5391, 0.1971, 0.1961], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,035][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.3832, 0.1256, 0.4523, 0.0389], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,036][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.4101, 0.0720, 0.5159, 0.0020], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,038][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0089, 0.6977, 0.0583, 0.2351], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,040][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.0395, 0.3560, 0.2634, 0.3411], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,040][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0144, 0.4654, 0.0067, 0.5135], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,041][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0505, 0.6171, 0.2962, 0.0362], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,042][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.7083, 0.0165, 0.2686, 0.0066], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,043][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0795, 0.4575, 0.1975, 0.2655], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,044][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.1048, 0.6173, 0.2768, 0.0011], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,046][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1140, 0.5097, 0.1559, 0.2204], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,047][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.7070, 0.1101, 0.1239, 0.0206, 0.0385], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,049][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0200, 0.4005, 0.1081, 0.2006, 0.2708], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,050][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.2176, 0.1569, 0.4006, 0.0196, 0.2052], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,052][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.4600, 0.0445, 0.4482, 0.0025, 0.0449], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,054][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0045, 0.1821, 0.1065, 0.0495, 0.6573], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,055][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0278, 0.2827, 0.1893, 0.2701, 0.2301], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,057][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0073, 0.3364, 0.0038, 0.3207, 0.3318], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,059][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0505, 0.3780, 0.1866, 0.0320, 0.3529], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,060][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.5364, 0.0314, 0.4044, 0.0049, 0.0229], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,062][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0249, 0.3144, 0.0806, 0.1633, 0.4168], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,064][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.2029, 0.3244, 0.4214, 0.0013, 0.0499], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,065][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0169, 0.3861, 0.0381, 0.1553, 0.4036], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,066][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ had] are: tensor([9.4407e-01, 1.0414e-03, 4.8267e-02, 8.8739e-04, 7.7947e-04, 4.9493e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,068][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0399, 0.3246, 0.1294, 0.1718, 0.2270, 0.1073], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,070][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.2930, 0.0795, 0.3233, 0.0155, 0.2366, 0.0520], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,071][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ had] are: tensor([8.3087e-01, 1.2374e-02, 1.3455e-01, 1.3019e-04, 2.9139e-03, 1.9165e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,072][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.0016, 0.1093, 0.0580, 0.0455, 0.4667, 0.3189], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,074][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.0254, 0.2196, 0.1600, 0.2062, 0.1754, 0.2133], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,076][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0076, 0.2300, 0.0032, 0.2526, 0.2162, 0.2904], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,077][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.1589, 0.1217, 0.2648, 0.0069, 0.3230, 0.1247], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,079][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.6953, 0.0072, 0.1572, 0.0028, 0.0170, 0.1205], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,081][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0315, 0.2338, 0.0626, 0.1253, 0.3353, 0.2116], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,082][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ had] are: tensor([6.9780e-01, 2.5660e-02, 2.0480e-01, 3.8288e-05, 1.6665e-03, 7.0040e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,084][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.0957, 0.2728, 0.0546, 0.1091, 0.2377, 0.2302], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,085][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([9.4082e-01, 1.0139e-03, 4.3275e-02, 8.5223e-04, 9.2421e-04, 1.1359e-02,
        1.7608e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,085][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0640, 0.2659, 0.1620, 0.1059, 0.1901, 0.0896, 0.1226],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,086][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.4448, 0.0463, 0.3719, 0.0099, 0.0781, 0.0371, 0.0119],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,087][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([4.8087e-01, 2.5180e-02, 2.8615e-01, 2.2624e-04, 5.6793e-03, 1.9717e-01,
        4.7179e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,088][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0064, 0.2738, 0.0656, 0.0790, 0.3417, 0.2305, 0.0031],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,089][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0195, 0.1822, 0.1324, 0.1716, 0.1485, 0.1747, 0.1711],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,091][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0043, 0.1950, 0.0015, 0.2035, 0.1468, 0.2246, 0.2244],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,092][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0494, 0.2512, 0.1596, 0.0105, 0.2459, 0.2050, 0.0785],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,094][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.8303, 0.0047, 0.1205, 0.0015, 0.0110, 0.0237, 0.0083],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,096][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0225, 0.1803, 0.0619, 0.0968, 0.2776, 0.1563, 0.2046],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,097][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([2.2954e-01, 6.2152e-02, 2.1640e-01, 1.2438e-04, 3.1560e-03, 4.8251e-01,
        6.1214e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,099][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0152, 0.2675, 0.0440, 0.0786, 0.3660, 0.1377, 0.0912],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,100][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.7353, 0.0167, 0.0994, 0.0196, 0.0028, 0.0728, 0.0094, 0.0440],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,102][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0139, 0.1994, 0.0598, 0.1264, 0.1135, 0.1479, 0.2295, 0.1095],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,104][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.2769, 0.0722, 0.2432, 0.0141, 0.0828, 0.0376, 0.0387, 0.2345],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,105][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([7.6503e-01, 1.5274e-03, 4.3657e-02, 9.8519e-06, 2.5321e-04, 3.7252e-03,
        5.3246e-05, 1.8574e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,106][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.0038, 0.2032, 0.0272, 0.1234, 0.2225, 0.3124, 0.0358, 0.0717],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,108][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0185, 0.1589, 0.1066, 0.1474, 0.1268, 0.1509, 0.1461, 0.1448],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,110][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0035, 0.1750, 0.0008, 0.1786, 0.1360, 0.1842, 0.1894, 0.1326],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,111][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.1507, 0.0482, 0.1162, 0.0012, 0.0228, 0.0407, 0.0061, 0.6143],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,113][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.3223, 0.0183, 0.1082, 0.0063, 0.0156, 0.2115, 0.0981, 0.2196],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,115][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0206, 0.1637, 0.0373, 0.0828, 0.1972, 0.1284, 0.2168, 0.1532],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,116][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([4.1269e-01, 3.6907e-03, 4.4962e-02, 3.4752e-06, 1.2592e-04, 1.4360e-02,
        8.7331e-05, 5.2408e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,118][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.0184, 0.1977, 0.0265, 0.0719, 0.1644, 0.1307, 0.1129, 0.2776],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,119][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.7920, 0.0058, 0.0823, 0.0066, 0.0032, 0.0594, 0.0143, 0.0328, 0.0037],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,121][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0164, 0.2226, 0.0810, 0.0942, 0.1568, 0.0883, 0.1769, 0.1117, 0.0520],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,123][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.4208, 0.0249, 0.3646, 0.0067, 0.0577, 0.0209, 0.0102, 0.0782, 0.0160],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,124][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ of] are: tensor([1.0858e-02, 1.1284e-03, 9.4635e-03, 2.1757e-05, 3.3993e-04, 7.7771e-03,
        7.5562e-04, 9.6925e-01, 4.0445e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,125][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0024, 0.0585, 0.0217, 0.2030, 0.5022, 0.1951, 0.0107, 0.0045, 0.0018],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,127][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0151, 0.1361, 0.0971, 0.1250, 0.1100, 0.1288, 0.1251, 0.1266, 0.1363],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,129][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0022, 0.1454, 0.0009, 0.1598, 0.1185, 0.1713, 0.1694, 0.1179, 0.1146],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,130][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0061, 0.0436, 0.0342, 0.0037, 0.1061, 0.0466, 0.0219, 0.7207, 0.0171],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,131][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3669, 0.0172, 0.1285, 0.0068, 0.0329, 0.0917, 0.0483, 0.1747, 0.1330],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,131][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0175, 0.1291, 0.0355, 0.0625, 0.1836, 0.1191, 0.1608, 0.1553, 0.1365],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,132][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ of] are: tensor([3.7481e-03, 1.4190e-03, 5.1399e-03, 5.2587e-06, 2.2945e-04, 1.2716e-02,
        4.2437e-04, 9.7617e-01, 1.4825e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,133][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0186, 0.1391, 0.0247, 0.0443, 0.2023, 0.0963, 0.0715, 0.2881, 0.1152],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,135][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.7496, 0.0181, 0.0818, 0.0084, 0.0153, 0.0123, 0.0236, 0.0770, 0.0060,
        0.0078], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,136][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0154, 0.1991, 0.0579, 0.0941, 0.0912, 0.0948, 0.1483, 0.1450, 0.0773,
        0.0769], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,138][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.0806, 0.0711, 0.1386, 0.0117, 0.1134, 0.0552, 0.0474, 0.3523, 0.0819,
        0.0478], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,139][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([1.7358e-02, 7.1016e-04, 6.7333e-03, 1.1692e-05, 2.0599e-04, 4.9245e-03,
        3.3516e-04, 9.6840e-01, 6.2385e-04, 6.9525e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,141][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0025, 0.0753, 0.0515, 0.0302, 0.4089, 0.0399, 0.0253, 0.0387, 0.0612,
        0.2665], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,142][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.0125, 0.1190, 0.0849, 0.1118, 0.0984, 0.1150, 0.1130, 0.1139, 0.1208,
        0.1108], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,144][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0025, 0.1207, 0.0006, 0.1216, 0.0896, 0.1389, 0.1441, 0.1032, 0.1018,
        0.1770], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,146][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.0089, 0.0282, 0.0197, 0.0016, 0.0278, 0.0451, 0.0126, 0.8381, 0.0140,
        0.0040], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,147][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.1298, 0.0140, 0.0928, 0.0098, 0.0197, 0.1457, 0.1329, 0.1308, 0.2289,
        0.0955], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,149][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0113, 0.1088, 0.0323, 0.0538, 0.1414, 0.0962, 0.1262, 0.1114, 0.1026,
        0.2160], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,150][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([2.0228e-03, 8.7860e-04, 1.9395e-03, 2.2933e-06, 4.9975e-05, 8.8444e-03,
        2.5798e-04, 9.8546e-01, 2.3165e-04, 3.0942e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,152][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0097, 0.1847, 0.0349, 0.0646, 0.1392, 0.1227, 0.0766, 0.2095, 0.0859,
        0.0724], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,154][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.5831, 0.0120, 0.0787, 0.0180, 0.0145, 0.1121, 0.0274, 0.1010, 0.0062,
        0.0224, 0.0246], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,156][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0142, 0.1377, 0.0672, 0.1211, 0.1025, 0.0958, 0.1497, 0.1040, 0.0660,
        0.0478, 0.0940], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,157][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.2132, 0.0563, 0.2845, 0.0229, 0.1129, 0.0435, 0.0335, 0.1228, 0.0450,
        0.0311, 0.0343], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,158][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ at] are: tensor([3.0152e-01, 1.2321e-03, 4.6424e-02, 5.1844e-05, 6.8863e-04, 6.4154e-03,
        2.5103e-04, 6.2432e-01, 7.4571e-04, 1.2313e-03, 1.7121e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,160][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0007, 0.1594, 0.0075, 0.2093, 0.1590, 0.0452, 0.0081, 0.0114, 0.0064,
        0.3797, 0.0133], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,162][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.0112, 0.1090, 0.0778, 0.1014, 0.0887, 0.1032, 0.1014, 0.1020, 0.1083,
        0.0981, 0.0989], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,164][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0018, 0.1110, 0.0008, 0.1202, 0.0893, 0.1232, 0.1290, 0.0890, 0.0890,
        0.1493, 0.0974], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,165][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0650, 0.0458, 0.0956, 0.0034, 0.1065, 0.0331, 0.0168, 0.5997, 0.0218,
        0.0050, 0.0073], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,167][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.3937, 0.0055, 0.1793, 0.0047, 0.0616, 0.0684, 0.0397, 0.1341, 0.0662,
        0.0310, 0.0157], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,169][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0125, 0.0924, 0.0289, 0.0482, 0.1151, 0.0823, 0.1147, 0.1044, 0.1091,
        0.2225, 0.0698], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,170][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ at] are: tensor([7.8248e-02, 3.2767e-03, 2.1750e-02, 1.3223e-05, 3.3110e-04, 1.0410e-02,
        2.8185e-04, 8.7143e-01, 4.6035e-04, 1.1045e-03, 1.2697e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,171][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0047, 0.0906, 0.0138, 0.0503, 0.1504, 0.0770, 0.0907, 0.2288, 0.1370,
        0.1251, 0.0315], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,173][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.9079e-01, 2.0098e-03, 5.1505e-02, 1.9930e-03, 1.9039e-03, 7.8655e-03,
        1.9383e-03, 3.1639e-02, 8.5567e-04, 3.6776e-03, 4.7927e-03, 1.0295e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,175][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0182, 0.1522, 0.0600, 0.0921, 0.1099, 0.0565, 0.0883, 0.0899, 0.0401,
        0.0529, 0.0688, 0.1709], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,175][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.2139, 0.0621, 0.2409, 0.0235, 0.1141, 0.0454, 0.0237, 0.1300, 0.0292,
        0.0207, 0.0358, 0.0607], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,176][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([2.9991e-01, 4.6785e-04, 3.1921e-02, 7.0342e-06, 2.3574e-04, 2.8569e-03,
        6.4525e-05, 6.4605e-01, 2.4836e-04, 5.1552e-04, 1.5642e-02, 2.0832e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,177][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([4.7022e-03, 4.1791e-02, 1.2542e-02, 2.1502e-02, 3.4892e-02, 1.5263e-01,
        1.1633e-03, 1.2344e-01, 1.4508e-02, 5.0198e-01, 9.0803e-02, 4.8148e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,178][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0150, 0.0967, 0.0700, 0.0907, 0.0813, 0.0929, 0.0914, 0.0907, 0.0949,
        0.0886, 0.0865, 0.1015], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,180][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0017, 0.0992, 0.0006, 0.1088, 0.0754, 0.1185, 0.1173, 0.0836, 0.0798,
        0.1405, 0.0889, 0.0857], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,181][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0566, 0.0396, 0.0664, 0.0032, 0.0618, 0.0409, 0.0117, 0.6627, 0.0211,
        0.0058, 0.0113, 0.0191], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,183][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.4871, 0.0052, 0.1091, 0.0026, 0.0266, 0.0550, 0.0168, 0.1504, 0.0429,
        0.0707, 0.0110, 0.0226], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,184][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0135, 0.0839, 0.0224, 0.0446, 0.1161, 0.0786, 0.1014, 0.1010, 0.0788,
        0.2183, 0.0566, 0.0848], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,186][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([8.6429e-02, 6.5331e-04, 1.5838e-02, 1.1551e-06, 4.8425e-05, 3.2755e-03,
        2.8942e-05, 8.8818e-01, 5.4482e-05, 2.4781e-04, 3.5989e-03, 1.6485e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,187][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0119, 0.1410, 0.0258, 0.0498, 0.1537, 0.0902, 0.0607, 0.1930, 0.1120,
        0.1059, 0.0287, 0.0274], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,189][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.6495, 0.0666, 0.0921, 0.0297, 0.0243, 0.0301, 0.0238, 0.0308, 0.0053,
        0.0107, 0.0113, 0.0102, 0.0157], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,191][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0079, 0.0916, 0.0322, 0.0512, 0.0589, 0.0633, 0.0844, 0.0766, 0.0369,
        0.0408, 0.0384, 0.1907, 0.2272], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,193][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.0821, 0.0635, 0.1366, 0.0096, 0.0808, 0.0513, 0.0367, 0.2516, 0.0365,
        0.0308, 0.0530, 0.1081, 0.0594], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,194][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ house] are: tensor([7.8589e-01, 2.5171e-04, 2.3674e-02, 3.6243e-06, 6.3095e-05, 9.0430e-04,
        1.4441e-05, 1.2884e-01, 8.9877e-05, 1.0798e-04, 4.7405e-03, 9.7675e-04,
        5.4444e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,196][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0024, 0.0537, 0.0138, 0.0087, 0.3070, 0.0441, 0.0071, 0.0042, 0.0053,
        0.1398, 0.0103, 0.0014, 0.4022], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,197][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.0101, 0.0906, 0.0634, 0.0842, 0.0731, 0.0850, 0.0826, 0.0811, 0.0907,
        0.0792, 0.0791, 0.0928, 0.0881], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,199][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0015, 0.0964, 0.0005, 0.0897, 0.0629, 0.1030, 0.1044, 0.0729, 0.0693,
        0.1165, 0.0787, 0.0723, 0.1319], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,201][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.1038, 0.0568, 0.0742, 0.0025, 0.0288, 0.0559, 0.0118, 0.4851, 0.0169,
        0.0051, 0.0122, 0.0163, 0.1307], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,203][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.3408, 0.0318, 0.1124, 0.0052, 0.0126, 0.1073, 0.0419, 0.0318, 0.1293,
        0.0156, 0.0266, 0.0912, 0.0536], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,204][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0058, 0.0835, 0.0240, 0.0383, 0.1035, 0.0587, 0.0767, 0.0732, 0.0742,
        0.1706, 0.0567, 0.0783, 0.1565], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,206][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ house] are: tensor([2.8241e-01, 8.1803e-04, 1.7669e-02, 2.0556e-06, 3.9998e-05, 7.0077e-03,
        5.4874e-05, 3.5419e-01, 1.1474e-04, 1.2220e-04, 5.1601e-03, 3.5739e-03,
        3.2884e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,207][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0071, 0.0985, 0.0223, 0.0310, 0.1259, 0.0878, 0.0523, 0.1837, 0.0860,
        0.0622, 0.0265, 0.0263, 0.1903], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,209][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.7813, 0.0035, 0.0918, 0.0042, 0.0035, 0.0298, 0.0075, 0.0401, 0.0023,
        0.0057, 0.0056, 0.0111, 0.0047, 0.0089], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,211][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0110, 0.1148, 0.0412, 0.0598, 0.0870, 0.0490, 0.0972, 0.0487, 0.0392,
        0.0521, 0.0552, 0.1408, 0.1394, 0.0646], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,212][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.3253, 0.0303, 0.2575, 0.0166, 0.0792, 0.0321, 0.0209, 0.0768, 0.0463,
        0.0146, 0.0194, 0.0435, 0.0151, 0.0224], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,214][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [.] are: tensor([7.5360e-01, 1.1906e-03, 4.7432e-02, 1.0115e-05, 2.2532e-04, 2.7370e-03,
        5.0211e-05, 1.6680e-01, 2.4893e-04, 2.0499e-04, 5.6557e-03, 1.1392e-03,
        1.4074e-02, 6.6363e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,215][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0011, 0.0821, 0.0045, 0.0981, 0.0372, 0.0784, 0.0032, 0.0107, 0.0094,
        0.0719, 0.0532, 0.0027, 0.4165, 0.1309], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,217][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0104, 0.0817, 0.0580, 0.0756, 0.0686, 0.0793, 0.0768, 0.0783, 0.0807,
        0.0755, 0.0729, 0.0847, 0.0813, 0.0763], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,219][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0021, 0.0693, 0.0010, 0.0692, 0.0581, 0.0797, 0.0826, 0.0652, 0.0644,
        0.1044, 0.0686, 0.0637, 0.1145, 0.1572], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,221][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0985, 0.0491, 0.0971, 0.0024, 0.0766, 0.0560, 0.0181, 0.3471, 0.0243,
        0.0032, 0.0148, 0.0293, 0.0730, 0.1107], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,223][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.1398, 0.0119, 0.0742, 0.0049, 0.0148, 0.0867, 0.0444, 0.1728, 0.0858,
        0.0689, 0.0159, 0.0727, 0.1203, 0.0869], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,224][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [.] are: tensor([0.0110, 0.0596, 0.0194, 0.0317, 0.0859, 0.0580, 0.0850, 0.0754, 0.0652,
        0.1592, 0.0484, 0.0716, 0.1708, 0.0590], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,225][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [.] are: tensor([3.2665e-01, 5.8574e-03, 4.1750e-02, 7.5316e-06, 2.2868e-04, 1.2133e-02,
        1.4814e-04, 3.4514e-01, 2.8751e-04, 3.4993e-04, 7.5774e-03, 2.3875e-03,
        2.4526e-01, 1.2229e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,226][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0118, 0.0329, 0.0173, 0.0190, 0.1151, 0.0599, 0.0568, 0.1304, 0.0580,
        0.1367, 0.0150, 0.0269, 0.2626, 0.0577], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,227][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.2666, 0.0889, 0.0541, 0.0172, 0.0189, 0.0688, 0.0908, 0.1547, 0.0097,
        0.0141, 0.0263, 0.0757, 0.0093, 0.0890, 0.0161], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,229][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0061, 0.0834, 0.0250, 0.0491, 0.0652, 0.0498, 0.0663, 0.0437, 0.0321,
        0.0254, 0.0483, 0.1656, 0.1044, 0.0828, 0.1527], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,230][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.2576, 0.0213, 0.2031, 0.0040, 0.0600, 0.0202, 0.0087, 0.0664, 0.0083,
        0.0156, 0.0228, 0.0264, 0.0284, 0.0278, 0.2294], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,232][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([3.2290e-01, 7.3567e-04, 4.4463e-02, 3.6783e-05, 9.3353e-04, 4.2908e-03,
        1.3625e-04, 3.6764e-01, 7.0381e-04, 1.1356e-03, 1.6522e-02, 6.3860e-03,
        1.6856e-01, 4.6215e-02, 1.9343e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,233][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0017, 0.0213, 0.0227, 0.0085, 0.2444, 0.0143, 0.0058, 0.0067, 0.0329,
        0.0048, 0.0698, 0.0030, 0.1157, 0.1413, 0.3071], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,235][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.0077, 0.0765, 0.0478, 0.0712, 0.0634, 0.0746, 0.0728, 0.0720, 0.0776,
        0.0690, 0.0697, 0.0805, 0.0779, 0.0750, 0.0643], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,237][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0014, 0.0626, 0.0005, 0.0599, 0.0501, 0.0649, 0.0702, 0.0576, 0.0523,
        0.0791, 0.0559, 0.0570, 0.0999, 0.1402, 0.1485], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,239][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0484, 0.0176, 0.0391, 0.0013, 0.0140, 0.0171, 0.0045, 0.5584, 0.0093,
        0.0043, 0.0088, 0.0141, 0.1119, 0.0927, 0.0585], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,240][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1332, 0.0108, 0.0756, 0.0012, 0.0027, 0.1136, 0.0636, 0.0634, 0.0492,
        0.0236, 0.0127, 0.2903, 0.1002, 0.0525, 0.0077], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,242][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0047, 0.0688, 0.0137, 0.0332, 0.0690, 0.0483, 0.0759, 0.0617, 0.0652,
        0.1328, 0.0447, 0.0662, 0.1166, 0.0662, 0.1330], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,244][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([3.1748e-02, 5.1595e-04, 7.7029e-03, 2.1064e-06, 1.1876e-04, 2.0192e-03,
        5.4787e-05, 2.4662e-01, 1.8359e-04, 5.7269e-04, 5.7975e-03, 7.5454e-03,
        6.3877e-01, 3.5287e-02, 2.3059e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,245][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0024, 0.0584, 0.0070, 0.0252, 0.0874, 0.0796, 0.0841, 0.1390, 0.0795,
        0.0560, 0.0354, 0.0397, 0.1661, 0.0738, 0.0667], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,247][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([8.3310e-01, 8.5026e-03, 6.2472e-02, 4.4305e-03, 3.3839e-03, 1.5212e-02,
        1.9396e-03, 1.6898e-02, 2.2426e-03, 1.9484e-03, 3.7561e-03, 3.3372e-03,
        6.0576e-04, 2.8658e-03, 2.0879e-03, 3.7218e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,248][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0081, 0.0797, 0.0346, 0.0610, 0.0621, 0.0339, 0.0819, 0.0503, 0.0338,
        0.0393, 0.0354, 0.1470, 0.1032, 0.0625, 0.0802, 0.0871],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,250][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.1785, 0.0261, 0.1368, 0.0066, 0.0843, 0.0229, 0.0216, 0.1141, 0.0261,
        0.0158, 0.0367, 0.0623, 0.0241, 0.0315, 0.1201, 0.0927],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,251][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([8.7887e-01, 1.6455e-04, 1.7294e-02, 4.1932e-06, 7.3398e-05, 3.5234e-04,
        6.0550e-06, 2.4625e-02, 3.8488e-05, 5.4538e-05, 1.9376e-03, 3.7237e-04,
        9.0445e-03, 7.8199e-03, 1.4685e-03, 5.7871e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,253][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0011, 0.0289, 0.0097, 0.0171, 0.1540, 0.0392, 0.0018, 0.0188, 0.0020,
        0.1996, 0.0262, 0.0028, 0.1954, 0.0349, 0.2246, 0.0440],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,255][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.0087, 0.0710, 0.0497, 0.0675, 0.0588, 0.0689, 0.0670, 0.0665, 0.0711,
        0.0633, 0.0633, 0.0744, 0.0699, 0.0685, 0.0604, 0.0710],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,257][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0008, 0.0580, 0.0004, 0.0622, 0.0478, 0.0652, 0.0660, 0.0470, 0.0434,
        0.0799, 0.0478, 0.0480, 0.0869, 0.1412, 0.1544, 0.0512],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,258][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.3326, 0.0220, 0.1177, 0.0013, 0.0350, 0.0121, 0.0033, 0.1599, 0.0073,
        0.0019, 0.0046, 0.0060, 0.0302, 0.0548, 0.0598, 0.1513],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,260][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.3921, 0.0105, 0.1310, 0.0059, 0.0081, 0.0690, 0.0303, 0.0924, 0.0252,
        0.0171, 0.0251, 0.0732, 0.0381, 0.0315, 0.0136, 0.0371],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,262][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([0.0114, 0.0596, 0.0149, 0.0298, 0.0730, 0.0483, 0.0687, 0.0544, 0.0527,
        0.1159, 0.0406, 0.0621, 0.1308, 0.0567, 0.1362, 0.0451],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,263][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([3.0440e-01, 1.2648e-04, 1.1270e-02, 6.0539e-07, 1.5573e-05, 5.1904e-04,
        4.5272e-06, 2.6429e-02, 1.1107e-05, 2.4959e-05, 6.9342e-04, 2.6172e-04,
        2.7788e-02, 2.5489e-03, 1.6282e-03, 6.2428e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,265][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0085, 0.0681, 0.0143, 0.0314, 0.0845, 0.0805, 0.0632, 0.1163, 0.0629,
        0.0591, 0.0198, 0.0328, 0.1616, 0.0581, 0.0743, 0.0647],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,266][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([8.3919e-01, 1.2976e-03, 4.2143e-02, 2.0069e-03, 9.5487e-04, 1.5668e-02,
        2.4500e-03, 1.3594e-02, 9.4487e-04, 1.9129e-03, 7.9960e-03, 2.5654e-03,
        6.2300e-04, 4.9803e-03, 6.1569e-04, 5.4624e-02, 8.4301e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,268][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0124, 0.0753, 0.0391, 0.0474, 0.0476, 0.0292, 0.0446, 0.0623, 0.0252,
        0.0355, 0.0365, 0.0978, 0.1491, 0.0377, 0.0736, 0.0691, 0.1176],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,269][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.3288, 0.0159, 0.2448, 0.0101, 0.0624, 0.0206, 0.0084, 0.0736, 0.0095,
        0.0098, 0.0208, 0.0249, 0.0135, 0.0137, 0.0751, 0.0528, 0.0155],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,270][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([6.7093e-01, 9.6739e-05, 1.7918e-02, 1.9951e-06, 4.7197e-05, 5.6761e-04,
        5.1788e-06, 6.8787e-02, 4.2020e-05, 5.3235e-05, 2.8123e-03, 3.0045e-04,
        1.6829e-02, 7.3483e-03, 1.4631e-03, 2.0117e-01, 1.1622e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,271][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([4.9001e-04, 8.7622e-03, 3.1943e-03, 3.2265e-03, 1.9656e-02, 1.4521e-02,
        8.9673e-05, 2.7696e-02, 2.4513e-03, 1.7224e-01, 6.0508e-03, 5.6389e-04,
        6.5341e-01, 3.1707e-02, 3.5671e-02, 2.0150e-02, 1.2130e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,272][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0095, 0.0655, 0.0482, 0.0616, 0.0554, 0.0630, 0.0624, 0.0619, 0.0653,
        0.0602, 0.0601, 0.0693, 0.0646, 0.0621, 0.0550, 0.0621, 0.0739],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,274][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0010, 0.0560, 0.0004, 0.0611, 0.0425, 0.0609, 0.0628, 0.0461, 0.0442,
        0.0759, 0.0489, 0.0484, 0.0858, 0.1290, 0.1308, 0.0498, 0.0564],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,275][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1945, 0.0354, 0.0988, 0.0015, 0.0263, 0.0123, 0.0037, 0.2363, 0.0079,
        0.0025, 0.0060, 0.0072, 0.0445, 0.0613, 0.0692, 0.1460, 0.0468],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,277][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.6280, 0.0029, 0.0896, 0.0010, 0.0072, 0.0296, 0.0059, 0.0393, 0.0140,
        0.0141, 0.0053, 0.0088, 0.0837, 0.0213, 0.0270, 0.0100, 0.0124],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,279][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0055, 0.0530, 0.0146, 0.0271, 0.0712, 0.0425, 0.0551, 0.0604, 0.0518,
        0.1190, 0.0354, 0.0520, 0.1322, 0.0527, 0.1325, 0.0436, 0.0514],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,280][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([1.0336e-01, 1.0577e-04, 4.7026e-03, 3.2270e-07, 5.5750e-06, 4.2434e-04,
        3.0282e-06, 4.7687e-02, 9.8961e-06, 2.4448e-05, 5.3584e-04, 1.9504e-04,
        5.5741e-02, 2.3945e-03, 1.5258e-03, 7.5635e-01, 2.6936e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,282][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0035, 0.0697, 0.0148, 0.0221, 0.1667, 0.0423, 0.0272, 0.1253, 0.0461,
        0.0658, 0.0173, 0.0154, 0.1246, 0.0504, 0.1630, 0.0288, 0.0169],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,284][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.6009, 0.0544, 0.0533, 0.0076, 0.0080, 0.0160, 0.0249, 0.0386, 0.0031,
        0.0066, 0.0127, 0.0324, 0.0064, 0.0332, 0.0061, 0.0185, 0.0647, 0.0124],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,285][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([0.0016, 0.0371, 0.0109, 0.0410, 0.0328, 0.0396, 0.0791, 0.0212, 0.0396,
        0.0377, 0.0418, 0.1654, 0.0995, 0.0631, 0.0744, 0.0613, 0.1320, 0.0217],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,287][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.2442, 0.0147, 0.1447, 0.0048, 0.0495, 0.0224, 0.0153, 0.0616, 0.0413,
        0.0127, 0.0310, 0.0440, 0.0176, 0.0169, 0.0802, 0.0881, 0.0209, 0.0899],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,288][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([6.5812e-01, 5.1257e-04, 2.2533e-02, 1.1048e-05, 1.2574e-04, 9.0510e-04,
        1.3735e-05, 1.9588e-02, 7.1484e-05, 7.5809e-05, 2.4294e-03, 3.8018e-04,
        6.7565e-03, 7.2024e-03, 2.1919e-03, 5.8874e-02, 7.2225e-03, 2.1299e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,290][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0007, 0.0610, 0.0073, 0.0066, 0.1355, 0.0073, 0.0046, 0.0332, 0.0161,
        0.0641, 0.0139, 0.0011, 0.2388, 0.0685, 0.1447, 0.0071, 0.0032, 0.1864],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,292][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.0081, 0.0616, 0.0410, 0.0579, 0.0499, 0.0604, 0.0586, 0.0564, 0.0615,
        0.0556, 0.0560, 0.0655, 0.0603, 0.0587, 0.0493, 0.0579, 0.0696, 0.0716],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,293][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([4.8198e-04, 6.0792e-02, 1.0795e-04, 5.8416e-02, 3.9209e-02, 6.2417e-02,
        6.2746e-02, 3.8043e-02, 4.1728e-02, 7.2909e-02, 4.2837e-02, 4.1216e-02,
        7.8319e-02, 1.4415e-01, 1.3641e-01, 4.5230e-02, 5.0845e-02, 2.4149e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,295][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.2458, 0.0179, 0.0627, 0.0007, 0.0114, 0.0133, 0.0027, 0.1786, 0.0069,
        0.0018, 0.0050, 0.0064, 0.0292, 0.0430, 0.0410, 0.0981, 0.0391, 0.1965],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,296][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([6.9345e-01, 2.9450e-03, 7.7710e-02, 5.7912e-04, 1.1400e-03, 1.2316e-02,
        7.8245e-03, 5.1246e-02, 4.9987e-03, 8.2423e-03, 7.7029e-03, 3.1771e-02,
        2.5248e-02, 1.3162e-02, 3.1983e-03, 9.4586e-03, 1.9121e-02, 2.9884e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,298][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([0.0052, 0.0518, 0.0135, 0.0263, 0.0603, 0.0421, 0.0595, 0.0546, 0.0466,
        0.0981, 0.0423, 0.0513, 0.0987, 0.0546, 0.1051, 0.0397, 0.0549, 0.0955],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,300][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([1.2490e-01, 7.8546e-04, 8.0347e-03, 2.7082e-06, 2.1625e-05, 1.1498e-03,
        2.0068e-05, 2.8875e-02, 4.9250e-05, 4.2867e-05, 1.4424e-03, 7.1114e-04,
        3.0979e-02, 5.4084e-03, 2.3767e-03, 2.5043e-01, 4.0428e-02, 5.0435e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,301][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0276, 0.0722, 0.0195, 0.0244, 0.0602, 0.0644, 0.0419, 0.1112, 0.0444,
        0.0527, 0.0158, 0.0238, 0.1076, 0.0583, 0.0479, 0.0565, 0.0292, 0.1425],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,303][circuit_model.py][line:2294][INFO] ##9-th layer ##Weight##: The head1 weight for token [ to] are: tensor([7.8847e-01, 3.4946e-03, 5.4552e-02, 2.9416e-03, 1.9412e-03, 3.4743e-02,
        6.0252e-03, 1.7783e-02, 7.9294e-04, 2.7602e-03, 7.3295e-03, 1.2524e-02,
        5.5654e-04, 4.1538e-03, 1.2218e-03, 2.4021e-02, 1.8521e-02, 9.7164e-04,
        1.7198e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,304][circuit_model.py][line:2297][INFO] ##9-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0166, 0.0800, 0.0433, 0.0367, 0.0367, 0.0309, 0.0447, 0.0356, 0.0189,
        0.0318, 0.0426, 0.0850, 0.1054, 0.0434, 0.0577, 0.0708, 0.1142, 0.0472,
        0.0584], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,306][circuit_model.py][line:2300][INFO] ##9-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.6847, 0.0032, 0.1713, 0.0010, 0.0114, 0.0035, 0.0013, 0.0156, 0.0022,
        0.0013, 0.0053, 0.0054, 0.0028, 0.0035, 0.0137, 0.0184, 0.0035, 0.0413,
        0.0107], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,308][circuit_model.py][line:2303][INFO] ##9-th layer ##Weight##: The head4 weight for token [ to] are: tensor([5.3111e-01, 5.0394e-05, 8.6998e-03, 3.6609e-07, 1.3091e-05, 1.7230e-04,
        1.3041e-06, 2.0930e-02, 7.6633e-06, 1.0905e-05, 7.1615e-04, 8.2028e-05,
        4.8067e-03, 2.4663e-03, 5.2111e-04, 6.0765e-02, 4.0555e-03, 3.4538e-01,
        2.0207e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,309][circuit_model.py][line:2306][INFO] ##9-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0015, 0.0524, 0.0056, 0.0292, 0.0731, 0.0361, 0.0022, 0.0106, 0.0132,
        0.0572, 0.0228, 0.0024, 0.4656, 0.0656, 0.0872, 0.0193, 0.0028, 0.0449,
        0.0082], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,311][circuit_model.py][line:2309][INFO] ##9-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0093, 0.0575, 0.0409, 0.0530, 0.0482, 0.0555, 0.0539, 0.0539, 0.0559,
        0.0526, 0.0517, 0.0599, 0.0561, 0.0541, 0.0479, 0.0538, 0.0639, 0.0652,
        0.0668], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,313][circuit_model.py][line:2312][INFO] ##9-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0007, 0.0497, 0.0004, 0.0537, 0.0389, 0.0575, 0.0570, 0.0432, 0.0399,
        0.0680, 0.0446, 0.0438, 0.0769, 0.1179, 0.1225, 0.0461, 0.0523, 0.0314,
        0.0554], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,314][circuit_model.py][line:2315][INFO] ##9-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.3003, 0.0142, 0.0896, 0.0003, 0.0104, 0.0062, 0.0013, 0.0793, 0.0026,
        0.0007, 0.0022, 0.0039, 0.0192, 0.0378, 0.0340, 0.0944, 0.0262, 0.1699,
        0.1075], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,315][circuit_model.py][line:2318][INFO] ##9-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.2223, 0.0114, 0.0786, 0.0051, 0.0070, 0.0509, 0.0216, 0.0545, 0.0412,
        0.0126, 0.0197, 0.0256, 0.0508, 0.0455, 0.0205, 0.0323, 0.0499, 0.1251,
        0.1255], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,316][circuit_model.py][line:2321][INFO] ##9-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0044, 0.0436, 0.0131, 0.0240, 0.0575, 0.0411, 0.0552, 0.0496, 0.0447,
        0.0998, 0.0311, 0.0499, 0.1009, 0.0412, 0.0998, 0.0388, 0.0525, 0.0971,
        0.0558], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,317][circuit_model.py][line:2324][INFO] ##9-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.5088e-02, 2.5971e-05, 2.3828e-03, 2.0916e-08, 1.0671e-06, 6.7712e-05,
        3.3112e-07, 7.6427e-03, 7.8589e-07, 1.8827e-06, 8.6805e-05, 2.2214e-05,
        6.1103e-03, 5.1346e-04, 3.2647e-04, 1.4018e-01, 4.3977e-03, 7.3252e-01,
        1.0640e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,319][circuit_model.py][line:2327][INFO] ##9-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0105, 0.0531, 0.0149, 0.0222, 0.0710, 0.0396, 0.0426, 0.1220, 0.0683,
        0.0487, 0.0141, 0.0243, 0.1080, 0.0501, 0.0564, 0.0424, 0.0306, 0.1532,
        0.0282], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,427][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:22,428][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,429][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,430][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,430][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,431][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,432][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,433][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,433][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,434][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,435][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,435][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,436][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,438][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.9948, 0.0052], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,439][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.1311, 0.8689], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,441][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.9612, 0.0388], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,442][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.9939, 0.0061], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,443][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.2982, 0.7018], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,444][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.7340, 0.2660], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,446][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.0100, 0.9900], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,447][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.6795, 0.3205], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,449][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.9564, 0.0436], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,451][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.0032, 0.9968], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,452][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.6077, 0.3923], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,454][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.0031, 0.9969], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,455][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.4225, 0.2183, 0.3593], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,457][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.0296, 0.7509, 0.2195], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,459][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.3089, 0.3261, 0.3650], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,459][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.5450, 0.0962, 0.3588], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,460][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.0331, 0.8436, 0.1233], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,461][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.2417, 0.6023, 0.1560], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,462][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.0014, 0.9636, 0.0350], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,462][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.1585, 0.5125, 0.3290], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,464][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.2087, 0.4766, 0.3147], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,465][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([2.9038e-04, 9.7004e-01, 2.9667e-02], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,466][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.3122, 0.3593, 0.3285], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,468][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.0093, 0.9341, 0.0566], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,469][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.8401, 0.0149, 0.1427, 0.0023], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,471][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0677, 0.5391, 0.1971, 0.1961], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,472][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.6207, 0.1089, 0.2531, 0.0173], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,474][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.4101, 0.0720, 0.5159, 0.0020], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,476][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.3418, 0.2408, 0.2763, 0.1412], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,477][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.5940, 0.2546, 0.1384, 0.0131], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,479][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0195, 0.4727, 0.1328, 0.3749], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,481][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0981, 0.5437, 0.3132, 0.0450], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,482][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.5348, 0.1913, 0.2443, 0.0296], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,484][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.0009, 0.1149, 0.0360, 0.8482], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,485][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.1266, 0.5848, 0.2854, 0.0031], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,487][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0079, 0.7996, 0.0310, 0.1614], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,489][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.6752, 0.0365, 0.2353, 0.0049, 0.0481], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,490][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0200, 0.4005, 0.1081, 0.2006, 0.2708], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,492][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.4058, 0.1626, 0.2897, 0.0171, 0.1248], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,494][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.4600, 0.0445, 0.4482, 0.0025, 0.0449], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,495][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0558, 0.3165, 0.1535, 0.1884, 0.2859], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,497][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.3555, 0.4158, 0.1032, 0.0219, 0.1036], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,498][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0036, 0.3108, 0.0533, 0.2407, 0.3916], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,500][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0916, 0.3819, 0.2264, 0.0520, 0.2481], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,502][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.2007, 0.4050, 0.1790, 0.0565, 0.1588], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,503][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([2.5083e-04, 1.0806e-01, 1.8237e-02, 5.9716e-01, 2.7630e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,504][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2102, 0.3284, 0.3879, 0.0036, 0.0699], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,505][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0046, 0.7239, 0.0248, 0.1145, 0.1321], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,506][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.8642, 0.0066, 0.1099, 0.0012, 0.0110, 0.0072], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,506][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0399, 0.3246, 0.1294, 0.1718, 0.2270, 0.1073], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,508][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.6726, 0.0468, 0.1857, 0.0054, 0.0766, 0.0129], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,509][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([8.3087e-01, 1.2374e-02, 1.3455e-01, 1.3019e-04, 2.9139e-03, 1.9165e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,511][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.1213, 0.2430, 0.1992, 0.1059, 0.1895, 0.1412], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,512][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.7065, 0.1153, 0.0889, 0.0033, 0.0246, 0.0614], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,514][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.0050, 0.1592, 0.0566, 0.1806, 0.2185, 0.3801], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,515][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.2064, 0.1540, 0.2734, 0.0117, 0.2111, 0.1433], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,517][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.3796, 0.2456, 0.1820, 0.0305, 0.0640, 0.0984], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,519][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.0006, 0.1095, 0.0223, 0.3731, 0.1875, 0.3070], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,520][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([6.1502e-01, 5.3118e-02, 2.2935e-01, 2.2218e-04, 5.5522e-03, 9.6737e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,521][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.0204, 0.3507, 0.0556, 0.0966, 0.1864, 0.2904], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:22,523][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.8839, 0.0096, 0.0892, 0.0011, 0.0071, 0.0081, 0.0011],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,525][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0640, 0.2659, 0.1620, 0.1059, 0.1901, 0.0896, 0.1226],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,526][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.6619, 0.0573, 0.2050, 0.0056, 0.0502, 0.0173, 0.0029],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,527][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([4.8087e-01, 2.5180e-02, 2.8615e-01, 2.2624e-04, 5.6793e-03, 1.9717e-01,
        4.7179e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,529][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2628, 0.1761, 0.2300, 0.0830, 0.1393, 0.0777, 0.0311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,531][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.6382, 0.1347, 0.1101, 0.0047, 0.0387, 0.0698, 0.0037],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,532][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0144, 0.2274, 0.0763, 0.1027, 0.1619, 0.2861, 0.1311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,534][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1126, 0.2550, 0.2218, 0.0111, 0.1676, 0.1872, 0.0447],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,536][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.3612, 0.2766, 0.1973, 0.0175, 0.0281, 0.0990, 0.0204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,537][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0007, 0.0356, 0.0215, 0.1901, 0.0870, 0.2388, 0.4263],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,539][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([2.3025e-01, 9.4403e-02, 2.3337e-01, 4.0546e-04, 7.5618e-03, 4.2407e-01,
        9.9391e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,540][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0131, 0.3386, 0.0317, 0.0675, 0.0764, 0.1885, 0.2841],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:22,542][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.7678, 0.0255, 0.1245, 0.0039, 0.0223, 0.0221, 0.0030, 0.0308],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,544][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.0139, 0.1994, 0.0598, 0.1264, 0.1135, 0.1479, 0.2295, 0.1095],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,545][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.5275, 0.0755, 0.2038, 0.0123, 0.0655, 0.0264, 0.0110, 0.0779],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,547][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([7.6503e-01, 1.5274e-03, 4.3657e-02, 9.8519e-06, 2.5321e-04, 3.7252e-03,
        5.3246e-05, 1.8574e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,548][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.0423, 0.1789, 0.0891, 0.1077, 0.1997, 0.1764, 0.0822, 0.1239],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,549][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.4788, 0.1571, 0.0898, 0.0071, 0.0445, 0.1049, 0.0066, 0.1111],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,550][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.0006, 0.1382, 0.0110, 0.1381, 0.1412, 0.2658, 0.2653, 0.0398],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,551][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.1371, 0.0882, 0.1358, 0.0052, 0.0421, 0.0774, 0.0104, 0.5038],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,551][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.2604, 0.2417, 0.1550, 0.0350, 0.0587, 0.1162, 0.0316, 0.1013],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,552][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([4.7265e-05, 1.9233e-02, 2.7441e-03, 1.1294e-01, 5.4827e-02, 1.7312e-01,
        6.0675e-01, 3.0342e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,554][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([3.7488e-01, 1.2157e-02, 7.0095e-02, 4.2650e-05, 8.7143e-04, 2.7764e-02,
        3.9459e-04, 5.1380e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,555][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.0012, 0.1508, 0.0063, 0.0649, 0.0557, 0.2338, 0.3051, 0.1822],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:22,557][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.7906, 0.0112, 0.1226, 0.0018, 0.0145, 0.0163, 0.0023, 0.0357, 0.0050],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,558][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0164, 0.2226, 0.0810, 0.0942, 0.1568, 0.0883, 0.1769, 0.1117, 0.0520],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,560][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.6046, 0.0394, 0.2319, 0.0054, 0.0509, 0.0149, 0.0033, 0.0436, 0.0059],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,561][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([1.0858e-02, 1.1284e-03, 9.4635e-03, 2.1757e-05, 3.3993e-04, 7.7771e-03,
        7.5562e-04, 9.6925e-01, 4.0445e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,563][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0895, 0.1319, 0.1335, 0.1308, 0.2181, 0.1406, 0.0444, 0.0486, 0.0625],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,565][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.4610, 0.1489, 0.0989, 0.0056, 0.0457, 0.1126, 0.0056, 0.1069, 0.0147],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,566][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0038, 0.1342, 0.0385, 0.0919, 0.1927, 0.2598, 0.1788, 0.0452, 0.0552],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,568][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0208, 0.0806, 0.0699, 0.0071, 0.1016, 0.0768, 0.0251, 0.5989, 0.0192],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,570][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.3196, 0.1565, 0.2257, 0.0193, 0.0544, 0.0900, 0.0182, 0.0834, 0.0329],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,571][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([7.3567e-05, 9.2370e-03, 4.0339e-03, 6.0911e-02, 3.1707e-02, 8.2181e-02,
        2.8803e-01, 1.7356e-02, 5.0647e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,572][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([1.0461e-02, 5.2620e-03, 1.3956e-02, 3.9838e-05, 1.0908e-03, 2.6300e-02,
        1.4786e-03, 9.4079e-01, 6.1795e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,574][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0020, 0.1940, 0.0113, 0.0530, 0.0556, 0.1148, 0.2333, 0.1147, 0.2213],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:22,575][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.8319, 0.0127, 0.0862, 0.0013, 0.0071, 0.0190, 0.0020, 0.0304, 0.0028,
        0.0065], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,577][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0154, 0.1991, 0.0579, 0.0941, 0.0912, 0.0948, 0.1483, 0.1450, 0.0773,
        0.0769], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,579][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.5574, 0.0589, 0.1806, 0.0050, 0.0568, 0.0222, 0.0057, 0.0779, 0.0113,
        0.0242], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,580][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([1.7358e-02, 7.1016e-04, 6.7333e-03, 1.1692e-05, 2.0599e-04, 4.9245e-03,
        3.3516e-04, 9.6840e-01, 6.2385e-04, 6.9525e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,582][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.0586, 0.1346, 0.1176, 0.0652, 0.1893, 0.0955, 0.0392, 0.0863, 0.1046,
        0.1092], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,583][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.4348, 0.1580, 0.0971, 0.0067, 0.0445, 0.0994, 0.0055, 0.1239, 0.0095,
        0.0206], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,585][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0029, 0.1008, 0.0263, 0.0640, 0.1244, 0.2021, 0.2147, 0.0573, 0.1257,
        0.0816], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,587][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.0280, 0.0598, 0.0467, 0.0036, 0.0383, 0.0787, 0.0150, 0.7048, 0.0165,
        0.0085], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,588][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.2827, 0.2669, 0.1369, 0.0156, 0.0337, 0.0748, 0.0223, 0.1103, 0.0423,
        0.0146], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,589][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([5.3896e-05, 3.4933e-03, 1.6937e-03, 1.9925e-02, 1.1485e-02, 3.3094e-02,
        1.3253e-01, 1.2321e-02, 2.6751e-01, 5.1789e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,591][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([6.4880e-03, 3.6606e-03, 6.6492e-03, 2.1363e-05, 3.4559e-04, 2.0136e-02,
        1.0077e-03, 9.5958e-01, 9.2821e-04, 1.1827e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,592][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.0047, 0.2219, 0.0217, 0.0292, 0.1012, 0.0841, 0.1240, 0.2148, 0.1452,
        0.0531], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:22,594][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.7979, 0.0141, 0.1001, 0.0024, 0.0086, 0.0145, 0.0026, 0.0263, 0.0042,
        0.0072, 0.0222], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,595][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0142, 0.1377, 0.0672, 0.1211, 0.1025, 0.0958, 0.1497, 0.1040, 0.0660,
        0.0478, 0.0940], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,596][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.5489, 0.0546, 0.2103, 0.0101, 0.0584, 0.0198, 0.0068, 0.0404, 0.0104,
        0.0202, 0.0199], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,597][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([3.0152e-01, 1.2321e-03, 4.6424e-02, 5.1844e-05, 6.8863e-04, 6.4154e-03,
        2.5103e-04, 6.2432e-01, 7.4571e-04, 1.2313e-03, 1.7121e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,598][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0402, 0.1261, 0.0678, 0.1163, 0.1699, 0.0886, 0.0650, 0.0432, 0.0819,
        0.1102, 0.0907], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,600][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.4046, 0.1623, 0.0955, 0.0072, 0.0517, 0.0980, 0.0070, 0.0998, 0.0123,
        0.0214, 0.0400], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,601][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0016, 0.0977, 0.0212, 0.0961, 0.1496, 0.1961, 0.2065, 0.0343, 0.0748,
        0.0493, 0.0727], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,603][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0973, 0.0775, 0.1220, 0.0075, 0.0937, 0.0559, 0.0174, 0.4794, 0.0216,
        0.0105, 0.0172], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,604][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.1203, 0.2338, 0.1094, 0.0369, 0.0504, 0.1483, 0.0445, 0.0744, 0.0700,
        0.0204, 0.0916], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,605][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([8.0029e-06, 5.7404e-03, 7.1612e-04, 5.9218e-02, 1.2160e-02, 4.1923e-02,
        2.4223e-01, 5.4259e-03, 3.0515e-01, 2.9992e-01, 2.7508e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,606][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([9.6296e-02, 1.1191e-02, 3.8768e-02, 1.0761e-04, 1.6813e-03, 2.2899e-02,
        1.0983e-03, 7.9344e-01, 1.7252e-03, 3.4874e-03, 2.9306e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,608][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0015, 0.1212, 0.0087, 0.0428, 0.0345, 0.1217, 0.2139, 0.1290, 0.1917,
        0.0445, 0.0904], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:22,610][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.8778, 0.0102, 0.0699, 0.0009, 0.0084, 0.0080, 0.0009, 0.0115, 0.0010,
        0.0027, 0.0073, 0.0014], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,612][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0182, 0.1522, 0.0600, 0.0921, 0.1099, 0.0565, 0.0883, 0.0899, 0.0401,
        0.0529, 0.0688, 0.1709], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,613][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.6276, 0.0513, 0.1723, 0.0077, 0.0523, 0.0132, 0.0029, 0.0301, 0.0042,
        0.0126, 0.0154, 0.0104], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,615][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([2.9991e-01, 4.6785e-04, 3.1921e-02, 7.0342e-06, 2.3574e-04, 2.8569e-03,
        6.4525e-05, 6.4605e-01, 2.4836e-04, 5.1552e-04, 1.5642e-02, 2.0832e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,616][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.3156, 0.0938, 0.1729, 0.0352, 0.0918, 0.0493, 0.0143, 0.0770, 0.0347,
        0.0397, 0.0647, 0.0110], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,618][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.7276, 0.0589, 0.0838, 0.0021, 0.0222, 0.0332, 0.0014, 0.0469, 0.0029,
        0.0063, 0.0110, 0.0038], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,620][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0041, 0.1338, 0.0287, 0.0994, 0.1175, 0.2213, 0.1137, 0.0381, 0.0494,
        0.0357, 0.0614, 0.0969], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,622][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1410, 0.0563, 0.1141, 0.0040, 0.0568, 0.0479, 0.0070, 0.5272, 0.0131,
        0.0063, 0.0155, 0.0109], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,623][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2174, 0.1698, 0.1350, 0.0276, 0.0320, 0.1398, 0.0192, 0.0677, 0.0424,
        0.0096, 0.1100, 0.0296], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,624][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([3.2275e-05, 4.3643e-03, 1.3832e-03, 4.2084e-02, 1.1898e-02, 3.4411e-02,
        1.1052e-01, 7.3559e-03, 1.8281e-01, 4.2405e-01, 2.7438e-02, 1.5366e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,626][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.1886e-01, 2.6637e-03, 2.9743e-02, 1.0377e-05, 2.9711e-04, 7.5280e-03,
        1.1770e-04, 8.2640e-01, 2.1746e-04, 7.9233e-04, 9.3056e-03, 4.0590e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,627][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0031, 0.1557, 0.0129, 0.0533, 0.0353, 0.0958, 0.1507, 0.1016, 0.1164,
        0.0362, 0.1230, 0.1160], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:22,629][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.8428, 0.0188, 0.0740, 0.0011, 0.0063, 0.0130, 0.0017, 0.0153, 0.0017,
        0.0027, 0.0114, 0.0018, 0.0094], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,631][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0079, 0.0916, 0.0322, 0.0512, 0.0589, 0.0633, 0.0844, 0.0766, 0.0369,
        0.0408, 0.0384, 0.1907, 0.2272], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,633][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.5995, 0.0485, 0.1619, 0.0034, 0.0358, 0.0179, 0.0037, 0.0505, 0.0050,
        0.0118, 0.0250, 0.0166, 0.0206], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,634][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([7.8589e-01, 2.5171e-04, 2.3674e-02, 3.6243e-06, 6.3095e-05, 9.0430e-04,
        1.4441e-05, 1.2884e-01, 8.9877e-05, 1.0798e-04, 4.7405e-03, 9.7675e-04,
        5.4444e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,635][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0711, 0.1461, 0.0822, 0.0537, 0.0949, 0.1036, 0.0369, 0.0721, 0.0754,
        0.0645, 0.0618, 0.0346, 0.1031], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,637][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.5711, 0.1054, 0.0820, 0.0035, 0.0337, 0.0681, 0.0029, 0.0503, 0.0075,
        0.0073, 0.0160, 0.0052, 0.0469], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,639][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0015, 0.1628, 0.0151, 0.0547, 0.0748, 0.1963, 0.1673, 0.0340, 0.0506,
        0.0310, 0.0646, 0.0895, 0.0579], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,640][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.1903, 0.0713, 0.1119, 0.0030, 0.0300, 0.0581, 0.0062, 0.4034, 0.0094,
        0.0048, 0.0154, 0.0082, 0.0878], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,641][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.3439, 0.1513, 0.1582, 0.0113, 0.0354, 0.0751, 0.0134, 0.0586, 0.0309,
        0.0060, 0.0545, 0.0211, 0.0405], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,642][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([3.1099e-05, 6.4292e-03, 9.7756e-04, 1.8696e-02, 6.7633e-03, 2.3582e-02,
        8.5747e-02, 6.8531e-03, 1.6439e-01, 2.4076e-01, 2.4852e-02, 8.3143e-02,
        3.3777e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,643][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([2.3673e-01, 2.6972e-03, 2.6404e-02, 1.3500e-05, 2.1928e-04, 1.2175e-02,
        1.5708e-04, 3.7375e-01, 3.2510e-04, 3.5546e-04, 1.1068e-02, 6.4403e-03,
        3.2966e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,644][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0019, 0.1552, 0.0095, 0.0361, 0.0473, 0.0664, 0.1100, 0.1112, 0.1123,
        0.0239, 0.0692, 0.0596, 0.1972], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:22,646][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([8.8527e-01, 3.9281e-03, 6.2458e-02, 5.6394e-04, 5.9568e-03, 5.7901e-03,
        6.5946e-04, 6.1382e-03, 1.0889e-03, 2.0209e-03, 5.0675e-03, 1.2672e-03,
        2.7868e-03, 1.7001e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,647][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0110, 0.1148, 0.0412, 0.0598, 0.0870, 0.0490, 0.0972, 0.0487, 0.0392,
        0.0521, 0.0552, 0.1408, 0.1394, 0.0646], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,649][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.7310, 0.0235, 0.1415, 0.0045, 0.0282, 0.0072, 0.0020, 0.0153, 0.0043,
        0.0065, 0.0072, 0.0059, 0.0060, 0.0167], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,650][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([7.5360e-01, 1.1906e-03, 4.7432e-02, 1.0115e-05, 2.2532e-04, 2.7370e-03,
        5.0211e-05, 1.6680e-01, 2.4893e-04, 2.0499e-04, 5.6557e-03, 1.1392e-03,
        1.4074e-02, 6.6363e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,652][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.1074, 0.1158, 0.1026, 0.0592, 0.0993, 0.0725, 0.0243, 0.0570, 0.0543,
        0.0339, 0.0602, 0.0332, 0.0501, 0.1302], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,654][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.6504, 0.0955, 0.0815, 0.0027, 0.0287, 0.0392, 0.0017, 0.0494, 0.0033,
        0.0070, 0.0112, 0.0031, 0.0191, 0.0071], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,656][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0025, 0.1003, 0.0210, 0.0619, 0.0912, 0.1656, 0.1043, 0.0264, 0.0544,
        0.0354, 0.0464, 0.0935, 0.0588, 0.1383], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,657][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.1342, 0.0700, 0.1186, 0.0038, 0.0659, 0.0660, 0.0125, 0.3108, 0.0171,
        0.0043, 0.0225, 0.0199, 0.0618, 0.0927], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,659][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.4285, 0.0995, 0.1498, 0.0121, 0.0246, 0.0415, 0.0083, 0.0364, 0.0255,
        0.0063, 0.0473, 0.0094, 0.0256, 0.0852], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,660][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([4.0210e-05, 2.7745e-03, 9.6419e-04, 2.4320e-02, 7.9175e-03, 2.3073e-02,
        8.5831e-02, 3.8807e-03, 1.4074e-01, 3.2587e-01, 1.5280e-02, 8.1747e-02,
        2.2594e-01, 6.1629e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,661][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([2.4964e-01, 1.4437e-02, 5.1405e-02, 4.8974e-05, 9.7106e-04, 2.0427e-02,
        4.6231e-04, 3.4365e-01, 8.6041e-04, 1.0402e-03, 1.5923e-02, 5.3358e-03,
        2.7638e-01, 1.9427e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,663][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0010, 0.1415, 0.0074, 0.0480, 0.0368, 0.0822, 0.1640, 0.0810, 0.1274,
        0.0415, 0.0665, 0.0764, 0.1115, 0.0148], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:22,665][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.6341, 0.0166, 0.1392, 0.0018, 0.0258, 0.0196, 0.0038, 0.0273, 0.0033,
        0.0089, 0.0245, 0.0061, 0.0161, 0.0352, 0.0377], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,667][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0061, 0.0834, 0.0250, 0.0491, 0.0652, 0.0498, 0.0663, 0.0437, 0.0321,
        0.0254, 0.0483, 0.1656, 0.1044, 0.0828, 0.1527], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,668][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.3815, 0.0503, 0.1595, 0.0061, 0.0444, 0.0257, 0.0058, 0.0461, 0.0062,
        0.0188, 0.0289, 0.0209, 0.0267, 0.0496, 0.1297], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,670][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([3.2290e-01, 7.3567e-04, 4.4463e-02, 3.6783e-05, 9.3353e-04, 4.2908e-03,
        1.3625e-04, 3.6764e-01, 7.0381e-04, 1.1356e-03, 1.6522e-02, 6.3860e-03,
        1.6856e-01, 4.6215e-02, 1.9343e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,671][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0314, 0.1037, 0.0623, 0.0585, 0.1125, 0.0594, 0.0316, 0.0453, 0.0497,
        0.0320, 0.0627, 0.0440, 0.0575, 0.1172, 0.1320], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,673][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.3310, 0.1349, 0.0604, 0.0068, 0.0482, 0.0863, 0.0069, 0.0838, 0.0146,
        0.0199, 0.0397, 0.0142, 0.0793, 0.0214, 0.0525], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,675][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0010, 0.0501, 0.0096, 0.0360, 0.0537, 0.1017, 0.1101, 0.0295, 0.0545,
        0.0263, 0.0406, 0.1377, 0.1049, 0.0899, 0.1544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,677][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0628, 0.0397, 0.0610, 0.0048, 0.0259, 0.0378, 0.0079, 0.3926, 0.0137,
        0.0120, 0.0222, 0.0189, 0.1340, 0.1025, 0.0643], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,679][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1007, 0.1218, 0.0839, 0.0160, 0.0623, 0.0585, 0.0157, 0.0541, 0.0354,
        0.0147, 0.0864, 0.0317, 0.0953, 0.1498, 0.0738], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,680][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([2.1807e-05, 3.1754e-03, 8.7200e-04, 1.8694e-02, 8.5860e-03, 4.9627e-02,
        7.0211e-02, 7.8551e-03, 7.5724e-02, 1.1320e-01, 1.8254e-02, 1.3023e-01,
        3.3845e-01, 5.5888e-02, 1.0922e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,681][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([4.3547e-02, 2.4008e-03, 1.5616e-02, 2.5340e-05, 6.9186e-04, 5.6409e-03,
        2.7019e-04, 2.4474e-01, 7.3897e-04, 1.8297e-03, 1.3235e-02, 1.4938e-02,
        5.7000e-01, 4.6665e-02, 3.9663e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,683][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0015, 0.1497, 0.0077, 0.0261, 0.0421, 0.0628, 0.1367, 0.0719, 0.1059,
        0.0459, 0.0846, 0.0616, 0.1301, 0.0339, 0.0395], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:22,684][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.7797, 0.0089, 0.0906, 0.0014, 0.0128, 0.0056, 0.0013, 0.0127, 0.0016,
        0.0027, 0.0080, 0.0018, 0.0043, 0.0190, 0.0109, 0.0387],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,685][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0081, 0.0797, 0.0346, 0.0610, 0.0621, 0.0339, 0.0819, 0.0503, 0.0338,
        0.0393, 0.0354, 0.1470, 0.1032, 0.0625, 0.0802, 0.0871],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,686][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.5765, 0.0247, 0.1279, 0.0034, 0.0378, 0.0098, 0.0028, 0.0248, 0.0044,
        0.0071, 0.0160, 0.0096, 0.0086, 0.0191, 0.0508, 0.0765],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,687][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([8.7887e-01, 1.6455e-04, 1.7294e-02, 4.1932e-06, 7.3398e-05, 3.5234e-04,
        6.0550e-06, 2.4625e-02, 3.8488e-05, 5.4538e-05, 1.9376e-03, 3.7237e-04,
        9.0445e-03, 7.8199e-03, 1.4685e-03, 5.7871e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,689][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0502, 0.1104, 0.0727, 0.0502, 0.1145, 0.0476, 0.0214, 0.0587, 0.0424,
        0.0364, 0.0457, 0.0221, 0.0445, 0.0966, 0.1033, 0.0833],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,690][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.5744, 0.0767, 0.0703, 0.0042, 0.0233, 0.0497, 0.0025, 0.0433, 0.0054,
        0.0070, 0.0148, 0.0048, 0.0196, 0.0094, 0.0185, 0.0760],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,692][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0009, 0.0652, 0.0110, 0.0612, 0.0741, 0.1040, 0.0958, 0.0189, 0.0323,
        0.0281, 0.0310, 0.0883, 0.0455, 0.1110, 0.1254, 0.1072],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,694][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.2509, 0.0387, 0.1232, 0.0034, 0.0404, 0.0230, 0.0037, 0.1858, 0.0072,
        0.0042, 0.0111, 0.0057, 0.0299, 0.0485, 0.0588, 0.1655],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,695][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.3642, 0.0927, 0.1228, 0.0132, 0.0317, 0.0425, 0.0071, 0.0337, 0.0105,
        0.0060, 0.0253, 0.0151, 0.0202, 0.0714, 0.0214, 0.1224],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,697][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([1.7336e-05, 2.7163e-03, 6.0153e-04, 2.8288e-02, 6.9594e-03, 1.9394e-02,
        9.2658e-02, 4.6209e-03, 1.2710e-01, 2.2725e-01, 1.4658e-02, 9.0033e-02,
        2.6667e-01, 5.4577e-02, 5.4055e-02, 1.0409e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,698][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([2.5967e-01, 9.5976e-04, 2.1041e-02, 1.0931e-05, 1.7136e-04, 2.2124e-03,
        4.0014e-05, 5.3598e-02, 9.0474e-05, 1.7674e-04, 3.0838e-03, 1.1900e-03,
        5.2240e-02, 6.9932e-03, 6.3185e-03, 5.9220e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,700][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0017, 0.0958, 0.0094, 0.0312, 0.0479, 0.0691, 0.1433, 0.0705, 0.0833,
        0.0385, 0.0458, 0.0678, 0.1017, 0.0296, 0.0441, 0.1203],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:22,701][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([9.1324e-01, 2.2860e-03, 4.6557e-02, 3.6744e-04, 3.1994e-03, 2.1873e-03,
        2.3219e-04, 3.3600e-03, 3.9617e-04, 6.0916e-04, 3.0694e-03, 5.1716e-04,
        1.3528e-03, 7.9134e-03, 2.1598e-03, 1.0252e-02, 2.3057e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,703][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0124, 0.0753, 0.0391, 0.0474, 0.0476, 0.0292, 0.0446, 0.0623, 0.0252,
        0.0355, 0.0365, 0.0978, 0.1491, 0.0377, 0.0736, 0.0691, 0.1176],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,705][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.7340, 0.0138, 0.1252, 0.0030, 0.0181, 0.0050, 0.0008, 0.0118, 0.0012,
        0.0038, 0.0073, 0.0033, 0.0041, 0.0077, 0.0230, 0.0327, 0.0052],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,706][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([6.7093e-01, 9.6739e-05, 1.7918e-02, 1.9951e-06, 4.7197e-05, 5.6761e-04,
        5.1788e-06, 6.8787e-02, 4.2020e-05, 5.3235e-05, 2.8123e-03, 3.0045e-04,
        1.6829e-02, 7.3483e-03, 1.4631e-03, 2.0117e-01, 1.1622e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,707][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1579, 0.0826, 0.1205, 0.0434, 0.0825, 0.0352, 0.0111, 0.0496, 0.0240,
        0.0306, 0.0494, 0.0205, 0.0362, 0.0750, 0.0779, 0.0735, 0.0301],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,709][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.6906, 0.0436, 0.0794, 0.0022, 0.0189, 0.0208, 0.0015, 0.0306, 0.0030,
        0.0048, 0.0123, 0.0036, 0.0124, 0.0069, 0.0139, 0.0440, 0.0115],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,711][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0033, 0.0644, 0.0201, 0.0556, 0.0635, 0.0962, 0.0582, 0.0224, 0.0314,
        0.0222, 0.0404, 0.0790, 0.0464, 0.0807, 0.0996, 0.1204, 0.0962],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,713][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.2555, 0.0447, 0.1204, 0.0021, 0.0268, 0.0156, 0.0021, 0.2143, 0.0046,
        0.0027, 0.0087, 0.0038, 0.0285, 0.0420, 0.0533, 0.1436, 0.0314],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,715][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.3591, 0.0904, 0.1189, 0.0083, 0.0191, 0.0329, 0.0041, 0.0291, 0.0104,
        0.0033, 0.0337, 0.0100, 0.0217, 0.0849, 0.0162, 0.1209, 0.0369],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,716][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([2.3752e-05, 2.4442e-03, 1.0144e-03, 3.1237e-02, 6.6086e-03, 1.9563e-02,
        5.9006e-02, 4.8586e-03, 9.3509e-02, 2.1450e-01, 1.6591e-02, 1.0288e-01,
        2.4495e-01, 6.3725e-02, 6.2153e-02, 1.5220e-02, 6.1711e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,717][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([1.0759e-01, 6.1056e-04, 9.4595e-03, 3.9634e-06, 5.3755e-05, 1.3912e-03,
        1.7891e-05, 7.3890e-02, 5.3569e-05, 1.1669e-04, 1.9359e-03, 6.7006e-04,
        8.0473e-02, 5.1388e-03, 4.7721e-03, 6.6074e-01, 5.3085e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,719][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0016, 0.1275, 0.0067, 0.0397, 0.0272, 0.0609, 0.1152, 0.0697, 0.0679,
        0.0265, 0.0655, 0.0787, 0.1055, 0.0175, 0.0197, 0.0685, 0.1017],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:22,721][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.7445, 0.0088, 0.0807, 0.0017, 0.0087, 0.0082, 0.0015, 0.0100, 0.0020,
        0.0025, 0.0097, 0.0026, 0.0047, 0.0178, 0.0083, 0.0333, 0.0078, 0.0472],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,723][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([0.0016, 0.0371, 0.0109, 0.0410, 0.0328, 0.0396, 0.0791, 0.0212, 0.0396,
        0.0377, 0.0418, 0.1654, 0.0995, 0.0631, 0.0744, 0.0613, 0.1320, 0.0217],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,724][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.4460, 0.0237, 0.1256, 0.0054, 0.0307, 0.0160, 0.0055, 0.0273, 0.0124,
        0.0116, 0.0257, 0.0195, 0.0130, 0.0209, 0.0467, 0.0743, 0.0179, 0.0780],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,726][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([6.5812e-01, 5.1257e-04, 2.2533e-02, 1.1048e-05, 1.2574e-04, 9.0510e-04,
        1.3735e-05, 1.9588e-02, 7.1484e-05, 7.5809e-05, 2.4294e-03, 3.8018e-04,
        6.7565e-03, 7.2024e-03, 2.1919e-03, 5.8874e-02, 7.2225e-03, 2.1299e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,728][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0163, 0.0828, 0.0314, 0.0382, 0.0746, 0.0482, 0.0279, 0.0407, 0.0564,
        0.0473, 0.0404, 0.0360, 0.0735, 0.0962, 0.1038, 0.0701, 0.0445, 0.0717],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,729][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.3199, 0.0747, 0.0608, 0.0052, 0.0258, 0.0718, 0.0050, 0.0403, 0.0103,
        0.0137, 0.0253, 0.0089, 0.0249, 0.0093, 0.0213, 0.0687, 0.0192, 0.1948],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,730][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0002, 0.0406, 0.0031, 0.0467, 0.0316, 0.1071, 0.1254, 0.0125, 0.0501,
        0.0345, 0.0324, 0.0906, 0.0527, 0.0956, 0.0811, 0.0780, 0.1086, 0.0092],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,731][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.1445, 0.0333, 0.0720, 0.0028, 0.0210, 0.0268, 0.0042, 0.1798, 0.0085,
        0.0052, 0.0133, 0.0079, 0.0343, 0.0433, 0.0474, 0.1168, 0.0417, 0.1969],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,732][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.2643, 0.0726, 0.1062, 0.0107, 0.0306, 0.0273, 0.0104, 0.0375, 0.0253,
        0.0090, 0.0332, 0.0227, 0.0316, 0.0591, 0.0370, 0.1208, 0.0326, 0.0693],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,733][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([2.0098e-06, 1.6374e-03, 1.4339e-04, 2.3618e-02, 4.0671e-03, 3.0360e-02,
        9.2752e-02, 2.7988e-03, 1.4045e-01, 1.4933e-01, 1.2956e-02, 1.1990e-01,
        2.5651e-01, 4.9089e-02, 5.3475e-02, 1.1763e-02, 4.9046e-02, 2.0913e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,734][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([1.0756e-01, 3.4183e-03, 1.4944e-02, 3.3902e-05, 2.1077e-04, 3.7499e-03,
        1.2372e-04, 5.0548e-02, 2.7271e-04, 2.5339e-04, 4.7948e-03, 2.3463e-03,
        4.8779e-02, 1.1096e-02, 7.6060e-03, 2.5805e-01, 7.0367e-02, 4.1584e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,736][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0011, 0.0651, 0.0069, 0.0264, 0.0250, 0.0596, 0.0919, 0.0593, 0.1000,
        0.0513, 0.0770, 0.0744, 0.1370, 0.0403, 0.0275, 0.0642, 0.0730, 0.0199],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:22,737][circuit_model.py][line:2332][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([9.3608e-01, 1.2821e-03, 2.8325e-02, 9.6981e-05, 8.4053e-04, 8.4540e-04,
        5.7292e-05, 1.4684e-03, 1.0897e-04, 2.2059e-04, 1.2133e-03, 1.7078e-04,
        4.9349e-04, 4.8289e-03, 7.1258e-04, 5.4701e-03, 8.3904e-04, 1.3152e-02,
        3.7973e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,739][circuit_model.py][line:2335][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0166, 0.0800, 0.0433, 0.0367, 0.0367, 0.0309, 0.0447, 0.0356, 0.0189,
        0.0318, 0.0426, 0.0850, 0.1054, 0.0434, 0.0577, 0.0708, 0.1142, 0.0472,
        0.0584], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,740][circuit_model.py][line:2338][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([8.3485e-01, 4.8039e-03, 7.9893e-02, 6.2110e-04, 5.9011e-03, 1.6030e-03,
        2.3261e-04, 4.6515e-03, 5.4158e-04, 8.9553e-04, 3.1887e-03, 1.3455e-03,
        1.4382e-03, 3.7324e-03, 7.4087e-03, 1.6861e-02, 2.1324e-03, 2.6233e-02,
        3.6624e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,741][circuit_model.py][line:2341][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([5.3111e-01, 5.0394e-05, 8.6998e-03, 3.6609e-07, 1.3091e-05, 1.7230e-04,
        1.3041e-06, 2.0930e-02, 7.6633e-06, 1.0905e-05, 7.1615e-04, 8.2028e-05,
        4.8067e-03, 2.4663e-03, 5.2111e-04, 6.0765e-02, 4.0555e-03, 3.4538e-01,
        2.0207e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,743][circuit_model.py][line:2344][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.1460, 0.0618, 0.0954, 0.0290, 0.1129, 0.0324, 0.0110, 0.0368, 0.0262,
        0.0178, 0.0309, 0.0178, 0.0302, 0.0539, 0.0896, 0.0685, 0.0271, 0.0852,
        0.0275], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,745][circuit_model.py][line:2347][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([6.8110e-01, 2.8746e-02, 6.4947e-02, 9.6461e-04, 1.2229e-02, 1.5756e-02,
        5.6713e-04, 2.1508e-02, 1.3407e-03, 3.0417e-03, 7.9599e-03, 1.8812e-03,
        7.0916e-03, 3.3697e-03, 9.5882e-03, 2.8863e-02, 5.2799e-03, 9.6726e-02,
        9.0369e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,746][circuit_model.py][line:2350][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0030, 0.0540, 0.0198, 0.0364, 0.0559, 0.0996, 0.0519, 0.0171, 0.0244,
        0.0143, 0.0300, 0.0648, 0.0311, 0.0676, 0.0752, 0.1168, 0.0925, 0.0202,
        0.1254], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,748][circuit_model.py][line:2353][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.3043, 0.0207, 0.1016, 0.0006, 0.0130, 0.0085, 0.0007, 0.0952, 0.0016,
        0.0008, 0.0037, 0.0020, 0.0118, 0.0250, 0.0304, 0.0988, 0.0175, 0.1787,
        0.0853], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,750][circuit_model.py][line:2356][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.5960, 0.0466, 0.1114, 0.0025, 0.0111, 0.0110, 0.0014, 0.0160, 0.0032,
        0.0013, 0.0114, 0.0034, 0.0057, 0.0268, 0.0101, 0.0645, 0.0138, 0.0407,
        0.0230], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,751][circuit_model.py][line:2359][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.0442e-05, 4.0597e-03, 1.2281e-03, 3.1720e-02, 9.7607e-03, 2.7339e-02,
        5.8601e-02, 5.6260e-03, 1.1044e-01, 1.4064e-01, 1.8244e-02, 1.1350e-01,
        1.8108e-01, 6.5976e-02, 7.7625e-02, 2.2703e-02, 5.5510e-02, 5.0302e-03,
        7.0875e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,753][circuit_model.py][line:2362][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([1.0423e-01, 2.0872e-04, 5.5288e-03, 4.7289e-07, 1.5212e-05, 3.3336e-04,
        3.0022e-06, 1.8220e-02, 7.0572e-06, 1.5099e-05, 4.7834e-04, 1.1858e-04,
        1.3521e-02, 1.5552e-03, 1.4668e-03, 1.8257e-01, 1.2828e-02, 6.3649e-01,
        2.2412e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,755][circuit_model.py][line:2365][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0032, 0.1649, 0.0127, 0.0300, 0.0306, 0.0438, 0.0822, 0.0531, 0.0623,
        0.0176, 0.0723, 0.0672, 0.0483, 0.0137, 0.0199, 0.0595, 0.0735, 0.0189,
        0.1263], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:22,758][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:22,761][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[6008],
        [  16],
        [ 216],
        [   1],
        [   1],
        [   6],
        [   3],
        [   3],
        [   5],
        [   3],
        [   2],
        [   9],
        [   3],
        [   5],
        [   8],
        [  13],
        [  12],
        [   2],
        [   1]], device='cuda:0')
[2024-07-24 10:25:22,763][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[4889],
        [  35],
        [ 891],
        [  17],
        [   2],
        [  51],
        [  20],
        [   5],
        [  13],
        [   3],
        [   5],
        [  17],
        [   8],
        [   8],
        [  20],
        [  23],
        [  16],
        [   4],
        [   7]], device='cuda:0')
[2024-07-24 10:25:22,765][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[11811],
        [11810],
        [ 8101],
        [ 4745],
        [ 2402],
        [ 3489],
        [ 3693],
        [ 1670],
        [ 1601],
        [ 2087],
        [ 3935],
        [ 2813],
        [ 3830],
        [ 1553],
        [10359],
        [ 3255],
        [ 5467],
        [ 6717],
        [ 5100]], device='cuda:0')
[2024-07-24 10:25:22,767][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[32974],
        [36854],
        [39313],
        [42078],
        [41274],
        [43638],
        [43586],
        [44412],
        [43699],
        [44542],
        [44416],
        [42630],
        [41464],
        [42286],
        [41896],
        [43030],
        [43023],
        [42477],
        [43162]], device='cuda:0')
[2024-07-24 10:25:22,769][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[21874],
        [27295],
        [41825],
        [44425],
        [38932],
        [37780],
        [43634],
        [45320],
        [45875],
        [44329],
        [42696],
        [41743],
        [41488],
        [41752],
        [38513],
        [39084],
        [42264],
        [41164],
        [43594]], device='cuda:0')
[2024-07-24 10:25:22,770][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[33528],
        [33648],
        [37185],
        [37411],
        [37889],
        [35122],
        [38856],
        [37103],
        [38606],
        [38601],
        [38899],
        [38839],
        [36564],
        [37033],
        [38528],
        [36646],
        [41111],
        [39553],
        [40806]], device='cuda:0')
[2024-07-24 10:25:22,772][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[ 5957],
        [22105],
        [17789],
        [17724],
        [ 9172],
        [19882],
        [18770],
        [23907],
        [15688],
        [ 7737],
        [10667],
        [13976],
        [24447],
        [27133],
        [ 8258],
        [12828],
        [30859],
        [18043],
        [26263]], device='cuda:0')
[2024-07-24 10:25:22,775][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[15030],
        [ 3509],
        [ 1787],
        [ 2930],
        [ 3024],
        [ 4199],
        [ 5163],
        [ 5976],
        [ 6752],
        [ 6765],
        [ 7157],
        [ 7800],
        [ 8190],
        [ 8379],
        [ 7777],
        [ 7721],
        [ 8179],
        [ 8119],
        [ 8019]], device='cuda:0')
[2024-07-24 10:25:22,776][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[36849],
        [15303],
        [15284],
        [10091],
        [ 3224],
        [ 2104],
        [ 1751],
        [ 1607],
        [ 1685],
        [ 2187],
        [ 1998],
        [ 1873],
        [ 1580],
        [ 1459],
        [ 1469],
        [ 1412],
        [ 1345],
        [ 1293],
        [ 1143]], device='cuda:0')
[2024-07-24 10:25:22,778][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[11393],
        [21522],
        [17726],
        [16539],
        [19872],
        [16089],
        [18714],
        [18712],
        [20906],
        [20936],
        [19229],
        [19626],
        [19265],
        [16570],
        [18669],
        [14693],
        [15555],
        [13298],
        [11430]], device='cuda:0')
[2024-07-24 10:25:22,779][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 1724],
        [ 1710],
        [  217],
        [  195],
        [  220],
        [ 3925],
        [  650],
        [28781],
        [19305],
        [27219],
        [12884],
        [19235],
        [17850],
        [28100],
        [24623],
        [15619],
        [11705],
        [ 7926],
        [18933]], device='cuda:0')
[2024-07-24 10:25:22,781][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[34786],
        [40250],
        [ 4464],
        [  916],
        [19355],
        [20186],
        [17062],
        [24164],
        [28772],
        [30542],
        [31375],
        [33398],
        [32090],
        [33237],
        [37887],
        [36363],
        [36079],
        [35051],
        [35433]], device='cuda:0')
[2024-07-24 10:25:22,783][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[16194],
        [10803],
        [10891],
        [11178],
        [11427],
        [13020],
        [13769],
        [14798],
        [15548],
        [15577],
        [15524],
        [15492],
        [16493],
        [16023],
        [17781],
        [28886],
        [28635],
        [22781],
        [21548]], device='cuda:0')
[2024-07-24 10:25:22,785][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[14962],
        [ 3989],
        [ 1870],
        [ 2012],
        [ 1881],
        [ 1772],
        [ 1740],
        [ 1814],
        [ 1696],
        [ 1896],
        [ 1926],
        [ 1852],
        [ 1803],
        [ 1947],
        [ 1852],
        [ 1900],
        [ 1748],
        [ 1845],
        [ 1894]], device='cuda:0')
[2024-07-24 10:25:22,787][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[40471],
        [ 5043],
        [ 7749],
        [  790],
        [ 1893],
        [ 1413],
        [ 1013],
        [ 1959],
        [ 1900],
        [ 3265],
        [ 2664],
        [ 2524],
        [ 3767],
        [ 2566],
        [ 4174],
        [ 4984],
        [ 3126],
        [ 5235],
        [ 1642]], device='cuda:0')
[2024-07-24 10:25:22,788][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[17967],
        [17330],
        [23978],
        [15632],
        [28123],
        [17739],
        [16509],
        [22876],
        [20149],
        [17610],
        [18691],
        [16914],
        [18594],
        [15858],
        [30165],
        [23828],
        [16518],
        [20600],
        [14902]], device='cuda:0')
[2024-07-24 10:25:22,790][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[ 4354],
        [ 8594],
        [10483],
        [14504],
        [18631],
        [19703],
        [20038],
        [19884],
        [20381],
        [20827],
        [23256],
        [22976],
        [21932],
        [21863],
        [20130],
        [21859],
        [21548],
        [21006],
        [20222]], device='cuda:0')
[2024-07-24 10:25:22,792][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[11928],
        [11799],
        [14878],
        [14324],
        [14452],
        [11551],
        [12066],
        [10100],
        [ 9953],
        [ 9561],
        [11120],
        [10949],
        [11402],
        [ 9810],
        [ 9697],
        [ 8124],
        [ 9346],
        [ 9566],
        [10851]], device='cuda:0')
[2024-07-24 10:25:22,794][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[20],
        [19],
        [18],
        [27],
        [19],
        [13],
        [11],
        [ 9],
        [30],
        [30],
        [21],
        [21],
        [ 7],
        [ 9],
        [21],
        [10],
        [10],
        [10],
        [16]], device='cuda:0')
[2024-07-24 10:25:22,796][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[20216],
        [17913],
        [17831],
        [20200],
        [18646],
        [21118],
        [22728],
        [25908],
        [27551],
        [31316],
        [31156],
        [29901],
        [28897],
        [29393],
        [28372],
        [27719],
        [27534],
        [28141],
        [26732]], device='cuda:0')
[2024-07-24 10:25:22,798][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[  575],
        [23088],
        [37600],
        [27879],
        [32136],
        [ 4951],
        [ 8032],
        [10032],
        [ 9928],
        [10974],
        [10735],
        [ 2012],
        [ 4635],
        [ 4395],
        [ 7826],
        [ 3225],
        [ 1428],
        [ 3369],
        [  479]], device='cuda:0')
[2024-07-24 10:25:22,800][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[ 7942],
        [45670],
        [45612],
        [44433],
        [46570],
        [42866],
        [41617],
        [36232],
        [38467],
        [29873],
        [32658],
        [36070],
        [35832],
        [38907],
        [36738],
        [40240],
        [39370],
        [35854],
        [40472]], device='cuda:0')
[2024-07-24 10:25:22,802][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[ 4557],
        [28729],
        [33635],
        [34498],
        [28010],
        [20146],
        [26577],
        [ 8146],
        [ 8784],
        [ 7349],
        [ 8947],
        [ 7307],
        [ 7891],
        [10554],
        [11132],
        [ 9063],
        [ 9121],
        [15275],
        [14561]], device='cuda:0')
[2024-07-24 10:25:22,804][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 3756],
        [ 5328],
        [31646],
        [31049],
        [29925],
        [32136],
        [32839],
        [32141],
        [33464],
        [32363],
        [33962],
        [33734],
        [33627],
        [30751],
        [29376],
        [30434],
        [30831],
        [29398],
        [26887]], device='cuda:0')
[2024-07-24 10:25:22,806][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 7669],
        [37927],
        [38031],
        [37464],
        [41755],
        [40169],
        [40460],
        [40756],
        [38518],
        [17446],
        [26949],
        [19372],
        [13951],
        [15538],
        [17653],
        [16535],
        [17605],
        [18309],
        [21157]], device='cuda:0')
[2024-07-24 10:25:22,808][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[ 3012],
        [14632],
        [20723],
        [27068],
        [23912],
        [ 6767],
        [28063],
        [30103],
        [34089],
        [33998],
        [33302],
        [33423],
        [22245],
        [23408],
        [20702],
        [21491],
        [23034],
        [22658],
        [23536]], device='cuda:0')
[2024-07-24 10:25:22,810][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 4737],
        [ 7808],
        [ 7687],
        [ 8079],
        [ 9911],
        [14039],
        [13235],
        [14284],
        [14053],
        [13118],
        [12689],
        [11759],
        [11559],
        [12302],
        [11702],
        [13446],
        [11363],
        [11495],
        [ 9873]], device='cuda:0')
[2024-07-24 10:25:22,812][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[50007],
        [43739],
        [31741],
        [30145],
        [29674],
        [43836],
        [36736],
        [38281],
        [33930],
        [35795],
        [35854],
        [38038],
        [41396],
        [41318],
        [37955],
        [43279],
        [43942],
        [41773],
        [43280]], device='cuda:0')
[2024-07-24 10:25:22,814][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[ 7842],
        [25124],
        [19880],
        [37990],
        [27780],
        [28273],
        [32063],
        [29887],
        [33340],
        [33103],
        [29143],
        [38956],
        [31729],
        [34282],
        [34496],
        [25970],
        [34198],
        [29341],
        [37999]], device='cuda:0')
[2024-07-24 10:25:22,816][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917],
        [23917]], device='cuda:0')
[2024-07-24 10:25:22,927][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:22,928][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,930][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,931][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,932][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,933][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,935][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,936][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,937][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,939][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,940][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,941][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,943][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:22,944][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.8428, 0.1572], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,946][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.0147, 0.9853], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,948][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.0894, 0.9106], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,949][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.4380, 0.5620], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,950][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.2818, 0.7182], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,951][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.9751, 0.0249], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,951][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.2867, 0.7133], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,952][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1527, 0.8473], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,953][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.3979, 0.6021], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,955][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.9771, 0.0229], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,956][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.6702, 0.3298], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,958][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.4513, 0.5487], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:22,959][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.0859, 0.5199, 0.3942], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,961][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.0050, 0.9193, 0.0756], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,962][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.0685, 0.7023, 0.2292], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,964][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.1913, 0.3546, 0.4541], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,966][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.1059, 0.6712, 0.2229], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,967][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.7440, 0.0243, 0.2317], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,969][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.0314, 0.7026, 0.2660], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,970][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.1979, 0.3666, 0.4355], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,972][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.0850, 0.6056, 0.3094], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,974][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.6729, 0.0384, 0.2887], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,975][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.2169, 0.3942, 0.3890], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,977][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.0107, 0.8757, 0.1136], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:22,979][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.3119, 0.1352, 0.4703, 0.0826], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,980][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.0127, 0.2064, 0.0786, 0.7023], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,982][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0962, 0.1953, 0.1688, 0.5398], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,983][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ and] are: tensor([0.2598, 0.1538, 0.4588, 0.1276], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,985][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.1346, 0.3676, 0.2277, 0.2701], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,987][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.8282, 0.0031, 0.1673, 0.0015], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,988][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0533, 0.2892, 0.2544, 0.4031], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,990][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.1697, 0.4697, 0.2471, 0.1134], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,991][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.3990, 0.1521, 0.3926, 0.0564], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,993][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.6311, 0.0044, 0.3607, 0.0038], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,994][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.3590, 0.3375, 0.2863, 0.0172], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,995][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.0375, 0.2032, 0.2164, 0.5429], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:22,995][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0614, 0.1292, 0.2577, 0.1708, 0.3809], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,996][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.0017, 0.1166, 0.0222, 0.2320, 0.6276], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,997][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0121, 0.1519, 0.0521, 0.5389, 0.2450], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:22,999][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0678, 0.1713, 0.2424, 0.1362, 0.3823], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,000][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0517, 0.3465, 0.1487, 0.2872, 0.1660], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,002][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.7915, 0.0035, 0.1920, 0.0027, 0.0104], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,003][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0134, 0.1162, 0.1159, 0.2726, 0.4820], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,005][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0691, 0.2851, 0.1612, 0.0529, 0.4317], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,007][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.1943, 0.0684, 0.3423, 0.0632, 0.3318], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,008][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.6782, 0.0040, 0.3048, 0.0037, 0.0093], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,010][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.2805, 0.2702, 0.2603, 0.0108, 0.1783], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,011][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0054, 0.2287, 0.0481, 0.4245, 0.2932], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,013][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.1867, 0.0795, 0.3200, 0.0410, 0.2223, 0.1506], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,015][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ had] are: tensor([0.0065, 0.0569, 0.0370, 0.2395, 0.4759, 0.1841], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,016][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.0696, 0.0736, 0.1301, 0.2894, 0.2125, 0.2249], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,018][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ had] are: tensor([0.2086, 0.0828, 0.3123, 0.0429, 0.2412, 0.1123], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,020][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.1478, 0.2593, 0.1814, 0.2317, 0.0704, 0.1094], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,021][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ had] are: tensor([9.4994e-01, 2.7734e-04, 3.2568e-02, 3.7838e-05, 2.6072e-04, 1.6919e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,022][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0837, 0.0610, 0.2208, 0.0986, 0.3818, 0.1540], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,024][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.1515, 0.2656, 0.1747, 0.0486, 0.2626, 0.0969], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,026][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.3707, 0.0468, 0.3758, 0.0297, 0.1392, 0.0377], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,027][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ had] are: tensor([9.0669e-01, 7.0306e-04, 6.2207e-02, 6.4338e-05, 4.0781e-04, 2.9924e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,028][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.3463, 0.1816, 0.2548, 0.0097, 0.1976, 0.0099], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,030][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.0769, 0.0846, 0.1605, 0.1530, 0.1655, 0.3594], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,032][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.3122, 0.0598, 0.2570, 0.0206, 0.1233, 0.1671, 0.0600],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,033][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0054, 0.0704, 0.0252, 0.1697, 0.3219, 0.1839, 0.2236],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,035][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1187, 0.0563, 0.1481, 0.1823, 0.1689, 0.2002, 0.1255],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,036][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.3404, 0.0534, 0.2763, 0.0301, 0.1577, 0.1067, 0.0355],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,038][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1778, 0.2758, 0.1462, 0.1444, 0.0311, 0.1411, 0.0836],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,039][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([8.7178e-01, 9.7589e-05, 3.0559e-02, 2.1891e-05, 8.6143e-05, 9.6455e-02,
        1.0023e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,040][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1259, 0.0796, 0.2052, 0.0783, 0.2169, 0.2140, 0.0802],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,041][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2536, 0.2261, 0.1524, 0.0307, 0.1889, 0.1111, 0.0373],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,041][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.5500, 0.0278, 0.3376, 0.0114, 0.0493, 0.0137, 0.0101],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,042][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([5.5915e-01, 2.0363e-04, 6.7760e-02, 4.0178e-05, 2.4158e-04, 3.7096e-01,
        1.6533e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,044][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.4814, 0.1901, 0.2212, 0.0054, 0.0910, 0.0094, 0.0015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,046][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0560, 0.0949, 0.1156, 0.1243, 0.1298, 0.3309, 0.1485],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,047][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0841, 0.0661, 0.1637, 0.0466, 0.1461, 0.2062, 0.0956, 0.1916],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,049][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([0.0005, 0.0486, 0.0051, 0.1570, 0.2245, 0.1345, 0.3419, 0.0879],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,051][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.0117, 0.0869, 0.0298, 0.2137, 0.1068, 0.2321, 0.1908, 0.1283],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,052][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([0.0599, 0.0653, 0.1311, 0.0644, 0.2093, 0.2044, 0.1082, 0.1574],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,054][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.1612, 0.1969, 0.1060, 0.0824, 0.0297, 0.1089, 0.0730, 0.2417],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,055][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([8.7051e-01, 3.3841e-05, 1.2157e-02, 4.0601e-06, 2.3981e-05, 1.0473e-02,
        6.7701e-05, 1.0673e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,057][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0124, 0.0488, 0.0595, 0.1102, 0.2201, 0.1759, 0.1723, 0.2009],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,059][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.1476, 0.1534, 0.1543, 0.0268, 0.1793, 0.1226, 0.0417, 0.1743],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,060][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.0572, 0.0906, 0.1331, 0.0666, 0.1666, 0.1301, 0.1246, 0.2311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,061][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([6.8221e-01, 8.2813e-05, 1.3462e-02, 5.2529e-06, 1.5521e-05, 1.5952e-02,
        3.9788e-05, 2.8823e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,063][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.3474, 0.1971, 0.1878, 0.0096, 0.2082, 0.0166, 0.0043, 0.0290],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,065][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([0.0051, 0.0634, 0.0226, 0.0967, 0.0904, 0.3286, 0.3063, 0.0868],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,066][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.2818, 0.0364, 0.2692, 0.0133, 0.0745, 0.1233, 0.0252, 0.1408, 0.0355],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,068][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0033, 0.0519, 0.0201, 0.1214, 0.1908, 0.1610, 0.1512, 0.0987, 0.2017],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,070][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0661, 0.0668, 0.1135, 0.1476, 0.1555, 0.1680, 0.0827, 0.1186, 0.0812],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,072][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1552, 0.0574, 0.2495, 0.0264, 0.1799, 0.0899, 0.0358, 0.1257, 0.0802],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,073][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0660, 0.1386, 0.0855, 0.0793, 0.0242, 0.1141, 0.0915, 0.2454, 0.1555],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,074][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ of] are: tensor([1.2960e-01, 3.9322e-05, 9.5836e-03, 9.6760e-06, 6.0367e-05, 3.3997e-02,
        6.7649e-04, 8.2568e-01, 3.5921e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,076][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0569, 0.0427, 0.1424, 0.0661, 0.2243, 0.1937, 0.0557, 0.1065, 0.1118],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,078][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0970, 0.1588, 0.1419, 0.0296, 0.1829, 0.0956, 0.0489, 0.1396, 0.1057],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,080][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3586, 0.0358, 0.3567, 0.0117, 0.0907, 0.0220, 0.0198, 0.0764, 0.0282],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,081][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ of] are: tensor([1.6301e-02, 1.7446e-05, 3.4687e-03, 6.3844e-06, 3.4555e-05, 1.7982e-02,
        1.7461e-04, 9.6195e-01, 6.7746e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,083][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.4139, 0.1919, 0.2162, 0.0054, 0.1380, 0.0092, 0.0015, 0.0234, 0.0005],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,084][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0229, 0.0445, 0.1046, 0.0707, 0.1538, 0.2344, 0.1861, 0.0860, 0.0970],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,085][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([0.1393, 0.0322, 0.1349, 0.0186, 0.0835, 0.1241, 0.0543, 0.2454, 0.0578,
        0.1100], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,087][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([0.0012, 0.0270, 0.0075, 0.0762, 0.2282, 0.0787, 0.1687, 0.0931, 0.2363,
        0.0829], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,088][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.0211, 0.0602, 0.0358, 0.2055, 0.0827, 0.1230, 0.1226, 0.1350, 0.1363,
        0.0780], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,090][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([0.1111, 0.0582, 0.1465, 0.0441, 0.1337, 0.1304, 0.0556, 0.1562, 0.0931,
        0.0711], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,092][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0770, 0.1835, 0.0642, 0.0979, 0.0230, 0.0804, 0.0682, 0.1925, 0.1779,
        0.0354], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,093][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([6.6179e-02, 9.1614e-06, 3.6834e-03, 5.9122e-06, 1.9381e-05, 2.0880e-02,
        1.2260e-03, 9.0731e-01, 5.6270e-04, 1.2201e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,095][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0283, 0.0305, 0.0562, 0.0389, 0.1064, 0.0782, 0.0529, 0.2100, 0.0885,
        0.3101], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,097][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.1715, 0.1708, 0.1247, 0.0213, 0.1596, 0.1046, 0.0347, 0.1326, 0.0415,
        0.0387], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,098][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.4927, 0.0184, 0.3196, 0.0067, 0.0519, 0.0237, 0.0110, 0.0495, 0.0141,
        0.0123], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,099][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([2.9691e-03, 4.4177e-06, 3.4868e-04, 1.1534e-06, 2.1035e-06, 9.5178e-03,
        1.4891e-04, 9.8689e-01, 9.0993e-05, 2.2636e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,101][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.5100, 0.1737, 0.1801, 0.0056, 0.0875, 0.0094, 0.0021, 0.0231, 0.0006,
        0.0079], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,103][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([0.0118, 0.0384, 0.0432, 0.0589, 0.0755, 0.2606, 0.1874, 0.1021, 0.1283,
        0.0938], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,104][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0614, 0.0544, 0.1250, 0.0403, 0.0813, 0.1543, 0.0880, 0.1549, 0.0786,
        0.0929, 0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,106][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ at] are: tensor([0.0006, 0.0163, 0.0066, 0.1017, 0.1406, 0.1083, 0.2118, 0.0609, 0.1998,
        0.0791, 0.0741], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,108][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0156, 0.0300, 0.0420, 0.1674, 0.1280, 0.1352, 0.1241, 0.0794, 0.1223,
        0.0697, 0.0863], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,110][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ at] are: tensor([0.0861, 0.0546, 0.1514, 0.0491, 0.1236, 0.0802, 0.0698, 0.1153, 0.1066,
        0.0838, 0.0795], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,111][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0679, 0.1030, 0.0829, 0.0854, 0.0348, 0.1022, 0.0692, 0.1937, 0.0984,
        0.0839, 0.0786], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,112][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ at] are: tensor([4.4333e-01, 1.1502e-04, 2.1722e-02, 4.5267e-05, 2.0509e-04, 2.4802e-02,
        9.0572e-04, 4.9921e-01, 7.7411e-04, 4.0731e-04, 8.4856e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,114][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0156, 0.0270, 0.0588, 0.0662, 0.1415, 0.1000, 0.1060, 0.1385, 0.1270,
        0.1780, 0.0415], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,116][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.0533, 0.1115, 0.0940, 0.0278, 0.1968, 0.1073, 0.0581, 0.1842, 0.0889,
        0.0390, 0.0391], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,118][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0955, 0.0386, 0.2002, 0.0391, 0.1348, 0.0497, 0.0671, 0.1370, 0.0741,
        0.0906, 0.0731], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,119][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ at] are: tensor([1.5162e-01, 7.5163e-05, 1.3434e-02, 2.8032e-05, 1.1965e-04, 1.3628e-02,
        2.3771e-04, 8.1724e-01, 1.8265e-04, 1.3848e-04, 3.2958e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,120][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.2717, 0.2562, 0.2060, 0.0174, 0.1681, 0.0209, 0.0066, 0.0303, 0.0015,
        0.0127, 0.0087], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,122][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ at] are: tensor([0.0128, 0.0348, 0.0508, 0.0816, 0.1162, 0.1745, 0.1944, 0.0836, 0.0589,
        0.0796, 0.1126], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,124][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1004, 0.0494, 0.1298, 0.0242, 0.0799, 0.1079, 0.0565, 0.1864, 0.0341,
        0.1146, 0.0506, 0.0662], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,126][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0009, 0.0246, 0.0050, 0.0999, 0.1086, 0.0551, 0.0942, 0.0445, 0.0818,
        0.0511, 0.0515, 0.3826], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,127][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0406, 0.0548, 0.0573, 0.1269, 0.0998, 0.1038, 0.0686, 0.1142, 0.0999,
        0.0652, 0.0851, 0.0839], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,128][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0814, 0.1040, 0.1139, 0.0481, 0.1778, 0.0927, 0.0360, 0.0818, 0.0591,
        0.0774, 0.0613, 0.0663], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,129][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1333, 0.1613, 0.0944, 0.0643, 0.0213, 0.0436, 0.0334, 0.1432, 0.0682,
        0.0523, 0.1013, 0.0835], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,129][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([7.8736e-01, 1.0236e-05, 8.6558e-03, 2.0819e-06, 1.0667e-05, 4.4344e-03,
        2.7899e-05, 1.9626e-01, 6.8005e-05, 2.2326e-05, 1.3739e-03, 1.7710e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,131][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0210, 0.0464, 0.0492, 0.0526, 0.0810, 0.0814, 0.0637, 0.1238, 0.0838,
        0.2173, 0.0409, 0.1388], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,132][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1306, 0.2093, 0.0880, 0.0253, 0.2207, 0.0707, 0.0207, 0.1180, 0.0459,
        0.0146, 0.0319, 0.0242], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,134][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2802, 0.0668, 0.2874, 0.0184, 0.0768, 0.0236, 0.0191, 0.0849, 0.0306,
        0.0378, 0.0358, 0.0385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,135][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.9285e-01, 7.6733e-06, 5.8097e-03, 1.4380e-06, 9.4232e-06, 3.8372e-03,
        1.5335e-05, 6.9484e-01, 2.5676e-05, 1.1404e-05, 1.1670e-03, 1.4303e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,136][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([5.1495e-01, 1.5472e-01, 1.9608e-01, 6.5972e-03, 8.4724e-02, 1.0024e-02,
        1.4148e-03, 1.9328e-02, 4.1503e-04, 5.1519e-03, 4.5538e-03, 2.0460e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,138][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0240, 0.0255, 0.0708, 0.0676, 0.1211, 0.1380, 0.0751, 0.0923, 0.0450,
        0.0806, 0.0683, 0.1917], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,140][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0685, 0.0500, 0.0786, 0.0211, 0.0399, 0.0873, 0.0447, 0.1003, 0.0350,
        0.0513, 0.0407, 0.0581, 0.3246], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,141][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ house] are: tensor([0.0007, 0.0248, 0.0041, 0.0514, 0.0546, 0.0451, 0.0861, 0.0418, 0.0907,
        0.0311, 0.0485, 0.2546, 0.2666], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,143][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.0128, 0.0401, 0.0292, 0.1726, 0.0822, 0.1225, 0.1157, 0.0544, 0.0709,
        0.0538, 0.0568, 0.0546, 0.1345], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,145][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ house] are: tensor([0.0681, 0.0763, 0.0822, 0.0314, 0.0904, 0.0980, 0.0406, 0.0795, 0.0710,
        0.0443, 0.0892, 0.0825, 0.1465], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,146][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0991, 0.2053, 0.0657, 0.0693, 0.0122, 0.0395, 0.0426, 0.0758, 0.0832,
        0.0350, 0.1000, 0.0649, 0.1075], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,148][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ house] are: tensor([9.3713e-01, 5.2265e-06, 3.4280e-03, 1.1537e-06, 1.7714e-06, 1.4976e-03,
        8.1533e-06, 1.6637e-02, 1.6279e-05, 2.5985e-06, 4.9589e-04, 4.1602e-04,
        4.0362e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,149][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0271, 0.0492, 0.0601, 0.0334, 0.0788, 0.0754, 0.0514, 0.1152, 0.0519,
        0.1402, 0.0242, 0.1040, 0.1890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,151][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.1110, 0.2043, 0.0809, 0.0201, 0.2097, 0.1237, 0.0269, 0.0962, 0.0311,
        0.0149, 0.0375, 0.0265, 0.0171], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,153][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.3531, 0.0313, 0.3250, 0.0090, 0.0641, 0.0210, 0.0152, 0.0577, 0.0249,
        0.0240, 0.0363, 0.0237, 0.0147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,154][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ house] are: tensor([7.5063e-01, 7.4485e-06, 4.5385e-03, 1.0144e-06, 1.7328e-06, 2.7509e-03,
        2.6681e-06, 7.8825e-02, 8.8066e-06, 1.9723e-06, 4.2059e-04, 2.5306e-04,
        1.6256e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,155][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ house] are: tensor([5.8566e-01, 1.7705e-01, 1.5994e-01, 2.5221e-03, 5.1847e-02, 3.8659e-03,
        8.0220e-04, 1.1280e-02, 1.5977e-04, 3.3438e-03, 2.4166e-03, 6.8535e-04,
        4.2713e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,157][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ house] are: tensor([0.0050, 0.0242, 0.0198, 0.0400, 0.0355, 0.1032, 0.1458, 0.0447, 0.0615,
        0.0487, 0.0505, 0.2387, 0.1824], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,159][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.1516, 0.0362, 0.1170, 0.0139, 0.0582, 0.0598, 0.0221, 0.0666, 0.0304,
        0.0403, 0.0239, 0.0273, 0.1982, 0.1544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,161][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [.] are: tensor([0.0007, 0.0100, 0.0035, 0.0438, 0.0746, 0.0276, 0.0625, 0.0292, 0.0624,
        0.0441, 0.0238, 0.1681, 0.2064, 0.2434], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,162][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.0176, 0.0504, 0.0368, 0.1063, 0.0700, 0.0884, 0.0763, 0.0629, 0.0856,
        0.0558, 0.0563, 0.0733, 0.1082, 0.1122], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,164][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [.] are: tensor([0.0749, 0.0502, 0.0992, 0.0263, 0.1462, 0.0598, 0.0307, 0.0502, 0.0563,
        0.0880, 0.0449, 0.0464, 0.1380, 0.0890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,166][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.1284, 0.1594, 0.0865, 0.0750, 0.0219, 0.0377, 0.0315, 0.0891, 0.0752,
        0.0406, 0.0593, 0.0481, 0.0749, 0.0722], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,167][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [.] are: tensor([7.9244e-01, 1.4972e-04, 1.8920e-02, 1.7813e-05, 7.3035e-05, 1.2884e-02,
        2.2746e-04, 9.4572e-02, 3.0436e-04, 6.1770e-05, 2.2967e-03, 3.0890e-03,
        6.5172e-02, 9.7949e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,169][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0587, 0.0386, 0.0849, 0.0261, 0.0720, 0.0535, 0.0268, 0.0697, 0.0504,
        0.1093, 0.0131, 0.0329, 0.1566, 0.2074], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,171][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0752, 0.2637, 0.0686, 0.0302, 0.2418, 0.0715, 0.0202, 0.0648, 0.0341,
        0.0182, 0.0168, 0.0177, 0.0060, 0.0714], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,172][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.2446, 0.0565, 0.2356, 0.0199, 0.1045, 0.0258, 0.0244, 0.0733, 0.0376,
        0.0356, 0.0339, 0.0403, 0.0231, 0.0449], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,172][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [.] are: tensor([8.1202e-01, 1.3285e-04, 1.7797e-02, 9.0652e-06, 2.5840e-05, 6.1882e-03,
        3.1788e-05, 1.0096e-01, 3.6451e-05, 1.0878e-05, 6.7099e-04, 6.3506e-04,
        5.6781e-02, 4.7047e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,173][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [.] are: tensor([5.6171e-01, 1.2348e-01, 1.6315e-01, 4.8543e-03, 8.9368e-02, 1.0182e-02,
        1.8510e-03, 1.4758e-02, 5.1170e-04, 4.7748e-03, 3.6694e-03, 1.6496e-03,
        1.1601e-03, 1.8895e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,175][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [.] are: tensor([0.0558, 0.0149, 0.1430, 0.0377, 0.1420, 0.1151, 0.0739, 0.0825, 0.0548,
        0.0378, 0.0188, 0.0785, 0.0882, 0.0569], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,176][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0097, 0.0156, 0.0261, 0.0207, 0.0266, 0.0825, 0.0619, 0.0623, 0.0365,
        0.0281, 0.0335, 0.0670, 0.3051, 0.1599, 0.0643], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,178][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([1.3465e-04, 8.8323e-03, 1.5435e-03, 2.5963e-02, 3.9064e-02, 2.8321e-02,
        5.5736e-02, 1.8321e-02, 5.7449e-02, 1.7135e-02, 2.1946e-02, 1.9699e-01,
        2.5027e-01, 1.6286e-01, 1.1545e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,180][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0026, 0.0355, 0.0101, 0.1272, 0.0473, 0.1127, 0.1088, 0.0386, 0.0734,
        0.0309, 0.0670, 0.0685, 0.1232, 0.1016, 0.0526], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,181][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([0.0103, 0.0325, 0.0366, 0.0318, 0.0743, 0.0930, 0.0509, 0.0692, 0.0670,
        0.0510, 0.0560, 0.0942, 0.1502, 0.0671, 0.1158], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,183][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0309, 0.0895, 0.0464, 0.0506, 0.0350, 0.0473, 0.0449, 0.0944, 0.0893,
        0.0521, 0.0499, 0.0773, 0.0906, 0.1318, 0.0700], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,184][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([1.3789e-01, 1.1671e-05, 5.2349e-03, 6.4038e-06, 3.6157e-05, 6.6683e-03,
        2.0978e-04, 1.8846e-01, 3.1635e-04, 1.6872e-04, 3.7333e-03, 7.6979e-03,
        6.2927e-01, 1.5738e-02, 4.5568e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,186][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0019, 0.0153, 0.0127, 0.0296, 0.0485, 0.0324, 0.0554, 0.0664, 0.0374,
        0.0835, 0.0138, 0.0877, 0.2411, 0.1448, 0.1297], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,188][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.0528, 0.1575, 0.0775, 0.0277, 0.2562, 0.0810, 0.0343, 0.0643, 0.0263,
        0.0144, 0.0214, 0.0206, 0.0112, 0.0495, 0.1052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,190][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0986, 0.0183, 0.1406, 0.0236, 0.1255, 0.0554, 0.0423, 0.0784, 0.0372,
        0.0364, 0.0487, 0.0516, 0.0466, 0.0451, 0.1519], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,191][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([3.8449e-02, 4.4275e-06, 2.0135e-03, 2.3215e-06, 7.7288e-06, 2.3715e-03,
        2.9927e-05, 1.6879e-01, 4.4210e-05, 3.1923e-05, 9.4274e-04, 2.3180e-03,
        7.6926e-01, 1.4803e-02, 9.2813e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,193][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.4113, 0.1075, 0.1963, 0.0058, 0.0606, 0.0119, 0.0047, 0.0231, 0.0012,
        0.0068, 0.0089, 0.0045, 0.0039, 0.0241, 0.1294], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,194][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.0013, 0.0220, 0.0073, 0.0458, 0.0321, 0.0818, 0.1068, 0.0307, 0.0553,
        0.0263, 0.0685, 0.1985, 0.2115, 0.0726, 0.0396], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,196][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0297, 0.0214, 0.0607, 0.0233, 0.0311, 0.0643, 0.0386, 0.0610, 0.0421,
        0.0427, 0.0251, 0.0480, 0.1922, 0.1524, 0.0470, 0.1204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,198][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([0.0004, 0.0183, 0.0032, 0.0552, 0.0358, 0.0299, 0.0513, 0.0233, 0.0561,
        0.0269, 0.0271, 0.1830, 0.1380, 0.2624, 0.0572, 0.0318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,200][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0121, 0.0326, 0.0357, 0.1065, 0.0652, 0.1021, 0.0715, 0.0595, 0.0666,
        0.0565, 0.0544, 0.0578, 0.0527, 0.0983, 0.0442, 0.0844],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,201][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([0.0535, 0.0663, 0.0976, 0.0230, 0.1017, 0.0452, 0.0266, 0.0516, 0.0378,
        0.0566, 0.0351, 0.0431, 0.0996, 0.0531, 0.1213, 0.0880],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,203][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0824, 0.0949, 0.0689, 0.0426, 0.0199, 0.0406, 0.0291, 0.1024, 0.0641,
        0.0405, 0.0488, 0.0633, 0.0791, 0.1055, 0.0350, 0.0830],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,204][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([7.9307e-01, 9.5415e-06, 4.8709e-03, 1.9520e-06, 7.2506e-06, 1.2478e-03,
        8.1603e-06, 1.1453e-02, 1.8512e-05, 1.2796e-05, 4.4319e-04, 3.4395e-04,
        3.3285e-02, 2.7300e-03, 3.1793e-04, 1.5218e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,206][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0058, 0.0290, 0.0250, 0.0401, 0.0637, 0.0398, 0.0362, 0.0497, 0.0369,
        0.1188, 0.0202, 0.0642, 0.1338, 0.2044, 0.0857, 0.0466],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,208][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.0819, 0.1332, 0.0975, 0.0223, 0.2076, 0.0793, 0.0269, 0.0602, 0.0299,
        0.0128, 0.0177, 0.0186, 0.0126, 0.0501, 0.1053, 0.0441],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,210][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.1015, 0.0324, 0.1582, 0.0240, 0.1057, 0.0305, 0.0427, 0.0761, 0.0428,
        0.0633, 0.0476, 0.0578, 0.0270, 0.0469, 0.0695, 0.0742],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,211][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([7.3800e-01, 1.0192e-05, 5.5304e-03, 1.8898e-06, 6.7991e-06, 6.3045e-04,
        1.4824e-06, 1.6606e-02, 5.4640e-06, 2.4215e-06, 1.8721e-04, 1.0655e-04,
        2.3478e-02, 2.8491e-03, 2.3891e-04, 2.1234e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,213][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([0.3186, 0.1140, 0.1829, 0.0076, 0.1035, 0.0097, 0.0028, 0.0148, 0.0007,
        0.0067, 0.0041, 0.0024, 0.0013, 0.0303, 0.1041, 0.0965],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,215][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([0.0197, 0.0417, 0.0363, 0.0357, 0.0571, 0.0865, 0.0609, 0.0653, 0.0274,
        0.0390, 0.0334, 0.0940, 0.1911, 0.0719, 0.0436, 0.0963],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,216][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1330, 0.0339, 0.0959, 0.0131, 0.0325, 0.0529, 0.0134, 0.0583, 0.0117,
        0.0214, 0.0234, 0.0259, 0.1283, 0.0961, 0.0540, 0.1421, 0.0644],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,216][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0006, 0.0122, 0.0038, 0.0549, 0.0475, 0.0319, 0.0397, 0.0198, 0.0386,
        0.0236, 0.0261, 0.1778, 0.1353, 0.1750, 0.0814, 0.0257, 0.1060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,217][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0378, 0.0281, 0.0545, 0.0949, 0.0667, 0.0582, 0.0420, 0.0586, 0.0484,
        0.0441, 0.0563, 0.0423, 0.0886, 0.0714, 0.0540, 0.0837, 0.0704],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,218][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0775, 0.0402, 0.0972, 0.0241, 0.1071, 0.0479, 0.0178, 0.0512, 0.0376,
        0.0482, 0.0419, 0.0363, 0.1008, 0.0467, 0.1051, 0.0863, 0.0340],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,220][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.2142, 0.0947, 0.0886, 0.0407, 0.0107, 0.0276, 0.0138, 0.0774, 0.0298,
        0.0316, 0.0552, 0.0406, 0.0545, 0.0724, 0.0207, 0.0955, 0.0320],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,221][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([7.0696e-01, 3.1592e-06, 3.3510e-03, 6.3115e-07, 2.1490e-06, 9.5442e-04,
        3.0090e-06, 1.4773e-02, 9.8247e-06, 2.8232e-06, 2.7491e-04, 2.0974e-04,
        2.5140e-02, 1.9199e-03, 1.5632e-04, 2.3029e-01, 1.5948e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,223][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0436, 0.0234, 0.0641, 0.0236, 0.0572, 0.0438, 0.0170, 0.0684, 0.0226,
        0.0739, 0.0189, 0.0418, 0.1724, 0.1094, 0.0904, 0.0878, 0.0418],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,225][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1642, 0.1773, 0.1014, 0.0218, 0.1595, 0.0579, 0.0150, 0.0535, 0.0258,
        0.0119, 0.0220, 0.0153, 0.0063, 0.0471, 0.0716, 0.0351, 0.0142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,227][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.2702, 0.0283, 0.2325, 0.0181, 0.0661, 0.0165, 0.0143, 0.0556, 0.0208,
        0.0221, 0.0326, 0.0281, 0.0142, 0.0233, 0.0608, 0.0606, 0.0361],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,228][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([4.1686e-01, 2.7040e-06, 2.9620e-03, 3.4092e-07, 1.4285e-06, 5.6091e-04,
        5.7224e-07, 2.6085e-02, 1.8500e-06, 7.2174e-07, 1.1717e-04, 8.3312e-05,
        4.8913e-02, 2.1792e-03, 1.3499e-04, 4.8446e-01, 1.7642e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,229][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([4.4191e-01, 9.0981e-02, 1.6650e-01, 4.4789e-03, 5.2338e-02, 7.3870e-03,
        9.1524e-04, 9.8365e-03, 2.2529e-04, 2.3713e-03, 3.0332e-03, 1.3025e-03,
        7.1876e-04, 9.7886e-03, 5.5818e-02, 8.1455e-02, 7.0944e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,231][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0402, 0.0270, 0.0739, 0.0287, 0.0426, 0.0616, 0.0313, 0.0560, 0.0204,
        0.0349, 0.0328, 0.0536, 0.1651, 0.0530, 0.0490, 0.0990, 0.1312],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,233][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0356, 0.0142, 0.0451, 0.0177, 0.0325, 0.0602, 0.0264, 0.0379, 0.0302,
        0.0208, 0.0274, 0.0499, 0.1560, 0.1060, 0.0526, 0.1559, 0.0819, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,234][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([1.7511e-04, 9.5974e-03, 1.4170e-03, 3.5630e-02, 2.9339e-02, 3.1805e-02,
        6.0404e-02, 1.3868e-02, 6.0582e-02, 2.3975e-02, 2.1153e-02, 1.4429e-01,
        2.6173e-01, 1.1577e-01, 7.7425e-02, 2.8781e-02, 7.5342e-02, 8.7086e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,236][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0075, 0.0349, 0.0163, 0.1075, 0.0483, 0.0723, 0.1012, 0.0424, 0.0571,
        0.0327, 0.0619, 0.0534, 0.0859, 0.0829, 0.0517, 0.0624, 0.0635, 0.0181],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,238][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([0.0350, 0.0373, 0.0569, 0.0312, 0.0820, 0.0761, 0.0349, 0.0456, 0.0426,
        0.0458, 0.0428, 0.0562, 0.1077, 0.0466, 0.1052, 0.0649, 0.0442, 0.0451],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,239][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0823, 0.0963, 0.0553, 0.0466, 0.0205, 0.0343, 0.0311, 0.0546, 0.0631,
        0.0235, 0.0603, 0.0639, 0.0677, 0.1030, 0.0349, 0.0609, 0.0500, 0.0518],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,241][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([5.9251e-01, 5.8158e-05, 8.5344e-03, 9.8613e-06, 2.8264e-05, 3.8558e-03,
        4.8006e-05, 1.2566e-02, 7.2110e-05, 2.3223e-05, 1.1459e-03, 8.1080e-04,
        2.9349e-02, 5.0561e-03, 6.6186e-04, 1.2662e-01, 2.7247e-02, 1.9140e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,242][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0036, 0.0208, 0.0135, 0.0362, 0.0399, 0.0494, 0.0559, 0.0455, 0.0539,
        0.0593, 0.0185, 0.0936, 0.1309, 0.1111, 0.0891, 0.0571, 0.0709, 0.0508],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,244][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.0857, 0.1123, 0.0935, 0.0224, 0.1427, 0.0591, 0.0218, 0.0597, 0.0334,
        0.0156, 0.0268, 0.0211, 0.0192, 0.0438, 0.0787, 0.0369, 0.0186, 0.1086],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,246][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([0.0387, 0.0315, 0.0718, 0.0325, 0.0581, 0.0628, 0.0493, 0.0727, 0.0556,
        0.0744, 0.0638, 0.0608, 0.0384, 0.0583, 0.0696, 0.0761, 0.0515, 0.0340],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,247][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([4.7168e-01, 6.7794e-05, 6.6491e-03, 5.3472e-06, 6.5897e-06, 2.6615e-03,
        8.8106e-06, 1.5217e-02, 2.0637e-05, 4.4938e-06, 5.2693e-04, 3.1245e-04,
        2.5967e-02, 4.9321e-03, 1.8613e-04, 2.3099e-01, 2.1248e-02, 2.1952e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,249][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([0.1722, 0.0988, 0.1272, 0.0084, 0.0864, 0.0141, 0.0055, 0.0142, 0.0011,
        0.0083, 0.0058, 0.0028, 0.0020, 0.0296, 0.1067, 0.1041, 0.1702, 0.0427],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,251][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([0.0046, 0.0353, 0.0150, 0.0370, 0.0335, 0.0626, 0.0774, 0.0264, 0.0444,
        0.0340, 0.0418, 0.1322, 0.0975, 0.1000, 0.0385, 0.0643, 0.1247, 0.0307],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,253][circuit_model.py][line:2294][INFO] ##10-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.2279, 0.0299, 0.1084, 0.0067, 0.0205, 0.0444, 0.0066, 0.0313, 0.0079,
        0.0076, 0.0161, 0.0181, 0.0540, 0.0883, 0.0305, 0.1323, 0.0366, 0.0708,
        0.0622], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,255][circuit_model.py][line:2297][INFO] ##10-th layer ##Weight##: The head2 weight for token [ to] are: tensor([0.0014, 0.0131, 0.0059, 0.0354, 0.0464, 0.0510, 0.0296, 0.0164, 0.0295,
        0.0120, 0.0225, 0.1325, 0.0884, 0.1542, 0.0610, 0.0396, 0.0748, 0.0198,
        0.1667], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,256][circuit_model.py][line:2300][INFO] ##10-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0683, 0.0329, 0.0743, 0.0792, 0.0818, 0.0607, 0.0343, 0.0471, 0.0430,
        0.0215, 0.0418, 0.0357, 0.0491, 0.0650, 0.0435, 0.0784, 0.0459, 0.0299,
        0.0675], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,258][circuit_model.py][line:2303][INFO] ##10-th layer ##Weight##: The head4 weight for token [ to] are: tensor([0.1219, 0.0638, 0.1189, 0.0200, 0.0891, 0.0376, 0.0126, 0.0270, 0.0150,
        0.0224, 0.0277, 0.0202, 0.0491, 0.0459, 0.0875, 0.0889, 0.0288, 0.0697,
        0.0539], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,260][circuit_model.py][line:2306][INFO] ##10-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.2231, 0.1154, 0.0875, 0.0323, 0.0101, 0.0252, 0.0096, 0.0444, 0.0231,
        0.0125, 0.0490, 0.0275, 0.0353, 0.0611, 0.0187, 0.0850, 0.0230, 0.0640,
        0.0532], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,261][circuit_model.py][line:2309][INFO] ##10-th layer ##Weight##: The head6 weight for token [ to] are: tensor([7.1831e-01, 1.5494e-06, 2.2955e-03, 1.4793e-07, 1.1369e-06, 5.0541e-04,
        7.6159e-07, 4.1805e-03, 2.5586e-06, 7.7179e-07, 1.2299e-04, 6.3760e-05,
        7.3885e-03, 9.4564e-04, 8.2011e-05, 1.1585e-01, 5.8508e-03, 1.2728e-01,
        1.7115e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,262][circuit_model.py][line:2312][INFO] ##10-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0363, 0.0301, 0.0474, 0.0256, 0.0430, 0.0481, 0.0150, 0.0376, 0.0243,
        0.0321, 0.0193, 0.0407, 0.0773, 0.1335, 0.0520, 0.0717, 0.0385, 0.0759,
        0.1518], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,263][circuit_model.py][line:2315][INFO] ##10-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1429, 0.1265, 0.1322, 0.0171, 0.1636, 0.0710, 0.0142, 0.0404, 0.0178,
        0.0081, 0.0182, 0.0164, 0.0058, 0.0436, 0.0658, 0.0385, 0.0130, 0.0492,
        0.0159], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,264][circuit_model.py][line:2318][INFO] ##10-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.3897, 0.0232, 0.2339, 0.0092, 0.0404, 0.0126, 0.0079, 0.0351, 0.0117,
        0.0125, 0.0267, 0.0193, 0.0113, 0.0179, 0.0315, 0.0478, 0.0166, 0.0228,
        0.0298], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,266][circuit_model.py][line:2321][INFO] ##10-th layer ##Weight##: The head10 weight for token [ to] are: tensor([5.1543e-01, 1.1696e-06, 2.3070e-03, 7.0734e-08, 4.2063e-07, 3.2608e-04,
        1.6794e-07, 7.4893e-03, 3.5436e-07, 1.0513e-07, 3.2272e-05, 2.7734e-05,
        8.3407e-03, 6.6200e-04, 3.0670e-05, 1.9260e-01, 6.6851e-03, 2.1161e-01,
        5.4461e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,267][circuit_model.py][line:2324][INFO] ##10-th layer ##Weight##: The head11 weight for token [ to] are: tensor([5.4950e-01, 7.5998e-02, 1.4506e-01, 2.8247e-03, 3.6080e-02, 4.8235e-03,
        6.0868e-04, 6.6316e-03, 1.8379e-04, 1.2451e-03, 2.3746e-03, 8.2600e-04,
        3.6467e-04, 8.5939e-03, 3.0635e-02, 6.2292e-02, 4.5226e-02, 2.0809e-02,
        5.9236e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,269][circuit_model.py][line:2327][INFO] ##10-th layer ##Weight##: The head12 weight for token [ to] are: tensor([0.0508, 0.0262, 0.0763, 0.0238, 0.0419, 0.0440, 0.0376, 0.0422, 0.0139,
        0.0141, 0.0254, 0.0647, 0.0722, 0.0559, 0.0439, 0.0688, 0.1182, 0.0443,
        0.1357], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,390][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:23,391][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,391][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,392][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,394][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,395][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,396][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,398][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,399][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,400][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,401][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,401][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,402][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,403][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.8428, 0.1572], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,403][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.0147, 0.9853], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,404][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0894, 0.9106], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,405][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.4380, 0.5620], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,406][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.2818, 0.7182], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,406][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.9751, 0.0249], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,407][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.2867, 0.7133], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,408][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1527, 0.8473], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,408][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.3979, 0.6021], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,409][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.9771, 0.0229], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,410][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.2465, 0.7535], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,411][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.4513, 0.5487], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,412][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.0859, 0.5199, 0.3942], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,414][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.0050, 0.9193, 0.0756], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,415][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.0685, 0.7023, 0.2292], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,417][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.1913, 0.3546, 0.4541], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,418][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.1059, 0.6712, 0.2229], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,420][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.7440, 0.0243, 0.2317], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,422][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.0314, 0.7026, 0.2660], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,423][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.1979, 0.3666, 0.4355], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,425][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.0850, 0.6056, 0.3094], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,426][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.6729, 0.0384, 0.2887], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,428][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.0801, 0.7322, 0.1877], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,429][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.0107, 0.8757, 0.1136], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,431][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.3119, 0.1352, 0.4703, 0.0826], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,433][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.0127, 0.2064, 0.0786, 0.7023], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,434][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0962, 0.1953, 0.1688, 0.5398], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,436][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([0.2598, 0.1538, 0.4588, 0.1276], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,438][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.1346, 0.3676, 0.2277, 0.2701], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,439][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.8282, 0.0031, 0.1673, 0.0015], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,441][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.0533, 0.2892, 0.2544, 0.4031], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,442][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.1697, 0.4697, 0.2471, 0.1134], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,444][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.3990, 0.1521, 0.3926, 0.0564], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,446][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.6311, 0.0044, 0.3607, 0.0038], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,446][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.2067, 0.5306, 0.2155, 0.0472], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,447][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.0375, 0.2032, 0.2164, 0.5429], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,448][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0614, 0.1292, 0.2577, 0.1708, 0.3809], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,449][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.0017, 0.1166, 0.0222, 0.2320, 0.6276], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,450][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0121, 0.1519, 0.0521, 0.5389, 0.2450], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,452][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0678, 0.1713, 0.2424, 0.1362, 0.3823], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,454][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0517, 0.3465, 0.1487, 0.2872, 0.1660], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,455][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.7915, 0.0035, 0.1920, 0.0027, 0.0104], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,457][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0134, 0.1162, 0.1159, 0.2726, 0.4820], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,458][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0691, 0.2851, 0.1612, 0.0529, 0.4317], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,460][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1943, 0.0684, 0.3423, 0.0632, 0.3318], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,462][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.6782, 0.0040, 0.3048, 0.0037, 0.0093], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,463][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.1407, 0.4931, 0.2044, 0.0484, 0.1134], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,465][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0054, 0.2287, 0.0481, 0.4245, 0.2932], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,466][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.1867, 0.0795, 0.3200, 0.0410, 0.2223, 0.1506], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,468][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([0.0065, 0.0569, 0.0370, 0.2395, 0.4759, 0.1841], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,470][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0696, 0.0736, 0.1301, 0.2894, 0.2125, 0.2249], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,471][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([0.2086, 0.0828, 0.3123, 0.0429, 0.2412, 0.1123], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,473][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.1478, 0.2593, 0.1814, 0.2317, 0.0704, 0.1094], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,474][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([9.4994e-01, 2.7734e-04, 3.2568e-02, 3.7838e-05, 2.6072e-04, 1.6919e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,475][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.0837, 0.0610, 0.2208, 0.0986, 0.3818, 0.1540], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,477][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.1515, 0.2656, 0.1747, 0.0486, 0.2626, 0.0969], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,478][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.3707, 0.0468, 0.3758, 0.0297, 0.1392, 0.0377], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,479][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([9.0669e-01, 7.0306e-04, 6.2207e-02, 6.4338e-05, 4.0781e-04, 2.9924e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,481][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.2918, 0.3536, 0.2246, 0.0323, 0.0799, 0.0178], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,483][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.0769, 0.0846, 0.1605, 0.1530, 0.1655, 0.3594], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,484][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.3122, 0.0598, 0.2570, 0.0206, 0.1233, 0.1671, 0.0600],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,486][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0054, 0.0704, 0.0252, 0.1697, 0.3219, 0.1839, 0.2236],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,487][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1187, 0.0563, 0.1481, 0.1823, 0.1689, 0.2002, 0.1255],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,489][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.3404, 0.0534, 0.2763, 0.0301, 0.1577, 0.1067, 0.0355],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,491][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1778, 0.2758, 0.1462, 0.1444, 0.0311, 0.1411, 0.0836],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,492][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([8.7178e-01, 9.7589e-05, 3.0559e-02, 2.1891e-05, 8.6143e-05, 9.6455e-02,
        1.0023e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,492][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.1259, 0.0796, 0.2052, 0.0783, 0.2169, 0.2140, 0.0802],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,493][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.2536, 0.2261, 0.1524, 0.0307, 0.1889, 0.1111, 0.0373],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,494][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.5500, 0.0278, 0.3376, 0.0114, 0.0493, 0.0137, 0.0101],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,495][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([5.5915e-01, 2.0363e-04, 6.7760e-02, 4.0178e-05, 2.4158e-04, 3.7096e-01,
        1.6533e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,497][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.4535, 0.2785, 0.1941, 0.0177, 0.0349, 0.0191, 0.0022],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,498][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0560, 0.0949, 0.1156, 0.1243, 0.1298, 0.3309, 0.1485],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:23,500][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.0841, 0.0661, 0.1637, 0.0466, 0.1461, 0.2062, 0.0956, 0.1916],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,502][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([0.0005, 0.0486, 0.0051, 0.1570, 0.2245, 0.1345, 0.3419, 0.0879],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,503][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.0117, 0.0869, 0.0298, 0.2137, 0.1068, 0.2321, 0.1908, 0.1283],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,505][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([0.0599, 0.0653, 0.1311, 0.0644, 0.2093, 0.2044, 0.1082, 0.1574],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,507][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.1612, 0.1969, 0.1060, 0.0824, 0.0297, 0.1089, 0.0730, 0.2417],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,508][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([8.7051e-01, 3.3841e-05, 1.2157e-02, 4.0601e-06, 2.3981e-05, 1.0473e-02,
        6.7701e-05, 1.0673e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,509][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.0124, 0.0488, 0.0595, 0.1102, 0.2201, 0.1759, 0.1723, 0.2009],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,511][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.1476, 0.1534, 0.1543, 0.0268, 0.1793, 0.1226, 0.0417, 0.1743],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,513][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.0572, 0.0906, 0.1331, 0.0666, 0.1666, 0.1301, 0.1246, 0.2311],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,514][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([6.8221e-01, 8.2813e-05, 1.3462e-02, 5.2529e-06, 1.5521e-05, 1.5952e-02,
        3.9788e-05, 2.8823e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,516][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.1202, 0.5264, 0.1156, 0.0475, 0.1163, 0.0535, 0.0157, 0.0049],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,517][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([0.0051, 0.0634, 0.0226, 0.0967, 0.0904, 0.3286, 0.3063, 0.0868],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:23,519][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.2818, 0.0364, 0.2692, 0.0133, 0.0745, 0.1233, 0.0252, 0.1408, 0.0355],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,521][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0033, 0.0519, 0.0201, 0.1214, 0.1908, 0.1610, 0.1512, 0.0987, 0.2017],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,522][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0661, 0.0668, 0.1135, 0.1476, 0.1555, 0.1680, 0.0827, 0.1186, 0.0812],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,524][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1552, 0.0574, 0.2495, 0.0264, 0.1799, 0.0899, 0.0358, 0.1257, 0.0802],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,525][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0660, 0.1386, 0.0855, 0.0793, 0.0242, 0.1141, 0.0915, 0.2454, 0.1555],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,527][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([1.2960e-01, 3.9322e-05, 9.5836e-03, 9.6760e-06, 6.0367e-05, 3.3997e-02,
        6.7649e-04, 8.2568e-01, 3.5921e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,528][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0569, 0.0427, 0.1424, 0.0661, 0.2243, 0.1937, 0.0557, 0.1065, 0.1118],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,530][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0970, 0.1588, 0.1419, 0.0296, 0.1829, 0.0956, 0.0489, 0.1396, 0.1057],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,532][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.3586, 0.0358, 0.3567, 0.0117, 0.0907, 0.0220, 0.0198, 0.0764, 0.0282],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,533][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([1.6301e-02, 1.7446e-05, 3.4687e-03, 6.3844e-06, 3.4555e-05, 1.7982e-02,
        1.7461e-04, 9.6195e-01, 6.7746e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,535][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.2814, 0.4442, 0.1582, 0.0230, 0.0553, 0.0309, 0.0036, 0.0024, 0.0010],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,536][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0229, 0.0445, 0.1046, 0.0707, 0.1538, 0.2344, 0.1861, 0.0860, 0.0970],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:23,538][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([0.1393, 0.0322, 0.1349, 0.0186, 0.0835, 0.1241, 0.0543, 0.2454, 0.0578,
        0.1100], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,540][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([0.0012, 0.0270, 0.0075, 0.0762, 0.2282, 0.0787, 0.1687, 0.0931, 0.2363,
        0.0829], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,542][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.0211, 0.0602, 0.0358, 0.2055, 0.0827, 0.1230, 0.1226, 0.1350, 0.1363,
        0.0780], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,542][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([0.1111, 0.0582, 0.1465, 0.0441, 0.1337, 0.1304, 0.0556, 0.1562, 0.0931,
        0.0711], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,543][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.0770, 0.1835, 0.0642, 0.0979, 0.0230, 0.0804, 0.0682, 0.1925, 0.1779,
        0.0354], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,544][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([6.6179e-02, 9.1614e-06, 3.6834e-03, 5.9122e-06, 1.9381e-05, 2.0880e-02,
        1.2260e-03, 9.0731e-01, 5.6270e-04, 1.2201e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,545][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.0283, 0.0305, 0.0562, 0.0389, 0.1064, 0.0782, 0.0529, 0.2100, 0.0885,
        0.3101], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,547][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.1715, 0.1708, 0.1247, 0.0213, 0.1596, 0.1046, 0.0347, 0.1326, 0.0415,
        0.0387], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,548][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.4927, 0.0184, 0.3196, 0.0067, 0.0519, 0.0237, 0.0110, 0.0495, 0.0141,
        0.0123], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,549][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([2.9691e-03, 4.4177e-06, 3.4868e-04, 1.1534e-06, 2.1035e-06, 9.5178e-03,
        1.4891e-04, 9.8689e-01, 9.0993e-05, 2.2636e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,551][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.2349, 0.4623, 0.1496, 0.0317, 0.0676, 0.0387, 0.0071, 0.0044, 0.0018,
        0.0018], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,553][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([0.0118, 0.0384, 0.0432, 0.0589, 0.0755, 0.2606, 0.1874, 0.1021, 0.1283,
        0.0938], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:23,554][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0614, 0.0544, 0.1250, 0.0403, 0.0813, 0.1543, 0.0880, 0.1549, 0.0786,
        0.0929, 0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,556][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([0.0006, 0.0163, 0.0066, 0.1017, 0.1406, 0.1083, 0.2118, 0.0609, 0.1998,
        0.0791, 0.0741], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,558][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0156, 0.0300, 0.0420, 0.1674, 0.1280, 0.1352, 0.1241, 0.0794, 0.1223,
        0.0697, 0.0863], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,559][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([0.0861, 0.0546, 0.1514, 0.0491, 0.1236, 0.0802, 0.0698, 0.1153, 0.1066,
        0.0838, 0.0795], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,561][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0679, 0.1030, 0.0829, 0.0854, 0.0348, 0.1022, 0.0692, 0.1937, 0.0984,
        0.0839, 0.0786], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,562][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([4.4333e-01, 1.1502e-04, 2.1722e-02, 4.5267e-05, 2.0509e-04, 2.4802e-02,
        9.0572e-04, 4.9921e-01, 7.7411e-04, 4.0731e-04, 8.4856e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,564][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.0156, 0.0270, 0.0588, 0.0662, 0.1415, 0.1000, 0.1060, 0.1385, 0.1270,
        0.1780, 0.0415], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,566][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.0533, 0.1115, 0.0940, 0.0278, 0.1968, 0.1073, 0.0581, 0.1842, 0.0889,
        0.0390, 0.0391], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,568][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.0955, 0.0386, 0.2002, 0.0391, 0.1348, 0.0497, 0.0671, 0.1370, 0.0741,
        0.0906, 0.0731], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,569][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([1.5162e-01, 7.5163e-05, 1.3434e-02, 2.8032e-05, 1.1965e-04, 1.3628e-02,
        2.3771e-04, 8.1724e-01, 1.8265e-04, 1.3848e-04, 3.2958e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,571][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.1870, 0.4519, 0.1471, 0.0519, 0.0866, 0.0492, 0.0129, 0.0045, 0.0021,
        0.0015, 0.0053], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,572][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([0.0128, 0.0348, 0.0508, 0.0816, 0.1162, 0.1745, 0.1944, 0.0836, 0.0589,
        0.0796, 0.1126], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:23,574][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1004, 0.0494, 0.1298, 0.0242, 0.0799, 0.1079, 0.0565, 0.1864, 0.0341,
        0.1146, 0.0506, 0.0662], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,576][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0009, 0.0246, 0.0050, 0.0999, 0.1086, 0.0551, 0.0942, 0.0445, 0.0818,
        0.0511, 0.0515, 0.3826], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,578][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0406, 0.0548, 0.0573, 0.1269, 0.0998, 0.1038, 0.0686, 0.1142, 0.0999,
        0.0652, 0.0851, 0.0839], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,579][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0814, 0.1040, 0.1139, 0.0481, 0.1778, 0.0927, 0.0360, 0.0818, 0.0591,
        0.0774, 0.0613, 0.0663], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,581][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1333, 0.1613, 0.0944, 0.0643, 0.0213, 0.0436, 0.0334, 0.1432, 0.0682,
        0.0523, 0.1013, 0.0835], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,582][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([7.8736e-01, 1.0236e-05, 8.6558e-03, 2.0819e-06, 1.0667e-05, 4.4344e-03,
        2.7899e-05, 1.9626e-01, 6.8005e-05, 2.2326e-05, 1.3739e-03, 1.7710e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,584][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0210, 0.0464, 0.0492, 0.0526, 0.0810, 0.0814, 0.0637, 0.1238, 0.0838,
        0.2173, 0.0409, 0.1388], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,586][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1306, 0.2093, 0.0880, 0.0253, 0.2207, 0.0707, 0.0207, 0.1180, 0.0459,
        0.0146, 0.0319, 0.0242], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,587][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2802, 0.0668, 0.2874, 0.0184, 0.0768, 0.0236, 0.0191, 0.0849, 0.0306,
        0.0378, 0.0358, 0.0385], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,588][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([2.9285e-01, 7.6733e-06, 5.8097e-03, 1.4380e-06, 9.4232e-06, 3.8372e-03,
        1.5335e-05, 6.9484e-01, 2.5676e-05, 1.1404e-05, 1.1670e-03, 1.4303e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,588][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3731, 0.3589, 0.1630, 0.0235, 0.0436, 0.0259, 0.0028, 0.0025, 0.0006,
        0.0006, 0.0028, 0.0027], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,589][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0240, 0.0255, 0.0708, 0.0676, 0.1211, 0.1380, 0.0751, 0.0923, 0.0450,
        0.0806, 0.0683, 0.1917], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:23,591][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.0685, 0.0500, 0.0786, 0.0211, 0.0399, 0.0873, 0.0447, 0.1003, 0.0350,
        0.0513, 0.0407, 0.0581, 0.3246], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,593][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([0.0007, 0.0248, 0.0041, 0.0514, 0.0546, 0.0451, 0.0861, 0.0418, 0.0907,
        0.0311, 0.0485, 0.2546, 0.2666], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,594][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.0128, 0.0401, 0.0292, 0.1726, 0.0822, 0.1225, 0.1157, 0.0544, 0.0709,
        0.0538, 0.0568, 0.0546, 0.1345], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,596][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([0.0681, 0.0763, 0.0822, 0.0314, 0.0904, 0.0980, 0.0406, 0.0795, 0.0710,
        0.0443, 0.0892, 0.0825, 0.1465], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,598][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0991, 0.2053, 0.0657, 0.0693, 0.0122, 0.0395, 0.0426, 0.0758, 0.0832,
        0.0350, 0.1000, 0.0649, 0.1075], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,599][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([9.3713e-01, 5.2265e-06, 3.4280e-03, 1.1537e-06, 1.7714e-06, 1.4976e-03,
        8.1533e-06, 1.6637e-02, 1.6279e-05, 2.5985e-06, 4.9589e-04, 4.1602e-04,
        4.0362e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,601][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([0.0271, 0.0492, 0.0601, 0.0334, 0.0788, 0.0754, 0.0514, 0.1152, 0.0519,
        0.1402, 0.0242, 0.1040, 0.1890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,602][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.1110, 0.2043, 0.0809, 0.0201, 0.2097, 0.1237, 0.0269, 0.0962, 0.0311,
        0.0149, 0.0375, 0.0265, 0.0171], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,604][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.3531, 0.0313, 0.3250, 0.0090, 0.0641, 0.0210, 0.0152, 0.0577, 0.0249,
        0.0240, 0.0363, 0.0237, 0.0147], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,605][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([7.5063e-01, 7.4485e-06, 4.5385e-03, 1.0144e-06, 1.7328e-06, 2.7509e-03,
        2.6681e-06, 7.8825e-02, 8.8066e-06, 1.9723e-06, 4.2059e-04, 2.5306e-04,
        1.6256e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,607][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([2.6101e-01, 5.8454e-01, 1.0240e-01, 1.2474e-02, 2.0132e-02, 1.1873e-02,
        2.6199e-03, 1.1042e-03, 3.5094e-04, 3.3758e-04, 1.7294e-03, 1.3853e-03,
        3.8863e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,608][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([0.0050, 0.0242, 0.0198, 0.0400, 0.0355, 0.1032, 0.1458, 0.0447, 0.0615,
        0.0487, 0.0505, 0.2387, 0.1824], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:23,610][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.1516, 0.0362, 0.1170, 0.0139, 0.0582, 0.0598, 0.0221, 0.0666, 0.0304,
        0.0403, 0.0239, 0.0273, 0.1982, 0.1544], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,612][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([0.0007, 0.0100, 0.0035, 0.0438, 0.0746, 0.0276, 0.0625, 0.0292, 0.0624,
        0.0441, 0.0238, 0.1681, 0.2064, 0.2434], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,613][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0176, 0.0504, 0.0368, 0.1063, 0.0700, 0.0884, 0.0763, 0.0629, 0.0856,
        0.0558, 0.0563, 0.0733, 0.1082, 0.1122], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,615][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([0.0749, 0.0502, 0.0992, 0.0263, 0.1462, 0.0598, 0.0307, 0.0502, 0.0563,
        0.0880, 0.0449, 0.0464, 0.1380, 0.0890], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,617][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.1284, 0.1594, 0.0865, 0.0750, 0.0219, 0.0377, 0.0315, 0.0891, 0.0752,
        0.0406, 0.0593, 0.0481, 0.0749, 0.0722], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,618][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([7.9244e-01, 1.4972e-04, 1.8920e-02, 1.7813e-05, 7.3035e-05, 1.2884e-02,
        2.2746e-04, 9.4572e-02, 3.0436e-04, 6.1770e-05, 2.2967e-03, 3.0890e-03,
        6.5172e-02, 9.7949e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,620][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([0.0587, 0.0386, 0.0849, 0.0261, 0.0720, 0.0535, 0.0268, 0.0697, 0.0504,
        0.1093, 0.0131, 0.0329, 0.1566, 0.2074], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,622][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0752, 0.2637, 0.0686, 0.0302, 0.2418, 0.0715, 0.0202, 0.0648, 0.0341,
        0.0182, 0.0168, 0.0177, 0.0060, 0.0714], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,623][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.2446, 0.0565, 0.2356, 0.0199, 0.1045, 0.0258, 0.0244, 0.0733, 0.0376,
        0.0356, 0.0339, 0.0403, 0.0231, 0.0449], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,625][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([8.1202e-01, 1.3285e-04, 1.7797e-02, 9.0652e-06, 2.5840e-05, 6.1882e-03,
        3.1788e-05, 1.0096e-01, 3.6451e-05, 1.0878e-05, 6.7099e-04, 6.3506e-04,
        5.6781e-02, 4.7047e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,626][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([3.5570e-01, 3.4684e-01, 1.5016e-01, 1.9554e-02, 5.3083e-02, 2.4412e-02,
        3.4757e-03, 2.2449e-03, 8.1631e-04, 6.4268e-04, 2.2964e-03, 2.2649e-03,
        1.6666e-04, 3.8339e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,628][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([0.0558, 0.0149, 0.1430, 0.0377, 0.1420, 0.1151, 0.0739, 0.0825, 0.0548,
        0.0378, 0.0188, 0.0785, 0.0882, 0.0569], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:23,629][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0097, 0.0156, 0.0261, 0.0207, 0.0266, 0.0825, 0.0619, 0.0623, 0.0365,
        0.0281, 0.0335, 0.0670, 0.3051, 0.1599, 0.0643], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,631][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([1.3465e-04, 8.8323e-03, 1.5435e-03, 2.5963e-02, 3.9064e-02, 2.8321e-02,
        5.5736e-02, 1.8321e-02, 5.7449e-02, 1.7135e-02, 2.1946e-02, 1.9699e-01,
        2.5027e-01, 1.6286e-01, 1.1545e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,632][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0026, 0.0355, 0.0101, 0.1272, 0.0473, 0.1127, 0.1088, 0.0386, 0.0734,
        0.0309, 0.0670, 0.0685, 0.1232, 0.1016, 0.0526], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,633][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([0.0103, 0.0325, 0.0366, 0.0318, 0.0743, 0.0930, 0.0509, 0.0692, 0.0670,
        0.0510, 0.0560, 0.0942, 0.1502, 0.0671, 0.1158], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,633][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0309, 0.0895, 0.0464, 0.0506, 0.0350, 0.0473, 0.0449, 0.0944, 0.0893,
        0.0521, 0.0499, 0.0773, 0.0906, 0.1318, 0.0700], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,634][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([1.3789e-01, 1.1671e-05, 5.2349e-03, 6.4038e-06, 3.6157e-05, 6.6683e-03,
        2.0978e-04, 1.8846e-01, 3.1635e-04, 1.6872e-04, 3.7333e-03, 7.6979e-03,
        6.2927e-01, 1.5738e-02, 4.5568e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,636][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.0019, 0.0153, 0.0127, 0.0296, 0.0485, 0.0324, 0.0554, 0.0664, 0.0374,
        0.0835, 0.0138, 0.0877, 0.2411, 0.1448, 0.1297], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,638][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.0528, 0.1575, 0.0775, 0.0277, 0.2562, 0.0810, 0.0343, 0.0643, 0.0263,
        0.0144, 0.0214, 0.0206, 0.0112, 0.0495, 0.1052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,639][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.0986, 0.0183, 0.1406, 0.0236, 0.1255, 0.0554, 0.0423, 0.0784, 0.0372,
        0.0364, 0.0487, 0.0516, 0.0466, 0.0451, 0.1519], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,641][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([3.8449e-02, 4.4275e-06, 2.0135e-03, 2.3215e-06, 7.7288e-06, 2.3715e-03,
        2.9927e-05, 1.6879e-01, 4.4210e-05, 3.1923e-05, 9.4274e-04, 2.3180e-03,
        7.6926e-01, 1.4803e-02, 9.2813e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,643][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.1474, 0.3244, 0.1548, 0.0386, 0.0707, 0.0681, 0.0237, 0.0091, 0.0045,
        0.0030, 0.0119, 0.0160, 0.0023, 0.0836, 0.0419], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,644][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.0013, 0.0220, 0.0073, 0.0458, 0.0321, 0.0818, 0.1068, 0.0307, 0.0553,
        0.0263, 0.0685, 0.1985, 0.2115, 0.0726, 0.0396], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:23,646][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0297, 0.0214, 0.0607, 0.0233, 0.0311, 0.0643, 0.0386, 0.0610, 0.0421,
        0.0427, 0.0251, 0.0480, 0.1922, 0.1524, 0.0470, 0.1204],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,648][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([0.0004, 0.0183, 0.0032, 0.0552, 0.0358, 0.0299, 0.0513, 0.0233, 0.0561,
        0.0269, 0.0271, 0.1830, 0.1380, 0.2624, 0.0572, 0.0318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,650][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0121, 0.0326, 0.0357, 0.1065, 0.0652, 0.1021, 0.0715, 0.0595, 0.0666,
        0.0565, 0.0544, 0.0578, 0.0527, 0.0983, 0.0442, 0.0844],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,651][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([0.0535, 0.0663, 0.0976, 0.0230, 0.1017, 0.0452, 0.0266, 0.0516, 0.0378,
        0.0566, 0.0351, 0.0431, 0.0996, 0.0531, 0.1213, 0.0880],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,653][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0824, 0.0949, 0.0689, 0.0426, 0.0199, 0.0406, 0.0291, 0.1024, 0.0641,
        0.0405, 0.0488, 0.0633, 0.0791, 0.1055, 0.0350, 0.0830],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,654][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([7.9307e-01, 9.5415e-06, 4.8709e-03, 1.9520e-06, 7.2506e-06, 1.2478e-03,
        8.1603e-06, 1.1453e-02, 1.8512e-05, 1.2796e-05, 4.4319e-04, 3.4395e-04,
        3.3285e-02, 2.7300e-03, 3.1793e-04, 1.5218e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,656][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([0.0058, 0.0290, 0.0250, 0.0401, 0.0637, 0.0398, 0.0362, 0.0497, 0.0369,
        0.1188, 0.0202, 0.0642, 0.1338, 0.2044, 0.0857, 0.0466],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,658][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.0819, 0.1332, 0.0975, 0.0223, 0.2076, 0.0793, 0.0269, 0.0602, 0.0299,
        0.0128, 0.0177, 0.0186, 0.0126, 0.0501, 0.1053, 0.0441],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,660][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.1015, 0.0324, 0.1582, 0.0240, 0.1057, 0.0305, 0.0427, 0.0761, 0.0428,
        0.0633, 0.0476, 0.0578, 0.0270, 0.0469, 0.0695, 0.0742],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,661][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([7.3800e-01, 1.0192e-05, 5.5304e-03, 1.8898e-06, 6.7991e-06, 6.3045e-04,
        1.4824e-06, 1.6606e-02, 5.4640e-06, 2.4215e-06, 1.8721e-04, 1.0655e-04,
        2.3478e-02, 2.8491e-03, 2.3891e-04, 2.1234e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,662][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([2.2091e-01, 3.5195e-01, 1.6876e-01, 3.0930e-02, 6.4611e-02, 2.6741e-02,
        7.1276e-03, 2.9021e-03, 1.3415e-03, 1.3971e-03, 3.0029e-03, 3.9961e-03,
        2.2320e-04, 6.6356e-02, 1.3534e-02, 3.6224e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,664][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([0.0197, 0.0417, 0.0363, 0.0357, 0.0571, 0.0865, 0.0609, 0.0653, 0.0274,
        0.0390, 0.0334, 0.0940, 0.1911, 0.0719, 0.0436, 0.0963],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:23,666][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1330, 0.0339, 0.0959, 0.0131, 0.0325, 0.0529, 0.0134, 0.0583, 0.0117,
        0.0214, 0.0234, 0.0259, 0.1283, 0.0961, 0.0540, 0.1421, 0.0644],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,668][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0006, 0.0122, 0.0038, 0.0549, 0.0475, 0.0319, 0.0397, 0.0198, 0.0386,
        0.0236, 0.0261, 0.1778, 0.1353, 0.1750, 0.0814, 0.0257, 0.1060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,669][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0378, 0.0281, 0.0545, 0.0949, 0.0667, 0.0582, 0.0420, 0.0586, 0.0484,
        0.0441, 0.0563, 0.0423, 0.0886, 0.0714, 0.0540, 0.0837, 0.0704],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,671][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0775, 0.0402, 0.0972, 0.0241, 0.1071, 0.0479, 0.0178, 0.0512, 0.0376,
        0.0482, 0.0419, 0.0363, 0.1008, 0.0467, 0.1051, 0.0863, 0.0340],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,673][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.2142, 0.0947, 0.0886, 0.0407, 0.0107, 0.0276, 0.0138, 0.0774, 0.0298,
        0.0316, 0.0552, 0.0406, 0.0545, 0.0724, 0.0207, 0.0955, 0.0320],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,674][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([7.0696e-01, 3.1592e-06, 3.3510e-03, 6.3115e-07, 2.1490e-06, 9.5442e-04,
        3.0090e-06, 1.4773e-02, 9.8247e-06, 2.8232e-06, 2.7491e-04, 2.0974e-04,
        2.5140e-02, 1.9199e-03, 1.5632e-04, 2.3029e-01, 1.5948e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,676][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0436, 0.0234, 0.0641, 0.0236, 0.0572, 0.0438, 0.0170, 0.0684, 0.0226,
        0.0739, 0.0189, 0.0418, 0.1724, 0.1094, 0.0904, 0.0878, 0.0418],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,677][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1642, 0.1773, 0.1014, 0.0218, 0.1595, 0.0579, 0.0150, 0.0535, 0.0258,
        0.0119, 0.0220, 0.0153, 0.0063, 0.0471, 0.0716, 0.0351, 0.0142],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,678][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2702, 0.0283, 0.2325, 0.0181, 0.0661, 0.0165, 0.0143, 0.0556, 0.0208,
        0.0221, 0.0326, 0.0281, 0.0142, 0.0233, 0.0608, 0.0606, 0.0361],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,679][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([4.1686e-01, 2.7040e-06, 2.9620e-03, 3.4092e-07, 1.4285e-06, 5.6091e-04,
        5.7224e-07, 2.6085e-02, 1.8500e-06, 7.2174e-07, 1.1717e-04, 8.3312e-05,
        4.8913e-02, 2.1792e-03, 1.3499e-04, 4.8446e-01, 1.7642e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,680][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([3.8278e-01, 2.6627e-01, 1.5491e-01, 1.5822e-02, 2.8984e-02, 1.8546e-02,
        1.6495e-03, 1.6714e-03, 3.5687e-04, 3.5158e-04, 2.0736e-03, 1.9007e-03,
        1.1092e-04, 2.2745e-02, 7.2439e-03, 2.9029e-02, 6.5558e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,682][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0402, 0.0270, 0.0739, 0.0287, 0.0426, 0.0616, 0.0313, 0.0560, 0.0204,
        0.0349, 0.0328, 0.0536, 0.1651, 0.0530, 0.0490, 0.0990, 0.1312],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:23,683][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0356, 0.0142, 0.0451, 0.0177, 0.0325, 0.0602, 0.0264, 0.0379, 0.0302,
        0.0208, 0.0274, 0.0499, 0.1560, 0.1060, 0.0526, 0.1559, 0.0819, 0.0496],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,685][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([1.7511e-04, 9.5974e-03, 1.4170e-03, 3.5630e-02, 2.9339e-02, 3.1805e-02,
        6.0404e-02, 1.3868e-02, 6.0582e-02, 2.3975e-02, 2.1153e-02, 1.4429e-01,
        2.6173e-01, 1.1577e-01, 7.7425e-02, 2.8781e-02, 7.5342e-02, 8.7086e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,686][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.0075, 0.0349, 0.0163, 0.1075, 0.0483, 0.0723, 0.1012, 0.0424, 0.0571,
        0.0327, 0.0619, 0.0534, 0.0859, 0.0829, 0.0517, 0.0624, 0.0635, 0.0181],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,688][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([0.0350, 0.0373, 0.0569, 0.0312, 0.0820, 0.0761, 0.0349, 0.0456, 0.0426,
        0.0458, 0.0428, 0.0562, 0.1077, 0.0466, 0.1052, 0.0649, 0.0442, 0.0451],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,690][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0823, 0.0963, 0.0553, 0.0466, 0.0205, 0.0343, 0.0311, 0.0546, 0.0631,
        0.0235, 0.0603, 0.0639, 0.0677, 0.1030, 0.0349, 0.0609, 0.0500, 0.0518],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,691][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([5.9251e-01, 5.8158e-05, 8.5344e-03, 9.8613e-06, 2.8264e-05, 3.8558e-03,
        4.8006e-05, 1.2566e-02, 7.2110e-05, 2.3223e-05, 1.1459e-03, 8.1080e-04,
        2.9349e-02, 5.0561e-03, 6.6186e-04, 1.2662e-01, 2.7247e-02, 1.9140e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,693][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([0.0036, 0.0208, 0.0135, 0.0362, 0.0399, 0.0494, 0.0559, 0.0455, 0.0539,
        0.0593, 0.0185, 0.0936, 0.1309, 0.1111, 0.0891, 0.0571, 0.0709, 0.0508],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,695][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.0857, 0.1123, 0.0935, 0.0224, 0.1427, 0.0591, 0.0218, 0.0597, 0.0334,
        0.0156, 0.0268, 0.0211, 0.0192, 0.0438, 0.0787, 0.0369, 0.0186, 0.1086],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,697][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.0387, 0.0315, 0.0718, 0.0325, 0.0581, 0.0628, 0.0493, 0.0727, 0.0556,
        0.0744, 0.0638, 0.0608, 0.0384, 0.0583, 0.0696, 0.0761, 0.0515, 0.0340],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,698][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([4.7168e-01, 6.7794e-05, 6.6491e-03, 5.3472e-06, 6.5897e-06, 2.6615e-03,
        8.8106e-06, 1.5217e-02, 2.0637e-05, 4.4938e-06, 5.2693e-04, 3.1245e-04,
        2.5967e-02, 4.9321e-03, 1.8613e-04, 2.3099e-01, 2.1248e-02, 2.1952e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,700][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([0.0782, 0.3785, 0.0836, 0.0347, 0.0587, 0.0397, 0.0140, 0.0027, 0.0023,
        0.0019, 0.0050, 0.0053, 0.0005, 0.0592, 0.0180, 0.0407, 0.1705, 0.0065],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,702][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([0.0046, 0.0353, 0.0150, 0.0370, 0.0335, 0.0626, 0.0774, 0.0264, 0.0444,
        0.0340, 0.0418, 0.1322, 0.0975, 0.1000, 0.0385, 0.0643, 0.1247, 0.0307],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:23,704][circuit_model.py][line:2332][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.2279, 0.0299, 0.1084, 0.0067, 0.0205, 0.0444, 0.0066, 0.0313, 0.0079,
        0.0076, 0.0161, 0.0181, 0.0540, 0.0883, 0.0305, 0.1323, 0.0366, 0.0708,
        0.0622], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,705][circuit_model.py][line:2335][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([0.0014, 0.0131, 0.0059, 0.0354, 0.0464, 0.0510, 0.0296, 0.0164, 0.0295,
        0.0120, 0.0225, 0.1325, 0.0884, 0.1542, 0.0610, 0.0396, 0.0748, 0.0198,
        0.1667], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,707][circuit_model.py][line:2338][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0683, 0.0329, 0.0743, 0.0792, 0.0818, 0.0607, 0.0343, 0.0471, 0.0430,
        0.0215, 0.0418, 0.0357, 0.0491, 0.0650, 0.0435, 0.0784, 0.0459, 0.0299,
        0.0675], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,709][circuit_model.py][line:2341][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([0.1219, 0.0638, 0.1189, 0.0200, 0.0891, 0.0376, 0.0126, 0.0270, 0.0150,
        0.0224, 0.0277, 0.0202, 0.0491, 0.0459, 0.0875, 0.0889, 0.0288, 0.0697,
        0.0539], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,711][circuit_model.py][line:2344][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.2231, 0.1154, 0.0875, 0.0323, 0.0101, 0.0252, 0.0096, 0.0444, 0.0231,
        0.0125, 0.0490, 0.0275, 0.0353, 0.0611, 0.0187, 0.0850, 0.0230, 0.0640,
        0.0532], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,712][circuit_model.py][line:2347][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([7.1831e-01, 1.5494e-06, 2.2955e-03, 1.4793e-07, 1.1369e-06, 5.0541e-04,
        7.6159e-07, 4.1805e-03, 2.5586e-06, 7.7179e-07, 1.2299e-04, 6.3760e-05,
        7.3885e-03, 9.4564e-04, 8.2011e-05, 1.1585e-01, 5.8508e-03, 1.2728e-01,
        1.7115e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,714][circuit_model.py][line:2350][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([0.0363, 0.0301, 0.0474, 0.0256, 0.0430, 0.0481, 0.0150, 0.0376, 0.0243,
        0.0321, 0.0193, 0.0407, 0.0773, 0.1335, 0.0520, 0.0717, 0.0385, 0.0759,
        0.1518], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,716][circuit_model.py][line:2353][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1429, 0.1265, 0.1322, 0.0171, 0.1636, 0.0710, 0.0142, 0.0404, 0.0178,
        0.0081, 0.0182, 0.0164, 0.0058, 0.0436, 0.0658, 0.0385, 0.0130, 0.0492,
        0.0159], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,718][circuit_model.py][line:2356][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([0.3897, 0.0232, 0.2339, 0.0092, 0.0404, 0.0126, 0.0079, 0.0351, 0.0117,
        0.0125, 0.0267, 0.0193, 0.0113, 0.0179, 0.0315, 0.0478, 0.0166, 0.0228,
        0.0298], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,719][circuit_model.py][line:2359][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([5.1543e-01, 1.1696e-06, 2.3070e-03, 7.0734e-08, 4.2063e-07, 3.2608e-04,
        1.6794e-07, 7.4893e-03, 3.5436e-07, 1.0513e-07, 3.2272e-05, 2.7734e-05,
        8.3407e-03, 6.6200e-04, 3.0670e-05, 1.9260e-01, 6.6851e-03, 2.1161e-01,
        5.4461e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,720][circuit_model.py][line:2362][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([4.0683e-01, 3.1651e-01, 1.2571e-01, 1.3133e-02, 2.1905e-02, 1.5059e-02,
        1.3673e-03, 1.1849e-03, 3.4756e-04, 2.2758e-04, 1.7697e-03, 1.2829e-03,
        6.8025e-05, 2.4250e-02, 3.9515e-03, 2.2511e-02, 3.6748e-02, 3.4185e-03,
        3.7218e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,721][circuit_model.py][line:2365][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([0.0508, 0.0262, 0.0763, 0.0238, 0.0419, 0.0440, 0.0376, 0.0422, 0.0139,
        0.0141, 0.0254, 0.0647, 0.0722, 0.0559, 0.0439, 0.0688, 0.1182, 0.0443,
        0.1357], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:23,725][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:23,727][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[2798],
        [ 163],
        [1688],
        [ 210],
        [ 368],
        [ 134],
        [  64],
        [  43],
        [  47],
        [  55],
        [  45],
        [  28],
        [  38],
        [  35],
        [ 239],
        [  76],
        [  19],
        [  35],
        [   7]], device='cuda:0')
[2024-07-24 10:25:23,729][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[1481],
        [ 106],
        [ 827],
        [  84],
        [  83],
        [  67],
        [  31],
        [  14],
        [  21],
        [  21],
        [  25],
        [  24],
        [  15],
        [  33],
        [ 114],
        [  62],
        [  17],
        [  10],
        [   7]], device='cuda:0')
[2024-07-24 10:25:23,731][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[20228],
        [ 9735],
        [37838],
        [49796],
        [22000],
        [31552],
        [30117],
        [16010],
        [36243],
        [20671],
        [13555],
        [16021],
        [ 4685],
        [ 6557],
        [ 2815],
        [ 3294],
        [ 4170],
        [ 2536],
        [ 5097]], device='cuda:0')
[2024-07-24 10:25:23,733][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[32344],
        [14748],
        [17173],
        [24024],
        [26724],
        [25523],
        [24782],
        [24584],
        [23510],
        [22335],
        [21994],
        [18302],
        [ 8887],
        [10233],
        [ 8717],
        [11938],
        [14470],
        [ 9698],
        [14728]], device='cuda:0')
[2024-07-24 10:25:23,734][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[16161],
        [20014],
        [36838],
        [35177],
        [27929],
        [29057],
        [30223],
        [19381],
        [22639],
        [15939],
        [18292],
        [20360],
        [15974],
        [17100],
        [16812],
        [17596],
        [18906],
        [17894],
        [20424]], device='cuda:0')
[2024-07-24 10:25:23,736][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[ 8006],
        [20803],
        [21688],
        [19146],
        [18869],
        [16934],
        [14551],
        [13662],
        [15502],
        [14997],
        [14819],
        [15401],
        [15221],
        [15652],
        [14441],
        [13032],
        [12358],
        [12120],
        [10865]], device='cuda:0')
[2024-07-24 10:25:23,738][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[21516],
        [22138],
        [21952],
        [23178],
        [27648],
        [26924],
        [26387],
        [21285],
        [20581],
        [21417],
        [24313],
        [23996],
        [26684],
        [24469],
        [26089],
        [25856],
        [25798],
        [26093],
        [26129]], device='cuda:0')
[2024-07-24 10:25:23,740][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[25471],
        [23246],
        [26531],
        [27845],
        [29149],
        [26513],
        [27653],
        [26703],
        [25585],
        [25492],
        [26298],
        [26626],
        [27953],
        [29355],
        [31659],
        [30708],
        [30390],
        [21321],
        [22515]], device='cuda:0')
[2024-07-24 10:25:23,742][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[ 8359],
        [ 8674],
        [14734],
        [18854],
        [16934],
        [21261],
        [18922],
        [14792],
        [15369],
        [16348],
        [12609],
        [10120],
        [ 9890],
        [ 7658],
        [ 5987],
        [ 6109],
        [ 7664],
        [ 5849],
        [ 6305]], device='cuda:0')
[2024-07-24 10:25:23,744][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[36035],
        [45347],
        [10736],
        [32797],
        [46364],
        [45174],
        [44733],
        [47482],
        [48145],
        [48590],
        [49366],
        [48996],
        [49112],
        [49074],
        [49111],
        [49099],
        [48709],
        [49199],
        [48422]], device='cuda:0')
[2024-07-24 10:25:23,746][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[41608],
        [ 4360],
        [ 3179],
        [ 6384],
        [10449],
        [12148],
        [16127],
        [16054],
        [15886],
        [19314],
        [20260],
        [16927],
        [18384],
        [16598],
        [18611],
        [19927],
        [20128],
        [20893],
        [21339]], device='cuda:0')
[2024-07-24 10:25:23,748][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[35859],
        [36911],
        [37499],
        [36124],
        [36372],
        [37963],
        [46689],
        [47883],
        [49238],
        [49228],
        [49233],
        [49193],
        [44583],
        [44279],
        [46041],
        [45666],
        [47554],
        [47890],
        [47734]], device='cuda:0')
[2024-07-24 10:25:23,750][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[16722],
        [44327],
        [49817],
        [49633],
        [49634],
        [49710],
        [49692],
        [49268],
        [49558],
        [49396],
        [49236],
        [49542],
        [49342],
        [49389],
        [49674],
        [49420],
        [49411],
        [48817],
        [49346]], device='cuda:0')
[2024-07-24 10:25:23,752][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[31474],
        [13631],
        [11728],
        [11611],
        [ 8513],
        [ 5719],
        [ 5100],
        [ 4678],
        [ 6835],
        [ 7288],
        [ 7772],
        [ 7627],
        [ 3957],
        [ 7494],
        [ 3553],
        [ 4219],
        [ 5191],
        [ 4938],
        [ 6746]], device='cuda:0')
[2024-07-24 10:25:23,754][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[20821],
        [18316],
        [21697],
        [11863],
        [19672],
        [ 6846],
        [10999],
        [17070],
        [22918],
        [16262],
        [16294],
        [ 5454],
        [16744],
        [15823],
        [20428],
        [ 7275],
        [10114],
        [18294],
        [ 7903]], device='cuda:0')
[2024-07-24 10:25:23,756][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[ 9360],
        [12748],
        [ 9793],
        [11171],
        [ 8089],
        [ 7410],
        [ 7880],
        [ 9487],
        [10933],
        [14787],
        [14687],
        [15327],
        [16749],
        [14606],
        [16274],
        [14315],
        [12794],
        [13207],
        [11098]], device='cuda:0')
[2024-07-24 10:25:23,758][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[40435],
        [34869],
        [34977],
        [31206],
        [38383],
        [39703],
        [41339],
        [40788],
        [38270],
        [37009],
        [37795],
        [35511],
        [36045],
        [37142],
        [37448],
        [37897],
        [39038],
        [38604],
        [37338]], device='cuda:0')
[2024-07-24 10:25:23,760][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[36067],
        [14370],
        [13499],
        [24178],
        [27653],
        [22933],
        [21643],
        [20034],
        [19333],
        [18741],
        [18090],
        [16023],
        [16036],
        [14815],
        [15393],
        [15430],
        [14603],
        [15643],
        [15952]], device='cuda:0')
[2024-07-24 10:25:23,762][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[34777],
        [18824],
        [23199],
        [20243],
        [11059],
        [15183],
        [17875],
        [14512],
        [14309],
        [15640],
        [15670],
        [13362],
        [15018],
        [13739],
        [12008],
        [10892],
        [11399],
        [12343],
        [11531]], device='cuda:0')
[2024-07-24 10:25:23,763][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[25490],
        [26573],
        [25579],
        [18658],
        [16794],
        [15023],
        [16381],
        [16377],
        [18237],
        [18580],
        [16154],
        [16983],
        [17494],
        [17776],
        [18422],
        [18184],
        [17307],
        [18667],
        [17264]], device='cuda:0')
[2024-07-24 10:25:23,765][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[23913],
        [27086],
        [31703],
        [29683],
        [29675],
        [26868],
        [31184],
        [24988],
        [20660],
        [20388],
        [21388],
        [22655],
        [20073],
        [18124],
        [ 6340],
        [22580],
        [21296],
        [20828],
        [23670]], device='cuda:0')
[2024-07-24 10:25:23,767][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[12526],
        [ 8465],
        [ 8664],
        [ 7808],
        [ 4655],
        [ 4824],
        [ 5515],
        [ 5500],
        [ 6007],
        [10202],
        [ 8871],
        [ 8039],
        [ 4650],
        [ 5257],
        [ 4066],
        [ 5636],
        [ 5371],
        [ 5728],
        [ 6352]], device='cuda:0')
[2024-07-24 10:25:23,769][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[22437],
        [ 7511],
        [11604],
        [11959],
        [11190],
        [11422],
        [11338],
        [ 9814],
        [ 9552],
        [ 9673],
        [ 9537],
        [ 9442],
        [ 9822],
        [ 8949],
        [ 9684],
        [ 9754],
        [ 9439],
        [ 9856],
        [ 9924]], device='cuda:0')
[2024-07-24 10:25:23,771][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[ 9779],
        [30665],
        [27068],
        [19419],
        [20362],
        [16887],
        [13615],
        [24098],
        [16756],
        [14616],
        [20736],
        [18580],
        [16896],
        [20028],
        [21327],
        [21605],
        [19847],
        [22520],
        [18532]], device='cuda:0')
[2024-07-24 10:25:23,772][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[31471],
        [30441],
        [24568],
        [25518],
        [26228],
        [29565],
        [21030],
        [13077],
        [10826],
        [10657],
        [10423],
        [ 9887],
        [19594],
        [21288],
        [10208],
        [20174],
        [18449],
        [17750],
        [18783]], device='cuda:0')
[2024-07-24 10:25:23,774][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[28798],
        [24548],
        [26308],
        [27732],
        [24662],
        [26822],
        [29006],
        [24782],
        [26593],
        [26163],
        [25888],
        [27356],
        [25807],
        [25795],
        [23759],
        [25343],
        [28137],
        [24274],
        [27507]], device='cuda:0')
[2024-07-24 10:25:23,776][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[ 9721],
        [ 5966],
        [ 5682],
        [22386],
        [20443],
        [24744],
        [25121],
        [24438],
        [24592],
        [23563],
        [24738],
        [25513],
        [30318],
        [25989],
        [30470],
        [27173],
        [25407],
        [26449],
        [22713]], device='cuda:0')
[2024-07-24 10:25:23,778][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[ 7524],
        [17764],
        [15845],
        [14208],
        [17993],
        [17763],
        [15920],
        [16467],
        [19598],
        [16561],
        [15493],
        [16976],
        [15354],
        [18189],
        [20969],
        [16782],
        [18026],
        [17702],
        [18401]], device='cuda:0')
[2024-07-24 10:25:23,780][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[17676],
        [16697],
        [16614],
        [14658],
        [12877],
        [23134],
        [ 8477],
        [15721],
        [11194],
        [22810],
        [12311],
        [18846],
        [18375],
        [11195],
        [23563],
        [29355],
        [14789],
        [16094],
        [17647]], device='cuda:0')
[2024-07-24 10:25:23,781][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547],
        [21547]], device='cuda:0')
[2024-07-24 10:25:23,912][circuit_model.py][line:1774][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:23,913][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,915][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,916][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,917][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,918][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,920][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,921][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,922][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,922][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,923][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,924][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,924][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:23,925][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [,] are: tensor([0.0831, 0.9169], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,926][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [,] are: tensor([0.9072, 0.0928], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,926][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [,] are: tensor([0.3436, 0.6564], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,927][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [,] are: tensor([0.9989, 0.0011], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,928][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [,] are: tensor([0.1422, 0.8578], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,929][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [,] are: tensor([0.6971, 0.3029], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,929][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [,] are: tensor([0.0970, 0.9030], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,930][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [,] are: tensor([0.1688, 0.8312], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,931][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [,] are: tensor([0.0895, 0.9105], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,932][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [,] are: tensor([0.0182, 0.9818], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,933][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [,] are: tensor([0.8068, 0.1932], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,934][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [,] are: tensor([0.7505, 0.2495], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:23,935][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Nathan] are: tensor([0.5142, 0.0763, 0.4095], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,937][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Nathan] are: tensor([0.7101, 0.0281, 0.2618], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,938][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Nathan] are: tensor([0.1120, 0.6567, 0.2313], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,940][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Nathan] are: tensor([0.7527, 0.0043, 0.2431], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,942][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Nathan] are: tensor([0.0667, 0.6921, 0.2412], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,943][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Nathan] are: tensor([0.2845, 0.3868, 0.3287], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,945][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Nathan] are: tensor([0.0864, 0.6925, 0.2211], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,946][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Nathan] are: tensor([0.5044, 0.1966, 0.2990], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,948][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Nathan] are: tensor([0.0052, 0.2689, 0.7260], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,950][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Nathan] are: tensor([0.0100, 0.7245, 0.2655], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,951][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Nathan] are: tensor([0.6182, 0.0509, 0.3308], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,953][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Nathan] are: tensor([0.9521, 0.0035, 0.0444], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:23,954][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ and] are: tensor([0.0195, 0.6284, 0.0949, 0.2571], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,955][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ and] are: tensor([0.4067, 0.0541, 0.5264, 0.0128], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,956][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ and] are: tensor([0.0439, 0.4626, 0.0795, 0.4140], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,957][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ and] are: tensor([7.6357e-01, 1.2144e-04, 2.3208e-01, 4.2258e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,957][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ and] are: tensor([0.0120, 0.3562, 0.0414, 0.5904], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,958][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ and] are: tensor([0.5115, 0.0378, 0.3823, 0.0684], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,960][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ and] are: tensor([0.0382, 0.5711, 0.0791, 0.3116], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,961][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ and] are: tensor([0.0923, 0.5793, 0.0963, 0.2320], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,963][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ and] are: tensor([0.0243, 0.1200, 0.4943, 0.3613], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,964][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ and] are: tensor([0.0032, 0.1341, 0.1149, 0.7477], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,966][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ and] are: tensor([0.4145, 0.0085, 0.5674, 0.0096], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,967][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ and] are: tensor([0.1745, 0.0152, 0.5478, 0.2625], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:23,969][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0394, 0.2179, 0.2608, 0.0580, 0.4239], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,970][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([0.5548, 0.0168, 0.3448, 0.0023, 0.0813], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,972][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0314, 0.3949, 0.0673, 0.3367, 0.1697], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,973][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([7.7734e-01, 3.9961e-04, 1.8834e-01, 5.0063e-03, 2.8910e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,975][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0114, 0.2802, 0.0525, 0.4846, 0.1713], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,977][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.3388, 0.0503, 0.2590, 0.0624, 0.2895], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,978][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0224, 0.3547, 0.0580, 0.3464, 0.2185], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,980][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.2658, 0.4080, 0.1537, 0.0749, 0.0976], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,981][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([6.4759e-04, 4.2090e-02, 1.1575e-01, 1.9308e-01, 6.4843e-01],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,982][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([0.0007, 0.0385, 0.0272, 0.2339, 0.6997], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,984][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.2647, 0.0067, 0.3437, 0.0119, 0.3731], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,986][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([0.7934, 0.0021, 0.1945, 0.0021, 0.0079], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:23,987][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ had] are: tensor([0.0173, 0.6090, 0.0627, 0.0638, 0.1226, 0.1245], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,988][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ had] are: tensor([6.1128e-01, 1.1082e-02, 2.6298e-01, 1.4565e-03, 1.1267e-01, 5.2562e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,990][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ had] are: tensor([0.0641, 0.2808, 0.0752, 0.2125, 0.1612, 0.2063], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,991][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ had] are: tensor([9.7366e-01, 2.3067e-05, 2.0755e-02, 2.9800e-05, 1.2890e-04, 5.4061e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,993][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ had] are: tensor([0.0067, 0.2217, 0.0244, 0.3122, 0.0778, 0.3573], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,994][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ had] are: tensor([0.4457, 0.0101, 0.3490, 0.0178, 0.1470, 0.0304], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,996][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ had] are: tensor([0.0193, 0.3302, 0.0319, 0.1534, 0.1152, 0.3500], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,998][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ had] are: tensor([0.2871, 0.4519, 0.1422, 0.0728, 0.0404, 0.0056], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:23,999][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ had] are: tensor([0.0063, 0.0447, 0.1379, 0.1319, 0.5596, 0.1197], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,000][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ had] are: tensor([0.0008, 0.0297, 0.0376, 0.1829, 0.5102, 0.2388], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,001][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ had] are: tensor([0.6813, 0.0027, 0.2413, 0.0010, 0.0560, 0.0178], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,002][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ had] are: tensor([0.8545, 0.0020, 0.1364, 0.0018, 0.0030, 0.0023], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,002][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0029, 0.3305, 0.0137, 0.0781, 0.0471, 0.1644, 0.3633],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,003][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([8.1388e-01, 5.3852e-03, 1.5655e-01, 8.8112e-04, 2.2805e-02, 4.7712e-04,
        2.9250e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,005][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0870, 0.2475, 0.0736, 0.1491, 0.1522, 0.1715, 0.1192],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,006][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([8.6849e-01, 4.8949e-06, 1.6868e-02, 6.8669e-05, 1.2982e-04, 1.1352e-01,
        9.1603e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,008][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0079, 0.1674, 0.0240, 0.2207, 0.0718, 0.2743, 0.2339],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,009][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.5818, 0.0119, 0.2416, 0.0130, 0.1182, 0.0292, 0.0044],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,011][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0180, 0.2302, 0.0274, 0.1044, 0.0695, 0.2670, 0.2834],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,013][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.2501, 0.4766, 0.1085, 0.1111, 0.0377, 0.0133, 0.0027],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,014][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0025, 0.0407, 0.0691, 0.1228, 0.3802, 0.1182, 0.2665],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,016][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0010, 0.0267, 0.0254, 0.1390, 0.3598, 0.2272, 0.2209],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,017][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([7.7767e-01, 1.0409e-03, 1.5642e-01, 6.5554e-04, 2.9136e-02, 3.4707e-02,
        3.7519e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,019][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.6364, 0.0025, 0.2543, 0.0148, 0.0114, 0.0742, 0.0064],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,020][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ lot] are: tensor([0.0013, 0.1290, 0.0050, 0.0498, 0.0242, 0.3401, 0.1060, 0.3445],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,022][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ lot] are: tensor([5.3555e-01, 4.0317e-02, 2.1796e-01, 5.2193e-03, 1.8734e-01, 7.6512e-03,
        4.7183e-04, 5.4899e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,023][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ lot] are: tensor([0.0365, 0.1937, 0.0419, 0.1549, 0.0950, 0.1520, 0.1460, 0.1799],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,024][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ lot] are: tensor([8.8737e-01, 5.0468e-06, 1.1638e-02, 1.7986e-05, 4.1163e-05, 7.2112e-02,
        3.4072e-04, 2.8480e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,026][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ lot] are: tensor([0.0077, 0.1145, 0.0266, 0.1845, 0.0601, 0.2490, 0.1947, 0.1628],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,028][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ lot] are: tensor([0.0882, 0.0465, 0.0854, 0.0799, 0.2268, 0.2698, 0.0710, 0.1323],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,029][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ lot] are: tensor([0.0095, 0.1305, 0.0162, 0.0783, 0.0617, 0.2887, 0.1919, 0.2232],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,031][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ lot] are: tensor([0.0833, 0.4241, 0.0507, 0.2634, 0.0506, 0.0795, 0.0090, 0.0393],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,033][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ lot] are: tensor([0.0011, 0.0228, 0.0579, 0.0830, 0.2599, 0.0889, 0.2000, 0.2865],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,034][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ lot] are: tensor([0.0004, 0.0152, 0.0088, 0.0946, 0.2387, 0.1930, 0.1789, 0.2704],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,036][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ lot] are: tensor([0.5732, 0.0023, 0.1210, 0.0034, 0.0628, 0.1715, 0.0020, 0.0638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,037][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ lot] are: tensor([9.4084e-01, 1.3985e-03, 3.8957e-02, 6.9257e-04, 6.0852e-04, 1.5058e-02,
        3.6214e-04, 2.0785e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,039][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0016, 0.0582, 0.0046, 0.0308, 0.0164, 0.1568, 0.1298, 0.4453, 0.1565],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,040][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ of] are: tensor([6.3073e-01, 6.7158e-03, 2.8530e-01, 2.5213e-03, 6.9672e-02, 3.9440e-03,
        7.7258e-05, 8.5109e-04, 1.8879e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,042][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0309, 0.1480, 0.0496, 0.1329, 0.0945, 0.1624, 0.1161, 0.1650, 0.1005],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,043][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ of] are: tensor([7.1577e-01, 2.7382e-06, 1.9225e-02, 5.4617e-05, 9.6729e-05, 8.8845e-02,
        1.0388e-03, 1.7357e-01, 1.4026e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,045][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0047, 0.0828, 0.0188, 0.1807, 0.0432, 0.1826, 0.1801, 0.1214, 0.1858],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,045][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.3837, 0.0137, 0.2892, 0.0268, 0.1191, 0.0979, 0.0127, 0.0216, 0.0353],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,046][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0037, 0.0985, 0.0106, 0.0699, 0.0359, 0.3179, 0.1702, 0.1645, 0.1287],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,047][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.1119, 0.5643, 0.0864, 0.1476, 0.0433, 0.0166, 0.0019, 0.0165, 0.0115],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,048][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0096, 0.0318, 0.0664, 0.0787, 0.2885, 0.0728, 0.1809, 0.1297, 0.1418],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,049][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0008, 0.0193, 0.0157, 0.1108, 0.2016, 0.1824, 0.1546, 0.1651, 0.1497],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,051][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ of] are: tensor([6.0800e-01, 7.8568e-04, 2.6085e-01, 1.4983e-03, 5.5059e-02, 3.8938e-02,
        5.5346e-04, 3.2744e-02, 1.5667e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,052][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.5937, 0.0016, 0.3206, 0.0128, 0.0055, 0.0468, 0.0024, 0.0137, 0.0028],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,054][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ fun] are: tensor([1.1993e-04, 4.2915e-02, 1.6247e-03, 3.5313e-02, 1.2954e-02, 1.2666e-01,
        1.6192e-01, 4.5542e-01, 1.6199e-01, 1.0800e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,055][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ fun] are: tensor([6.5761e-01, 3.5786e-02, 2.1348e-01, 6.0672e-03, 7.3405e-02, 7.7000e-03,
        2.6224e-04, 4.0522e-03, 8.6361e-04, 7.7267e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,056][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ fun] are: tensor([0.0305, 0.1660, 0.0307, 0.1115, 0.0550, 0.1128, 0.1011, 0.1309, 0.1092,
        0.1523], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,057][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ fun] are: tensor([1.3518e-01, 4.5786e-06, 5.8747e-03, 1.2983e-04, 9.6363e-05, 1.8809e-01,
        8.4634e-03, 6.4582e-01, 1.5909e-02, 4.2839e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,059][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ fun] are: tensor([0.0056, 0.0869, 0.0240, 0.1319, 0.0526, 0.1601, 0.1526, 0.1198, 0.1439,
        0.1226], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,061][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ fun] are: tensor([0.4844, 0.0167, 0.1540, 0.0166, 0.1584, 0.0421, 0.0125, 0.0453, 0.0502,
        0.0198], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,062][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ fun] are: tensor([0.0068, 0.0524, 0.0084, 0.0301, 0.0202, 0.1540, 0.1288, 0.1492, 0.1534,
        0.2968], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,064][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ fun] are: tensor([0.5058, 0.1801, 0.1176, 0.0734, 0.0403, 0.0295, 0.0042, 0.0186, 0.0212,
        0.0094], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,066][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ fun] are: tensor([0.0006, 0.0162, 0.0370, 0.0580, 0.2125, 0.0596, 0.1487, 0.1397, 0.1983,
        0.1293], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,068][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ fun] are: tensor([0.0004, 0.0132, 0.0087, 0.0681, 0.1650, 0.1254, 0.1184, 0.1733, 0.1109,
        0.2165], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,069][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ fun] are: tensor([0.6356, 0.0026, 0.1893, 0.0013, 0.0532, 0.0744, 0.0010, 0.0386, 0.0034,
        0.0007], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,070][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ fun] are: tensor([6.4499e-01, 3.0049e-03, 1.5240e-01, 1.2088e-02, 7.4807e-03, 1.0907e-01,
        1.6230e-02, 4.6077e-02, 8.3638e-03, 2.9602e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,072][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ at] are: tensor([0.0018, 0.1265, 0.0095, 0.0435, 0.0319, 0.1281, 0.1079, 0.3872, 0.1439,
        0.0048, 0.0148], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,073][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ at] are: tensor([7.3360e-01, 5.5946e-03, 1.9891e-01, 1.9634e-03, 5.6137e-02, 1.6736e-03,
        7.3776e-05, 1.3464e-03, 1.4599e-04, 2.3029e-04, 3.2039e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,075][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ at] are: tensor([0.0316, 0.1169, 0.0444, 0.1047, 0.0817, 0.1157, 0.0983, 0.1261, 0.0716,
        0.0898, 0.1192], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,076][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ at] are: tensor([7.8706e-01, 1.4247e-05, 2.4904e-02, 7.8512e-05, 1.8593e-04, 4.0477e-02,
        5.1394e-04, 1.4329e-01, 1.6416e-03, 3.1372e-04, 1.5229e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,078][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ at] are: tensor([0.0055, 0.0616, 0.0190, 0.1509, 0.0483, 0.1660, 0.1435, 0.0830, 0.1340,
        0.0884, 0.0996], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,080][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ at] are: tensor([0.1165, 0.0175, 0.1227, 0.0820, 0.2188, 0.1538, 0.0476, 0.0792, 0.1004,
        0.0307, 0.0308], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,081][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ at] are: tensor([0.0042, 0.0730, 0.0090, 0.0604, 0.0301, 0.2130, 0.1393, 0.0996, 0.0995,
        0.1486, 0.1234], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,083][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ at] are: tensor([0.1449, 0.2860, 0.1246, 0.1961, 0.0553, 0.0399, 0.0053, 0.0522, 0.0277,
        0.0233, 0.0447], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,085][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ at] are: tensor([0.0016, 0.0192, 0.0531, 0.0626, 0.2111, 0.0499, 0.1209, 0.1165, 0.1425,
        0.0957, 0.1269], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,086][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ at] are: tensor([0.0003, 0.0093, 0.0084, 0.0676, 0.1514, 0.1239, 0.1162, 0.1433, 0.1047,
        0.1947, 0.0800], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,088][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ at] are: tensor([0.5018, 0.0010, 0.3057, 0.0015, 0.1074, 0.0329, 0.0008, 0.0414, 0.0016,
        0.0007, 0.0052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,089][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ at] are: tensor([7.7764e-01, 2.5753e-03, 1.6552e-01, 7.7532e-03, 4.4138e-03, 1.7045e-02,
        1.7905e-03, 9.4723e-03, 1.9719e-03, 9.3498e-05, 1.1729e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,090][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0017, 0.1238, 0.0087, 0.0314, 0.0232, 0.0807, 0.0975, 0.4126, 0.1436,
        0.0086, 0.0088, 0.0596], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,091][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([6.8300e-01, 2.5183e-02, 1.9321e-01, 4.1645e-03, 9.1217e-02, 1.1206e-03,
        4.8332e-05, 1.1188e-03, 2.1352e-04, 2.6041e-04, 3.2068e-04, 1.4866e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,092][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0588, 0.1439, 0.0477, 0.0753, 0.0776, 0.0821, 0.0530, 0.1259, 0.0435,
        0.1071, 0.1163, 0.0688], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,093][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([8.5543e-01, 4.0422e-06, 1.1925e-02, 5.4639e-05, 8.4539e-05, 5.0762e-02,
        1.8606e-04, 7.8979e-02, 8.4165e-04, 2.2025e-04, 7.7447e-04, 7.3936e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,094][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0046, 0.0581, 0.0107, 0.1153, 0.0256, 0.1252, 0.0999, 0.0634, 0.1210,
        0.0563, 0.0793, 0.2407], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,096][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1093, 0.0523, 0.1304, 0.0751, 0.2948, 0.1029, 0.0283, 0.0212, 0.0707,
        0.0340, 0.0290, 0.0520], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,098][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0063, 0.1082, 0.0088, 0.0458, 0.0230, 0.1267, 0.0806, 0.0834, 0.0623,
        0.1599, 0.1490, 0.1459], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,099][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0419, 0.6444, 0.0305, 0.1512, 0.0247, 0.0127, 0.0019, 0.0096, 0.0089,
        0.0097, 0.0161, 0.0484], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,101][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0021, 0.0152, 0.0394, 0.0546, 0.2325, 0.0455, 0.1165, 0.0953, 0.1049,
        0.1010, 0.0984, 0.0947], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,103][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0004, 0.0104, 0.0088, 0.0663, 0.1362, 0.1015, 0.0938, 0.1258, 0.0865,
        0.1734, 0.0688, 0.1281], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,104][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([7.0926e-01, 1.8214e-03, 1.5583e-01, 1.1183e-03, 6.4401e-02, 1.6897e-02,
        2.0932e-04, 4.6375e-02, 9.8871e-04, 4.4519e-04, 2.0406e-03, 6.1567e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,105][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([5.1253e-01, 3.3123e-03, 2.2077e-01, 3.4830e-02, 1.5923e-02, 1.0324e-01,
        6.0256e-03, 2.1192e-02, 1.9818e-03, 1.5342e-04, 2.4804e-02, 5.5236e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,107][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ house] are: tensor([0.0005, 0.1454, 0.0042, 0.0631, 0.0100, 0.0858, 0.1950, 0.3025, 0.1026,
        0.0003, 0.0112, 0.0489, 0.0304], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,108][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ house] are: tensor([7.8769e-01, 2.7334e-02, 1.6734e-01, 1.7095e-03, 1.2402e-02, 2.5402e-03,
        8.0247e-05, 5.5441e-04, 1.0309e-04, 5.9678e-05, 8.0975e-05, 8.3460e-05,
        2.5389e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,110][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ house] are: tensor([0.0656, 0.2178, 0.0365, 0.0515, 0.0254, 0.0490, 0.0382, 0.1018, 0.0390,
        0.0758, 0.0896, 0.0574, 0.1526], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,111][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ house] are: tensor([9.9358e-01, 5.4882e-07, 2.6211e-03, 1.3175e-06, 5.3083e-07, 1.4321e-03,
        1.7393e-06, 3.9409e-04, 1.4970e-05, 5.3964e-07, 1.0184e-05, 1.2493e-05,
        1.9266e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,113][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ house] are: tensor([0.0047, 0.0403, 0.0096, 0.0475, 0.0160, 0.0501, 0.0588, 0.0519, 0.0578,
        0.0360, 0.0593, 0.1812, 0.3869], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,114][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ house] are: tensor([0.1381, 0.0118, 0.1022, 0.0346, 0.1464, 0.1788, 0.0509, 0.0714, 0.1089,
        0.0279, 0.0234, 0.0696, 0.0360], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,116][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ house] are: tensor([0.0101, 0.0828, 0.0077, 0.0155, 0.0121, 0.0696, 0.0726, 0.0925, 0.0718,
        0.1856, 0.1274, 0.0812, 0.1712], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,118][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ house] are: tensor([0.2888, 0.2851, 0.1204, 0.0889, 0.0603, 0.0275, 0.0034, 0.0204, 0.0122,
        0.0101, 0.0160, 0.0460, 0.0209], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,120][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ house] are: tensor([0.0002, 0.0098, 0.0201, 0.0367, 0.1136, 0.0322, 0.0902, 0.0805, 0.1322,
        0.0761, 0.0875, 0.1072, 0.2138], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,122][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ house] are: tensor([0.0003, 0.0083, 0.0065, 0.0460, 0.0987, 0.0794, 0.0772, 0.1087, 0.0693,
        0.1334, 0.0532, 0.0885, 0.2302], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,123][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ house] are: tensor([8.5612e-01, 1.0298e-03, 8.9442e-02, 5.8329e-04, 9.1915e-03, 2.1318e-02,
        8.7296e-05, 1.0642e-02, 6.2090e-04, 1.7280e-04, 1.9516e-03, 2.6591e-04,
        8.5789e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,124][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ house] are: tensor([9.4690e-01, 4.2307e-04, 4.2925e-02, 4.6012e-04, 5.0808e-04, 5.9987e-03,
        3.3854e-05, 3.7699e-04, 6.1351e-05, 3.4856e-06, 3.7124e-04, 8.2940e-04,
        1.1120e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,126][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [.] are: tensor([0.0412, 0.3238, 0.0788, 0.0770, 0.0311, 0.0822, 0.0520, 0.1049, 0.0435,
        0.0025, 0.0153, 0.0705, 0.0079, 0.0693], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,127][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [.] are: tensor([5.5519e-01, 4.0504e-02, 2.6900e-01, 2.9397e-03, 1.1600e-01, 1.9895e-03,
        5.0057e-05, 1.3101e-03, 3.3435e-04, 1.9423e-04, 2.6462e-04, 8.7194e-05,
        1.1666e-04, 1.2031e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,129][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [.] are: tensor([0.0381, 0.1011, 0.0323, 0.0546, 0.0522, 0.0543, 0.0443, 0.0875, 0.0369,
        0.0960, 0.0854, 0.0558, 0.1960, 0.0655], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,130][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [.] are: tensor([9.8441e-01, 2.5145e-06, 6.8390e-03, 5.3136e-06, 1.1190e-05, 2.6639e-03,
        6.6622e-06, 2.6865e-03, 7.9300e-05, 7.6661e-06, 5.5873e-05, 2.9017e-05,
        1.5253e-03, 1.6751e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,132][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [.] are: tensor([0.0017, 0.0279, 0.0050, 0.0634, 0.0119, 0.0785, 0.0593, 0.0364, 0.0657,
        0.0394, 0.0416, 0.1158, 0.2802, 0.1732], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,134][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [.] are: tensor([0.0602, 0.0127, 0.0598, 0.0277, 0.0872, 0.0446, 0.0096, 0.0140, 0.0233,
        0.0101, 0.0110, 0.0083, 0.0160, 0.6155], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,135][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [.] are: tensor([0.0039, 0.0542, 0.0051, 0.0221, 0.0156, 0.0827, 0.0544, 0.0665, 0.0495,
        0.0968, 0.0915, 0.0809, 0.1957, 0.1810], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,135][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [.] are: tensor([0.0485, 0.3802, 0.0279, 0.0669, 0.0161, 0.0129, 0.0009, 0.0057, 0.0058,
        0.0027, 0.0070, 0.0206, 0.0098, 0.3950], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,136][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [.] are: tensor([0.0044, 0.0188, 0.0554, 0.0511, 0.2170, 0.0418, 0.0866, 0.0895, 0.0694,
        0.0666, 0.0724, 0.0672, 0.1228, 0.0370], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,138][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [.] are: tensor([1.9525e-04, 8.2158e-03, 7.4075e-03, 4.6084e-02, 1.1046e-01, 7.5759e-02,
        7.0506e-02, 9.2560e-02, 6.3288e-02, 1.1829e-01, 4.5239e-02, 8.1788e-02,
        2.1456e-01, 6.5640e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,139][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [.] are: tensor([6.1857e-01, 5.0547e-03, 2.1749e-01, 1.9573e-03, 1.0197e-01, 1.2144e-02,
        2.3518e-04, 6.1220e-03, 4.5176e-04, 1.7607e-04, 1.0123e-03, 4.1308e-04,
        4.1458e-04, 3.3992e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,140][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [.] are: tensor([7.9932e-01, 2.1904e-03, 1.1177e-01, 9.3029e-03, 3.8729e-03, 2.2721e-02,
        1.0917e-03, 1.3044e-03, 6.2363e-04, 1.8389e-05, 3.9483e-03, 5.7424e-03,
        1.4050e-03, 3.6688e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,142][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Katie] are: tensor([0.0070, 0.0582, 0.0504, 0.0138, 0.0983, 0.0560, 0.0386, 0.2455, 0.0451,
        0.0030, 0.0089, 0.0329, 0.0191, 0.0485, 0.2747], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,143][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Katie] are: tensor([6.2601e-01, 2.4318e-02, 2.4077e-01, 3.7919e-03, 5.4165e-02, 6.4766e-03,
        2.4924e-04, 5.3739e-03, 4.1852e-04, 6.6346e-04, 1.6386e-03, 7.2257e-04,
        1.1442e-03, 2.0677e-02, 1.3577e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,145][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Katie] are: tensor([0.0224, 0.1587, 0.0291, 0.0861, 0.0426, 0.0794, 0.0638, 0.0711, 0.0624,
        0.0311, 0.0779, 0.0764, 0.1053, 0.0613, 0.0322], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,146][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Katie] are: tensor([6.0761e-02, 1.0591e-06, 2.9877e-03, 3.0769e-05, 7.3019e-05, 3.4700e-02,
        1.4328e-03, 2.0866e-01, 3.7082e-03, 3.6748e-04, 6.4417e-03, 1.5298e-02,
        5.6863e-01, 9.5740e-02, 1.1679e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,148][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Katie] are: tensor([0.0026, 0.0252, 0.0091, 0.0553, 0.0189, 0.0848, 0.0644, 0.0438, 0.0629,
        0.0506, 0.0623, 0.1190, 0.2248, 0.1455, 0.0308], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,150][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Katie] are: tensor([0.1409, 0.0114, 0.0753, 0.0244, 0.0622, 0.1390, 0.0324, 0.0505, 0.0420,
        0.0089, 0.0241, 0.0435, 0.0460, 0.2682, 0.0312], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,151][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Katie] are: tensor([0.0034, 0.0317, 0.0053, 0.0312, 0.0113, 0.0913, 0.1043, 0.0628, 0.1164,
        0.0626, 0.1147, 0.1466, 0.1100, 0.0884, 0.0199], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,153][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Katie] are: tensor([0.2467, 0.2259, 0.0891, 0.0637, 0.0489, 0.0194, 0.0039, 0.0268, 0.0142,
        0.0062, 0.0214, 0.0341, 0.0322, 0.1557, 0.0117], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,155][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Katie] are: tensor([0.0002, 0.0067, 0.0157, 0.0260, 0.0766, 0.0333, 0.0723, 0.0608, 0.1036,
        0.0588, 0.0595, 0.0772, 0.1846, 0.0418, 0.1830], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,156][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Katie] are: tensor([1.4848e-04, 5.4051e-03, 2.5857e-03, 3.7597e-02, 6.0211e-02, 6.1627e-02,
        5.9077e-02, 6.8198e-02, 4.6547e-02, 7.1733e-02, 4.0963e-02, 8.1024e-02,
        2.0439e-01, 5.6404e-02, 2.0409e-01], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,158][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Katie] are: tensor([0.4296, 0.0011, 0.1233, 0.0021, 0.0408, 0.1531, 0.0023, 0.0616, 0.0053,
        0.0010, 0.0168, 0.0048, 0.0101, 0.1218, 0.0262], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,159][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Katie] are: tensor([8.2622e-01, 5.2050e-04, 1.0276e-01, 1.2901e-03, 1.3190e-03, 1.1797e-02,
        8.8129e-04, 2.5575e-03, 1.2589e-03, 1.2935e-05, 3.1922e-03, 1.3570e-02,
        2.3375e-03, 3.1439e-02, 8.4776e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,161][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ gave] are: tensor([0.0246, 0.2347, 0.0661, 0.0220, 0.0455, 0.0840, 0.0263, 0.0953, 0.0167,
        0.0004, 0.0037, 0.0234, 0.0033, 0.0126, 0.0414, 0.3001],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,162][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ gave] are: tensor([7.1423e-01, 1.4357e-02, 2.3665e-01, 9.1493e-04, 2.8666e-02, 5.6000e-04,
        2.0270e-05, 3.5670e-04, 3.6648e-05, 1.0545e-04, 6.0217e-05, 3.3048e-05,
        3.3673e-05, 1.9978e-03, 1.2506e-03, 7.2784e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,164][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ gave] are: tensor([0.0565, 0.1375, 0.0489, 0.0696, 0.0488, 0.0576, 0.0432, 0.0636, 0.0383,
        0.0527, 0.0648, 0.0604, 0.1210, 0.0546, 0.0403, 0.0424],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,165][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ gave] are: tensor([9.5939e-01, 4.6016e-06, 4.2387e-03, 8.2529e-06, 8.8964e-06, 4.2176e-03,
        3.6529e-06, 1.9352e-03, 3.0196e-05, 1.4544e-05, 6.7073e-05, 3.8361e-05,
        6.3228e-03, 5.0749e-03, 2.1764e-05, 1.8621e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,167][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ gave] are: tensor([0.0035, 0.0381, 0.0086, 0.0628, 0.0189, 0.0729, 0.0494, 0.0354, 0.0625,
        0.0422, 0.0452, 0.0972, 0.1980, 0.1225, 0.0259, 0.1168],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,169][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ gave] are: tensor([0.1716, 0.0105, 0.1837, 0.0329, 0.1387, 0.0514, 0.0140, 0.0205, 0.0203,
        0.0119, 0.0058, 0.0177, 0.0105, 0.2543, 0.0245, 0.0318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,171][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ gave] are: tensor([0.0071, 0.0976, 0.0087, 0.0256, 0.0111, 0.0557, 0.0462, 0.0576, 0.0579,
        0.0977, 0.0804, 0.0755, 0.1062, 0.0886, 0.0209, 0.1633],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,172][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ gave] are: tensor([0.1383, 0.2593, 0.1077, 0.0712, 0.0411, 0.0170, 0.0013, 0.0144, 0.0074,
        0.0076, 0.0131, 0.0312, 0.0211, 0.1791, 0.0042, 0.0861],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,174][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ gave] are: tensor([0.0003, 0.0085, 0.0191, 0.0267, 0.0878, 0.0232, 0.0540, 0.0600, 0.0778,
        0.0520, 0.0508, 0.0659, 0.1388, 0.0397, 0.2462, 0.0489],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,175][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ gave] are: tensor([1.6938e-04, 5.4552e-03, 5.4144e-03, 3.6542e-02, 7.9588e-02, 5.5169e-02,
        5.4017e-02, 6.7956e-02, 4.8386e-02, 9.5271e-02, 3.6612e-02, 6.6855e-02,
        1.6253e-01, 4.7661e-02, 1.8708e-01, 5.1295e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,177][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ gave] are: tensor([8.6324e-01, 6.2156e-04, 1.0640e-01, 1.9741e-04, 1.0751e-02, 4.5056e-03,
        2.5641e-05, 2.3028e-03, 1.2383e-04, 6.2688e-05, 3.7507e-04, 8.7875e-05,
        5.7992e-04, 4.8759e-03, 3.2464e-03, 2.6045e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,178][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ gave] are: tensor([9.4985e-01, 3.1246e-04, 3.5718e-02, 3.6179e-04, 3.0420e-04, 1.0610e-03,
        1.6093e-05, 1.1979e-04, 3.3882e-05, 5.8005e-07, 1.4912e-04, 2.7447e-04,
        1.0239e-04, 2.4735e-03, 6.3457e-05, 9.1545e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,179][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0076, 0.1971, 0.0194, 0.0241, 0.0158, 0.0396, 0.0271, 0.0850, 0.0291,
        0.0010, 0.0058, 0.0287, 0.0132, 0.0308, 0.0241, 0.2781, 0.1734],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,180][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([7.8309e-01, 1.3183e-02, 1.6816e-01, 1.3694e-03, 2.8609e-02, 4.4519e-04,
        1.5051e-05, 3.2137e-04, 5.2400e-05, 5.8969e-05, 9.0037e-05, 4.6995e-05,
        3.8831e-05, 2.4248e-03, 1.3542e-03, 6.5677e-04, 8.8655e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,181][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0443, 0.1363, 0.0349, 0.0596, 0.0490, 0.0461, 0.0309, 0.0711, 0.0304,
        0.0588, 0.0816, 0.0487, 0.1301, 0.0563, 0.0464, 0.0377, 0.0379],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,182][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([9.6723e-01, 2.7734e-07, 3.5705e-03, 1.8983e-06, 5.1827e-06, 1.7649e-03,
        1.0924e-06, 1.9086e-03, 1.4660e-05, 5.3265e-06, 3.8521e-05, 1.6523e-05,
        7.0454e-03, 1.5986e-03, 2.1186e-05, 1.5874e-02, 8.9852e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,184][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0020, 0.0288, 0.0054, 0.0539, 0.0127, 0.0615, 0.0468, 0.0296, 0.0456,
        0.0297, 0.0377, 0.1004, 0.2164, 0.1243, 0.0171, 0.0789, 0.1090],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,185][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1918, 0.0207, 0.1635, 0.0340, 0.1376, 0.0390, 0.0048, 0.0075, 0.0134,
        0.0102, 0.0090, 0.0148, 0.0103, 0.2798, 0.0355, 0.0114, 0.0165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,187][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0091, 0.0852, 0.0089, 0.0245, 0.0105, 0.0485, 0.0366, 0.0522, 0.0297,
        0.0663, 0.0864, 0.0673, 0.1138, 0.0908, 0.0201, 0.1221, 0.1280],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,189][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1724, 0.4015, 0.0615, 0.0657, 0.0230, 0.0105, 0.0008, 0.0056, 0.0032,
        0.0049, 0.0095, 0.0237, 0.0120, 0.1781, 0.0029, 0.0187, 0.0060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,190][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0009, 0.0120, 0.0203, 0.0377, 0.1169, 0.0332, 0.0716, 0.0503, 0.0700,
        0.0538, 0.0657, 0.0668, 0.0981, 0.0326, 0.1720, 0.0410, 0.0570],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,192][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0003, 0.0064, 0.0061, 0.0367, 0.0732, 0.0529, 0.0478, 0.0612, 0.0423,
        0.0938, 0.0364, 0.0627, 0.1681, 0.0459, 0.1785, 0.0438, 0.0441],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,193][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([8.5091e-01, 9.9896e-04, 1.1344e-01, 3.9133e-04, 1.3127e-02, 6.3510e-03,
        2.8508e-05, 1.9888e-03, 7.4567e-05, 3.7214e-05, 4.3802e-04, 8.9515e-05,
        6.1168e-04, 6.6076e-03, 2.8346e-03, 1.8474e-03, 2.2169e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,195][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([8.1684e-01, 6.7085e-04, 1.0932e-01, 3.3131e-03, 1.4216e-03, 7.4302e-03,
        1.7285e-04, 6.7572e-04, 1.0105e-04, 5.9161e-06, 1.0352e-03, 3.2199e-03,
        2.2290e-03, 1.8580e-02, 2.5839e-04, 1.7880e-02, 1.6842e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,196][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ snack] are: tensor([0.0208, 0.2562, 0.0768, 0.0261, 0.0484, 0.0917, 0.0191, 0.0315, 0.0199,
        0.0004, 0.0074, 0.0240, 0.0079, 0.0257, 0.0467, 0.1630, 0.0608, 0.0736],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,198][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ snack] are: tensor([7.7239e-01, 2.9773e-02, 1.5566e-01, 1.0681e-03, 2.4975e-02, 1.6308e-03,
        7.1861e-05, 4.7673e-04, 8.5296e-05, 7.5851e-05, 3.3136e-04, 9.1908e-05,
        6.3185e-05, 2.0227e-03, 2.4751e-03, 1.5589e-03, 2.5448e-04, 6.9991e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,200][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ snack] are: tensor([0.0432, 0.1349, 0.0377, 0.0667, 0.0371, 0.0632, 0.0478, 0.0544, 0.0534,
        0.0348, 0.0702, 0.0597, 0.0876, 0.0536, 0.0284, 0.0384, 0.0506, 0.0384],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,201][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ snack] are: tensor([9.5995e-01, 3.2069e-06, 3.4453e-03, 1.9368e-06, 9.5220e-06, 1.4100e-03,
        8.0455e-06, 5.4644e-04, 4.0064e-05, 3.7250e-06, 1.0535e-04, 8.1610e-05,
        3.2346e-03, 2.2366e-03, 2.9310e-05, 1.3229e-02, 4.6212e-03, 1.1046e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,203][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ snack] are: tensor([0.0045, 0.0256, 0.0107, 0.0456, 0.0168, 0.0590, 0.0531, 0.0385, 0.0560,
        0.0493, 0.0493, 0.0783, 0.1748, 0.1055, 0.0265, 0.0838, 0.0960, 0.0266],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,205][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ snack] are: tensor([0.1557, 0.0627, 0.0577, 0.0310, 0.0579, 0.0882, 0.0238, 0.0434, 0.0491,
        0.0110, 0.0227, 0.0325, 0.0224, 0.1940, 0.0205, 0.0512, 0.0343, 0.0421],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,206][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ snack] are: tensor([0.0189, 0.0509, 0.0139, 0.0210, 0.0125, 0.0630, 0.0578, 0.0594, 0.0783,
        0.0742, 0.0815, 0.0571, 0.0726, 0.0578, 0.0203, 0.1107, 0.1029, 0.0471],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,208][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ snack] are: tensor([0.3705, 0.1817, 0.0576, 0.0308, 0.0121, 0.0187, 0.0021, 0.0079, 0.0049,
        0.0022, 0.0130, 0.0158, 0.0078, 0.0904, 0.0034, 0.0518, 0.0117, 0.1175],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,210][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ snack] are: tensor([7.9359e-05, 7.5501e-03, 1.0367e-02, 3.1426e-02, 6.4353e-02, 2.4804e-02,
        6.0703e-02, 4.9487e-02, 9.0477e-02, 5.0278e-02, 5.6932e-02, 7.4340e-02,
        1.4029e-01, 3.6436e-02, 1.3763e-01, 4.4901e-02, 6.3979e-02, 5.5962e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,211][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ snack] are: tensor([1.6550e-04, 4.8474e-03, 1.9880e-03, 2.7051e-02, 5.7251e-02, 5.1015e-02,
        4.6307e-02, 4.9508e-02, 3.7574e-02, 7.1348e-02, 3.0765e-02, 5.7755e-02,
        1.6936e-01, 4.9825e-02, 1.9306e-01, 5.4159e-02, 4.3765e-02, 5.4261e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,212][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ snack] are: tensor([7.8928e-01, 1.2643e-02, 1.3663e-01, 1.1378e-03, 1.5529e-02, 1.6850e-02,
        2.8580e-04, 1.0323e-03, 3.3469e-04, 7.2389e-05, 2.0483e-03, 4.6576e-04,
        7.9359e-04, 1.3531e-02, 1.8884e-03, 3.1842e-03, 8.2941e-04, 3.4703e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,214][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ snack] are: tensor([9.9108e-01, 2.2071e-04, 7.1037e-03, 4.5463e-06, 3.2055e-05, 1.8966e-04,
        1.5320e-06, 2.1617e-06, 1.4511e-06, 1.7382e-08, 1.6947e-05, 4.8921e-06,
        1.8124e-06, 1.1714e-04, 3.8769e-06, 7.6365e-04, 8.3320e-05, 3.7568e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,216][circuit_model.py][line:2294][INFO] ##11-th layer ##Weight##: The head1 weight for token [ to] are: tensor([0.0154, 0.1372, 0.0274, 0.0232, 0.0138, 0.0635, 0.0176, 0.0237, 0.0142,
        0.0006, 0.0041, 0.0137, 0.0061, 0.0246, 0.0097, 0.2418, 0.1845, 0.1108,
        0.0680], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,217][circuit_model.py][line:2297][INFO] ##11-th layer ##Weight##: The head2 weight for token [ to] are: tensor([6.5016e-01, 3.1818e-02, 2.2180e-01, 3.5683e-03, 7.3800e-02, 2.2355e-03,
        5.0032e-05, 3.4309e-04, 7.0270e-05, 9.0435e-05, 1.3641e-04, 1.0073e-04,
        4.9024e-05, 5.1231e-03, 2.5021e-03, 1.1477e-03, 2.2933e-04, 6.2682e-03,
        5.0319e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,219][circuit_model.py][line:2300][INFO] ##11-th layer ##Weight##: The head3 weight for token [ to] are: tensor([0.0383, 0.1292, 0.0321, 0.0564, 0.0451, 0.0490, 0.0372, 0.0594, 0.0256,
        0.0483, 0.0710, 0.0509, 0.1159, 0.0500, 0.0399, 0.0305, 0.0449, 0.0321,
        0.0441], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,220][circuit_model.py][line:2303][INFO] ##11-th layer ##Weight##: The head4 weight for token [ to] are: tensor([9.8878e-01, 1.0837e-07, 1.4093e-03, 2.0577e-07, 6.2578e-07, 4.9107e-04,
        1.5984e-07, 1.3737e-04, 1.0721e-06, 3.3307e-07, 5.2289e-06, 2.7086e-06,
        1.1595e-03, 3.8985e-04, 1.5329e-06, 1.7082e-03, 2.0545e-04, 2.4742e-03,
        3.2324e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,222][circuit_model.py][line:2306][INFO] ##11-th layer ##Weight##: The head5 weight for token [ to] are: tensor([0.0025, 0.0250, 0.0055, 0.0483, 0.0099, 0.0581, 0.0399, 0.0229, 0.0473,
        0.0281, 0.0349, 0.0815, 0.2000, 0.1115, 0.0124, 0.0724, 0.0887, 0.0138,
        0.0974], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,223][circuit_model.py][line:2309][INFO] ##11-th layer ##Weight##: The head6 weight for token [ to] are: tensor([0.0869, 0.0271, 0.0647, 0.0361, 0.0516, 0.0736, 0.0074, 0.0059, 0.0120,
        0.0049, 0.0165, 0.0146, 0.0072, 0.5077, 0.0067, 0.0162, 0.0175, 0.0205,
        0.0228], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,224][circuit_model.py][line:2312][INFO] ##11-th layer ##Weight##: The head7 weight for token [ to] are: tensor([0.0083, 0.0886, 0.0099, 0.0226, 0.0113, 0.0447, 0.0297, 0.0467, 0.0254,
        0.0795, 0.0588, 0.0553, 0.1266, 0.0808, 0.0151, 0.0951, 0.0848, 0.0563,
        0.0606], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,225][circuit_model.py][line:2315][INFO] ##11-th layer ##Weight##: The head8 weight for token [ to] are: tensor([0.1686, 0.3114, 0.0385, 0.0442, 0.0125, 0.0128, 0.0007, 0.0033, 0.0038,
        0.0023, 0.0086, 0.0218, 0.0082, 0.2370, 0.0020, 0.0225, 0.0072, 0.0751,
        0.0197], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,226][circuit_model.py][line:2318][INFO] ##11-th layer ##Weight##: The head9 weight for token [ to] are: tensor([0.0049, 0.0140, 0.0331, 0.0338, 0.1274, 0.0291, 0.0600, 0.0518, 0.0473,
        0.0437, 0.0554, 0.0458, 0.0771, 0.0262, 0.1686, 0.0441, 0.0481, 0.0482,
        0.0415], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,227][circuit_model.py][line:2321][INFO] ##11-th layer ##Weight##: The head10 weight for token [ to] are: tensor([0.0003, 0.0072, 0.0052, 0.0358, 0.0653, 0.0532, 0.0485, 0.0486, 0.0430,
        0.0829, 0.0324, 0.0566, 0.1384, 0.0444, 0.1513, 0.0426, 0.0403, 0.0483,
        0.0558], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,228][circuit_model.py][line:2324][INFO] ##11-th layer ##Weight##: The head11 weight for token [ to] are: tensor([9.0365e-01, 5.0269e-04, 7.2876e-02, 1.1114e-04, 4.5436e-03, 2.8699e-03,
        9.4545e-06, 6.2939e-04, 2.8258e-05, 1.0442e-05, 1.7580e-04, 4.8544e-05,
        4.5251e-04, 5.8507e-03, 1.0898e-03, 9.1093e-04, 8.7025e-05, 5.7484e-03,
        4.0398e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,230][circuit_model.py][line:2327][INFO] ##11-th layer ##Weight##: The head12 weight for token [ to] are: tensor([7.5513e-01, 9.6764e-04, 9.9109e-02, 3.1493e-03, 8.3242e-04, 9.9674e-03,
        5.6490e-05, 3.7422e-04, 7.1252e-05, 3.0831e-06, 1.1749e-03, 2.4197e-03,
        1.4902e-03, 2.9939e-02, 1.3232e-04, 1.6785e-02, 1.0361e-02, 1.4897e-02,
        5.3142e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,349][circuit_model.py][line:1879][INFO] ############showing the attention weight of each circuit
[2024-07-24 10:25:24,350][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,352][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,353][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,354][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,355][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,356][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,356][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,357][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,358][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,359][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,359][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,360][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [Then] are: tensor([1.], device='cuda:0') for source tokens [Then]
[2024-07-24 10:25:24,362][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [,] are: tensor([0.0831, 0.9169], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,364][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [,] are: tensor([0.9072, 0.0928], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,365][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [,] are: tensor([0.0688, 0.9312], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,366][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [,] are: tensor([0.9989, 0.0011], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,367][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [,] are: tensor([0.0796, 0.9204], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,368][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [,] are: tensor([0.6971, 0.3029], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,370][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [,] are: tensor([0.7520, 0.2480], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,371][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [,] are: tensor([0.1688, 0.8312], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,372][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [,] are: tensor([0.0685, 0.9315], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,373][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [,] are: tensor([0.6886, 0.3114], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,374][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [,] are: tensor([0.8068, 0.1932], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,374][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [,] are: tensor([0.7505, 0.2495], device='cuda:0') for source tokens [Then,]
[2024-07-24 10:25:24,375][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Nathan] are: tensor([0.5142, 0.0763, 0.4095], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,377][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Nathan] are: tensor([0.7101, 0.0281, 0.2618], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,378][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Nathan] are: tensor([0.3191, 0.3747, 0.3062], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,380][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Nathan] are: tensor([0.7527, 0.0043, 0.2431], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,381][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Nathan] are: tensor([0.2596, 0.2526, 0.4878], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,382][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Nathan] are: tensor([0.2845, 0.3868, 0.3287], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,384][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Nathan] are: tensor([0.5838, 0.0846, 0.3316], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,385][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Nathan] are: tensor([0.5044, 0.1966, 0.2990], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,387][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Nathan] are: tensor([0.5390, 0.0345, 0.4266], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,389][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Nathan] are: tensor([0.4112, 0.1832, 0.4056], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,390][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Nathan] are: tensor([0.6182, 0.0509, 0.3308], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,392][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Nathan] are: tensor([0.9521, 0.0035, 0.0444], device='cuda:0') for source tokens [Then, Nathan]
[2024-07-24 10:25:24,393][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ and] are: tensor([0.0195, 0.6284, 0.0949, 0.2571], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,395][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ and] are: tensor([0.4067, 0.0541, 0.5264, 0.0128], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,396][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ and] are: tensor([0.0510, 0.5565, 0.2419, 0.1506], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,398][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ and] are: tensor([7.6357e-01, 1.2144e-04, 2.3208e-01, 4.2258e-03], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,399][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ and] are: tensor([0.0052, 0.0418, 0.0294, 0.9236], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,401][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ and] are: tensor([0.5115, 0.0378, 0.3823, 0.0684], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,402][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ and] are: tensor([0.5987, 0.1118, 0.2818, 0.0077], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,404][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ and] are: tensor([0.0923, 0.5793, 0.0963, 0.2320], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,406][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ and] are: tensor([0.0659, 0.1413, 0.0721, 0.7206], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,407][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ and] are: tensor([0.5674, 0.0148, 0.3657, 0.0522], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,409][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ and] are: tensor([0.4145, 0.0085, 0.5674, 0.0096], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,410][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ and] are: tensor([0.1745, 0.0152, 0.5478, 0.2625], device='cuda:0') for source tokens [Then, Nathan and]
[2024-07-24 10:25:24,412][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0394, 0.2179, 0.2608, 0.0580, 0.4239], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,414][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([0.5548, 0.0168, 0.3448, 0.0023, 0.0813], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,415][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0936, 0.4169, 0.2154, 0.1143, 0.1598], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,416][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([7.7734e-01, 3.9961e-04, 1.8834e-01, 5.0063e-03, 2.8910e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,417][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0235, 0.0423, 0.1048, 0.5333, 0.2961], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,418][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.3388, 0.0503, 0.2590, 0.0624, 0.2895], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,419][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.5764, 0.0369, 0.3522, 0.0134, 0.0211], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,420][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.2658, 0.4080, 0.1537, 0.0749, 0.0976], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,421][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1394, 0.0099, 0.3224, 0.1878, 0.3405], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,423][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.3468, 0.0091, 0.1945, 0.0281, 0.4215], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,424][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.2647, 0.0067, 0.3437, 0.0119, 0.3731], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,426][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([0.7934, 0.0021, 0.1945, 0.0021, 0.0079], device='cuda:0') for source tokens [Then, Nathan and Katie]
[2024-07-24 10:25:24,427][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ had] are: tensor([0.0173, 0.6090, 0.0627, 0.0638, 0.1226, 0.1245], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,428][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ had] are: tensor([6.1128e-01, 1.1082e-02, 2.6298e-01, 1.4565e-03, 1.1267e-01, 5.2562e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,430][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ had] are: tensor([0.0994, 0.2205, 0.2700, 0.0795, 0.2681, 0.0624], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,431][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ had] are: tensor([9.7366e-01, 2.3067e-05, 2.0755e-02, 2.9800e-05, 1.2890e-04, 5.4061e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,433][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ had] are: tensor([0.0018, 0.0404, 0.0174, 0.7835, 0.0729, 0.0840], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,434][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ had] are: tensor([0.4457, 0.0101, 0.3490, 0.0178, 0.1470, 0.0304], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,436][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ had] are: tensor([0.6937, 0.0617, 0.2281, 0.0058, 0.0060, 0.0047], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,437][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ had] are: tensor([0.2871, 0.4519, 0.1422, 0.0728, 0.0404, 0.0056], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,439][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ had] are: tensor([0.2427, 0.0373, 0.2968, 0.2037, 0.1058, 0.1137], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,441][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ had] are: tensor([0.5120, 0.0018, 0.2890, 0.0107, 0.1769, 0.0096], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,443][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ had] are: tensor([0.6813, 0.0027, 0.2413, 0.0010, 0.0560, 0.0178], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,444][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ had] are: tensor([0.8545, 0.0020, 0.1364, 0.0018, 0.0030, 0.0023], device='cuda:0') for source tokens [Then, Nathan and Katie had]
[2024-07-24 10:25:24,446][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0029, 0.3305, 0.0137, 0.0781, 0.0471, 0.1644, 0.3633],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,447][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([8.1388e-01, 5.3852e-03, 1.5655e-01, 8.8112e-04, 2.2805e-02, 4.7712e-04,
        2.9250e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,448][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.2284, 0.1699, 0.2444, 0.0474, 0.1883, 0.1159, 0.0058],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,450][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([8.6849e-01, 4.8949e-06, 1.6868e-02, 6.8669e-05, 1.2982e-04, 1.1352e-01,
        9.1603e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,451][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0089, 0.0312, 0.0312, 0.6335, 0.0776, 0.1744, 0.0430],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,453][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.5818, 0.0119, 0.2416, 0.0130, 0.1182, 0.0292, 0.0044],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,454][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([7.5544e-01, 4.4283e-02, 1.8296e-01, 4.6629e-03, 4.4767e-03, 7.8754e-03,
        3.0340e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,456][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.2501, 0.4766, 0.1085, 0.1111, 0.0377, 0.0133, 0.0027],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,457][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2054, 0.0379, 0.0585, 0.1590, 0.0296, 0.4969, 0.0128],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,458][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([7.4417e-01, 1.2002e-03, 1.8345e-01, 3.0255e-03, 6.3688e-02, 4.1634e-03,
        3.0532e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,460][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([7.7767e-01, 1.0409e-03, 1.5642e-01, 6.5554e-04, 2.9136e-02, 3.4707e-02,
        3.7519e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,461][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.6364, 0.0025, 0.2543, 0.0148, 0.0114, 0.0742, 0.0064],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a]
[2024-07-24 10:25:24,462][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ lot] are: tensor([0.0013, 0.1290, 0.0050, 0.0498, 0.0242, 0.3401, 0.1060, 0.3445],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,463][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ lot] are: tensor([5.3555e-01, 4.0317e-02, 2.1796e-01, 5.2193e-03, 1.8734e-01, 7.6512e-03,
        4.7183e-04, 5.4899e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,464][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ lot] are: tensor([0.0751, 0.2347, 0.1116, 0.1599, 0.1073, 0.2481, 0.0360, 0.0274],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,465][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ lot] are: tensor([8.8737e-01, 5.0468e-06, 1.1638e-02, 1.7986e-05, 4.1163e-05, 7.2112e-02,
        3.4072e-04, 2.8480e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,465][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ lot] are: tensor([0.0144, 0.0375, 0.0487, 0.4952, 0.0876, 0.1636, 0.0554, 0.0975],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,467][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ lot] are: tensor([0.0882, 0.0465, 0.0854, 0.0799, 0.2268, 0.2698, 0.0710, 0.1323],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,469][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ lot] are: tensor([0.4089, 0.0937, 0.2481, 0.0350, 0.0498, 0.1410, 0.0052, 0.0182],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,470][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ lot] are: tensor([0.0833, 0.4241, 0.0507, 0.2634, 0.0506, 0.0795, 0.0090, 0.0393],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,472][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ lot] are: tensor([0.5083, 0.0053, 0.0564, 0.0145, 0.0052, 0.2988, 0.0105, 0.1010],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,474][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ lot] are: tensor([0.1612, 0.0103, 0.1013, 0.0405, 0.3253, 0.1560, 0.0188, 0.1866],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,475][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ lot] are: tensor([0.5732, 0.0023, 0.1210, 0.0034, 0.0628, 0.1715, 0.0020, 0.0638],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,476][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ lot] are: tensor([9.4084e-01, 1.3985e-03, 3.8957e-02, 6.9257e-04, 6.0852e-04, 1.5058e-02,
        3.6214e-04, 2.0785e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot]
[2024-07-24 10:25:24,478][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0016, 0.0582, 0.0046, 0.0308, 0.0164, 0.1568, 0.1298, 0.4453, 0.1565],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,479][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([6.3073e-01, 6.7158e-03, 2.8530e-01, 2.5213e-03, 6.9672e-02, 3.9440e-03,
        7.7258e-05, 8.5109e-04, 1.8879e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,481][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.1966, 0.0906, 0.2750, 0.0928, 0.0885, 0.2229, 0.0119, 0.0089, 0.0128],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,482][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([7.1577e-01, 2.7382e-06, 1.9225e-02, 5.4617e-05, 9.6729e-05, 8.8845e-02,
        1.0388e-03, 1.7357e-01, 1.4026e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,484][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0055, 0.0216, 0.0237, 0.6485, 0.0501, 0.1424, 0.0355, 0.0198, 0.0529],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,486][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.3837, 0.0137, 0.2892, 0.0268, 0.1191, 0.0979, 0.0127, 0.0216, 0.0353],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,487][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.6072, 0.0723, 0.2528, 0.0124, 0.0088, 0.0415, 0.0007, 0.0032, 0.0011],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,489][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1119, 0.5643, 0.0864, 0.1476, 0.0433, 0.0166, 0.0019, 0.0165, 0.0115],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,491][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.2951, 0.0884, 0.0312, 0.0872, 0.0051, 0.4721, 0.0025, 0.0067, 0.0116],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,492][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.5726, 0.0039, 0.3361, 0.0095, 0.0485, 0.0231, 0.0009, 0.0039, 0.0015],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,494][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([6.0800e-01, 7.8568e-04, 2.6085e-01, 1.4983e-03, 5.5059e-02, 3.8938e-02,
        5.5346e-04, 3.2744e-02, 1.5667e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,495][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.5937, 0.0016, 0.3206, 0.0128, 0.0055, 0.0468, 0.0024, 0.0137, 0.0028],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of]
[2024-07-24 10:25:24,497][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ fun] are: tensor([1.1993e-04, 4.2915e-02, 1.6247e-03, 3.5313e-02, 1.2954e-02, 1.2666e-01,
        1.6192e-01, 4.5542e-01, 1.6199e-01, 1.0800e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,498][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ fun] are: tensor([6.5761e-01, 3.5786e-02, 2.1348e-01, 6.0672e-03, 7.3405e-02, 7.7000e-03,
        2.6224e-04, 4.0522e-03, 8.6361e-04, 7.7267e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,500][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ fun] are: tensor([0.0834, 0.3315, 0.1149, 0.0923, 0.0557, 0.2348, 0.0188, 0.0195, 0.0367,
        0.0124], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,501][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ fun] are: tensor([1.3518e-01, 4.5786e-06, 5.8747e-03, 1.2983e-04, 9.6363e-05, 1.8809e-01,
        8.4634e-03, 6.4582e-01, 1.5909e-02, 4.2839e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,503][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ fun] are: tensor([0.0166, 0.0372, 0.0600, 0.4355, 0.0946, 0.1893, 0.0410, 0.0547, 0.0593,
        0.0117], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,504][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ fun] are: tensor([0.4844, 0.0167, 0.1540, 0.0166, 0.1584, 0.0421, 0.0125, 0.0453, 0.0502,
        0.0198], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,506][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ fun] are: tensor([0.7345, 0.0338, 0.1693, 0.0047, 0.0092, 0.0357, 0.0008, 0.0085, 0.0014,
        0.0021], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,508][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ fun] are: tensor([0.5058, 0.1801, 0.1176, 0.0734, 0.0403, 0.0295, 0.0042, 0.0186, 0.0212,
        0.0094], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,509][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ fun] are: tensor([0.0897, 0.0494, 0.0200, 0.0879, 0.0039, 0.6319, 0.0125, 0.0281, 0.0743,
        0.0023], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,509][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ fun] are: tensor([0.5217, 0.0038, 0.1735, 0.0056, 0.1851, 0.0170, 0.0015, 0.0854, 0.0023,
        0.0042], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,510][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ fun] are: tensor([0.6356, 0.0026, 0.1893, 0.0013, 0.0532, 0.0744, 0.0010, 0.0386, 0.0034,
        0.0007], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,511][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ fun] are: tensor([6.4499e-01, 3.0049e-03, 1.5240e-01, 1.2088e-02, 7.4807e-03, 1.0907e-01,
        1.6230e-02, 4.6077e-02, 8.3638e-03, 2.9602e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun]
[2024-07-24 10:25:24,513][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ at] are: tensor([0.0018, 0.1265, 0.0095, 0.0435, 0.0319, 0.1281, 0.1079, 0.3872, 0.1439,
        0.0048, 0.0148], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,514][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ at] are: tensor([7.3360e-01, 5.5946e-03, 1.9891e-01, 1.9634e-03, 5.6137e-02, 1.6736e-03,
        7.3776e-05, 1.3464e-03, 1.4599e-04, 2.3029e-04, 3.2039e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,516][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ at] are: tensor([0.0703, 0.1317, 0.1826, 0.1083, 0.1485, 0.2517, 0.0228, 0.0216, 0.0219,
        0.0042, 0.0364], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,517][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ at] are: tensor([7.8706e-01, 1.4247e-05, 2.4904e-02, 7.8512e-05, 1.8593e-04, 4.0477e-02,
        5.1394e-04, 1.4329e-01, 1.6416e-03, 3.1372e-04, 1.5229e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,518][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ at] are: tensor([0.0118, 0.0222, 0.0378, 0.5515, 0.0868, 0.1546, 0.0321, 0.0241, 0.0491,
        0.0057, 0.0242], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,520][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ at] are: tensor([0.1165, 0.0175, 0.1227, 0.0820, 0.2188, 0.1538, 0.0476, 0.0792, 0.1004,
        0.0307, 0.0308], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,522][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ at] are: tensor([0.6080, 0.0663, 0.2526, 0.0155, 0.0074, 0.0390, 0.0012, 0.0054, 0.0017,
        0.0008, 0.0021], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,523][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ at] are: tensor([0.1449, 0.2860, 0.1246, 0.1961, 0.0553, 0.0399, 0.0053, 0.0522, 0.0277,
        0.0233, 0.0447], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,525][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ at] are: tensor([0.4425, 0.0304, 0.0517, 0.0540, 0.0100, 0.1984, 0.0103, 0.0350, 0.0437,
        0.0022, 0.1216], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,527][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ at] are: tensor([0.2710, 0.0026, 0.2522, 0.0246, 0.2664, 0.0651, 0.0070, 0.0704, 0.0083,
        0.0071, 0.0253], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,529][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ at] are: tensor([0.5018, 0.0010, 0.3057, 0.0015, 0.1074, 0.0329, 0.0008, 0.0414, 0.0016,
        0.0007, 0.0052], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,530][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ at] are: tensor([7.7764e-01, 2.5753e-03, 1.6552e-01, 7.7532e-03, 4.4138e-03, 1.7045e-02,
        1.7905e-03, 9.4723e-03, 1.9719e-03, 9.3498e-05, 1.1729e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at]
[2024-07-24 10:25:24,531][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0017, 0.1238, 0.0087, 0.0314, 0.0232, 0.0807, 0.0975, 0.4126, 0.1436,
        0.0086, 0.0088, 0.0596], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,533][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([6.8300e-01, 2.5183e-02, 1.9321e-01, 4.1645e-03, 9.1217e-02, 1.1206e-03,
        4.8332e-05, 1.1188e-03, 2.1352e-04, 2.6041e-04, 3.2068e-04, 1.4866e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,534][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0698, 0.2959, 0.1383, 0.0602, 0.1770, 0.1107, 0.0097, 0.0329, 0.0115,
        0.0118, 0.0272, 0.0550], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,535][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([8.5543e-01, 4.0422e-06, 1.1925e-02, 5.4639e-05, 8.4539e-05, 5.0762e-02,
        1.8606e-04, 7.8979e-02, 8.4165e-04, 2.2025e-04, 7.7447e-04, 7.3936e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,537][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0030, 0.0314, 0.0088, 0.5031, 0.0289, 0.0929, 0.0174, 0.0156, 0.0534,
        0.0021, 0.0133, 0.2302], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,539][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1093, 0.0523, 0.1304, 0.0751, 0.2948, 0.1029, 0.0283, 0.0212, 0.0707,
        0.0340, 0.0290, 0.0520], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,541][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.5677, 0.2038, 0.1695, 0.0119, 0.0067, 0.0258, 0.0008, 0.0045, 0.0006,
        0.0013, 0.0017, 0.0057], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,542][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0419, 0.6444, 0.0305, 0.1512, 0.0247, 0.0127, 0.0019, 0.0096, 0.0089,
        0.0097, 0.0161, 0.0484], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,544][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.5500, 0.0275, 0.0576, 0.0632, 0.0215, 0.1154, 0.0033, 0.0515, 0.0432,
        0.0029, 0.0600, 0.0039], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,546][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.3315, 0.0041, 0.2303, 0.0312, 0.2353, 0.0461, 0.0035, 0.0624, 0.0046,
        0.0100, 0.0175, 0.0236], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,547][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([7.0926e-01, 1.8214e-03, 1.5583e-01, 1.1183e-03, 6.4401e-02, 1.6897e-02,
        2.0932e-04, 4.6375e-02, 9.8871e-04, 4.4519e-04, 2.0406e-03, 6.1567e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,549][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([5.1253e-01, 3.3123e-03, 2.2077e-01, 3.4830e-02, 1.5923e-02, 1.0324e-01,
        6.0256e-03, 2.1192e-02, 1.9818e-03, 1.5342e-04, 2.4804e-02, 5.5236e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the]
[2024-07-24 10:25:24,550][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ house] are: tensor([0.0005, 0.1454, 0.0042, 0.0631, 0.0100, 0.0858, 0.1950, 0.3025, 0.1026,
        0.0003, 0.0112, 0.0489, 0.0304], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,552][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ house] are: tensor([7.8769e-01, 2.7334e-02, 1.6734e-01, 1.7095e-03, 1.2402e-02, 2.5402e-03,
        8.0247e-05, 5.5441e-04, 1.0309e-04, 5.9678e-05, 8.0975e-05, 8.3460e-05,
        2.5389e-05], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,553][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ house] are: tensor([0.0493, 0.3382, 0.0786, 0.0566, 0.0170, 0.1978, 0.0180, 0.0161, 0.0173,
        0.0033, 0.0184, 0.1544, 0.0351], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,554][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ house] are: tensor([9.9358e-01, 5.4882e-07, 2.6211e-03, 1.3175e-06, 5.3083e-07, 1.4321e-03,
        1.7393e-06, 3.9409e-04, 1.4970e-05, 5.3964e-07, 1.0184e-05, 1.2493e-05,
        1.9266e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,555][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ house] are: tensor([0.0035, 0.0099, 0.0102, 0.1248, 0.0131, 0.0117, 0.0090, 0.0092, 0.0098,
        0.0009, 0.0045, 0.1690, 0.6243], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,555][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ house] are: tensor([0.1381, 0.0118, 0.1022, 0.0346, 0.1464, 0.1788, 0.0509, 0.0714, 0.1089,
        0.0279, 0.0234, 0.0696, 0.0360], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,557][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ house] are: tensor([6.1465e-01, 1.2042e-01, 1.9669e-01, 6.9343e-03, 4.5100e-03, 3.8101e-02,
        1.3782e-03, 3.9743e-03, 1.0894e-03, 1.0606e-03, 2.1385e-03, 8.5307e-03,
        5.2526e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,558][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ house] are: tensor([0.2888, 0.2851, 0.1204, 0.0889, 0.0603, 0.0275, 0.0034, 0.0204, 0.0122,
        0.0101, 0.0160, 0.0460, 0.0209], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,560][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ house] are: tensor([0.0080, 0.0112, 0.0119, 0.1342, 0.0018, 0.2218, 0.0074, 0.0216, 0.1056,
        0.0019, 0.0877, 0.0109, 0.3761], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,561][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ house] are: tensor([7.8064e-01, 1.3692e-03, 1.4464e-01, 3.0254e-03, 3.2142e-02, 9.3208e-03,
        6.1674e-04, 1.7878e-02, 1.0031e-03, 9.0881e-04, 4.0442e-03, 1.9109e-03,
        2.5060e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,563][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ house] are: tensor([8.5612e-01, 1.0298e-03, 8.9442e-02, 5.8329e-04, 9.1915e-03, 2.1318e-02,
        8.7296e-05, 1.0642e-02, 6.2090e-04, 1.7280e-04, 1.9516e-03, 2.6591e-04,
        8.5789e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,564][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ house] are: tensor([9.4690e-01, 4.2307e-04, 4.2925e-02, 4.6012e-04, 5.0808e-04, 5.9987e-03,
        3.3854e-05, 3.7699e-04, 6.1351e-05, 3.4856e-06, 3.7124e-04, 8.2940e-04,
        1.1120e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house]
[2024-07-24 10:25:24,565][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [.] are: tensor([0.0412, 0.3238, 0.0788, 0.0770, 0.0311, 0.0822, 0.0520, 0.1049, 0.0435,
        0.0025, 0.0153, 0.0705, 0.0079, 0.0693], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,567][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [.] are: tensor([5.5519e-01, 4.0504e-02, 2.6900e-01, 2.9397e-03, 1.1600e-01, 1.9895e-03,
        5.0057e-05, 1.3101e-03, 3.3435e-04, 1.9423e-04, 2.6462e-04, 8.7194e-05,
        1.1666e-04, 1.2031e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,568][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [.] are: tensor([0.0656, 0.1474, 0.1331, 0.0517, 0.2256, 0.0602, 0.0042, 0.0042, 0.0038,
        0.0029, 0.0058, 0.0184, 0.0094, 0.2677], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,570][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [.] are: tensor([9.8441e-01, 2.5145e-06, 6.8390e-03, 5.3136e-06, 1.1190e-05, 2.6639e-03,
        6.6622e-06, 2.6865e-03, 7.9300e-05, 7.6661e-06, 5.5873e-05, 2.9017e-05,
        1.5253e-03, 1.6751e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,571][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [.] are: tensor([0.0005, 0.0100, 0.0029, 0.2434, 0.0134, 0.0643, 0.0094, 0.0049, 0.0300,
        0.0014, 0.0042, 0.0502, 0.2229, 0.3424], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,573][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [.] are: tensor([0.0602, 0.0127, 0.0598, 0.0277, 0.0872, 0.0446, 0.0096, 0.0140, 0.0233,
        0.0101, 0.0110, 0.0083, 0.0160, 0.6155], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,574][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [.] are: tensor([5.5550e-01, 2.0484e-01, 1.6914e-01, 1.0076e-02, 7.6586e-03, 1.9150e-02,
        3.1392e-04, 9.8130e-04, 3.3184e-04, 2.0985e-04, 5.8997e-04, 1.9273e-03,
        2.0034e-04, 2.9085e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,576][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [.] are: tensor([0.0485, 0.3802, 0.0279, 0.0669, 0.0161, 0.0129, 0.0009, 0.0057, 0.0058,
        0.0027, 0.0070, 0.0206, 0.0098, 0.3950], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,578][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [.] are: tensor([0.1367, 0.1026, 0.0246, 0.1197, 0.0096, 0.2556, 0.0070, 0.0350, 0.0373,
        0.0017, 0.0552, 0.0053, 0.0488, 0.1608], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,580][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [.] are: tensor([0.4677, 0.0077, 0.2301, 0.0206, 0.1484, 0.0593, 0.0021, 0.0150, 0.0024,
        0.0010, 0.0047, 0.0037, 0.0044, 0.0327], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,581][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [.] are: tensor([6.1857e-01, 5.0547e-03, 2.1749e-01, 1.9573e-03, 1.0197e-01, 1.2144e-02,
        2.3518e-04, 6.1220e-03, 4.5176e-04, 1.7607e-04, 1.0123e-03, 4.1308e-04,
        4.1458e-04, 3.3992e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,582][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [.] are: tensor([7.9932e-01, 2.1904e-03, 1.1177e-01, 9.3029e-03, 3.8729e-03, 2.2721e-02,
        1.0917e-03, 1.3044e-03, 6.2363e-04, 1.8389e-05, 3.9483e-03, 5.7424e-03,
        1.4050e-03, 3.6688e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house.]
[2024-07-24 10:25:24,584][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Katie] are: tensor([0.0070, 0.0582, 0.0504, 0.0138, 0.0983, 0.0560, 0.0386, 0.2455, 0.0451,
        0.0030, 0.0089, 0.0329, 0.0191, 0.0485, 0.2747], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,585][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Katie] are: tensor([6.2601e-01, 2.4318e-02, 2.4077e-01, 3.7919e-03, 5.4165e-02, 6.4766e-03,
        2.4924e-04, 5.3739e-03, 4.1852e-04, 6.6346e-04, 1.6386e-03, 7.2257e-04,
        1.1442e-03, 2.0677e-02, 1.3577e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,587][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Katie] are: tensor([0.0510, 0.1765, 0.0470, 0.0494, 0.0319, 0.1679, 0.0105, 0.0087, 0.0182,
        0.0007, 0.0154, 0.1026, 0.0141, 0.3009, 0.0053], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,588][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Katie] are: tensor([6.0761e-02, 1.0591e-06, 2.9877e-03, 3.0769e-05, 7.3019e-05, 3.4700e-02,
        1.4328e-03, 2.0866e-01, 3.7082e-03, 3.6748e-04, 6.4417e-03, 1.5298e-02,
        5.6863e-01, 9.5740e-02, 1.1679e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,590][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Katie] are: tensor([0.0061, 0.0029, 0.0128, 0.0547, 0.0157, 0.1120, 0.0096, 0.0192, 0.0181,
        0.0034, 0.0207, 0.0773, 0.3410, 0.2926, 0.0139], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,592][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Katie] are: tensor([0.1409, 0.0114, 0.0753, 0.0244, 0.0622, 0.1390, 0.0324, 0.0505, 0.0420,
        0.0089, 0.0241, 0.0435, 0.0460, 0.2682, 0.0312], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,594][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Katie] are: tensor([0.4516, 0.0309, 0.2065, 0.0182, 0.0138, 0.0918, 0.0051, 0.0137, 0.0053,
        0.0008, 0.0104, 0.0600, 0.0028, 0.0869, 0.0022], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,595][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Katie] are: tensor([0.2467, 0.2259, 0.0891, 0.0637, 0.0489, 0.0194, 0.0039, 0.0268, 0.0142,
        0.0062, 0.0214, 0.0341, 0.0322, 0.1557, 0.0117], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,597][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Katie] are: tensor([0.1569, 0.0027, 0.0516, 0.0122, 0.0061, 0.0648, 0.0095, 0.0455, 0.0541,
        0.0020, 0.1593, 0.0324, 0.2666, 0.1256, 0.0105], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,598][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Katie] are: tensor([0.1750, 0.0120, 0.0915, 0.0561, 0.1102, 0.1365, 0.0199, 0.0809, 0.0097,
        0.0014, 0.0509, 0.0807, 0.0409, 0.0774, 0.0570], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,599][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Katie] are: tensor([0.4296, 0.0011, 0.1233, 0.0021, 0.0408, 0.1531, 0.0023, 0.0616, 0.0053,
        0.0010, 0.0168, 0.0048, 0.0101, 0.1218, 0.0262], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,600][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Katie] are: tensor([8.2622e-01, 5.2050e-04, 1.0276e-01, 1.2901e-03, 1.3190e-03, 1.1797e-02,
        8.8129e-04, 2.5575e-03, 1.2589e-03, 1.2935e-05, 3.1922e-03, 1.3570e-02,
        2.3375e-03, 3.1439e-02, 8.4776e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie]
[2024-07-24 10:25:24,601][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ gave] are: tensor([0.0246, 0.2347, 0.0661, 0.0220, 0.0455, 0.0840, 0.0263, 0.0953, 0.0167,
        0.0004, 0.0037, 0.0234, 0.0033, 0.0126, 0.0414, 0.3001],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,602][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ gave] are: tensor([7.1423e-01, 1.4357e-02, 2.3665e-01, 9.1493e-04, 2.8666e-02, 5.6000e-04,
        2.0270e-05, 3.5670e-04, 3.6648e-05, 1.0545e-04, 6.0217e-05, 3.3048e-05,
        3.3673e-05, 1.9978e-03, 1.2506e-03, 7.2784e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,604][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ gave] are: tensor([0.0704, 0.1482, 0.1896, 0.0596, 0.0989, 0.0676, 0.0077, 0.0053, 0.0074,
        0.0023, 0.0055, 0.0671, 0.0090, 0.2324, 0.0059, 0.0233],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,605][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ gave] are: tensor([9.5939e-01, 4.6016e-06, 4.2387e-03, 8.2529e-06, 8.8964e-06, 4.2176e-03,
        3.6529e-06, 1.9352e-03, 3.0196e-05, 1.4544e-05, 6.7073e-05, 3.8361e-05,
        6.3228e-03, 5.0749e-03, 2.1764e-05, 1.8621e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,607][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ gave] are: tensor([0.0020, 0.0156, 0.0102, 0.2133, 0.0317, 0.0553, 0.0102, 0.0070, 0.0161,
        0.0025, 0.0077, 0.0742, 0.2301, 0.2308, 0.0093, 0.0841],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,609][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ gave] are: tensor([0.1716, 0.0105, 0.1837, 0.0329, 0.1387, 0.0514, 0.0140, 0.0205, 0.0203,
        0.0119, 0.0058, 0.0177, 0.0105, 0.2543, 0.0245, 0.0318],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,610][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ gave] are: tensor([3.7726e-01, 2.0138e-01, 2.3947e-01, 3.0672e-02, 6.6802e-03, 3.3776e-02,
        1.3063e-03, 4.2840e-03, 1.2916e-03, 8.8399e-04, 1.7040e-03, 1.3487e-02,
        5.3893e-04, 6.3429e-02, 2.6537e-04, 2.3564e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,612][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ gave] are: tensor([0.1383, 0.2593, 0.1077, 0.0712, 0.0411, 0.0170, 0.0013, 0.0144, 0.0074,
        0.0076, 0.0131, 0.0312, 0.0211, 0.1791, 0.0042, 0.0861],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,614][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ gave] are: tensor([0.2380, 0.0066, 0.0933, 0.0300, 0.0106, 0.0545, 0.0051, 0.0504, 0.0638,
        0.0022, 0.1220, 0.0089, 0.1381, 0.1010, 0.0077, 0.0680],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,615][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ gave] are: tensor([0.3655, 0.0064, 0.3051, 0.0307, 0.1416, 0.0351, 0.0030, 0.0278, 0.0032,
        0.0017, 0.0093, 0.0099, 0.0066, 0.0219, 0.0129, 0.0193],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,617][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ gave] are: tensor([8.6324e-01, 6.2156e-04, 1.0640e-01, 1.9741e-04, 1.0751e-02, 4.5056e-03,
        2.5641e-05, 2.3028e-03, 1.2383e-04, 6.2688e-05, 3.7507e-04, 8.7875e-05,
        5.7992e-04, 4.8759e-03, 3.2464e-03, 2.6045e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,618][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ gave] are: tensor([9.4985e-01, 3.1246e-04, 3.5718e-02, 3.6179e-04, 3.0420e-04, 1.0610e-03,
        1.6093e-05, 1.1979e-04, 3.3882e-05, 5.8005e-07, 1.4912e-04, 2.7447e-04,
        1.0239e-04, 2.4735e-03, 6.3457e-05, 9.1545e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave]
[2024-07-24 10:25:24,620][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0076, 0.1971, 0.0194, 0.0241, 0.0158, 0.0396, 0.0271, 0.0850, 0.0291,
        0.0010, 0.0058, 0.0287, 0.0132, 0.0308, 0.0241, 0.2781, 0.1734],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,621][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([7.8309e-01, 1.3183e-02, 1.6816e-01, 1.3694e-03, 2.8609e-02, 4.4519e-04,
        1.5051e-05, 3.2137e-04, 5.2400e-05, 5.8969e-05, 9.0037e-05, 4.6995e-05,
        3.8831e-05, 2.4248e-03, 1.3542e-03, 6.5677e-04, 8.8655e-05],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,623][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0727, 0.2394, 0.1118, 0.0469, 0.1231, 0.0449, 0.0020, 0.0057, 0.0035,
        0.0019, 0.0071, 0.0216, 0.0098, 0.2698, 0.0118, 0.0213, 0.0067],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,624][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([9.6723e-01, 2.7734e-07, 3.5705e-03, 1.8983e-06, 5.1827e-06, 1.7649e-03,
        1.0924e-06, 1.9086e-03, 1.4660e-05, 5.3265e-06, 3.8521e-05, 1.6523e-05,
        7.0454e-03, 1.5986e-03, 2.1186e-05, 1.5874e-02, 8.9852e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,626][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0014, 0.0118, 0.0057, 0.1847, 0.0165, 0.0618, 0.0061, 0.0069, 0.0185,
        0.0015, 0.0064, 0.0797, 0.2659, 0.2732, 0.0066, 0.0332, 0.0199],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,628][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1918, 0.0207, 0.1635, 0.0340, 0.1376, 0.0390, 0.0048, 0.0075, 0.0134,
        0.0102, 0.0090, 0.0148, 0.0103, 0.2798, 0.0355, 0.0114, 0.0165],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,629][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([6.0223e-01, 1.5094e-01, 1.7554e-01, 9.5567e-03, 5.1198e-03, 1.5277e-02,
        2.8462e-04, 2.5638e-03, 2.5772e-04, 4.0302e-04, 1.0610e-03, 3.7855e-03,
        5.0665e-04, 2.1722e-02, 2.4768e-04, 6.4779e-03, 4.0310e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,631][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1724, 0.4015, 0.0615, 0.0657, 0.0230, 0.0105, 0.0008, 0.0056, 0.0032,
        0.0049, 0.0095, 0.0237, 0.0120, 0.1781, 0.0029, 0.0187, 0.0060],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,632][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([6.0049e-01, 3.2047e-02, 4.4850e-02, 4.0836e-02, 8.7540e-03, 1.0294e-01,
        1.0380e-03, 7.8542e-03, 8.8166e-03, 4.2699e-04, 1.8906e-02, 1.2927e-03,
        3.7367e-02, 5.5262e-02, 1.4327e-03, 2.7919e-02, 9.7708e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,634][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.4994, 0.0052, 0.3014, 0.0177, 0.0967, 0.0246, 0.0006, 0.0121, 0.0006,
        0.0011, 0.0047, 0.0036, 0.0079, 0.0106, 0.0093, 0.0034, 0.0009],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,635][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([8.5091e-01, 9.9896e-04, 1.1344e-01, 3.9133e-04, 1.3127e-02, 6.3510e-03,
        2.8508e-05, 1.9888e-03, 7.4567e-05, 3.7214e-05, 4.3802e-04, 8.9515e-05,
        6.1168e-04, 6.6076e-03, 2.8346e-03, 1.8474e-03, 2.2169e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,636][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([8.1684e-01, 6.7085e-04, 1.0932e-01, 3.3131e-03, 1.4216e-03, 7.4302e-03,
        1.7285e-04, 6.7572e-04, 1.0105e-04, 5.9161e-06, 1.0352e-03, 3.2199e-03,
        2.2290e-03, 1.8580e-02, 2.5839e-04, 1.7880e-02, 1.6842e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a]
[2024-07-24 10:25:24,638][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ snack] are: tensor([0.0208, 0.2562, 0.0768, 0.0261, 0.0484, 0.0917, 0.0191, 0.0315, 0.0199,
        0.0004, 0.0074, 0.0240, 0.0079, 0.0257, 0.0467, 0.1630, 0.0608, 0.0736],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,640][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ snack] are: tensor([7.7239e-01, 2.9773e-02, 1.5566e-01, 1.0681e-03, 2.4975e-02, 1.6308e-03,
        7.1861e-05, 4.7673e-04, 8.5296e-05, 7.5851e-05, 3.3136e-04, 9.1908e-05,
        6.3185e-05, 2.0227e-03, 2.4751e-03, 1.5589e-03, 2.5448e-04, 6.9991e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,642][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ snack] are: tensor([0.2797, 0.2198, 0.1386, 0.0226, 0.0239, 0.0619, 0.0027, 0.0029, 0.0087,
        0.0004, 0.0064, 0.0255, 0.0042, 0.1290, 0.0022, 0.0311, 0.0133, 0.0270],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,643][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ snack] are: tensor([9.5995e-01, 3.2069e-06, 3.4453e-03, 1.9368e-06, 9.5220e-06, 1.4100e-03,
        8.0455e-06, 5.4644e-04, 4.0064e-05, 3.7250e-06, 1.0535e-04, 8.1610e-05,
        3.2346e-03, 2.2366e-03, 2.9310e-05, 1.3229e-02, 4.6212e-03, 1.1046e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,644][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ snack] are: tensor([0.0128, 0.0076, 0.0231, 0.0800, 0.0188, 0.0418, 0.0118, 0.0093, 0.0115,
        0.0051, 0.0139, 0.0537, 0.3306, 0.2257, 0.0114, 0.0757, 0.0372, 0.0301],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,645][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ snack] are: tensor([0.1557, 0.0627, 0.0577, 0.0310, 0.0579, 0.0882, 0.0238, 0.0434, 0.0491,
        0.0110, 0.0227, 0.0325, 0.0224, 0.1940, 0.0205, 0.0512, 0.0343, 0.0421],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,646][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ snack] are: tensor([6.9515e-01, 3.5841e-02, 1.5574e-01, 5.7106e-03, 3.8524e-03, 2.1438e-02,
        6.6604e-04, 1.5881e-03, 1.0447e-03, 2.5254e-04, 1.9051e-03, 3.4852e-03,
        2.2954e-04, 1.8811e-02, 3.1229e-04, 1.5597e-02, 7.0623e-03, 3.1319e-02],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,648][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ snack] are: tensor([0.3705, 0.1817, 0.0576, 0.0308, 0.0121, 0.0187, 0.0021, 0.0079, 0.0049,
        0.0022, 0.0130, 0.0158, 0.0078, 0.0904, 0.0034, 0.0518, 0.0117, 0.1175],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,649][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ snack] are: tensor([0.1347, 0.0170, 0.0469, 0.0134, 0.0033, 0.0795, 0.0099, 0.0372, 0.0511,
        0.0029, 0.0650, 0.0221, 0.1235, 0.0913, 0.0040, 0.0644, 0.1518, 0.0822],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,651][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ snack] are: tensor([0.4762, 0.0265, 0.0790, 0.0223, 0.0588, 0.0536, 0.0038, 0.0176, 0.0025,
        0.0006, 0.0119, 0.0107, 0.0118, 0.0506, 0.0210, 0.0586, 0.0060, 0.0884],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,653][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ snack] are: tensor([7.8928e-01, 1.2643e-02, 1.3663e-01, 1.1378e-03, 1.5529e-02, 1.6850e-02,
        2.8580e-04, 1.0323e-03, 3.3469e-04, 7.2389e-05, 2.0483e-03, 4.6576e-04,
        7.9359e-04, 1.3531e-02, 1.8884e-03, 3.1842e-03, 8.2941e-04, 3.4703e-03],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,654][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ snack] are: tensor([9.9108e-01, 2.2071e-04, 7.1037e-03, 4.5463e-06, 3.2055e-05, 1.8966e-04,
        1.5320e-06, 2.1617e-06, 1.4511e-06, 1.7382e-08, 1.6947e-05, 4.8921e-06,
        1.8124e-06, 1.1714e-04, 3.8769e-06, 7.6365e-04, 8.3320e-05, 3.7568e-04],
       device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack]
[2024-07-24 10:25:24,656][circuit_model.py][line:2332][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ to] are: tensor([0.0154, 0.1372, 0.0274, 0.0232, 0.0138, 0.0635, 0.0176, 0.0237, 0.0142,
        0.0006, 0.0041, 0.0137, 0.0061, 0.0246, 0.0097, 0.2418, 0.1845, 0.1108,
        0.0680], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,657][circuit_model.py][line:2335][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ to] are: tensor([6.5016e-01, 3.1818e-02, 2.2180e-01, 3.5683e-03, 7.3800e-02, 2.2355e-03,
        5.0032e-05, 3.4309e-04, 7.0270e-05, 9.0435e-05, 1.3641e-04, 1.0073e-04,
        4.9024e-05, 5.1231e-03, 2.5021e-03, 1.1477e-03, 2.2933e-04, 6.2682e-03,
        5.0319e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,659][circuit_model.py][line:2338][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ to] are: tensor([0.0370, 0.3033, 0.0686, 0.0371, 0.1063, 0.0532, 0.0028, 0.0041, 0.0027,
        0.0016, 0.0072, 0.0202, 0.0105, 0.2437, 0.0095, 0.0122, 0.0085, 0.0174,
        0.0540], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,660][circuit_model.py][line:2341][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ to] are: tensor([9.8878e-01, 1.0837e-07, 1.4093e-03, 2.0577e-07, 6.2578e-07, 4.9107e-04,
        1.5984e-07, 1.3737e-04, 1.0721e-06, 3.3307e-07, 5.2289e-06, 2.7086e-06,
        1.1595e-03, 3.8985e-04, 1.5329e-06, 1.7082e-03, 2.0545e-04, 2.4742e-03,
        3.2324e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,662][circuit_model.py][line:2344][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ to] are: tensor([0.0018, 0.0079, 0.0050, 0.1378, 0.0099, 0.0440, 0.0042, 0.0021, 0.0136,
        0.0009, 0.0043, 0.0541, 0.3026, 0.3188, 0.0047, 0.0288, 0.0141, 0.0030,
        0.0425], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,664][circuit_model.py][line:2347][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ to] are: tensor([0.0869, 0.0271, 0.0647, 0.0361, 0.0516, 0.0736, 0.0074, 0.0059, 0.0120,
        0.0049, 0.0165, 0.0146, 0.0072, 0.5077, 0.0067, 0.0162, 0.0175, 0.0205,
        0.0228], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,665][circuit_model.py][line:2350][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ to] are: tensor([4.0054e-01, 3.0343e-01, 1.2877e-01, 1.8991e-02, 4.9940e-03, 3.3977e-02,
        4.8953e-04, 2.7122e-03, 9.5755e-04, 1.1641e-03, 1.7108e-03, 6.4084e-03,
        1.5579e-03, 5.2278e-02, 2.1913e-04, 8.3377e-03, 3.9216e-03, 2.3228e-02,
        6.3147e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,667][circuit_model.py][line:2353][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ to] are: tensor([0.1686, 0.3114, 0.0385, 0.0442, 0.0125, 0.0128, 0.0007, 0.0033, 0.0038,
        0.0023, 0.0086, 0.0218, 0.0082, 0.2370, 0.0020, 0.0225, 0.0072, 0.0751,
        0.0197], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,668][circuit_model.py][line:2356][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ to] are: tensor([9.1075e-01, 1.6392e-02, 2.0885e-02, 5.4605e-03, 1.4051e-03, 1.1161e-02,
        5.0975e-05, 4.5217e-04, 3.9504e-04, 1.2952e-05, 3.3541e-03, 8.0672e-05,
        1.5399e-03, 9.6022e-03, 1.3424e-04, 4.4430e-03, 7.0624e-04, 6.6204e-03,
        6.5587e-03], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,670][circuit_model.py][line:2359][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ to] are: tensor([0.3157, 0.0527, 0.2559, 0.0588, 0.0822, 0.0680, 0.0024, 0.0067, 0.0029,
        0.0012, 0.0128, 0.0088, 0.0047, 0.0438, 0.0081, 0.0121, 0.0023, 0.0538,
        0.0071], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,672][circuit_model.py][line:2362][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ to] are: tensor([9.0365e-01, 5.0269e-04, 7.2876e-02, 1.1114e-04, 4.5436e-03, 2.8699e-03,
        9.4545e-06, 6.2939e-04, 2.8258e-05, 1.0442e-05, 1.7580e-04, 4.8544e-05,
        4.5251e-04, 5.8507e-03, 1.0898e-03, 9.1093e-04, 8.7025e-05, 5.7484e-03,
        4.0398e-04], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,673][circuit_model.py][line:2365][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ to] are: tensor([7.5513e-01, 9.6764e-04, 9.9109e-02, 3.1493e-03, 8.3242e-04, 9.9674e-03,
        5.6490e-05, 3.7422e-04, 7.1252e-05, 3.0831e-06, 1.1749e-03, 2.4197e-03,
        1.4902e-03, 2.9939e-02, 1.3232e-04, 1.6785e-02, 1.0361e-02, 1.4897e-02,
        5.3142e-02], device='cuda:0') for source tokens [Then, Nathan and Katie had a lot of fun at the house. Katie gave a snack to]
[2024-07-24 10:25:24,677][circuit_model.py][line:2041][INFO] ############showing the lable-rank of each circuit
[2024-07-24 10:25:24,679][circuit_model.py][line:2228][INFO] The CircuitSUM has label_rank 
 tensor([[2062],
        [  42],
        [ 174],
        [  18],
        [  21],
        [  19],
        [  11],
        [   9],
        [  10],
        [  22],
        [   7],
        [  15],
        [  12],
        [  38],
        [ 140],
        [  32],
        [  11],
        [  30],
        [   1]], device='cuda:0')
[2024-07-24 10:25:24,681][circuit_model.py][line:2230][INFO] The Circuit0 has label_rank 
 tensor([[1958],
        [  29],
        [ 136],
        [  16],
        [  25],
        [  17],
        [  14],
        [   7],
        [  10],
        [  23],
        [  13],
        [  15],
        [  17],
        [  46],
        [ 154],
        [  38],
        [  17],
        [  35],
        [   2]], device='cuda:0')
[2024-07-24 10:25:24,683][circuit_model.py][line:2232][INFO] The Circuit1 has label_rank 
 tensor([[11103],
        [42609],
        [ 8410],
        [40034],
        [21769],
        [38044],
        [34887],
        [29583],
        [26308],
        [25909],
        [27562],
        [26586],
        [28566],
        [33197],
        [21848],
        [31730],
        [33117],
        [32421],
        [34273]], device='cuda:0')
[2024-07-24 10:25:24,685][circuit_model.py][line:2234][INFO] The Circuit2 has label_rank 
 tensor([[  937],
        [  584],
        [ 4917],
        [13227],
        [13435],
        [12229],
        [ 3387],
        [12370],
        [10231],
        [ 5830],
        [ 5988],
        [ 6831],
        [ 2829],
        [10105],
        [ 6310],
        [ 5686],
        [ 3666],
        [ 2781],
        [ 6288]], device='cuda:0')
[2024-07-24 10:25:24,687][circuit_model.py][line:2236][INFO] The Circuit3 has label_rank 
 tensor([[ 9471],
        [33241],
        [44974],
        [24727],
        [18702],
        [10386],
        [10984],
        [ 6861],
        [ 5145],
        [ 5543],
        [ 4583],
        [ 4874],
        [ 7521],
        [ 5931],
        [ 6029],
        [ 6885],
        [ 6889],
        [ 6620],
        [ 6903]], device='cuda:0')
[2024-07-24 10:25:24,689][circuit_model.py][line:2238][INFO] The Circuit4 has label_rank 
 tensor([[23279],
        [23872],
        [49898],
        [49890],
        [49695],
        [34471],
        [46651],
        [46323],
        [48175],
        [48293],
        [47977],
        [47155],
        [25674],
        [30012],
        [42438],
        [34818],
        [32245],
        [35131],
        [27678]], device='cuda:0')
[2024-07-24 10:25:24,690][circuit_model.py][line:2240][INFO] The Circuit5 has label_rank 
 tensor([[6521],
        [6646],
        [7865],
        [9217],
        [8820],
        [6351],
        [5697],
        [5768],
        [4822],
        [4770],
        [4729],
        [3796],
        [3823],
        [3413],
        [3382],
        [3583],
        [3258],
        [3406],
        [3016]], device='cuda:0')
[2024-07-24 10:25:24,692][circuit_model.py][line:2242][INFO] The Circuit6 has label_rank 
 tensor([[16355],
        [29799],
        [23392],
        [10992],
        [12158],
        [10167],
        [10963],
        [20785],
        [14498],
        [16816],
        [19780],
        [17532],
        [19884],
        [19107],
        [19077],
        [16585],
        [16427],
        [21405],
        [19628]], device='cuda:0')
[2024-07-24 10:25:24,693][circuit_model.py][line:2244][INFO] The Circuit7 has label_rank 
 tensor([[20262],
        [ 5788],
        [ 9764],
        [ 4490],
        [ 3579],
        [ 6101],
        [ 7953],
        [ 8757],
        [ 9927],
        [12188],
        [11889],
        [14007],
        [13844],
        [15075],
        [15921],
        [14982],
        [17284],
        [17630],
        [17215]], device='cuda:0')
[2024-07-24 10:25:24,695][circuit_model.py][line:2246][INFO] The Circuit8 has label_rank 
 tensor([[48616],
        [46490],
        [44913],
        [46285],
        [46990],
        [46841],
        [46950],
        [47079],
        [46779],
        [47865],
        [46876],
        [46651],
        [47223],
        [46099],
        [46977],
        [45903],
        [46647],
        [47706],
        [47002]], device='cuda:0')
[2024-07-24 10:25:24,697][circuit_model.py][line:2248][INFO] The Circuit9 has label_rank 
 tensor([[ 8807],
        [29585],
        [23379],
        [16832],
        [ 6622],
        [ 6748],
        [ 6665],
        [ 7415],
        [ 7165],
        [ 7225],
        [ 7354],
        [ 7166],
        [ 6998],
        [ 7109],
        [ 6754],
        [ 6685],
        [ 6750],
        [ 6792],
        [ 6755]], device='cuda:0')
[2024-07-24 10:25:24,699][circuit_model.py][line:2250][INFO] The Circuit10 has label_rank 
 tensor([[ 8061],
        [21531],
        [27016],
        [15518],
        [ 6586],
        [ 6796],
        [ 6652],
        [ 7160],
        [ 7619],
        [ 8876],
        [ 9053],
        [ 9406],
        [ 6882],
        [ 6719],
        [ 5113],
        [ 5828],
        [ 5911],
        [ 5543],
        [ 6113]], device='cuda:0')
[2024-07-24 10:25:24,701][circuit_model.py][line:2252][INFO] The Circuit11 has label_rank 
 tensor([[31505],
        [24248],
        [   14],
        [    2],
        [  886],
        [   80],
        [  764],
        [13167],
        [  295],
        [ 2148],
        [  352],
        [ 2297],
        [ 7063],
        [  671],
        [16940],
        [ 2664],
        [ 2128],
        [ 1685],
        [ 8195]], device='cuda:0')
[2024-07-24 10:25:24,703][circuit_model.py][line:2254][INFO] The Circuit12 has label_rank 
 tensor([[34050],
        [10513],
        [36608],
        [33730],
        [40654],
        [39997],
        [40710],
        [38062],
        [40947],
        [39807],
        [40306],
        [36013],
        [37460],
        [33464],
        [33029],
        [35962],
        [34445],
        [34647],
        [27032]], device='cuda:0')
[2024-07-24 10:25:24,705][circuit_model.py][line:2256][INFO] The Circuit13 has label_rank 
 tensor([[ 9530],
        [19129],
        [ 9513],
        [13380],
        [ 8580],
        [11040],
        [ 9554],
        [ 5533],
        [ 4629],
        [ 9814],
        [11681],
        [15204],
        [ 9110],
        [ 3471],
        [ 4575],
        [ 6355],
        [10363],
        [ 7595],
        [ 9259]], device='cuda:0')
[2024-07-24 10:25:24,707][circuit_model.py][line:2258][INFO] The Circuit14 has label_rank 
 tensor([[16336],
        [ 9393],
        [10859],
        [ 9179],
        [ 8343],
        [ 8741],
        [ 8634],
        [ 8818],
        [ 9672],
        [ 9739],
        [ 9658],
        [ 9944],
        [ 9710],
        [ 9616],
        [ 9982],
        [10088],
        [10550],
        [ 9864],
        [10436]], device='cuda:0')
[2024-07-24 10:25:24,708][circuit_model.py][line:2260][INFO] The Circuit15 has label_rank 
 tensor([[11914],
        [15070],
        [14196],
        [14250],
        [12714],
        [12155],
        [13034],
        [11450],
        [12691],
        [13493],
        [12778],
        [12814],
        [14015],
        [12986],
        [13705],
        [13513],
        [13371],
        [14310],
        [13592]], device='cuda:0')
[2024-07-24 10:25:24,710][circuit_model.py][line:2262][INFO] The Circuit16 has label_rank 
 tensor([[24182],
        [17505],
        [17008],
        [18598],
        [19430],
        [19396],
        [20020],
        [24050],
        [23254],
        [23362],
        [23153],
        [21400],
        [23971],
        [24926],
        [28080],
        [25732],
        [25719],
        [25055],
        [25553]], device='cuda:0')
[2024-07-24 10:25:24,712][circuit_model.py][line:2264][INFO] The Circuit17 has label_rank 
 tensor([[3366],
        [3344],
        [6680],
        [6514],
        [4950],
        [3565],
        [4617],
        [4391],
        [6226],
        [7647],
        [6445],
        [5308],
        [3377],
        [3461],
        [5196],
        [4207],
        [4267],
        [4082],
        [3522]], device='cuda:0')
[2024-07-24 10:25:24,714][circuit_model.py][line:2266][INFO] The Circuit18 has label_rank 
 tensor([[19692],
        [44198],
        [45937],
        [42984],
        [45031],
        [44950],
        [46417],
        [47285],
        [46249],
        [47565],
        [46693],
        [45180],
        [42309],
        [40146],
        [40977],
        [42069],
        [41068],
        [41183],
        [39675]], device='cuda:0')
[2024-07-24 10:25:24,716][circuit_model.py][line:2268][INFO] The Circuit19 has label_rank 
 tensor([[14871],
        [12053],
        [12002],
        [13757],
        [18779],
        [16789],
        [17077],
        [10593],
        [12260],
        [13013],
        [10096],
        [13275],
        [ 8939],
        [12780],
        [10778],
        [13799],
        [14507],
        [10302],
        [12065]], device='cuda:0')
[2024-07-24 10:25:24,718][circuit_model.py][line:2270][INFO] The Circuit20 has label_rank 
 tensor([[16252],
        [23090],
        [18676],
        [20008],
        [16921],
        [18856],
        [18390],
        [19947],
        [18804],
        [18296],
        [19006],
        [22008],
        [20555],
        [22869],
        [23273],
        [22103],
        [21870],
        [19274],
        [22995]], device='cuda:0')
[2024-07-24 10:25:24,720][circuit_model.py][line:2272][INFO] The Circuit21 has label_rank 
 tensor([[16177],
        [ 9468],
        [ 7050],
        [ 9493],
        [ 8856],
        [ 8780],
        [ 9246],
        [ 9851],
        [ 9501],
        [ 9963],
        [ 9976],
        [10061],
        [ 9416],
        [11071],
        [10153],
        [ 9480],
        [10205],
        [ 9054],
        [ 9959]], device='cuda:0')
[2024-07-24 10:25:24,722][circuit_model.py][line:2274][INFO] The Circuit22 has label_rank 
 tensor([[15687],
        [16034],
        [15719],
        [12512],
        [12312],
        [14488],
        [ 9279],
        [15539],
        [13432],
        [12409],
        [15082],
        [15411],
        [12052],
        [10736],
        [ 8452],
        [ 6591],
        [15410],
        [ 8404],
        [15671]], device='cuda:0')
[2024-07-24 10:25:24,724][circuit_model.py][line:2276][INFO] The Circuit23 has label_rank 
 tensor([[ 7385],
        [ 7212],
        [ 7724],
        [10529],
        [15191],
        [14345],
        [12779],
        [23840],
        [13720],
        [20385],
        [19815],
        [19438],
        [15710],
        [17679],
        [19105],
        [16915],
        [15493],
        [19161],
        [13934]], device='cuda:0')
[2024-07-24 10:25:24,726][circuit_model.py][line:2278][INFO] The Circuit24 has label_rank 
 tensor([[10932],
        [ 9361],
        [ 6427],
        [ 5832],
        [ 5686],
        [ 5763],
        [ 6559],
        [ 8247],
        [ 6266],
        [ 6883],
        [ 6231],
        [ 6648],
        [ 8040],
        [ 5959],
        [ 7930],
        [ 8109],
        [ 7840],
        [ 7156],
        [ 9220]], device='cuda:0')
[2024-07-24 10:25:24,728][circuit_model.py][line:2280][INFO] The Circuit25 has label_rank 
 tensor([[4743],
        [4764],
        [5365],
        [6534],
        [6292],
        [6214],
        [4772],
        [5236],
        [5167],
        [3509],
        [5492],
        [4570],
        [5424],
        [6062],
        [6543],
        [5392],
        [6276],
        [4848],
        [6792]], device='cuda:0')
[2024-07-24 10:25:24,730][circuit_model.py][line:2282][INFO] The Circuit26 has label_rank 
 tensor([[32018],
        [33145],
        [32666],
        [31511],
        [32177],
        [33285],
        [33026],
        [30733],
        [32243],
        [30877],
        [31518],
        [31468],
        [33130],
        [31767],
        [30105],
        [31611],
        [31019],
        [32385],
        [31252]], device='cuda:0')
[2024-07-24 10:25:24,732][circuit_model.py][line:2284][INFO] The Circuit27 has label_rank 
 tensor([[35487],
        [12084],
        [31276],
        [19943],
        [28039],
        [29380],
        [22500],
        [32630],
        [34911],
        [30485],
        [25615],
        [16644],
        [34050],
        [32073],
        [30351],
        [28972],
        [22823],
        [31582],
        [26046]], device='cuda:0')
[2024-07-24 10:25:24,734][circuit_model.py][line:2286][INFO] The Circuit28 has label_rank 
 tensor([[11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256],
        [11256]], device='cuda:0')
