[2024-06-27 16:09:50,009][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,010][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,010][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,011][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,011][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,012][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,013][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,013][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,014][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,014][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,015][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,015][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,016][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.8760, 0.1240], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,017][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.0013, 0.9987], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,017][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.8840, 0.1160], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,017][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.0567, 0.9433], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,018][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.5982, 0.4018], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,018][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,019][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.8151, 0.1849], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,019][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9414, 0.0586], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,020][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9035, 0.0965], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,020][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8480, 0.1520], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,021][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.7101, 0.2899], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,021][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.7987, 0.2013], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,022][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5919, 0.3569, 0.0512], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,022][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0016, 0.0057, 0.9927], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,023][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.6998, 0.1853, 0.1149], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,023][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0228, 0.2130, 0.7642], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,024][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.3379, 0.2728, 0.3893], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,024][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0091, 0.0010, 0.9899], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,025][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.4141, 0.3172, 0.2686], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,025][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.5601, 0.2736, 0.1663], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,026][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.7863, 0.1338, 0.0799], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,026][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.6896, 0.2534, 0.0571], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,027][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.5777, 0.1519, 0.2704], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,027][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.6712, 0.1580, 0.1708], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,028][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5658, 0.2750, 0.0975, 0.0617], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,028][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ of] are: tensor([2.7660e-03, 7.1156e-04, 1.6157e-04, 9.9636e-01], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,029][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.5853, 0.1283, 0.1155, 0.1708], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,029][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0460, 0.3025, 0.1767, 0.4748], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,030][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2837, 0.4413, 0.1808, 0.0943], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,030][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.2564, 0.0711, 0.0179, 0.6546], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,031][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2911, 0.3001, 0.3820, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,031][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3290, 0.1464, 0.3082, 0.2164], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,032][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3944, 0.0866, 0.0580, 0.4609], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,032][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.5378, 0.1552, 0.0942, 0.2128], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,033][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5119, 0.1236, 0.0501, 0.3144], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,033][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.4950, 0.1927, 0.2060, 0.1064], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,034][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.4580, 0.1367, 0.1780, 0.1396, 0.0877], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,034][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ De] are: tensor([4.7577e-04, 1.8430e-03, 1.9662e-03, 7.1858e-04, 9.9500e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,035][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.4633, 0.1429, 0.1551, 0.1451, 0.0936], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,035][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ De] are: tensor([7.3997e-03, 1.5697e-03, 4.9343e-04, 1.9224e-03, 9.8861e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,036][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0848, 0.0304, 0.0194, 0.0221, 0.8434], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,036][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ De] are: tensor([2.5107e-02, 1.8511e-03, 3.3911e-05, 1.8053e-05, 9.7299e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,037][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.4389, 0.1573, 0.2067, 0.0502, 0.1469], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,037][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2565, 0.0871, 0.0704, 0.3043, 0.2817], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,038][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.4489, 0.0990, 0.0623, 0.2324, 0.1574], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,038][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.5173, 0.1428, 0.0882, 0.1776, 0.0742], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,039][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.4806, 0.1108, 0.0556, 0.1029, 0.2502], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,039][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.3248, 0.2346, 0.3069, 0.0896, 0.0441], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,040][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.3910, 0.0952, 0.1772, 0.1601, 0.1248, 0.0517], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,040][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([8.9033e-05, 4.7931e-03, 1.5460e-04, 1.4006e-03, 4.8277e-04, 9.9308e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,041][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.3253, 0.1499, 0.2778, 0.0976, 0.1197, 0.0297], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,041][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([1.3443e-03, 5.2498e-04, 2.4231e-05, 4.8777e-05, 9.8004e-04, 9.9708e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,042][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.6387e-03, 2.0815e-03, 8.9040e-04, 2.3722e-03, 1.8421e-02, 9.6660e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,042][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([4.1795e-03, 6.9115e-04, 3.2217e-05, 3.0980e-06, 3.2352e-04, 9.9477e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,043][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.3693, 0.1463, 0.2547, 0.0666, 0.0952, 0.0678], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,043][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.1704, 0.1249, 0.0505, 0.2912, 0.2461, 0.1170], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,044][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2367, 0.0718, 0.3045, 0.1910, 0.0930, 0.1031], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,044][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.3890, 0.1462, 0.1505, 0.1771, 0.1211, 0.0160], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,045][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.3530, 0.1560, 0.0485, 0.1120, 0.0644, 0.2661], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,046][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.3704, 0.1091, 0.1418, 0.1254, 0.0552, 0.1981], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,046][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.2608, 0.1815, 0.1922, 0.0788, 0.0981, 0.1201, 0.0684],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,047][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([1.0192e-06, 8.7755e-06, 3.8803e-04, 2.7981e-06, 3.3821e-05, 1.3176e-05,
        9.9955e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,047][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.3722, 0.1217, 0.1065, 0.0873, 0.2095, 0.0315, 0.0713],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,048][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([6.4776e-05, 1.0527e-04, 1.1847e-04, 9.7527e-06, 2.5299e-03, 2.3382e-03,
        9.9483e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,048][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([2.1835e-03, 9.6791e-04, 1.3731e-03, 5.5387e-04, 2.0302e-03, 4.1934e-01,
        5.7355e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,049][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([3.9244e-05, 4.0222e-06, 1.0783e-05, 3.0826e-09, 6.8012e-07, 4.1481e-06,
        9.9994e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,049][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.3578, 0.0460, 0.1112, 0.0472, 0.1037, 0.2075, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,050][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.1385, 0.0366, 0.0270, 0.0995, 0.1631, 0.5167, 0.0187],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,050][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3253, 0.0636, 0.0886, 0.1041, 0.2854, 0.0992, 0.0339],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,051][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.3516, 0.1608, 0.1300, 0.1073, 0.0936, 0.1321, 0.0246],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,051][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3575, 0.0932, 0.0864, 0.1007, 0.0738, 0.0333, 0.2552],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,052][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2338, 0.1569, 0.1996, 0.1117, 0.0276, 0.1409, 0.1294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,052][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.3167, 0.0847, 0.0889, 0.1721, 0.0527, 0.0848, 0.1027, 0.0974],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,053][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([1.8418e-05, 1.5845e-04, 3.4632e-05, 6.7318e-05, 1.0771e-04, 1.1345e-03,
        4.4688e-05, 9.9843e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,053][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2892, 0.1284, 0.1390, 0.1031, 0.1174, 0.0236, 0.1353, 0.0639],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,054][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([1.5037e-05, 2.4666e-06, 2.9078e-07, 2.2765e-07, 1.2378e-05, 3.5374e-04,
        3.4359e-06, 9.9961e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,054][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0191, 0.0041, 0.0011, 0.0074, 0.0135, 0.0872, 0.0397, 0.8278],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,055][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([1.0356e-03, 6.9173e-05, 2.1573e-06, 1.5790e-07, 1.1900e-05, 8.2333e-05,
        4.3949e-05, 9.9875e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,056][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.2490, 0.0573, 0.2070, 0.0563, 0.0739, 0.0955, 0.2027, 0.0582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,056][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1548, 0.0512, 0.0824, 0.1428, 0.1735, 0.1805, 0.1188, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,057][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.3282, 0.1681, 0.0635, 0.1119, 0.0505, 0.0399, 0.0711, 0.1668],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,057][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.3182, 0.1073, 0.0941, 0.1434, 0.1170, 0.1257, 0.0775, 0.0168],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,058][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.3652, 0.0983, 0.0357, 0.1019, 0.0610, 0.0540, 0.0257, 0.2582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,058][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.2492, 0.1058, 0.1344, 0.1007, 0.0442, 0.1888, 0.0806, 0.0963],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,059][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2609, 0.0855, 0.0964, 0.0838, 0.0518, 0.2071, 0.0757, 0.0482, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,059][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([2.1647e-05, 2.2618e-04, 7.5674e-04, 5.2729e-05, 4.7814e-04, 8.4970e-04,
        2.2010e-02, 4.4437e-03, 9.7116e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,060][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2813, 0.1034, 0.1891, 0.0709, 0.0713, 0.0681, 0.0694, 0.0395, 0.1068],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,060][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([3.1611e-04, 1.1498e-05, 6.2957e-06, 6.2300e-06, 1.8404e-04, 1.1954e-03,
        2.0652e-03, 3.0892e-03, 9.9313e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,061][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([3.4240e-03, 6.8455e-04, 1.7888e-03, 1.8692e-03, 5.7352e-03, 1.2699e-01,
        2.1376e-02, 1.0442e-01, 7.3372e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,062][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([2.7098e-03, 6.6089e-05, 4.2695e-05, 1.3363e-06, 1.5544e-05, 4.6581e-05,
        1.4676e-03, 3.3913e-05, 9.9562e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,062][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1964, 0.0446, 0.0970, 0.0334, 0.1440, 0.1716, 0.0924, 0.1504, 0.0703],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,063][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2108, 0.0727, 0.0266, 0.1216, 0.2963, 0.0979, 0.0108, 0.0621, 0.1013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,063][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.3112, 0.0957, 0.0654, 0.1124, 0.1428, 0.1106, 0.0355, 0.1010, 0.0254],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,064][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2871, 0.1045, 0.1239, 0.0954, 0.1186, 0.0917, 0.0766, 0.0803, 0.0218],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,064][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2810, 0.0887, 0.0638, 0.1092, 0.0574, 0.0646, 0.0869, 0.0316, 0.2168],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,065][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1898, 0.1720, 0.1684, 0.0619, 0.0262, 0.0930, 0.0876, 0.1385, 0.0625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,065][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.2441, 0.0991, 0.1640, 0.0518, 0.0870, 0.0434, 0.0966, 0.0902, 0.0689,
        0.0549], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,066][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ et] are: tensor([4.7677e-04, 9.6306e-04, 7.8710e-04, 1.2303e-03, 5.0300e-04, 2.9748e-04,
        5.6700e-04, 1.0168e-04, 1.6024e-03, 9.9347e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,067][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.2322, 0.0570, 0.1523, 0.1388, 0.1114, 0.0279, 0.1442, 0.0560, 0.0526,
        0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,067][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ et] are: tensor([2.3542e-04, 2.7035e-06, 2.9933e-06, 1.7182e-06, 1.9735e-05, 3.0432e-04,
        1.0826e-04, 1.1600e-03, 1.2660e-03, 9.9690e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,068][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0103, 0.0012, 0.0031, 0.0103, 0.0160, 0.0452, 0.0515, 0.0215, 0.0980,
        0.7429], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,068][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ et] are: tensor([6.6580e-04, 9.9814e-05, 1.2676e-05, 1.6858e-06, 2.2680e-05, 5.2646e-06,
        3.2962e-05, 2.5723e-06, 4.2750e-06, 9.9915e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,069][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.1125, 0.0502, 0.1162, 0.0187, 0.0562, 0.0757, 0.1488, 0.0737, 0.1571,
        0.1908], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,069][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1659, 0.0244, 0.0072, 0.0892, 0.0673, 0.0498, 0.0915, 0.0927, 0.2534,
        0.1588], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,070][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.2893, 0.1517, 0.0633, 0.2019, 0.0665, 0.0686, 0.0207, 0.0704, 0.0518,
        0.0158], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,070][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.2545, 0.1006, 0.1058, 0.1149, 0.0908, 0.0798, 0.0611, 0.0640, 0.0951,
        0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,071][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.2180, 0.1248, 0.0577, 0.1130, 0.0591, 0.0506, 0.0317, 0.0346, 0.0288,
        0.2817], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,072][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2041, 0.0920, 0.1192, 0.0695, 0.0307, 0.1229, 0.0598, 0.0869, 0.0556,
        0.1592], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,072][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.1812, 0.0553, 0.0874, 0.0538, 0.0374, 0.0381, 0.0958, 0.0798, 0.1372,
        0.1057, 0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,073][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([3.3097e-05, 2.2359e-04, 1.7588e-04, 4.3431e-05, 1.2347e-03, 1.5803e-04,
        4.7333e-04, 3.8590e-04, 1.7457e-04, 1.2392e-05, 9.9709e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,073][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.2732, 0.0515, 0.0984, 0.0803, 0.0509, 0.0455, 0.0462, 0.0600, 0.1690,
        0.0592, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,074][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([4.4071e-04, 1.7670e-05, 2.9868e-06, 1.0578e-06, 3.7284e-04, 6.4737e-04,
        1.1974e-04, 1.1023e-02, 2.9585e-03, 1.4759e-02, 9.6966e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,074][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0062, 0.0013, 0.0012, 0.0016, 0.0095, 0.0076, 0.0021, 0.0359, 0.0057,
        0.0430, 0.8860], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,075][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([1.7796e-04, 4.5544e-06, 1.0613e-06, 5.2177e-08, 1.2367e-05, 3.1378e-06,
        9.0654e-07, 5.1383e-05, 3.7395e-06, 3.3069e-07, 9.9974e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,076][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.1427, 0.0773, 0.1225, 0.0259, 0.0431, 0.0874, 0.1447, 0.0785, 0.1333,
        0.0724, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,076][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0949, 0.0229, 0.0258, 0.0527, 0.0250, 0.0513, 0.0445, 0.0423, 0.0832,
        0.3576, 0.1998], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,077][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.1804, 0.0613, 0.0599, 0.1166, 0.0567, 0.0692, 0.0987, 0.0650, 0.0953,
        0.0497, 0.1473], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,077][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2115, 0.0904, 0.1025, 0.1028, 0.0736, 0.1517, 0.0630, 0.0622, 0.0730,
        0.0602, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,078][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2339, 0.0713, 0.0406, 0.0957, 0.0639, 0.0431, 0.0316, 0.0534, 0.0286,
        0.0444, 0.2935], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,078][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.1835, 0.0823, 0.1299, 0.0683, 0.0266, 0.0865, 0.0563, 0.0819, 0.0699,
        0.0974, 0.1175], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,079][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2061, 0.0661, 0.0753, 0.0620, 0.0414, 0.1676, 0.0582, 0.0389, 0.0737,
        0.0778, 0.0566, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,080][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([8.6567e-06, 7.8134e-05, 2.7261e-04, 1.7985e-05, 1.8909e-04, 3.3536e-04,
        1.1171e-02, 1.7158e-03, 5.1216e-01, 4.4262e-05, 3.1099e-04, 4.7369e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,080][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2305, 0.0863, 0.1576, 0.0584, 0.0608, 0.0616, 0.0612, 0.0364, 0.0925,
        0.0278, 0.0359, 0.0911], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,081][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.0443e-04, 1.9482e-06, 9.9700e-07, 8.5760e-07, 2.4087e-05, 1.6049e-04,
        3.2396e-04, 4.3607e-04, 1.6716e-01, 6.0149e-03, 3.2648e-02, 7.9313e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,081][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([1.5673e-03, 2.4146e-04, 6.2082e-04, 6.6058e-04, 1.6916e-03, 4.0039e-02,
        7.2441e-03, 3.3730e-02, 2.5429e-01, 7.8436e-03, 1.3365e-01, 5.1842e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,082][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([1.2523e-03, 3.1623e-05, 2.0093e-05, 6.3776e-07, 7.9575e-06, 2.1615e-05,
        7.0141e-04, 1.6131e-05, 5.3550e-01, 6.5836e-06, 4.8697e-05, 4.6239e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,083][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1349, 0.0309, 0.0667, 0.0218, 0.1030, 0.1296, 0.0652, 0.1083, 0.0522,
        0.1469, 0.0874, 0.0530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,083][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1671, 0.0387, 0.0137, 0.0610, 0.1336, 0.0521, 0.0053, 0.0290, 0.0523,
        0.0668, 0.2698, 0.1105], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,084][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2079, 0.0639, 0.0469, 0.0733, 0.0967, 0.0797, 0.0248, 0.0714, 0.0177,
        0.1571, 0.1423, 0.0182], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,084][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2285, 0.0839, 0.1033, 0.0758, 0.0990, 0.0762, 0.0662, 0.0664, 0.0181,
        0.0816, 0.0824, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,085][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1796, 0.0584, 0.0463, 0.0744, 0.0429, 0.0502, 0.0706, 0.0247, 0.1896,
        0.0402, 0.0444, 0.1786], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,085][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1531, 0.1409, 0.1372, 0.0496, 0.0227, 0.0749, 0.0725, 0.1105, 0.0511,
        0.0794, 0.0538, 0.0543], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,086][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1593, 0.0595, 0.0350, 0.0117, 0.0350, 0.1127, 0.0594, 0.1245, 0.1007,
        0.0924, 0.0672, 0.1152, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,087][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ is] are: tensor([2.6377e-03, 5.3790e-04, 3.5144e-04, 4.8993e-03, 3.4439e-04, 1.7886e-04,
        1.4119e-04, 2.2006e-04, 3.7787e-04, 5.6773e-04, 4.3503e-04, 3.4606e-04,
        9.8896e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,087][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.2076, 0.0671, 0.0584, 0.1016, 0.0632, 0.0229, 0.1146, 0.0289, 0.0556,
        0.0608, 0.0346, 0.0583, 0.1264], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,088][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ is] are: tensor([8.9733e-03, 2.5728e-04, 6.3198e-05, 4.2710e-04, 1.2356e-04, 1.8010e-04,
        5.7753e-04, 1.3562e-03, 1.5530e-03, 1.7582e-02, 1.2350e-02, 8.0742e-03,
        9.4848e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,088][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0735, 0.0252, 0.0096, 0.0248, 0.0113, 0.0143, 0.0121, 0.0525, 0.0170,
        0.0705, 0.1032, 0.0380, 0.5479], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,089][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ is] are: tensor([2.4296e-02, 2.0065e-03, 2.4683e-03, 3.2399e-03, 9.1281e-04, 2.6412e-04,
        1.3690e-04, 2.7982e-04, 1.0724e-03, 3.4749e-04, 8.2915e-04, 8.5929e-04,
        9.6329e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,090][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0442, 0.0710, 0.0912, 0.0062, 0.0433, 0.0565, 0.1127, 0.1228, 0.0924,
        0.1439, 0.0943, 0.1050, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,090][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0382, 0.0055, 0.0063, 0.0105, 0.0089, 0.0068, 0.0116, 0.0223, 0.0553,
        0.1197, 0.0742, 0.1679, 0.4729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,091][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1005, 0.0181, 0.0222, 0.1145, 0.0270, 0.0361, 0.0120, 0.0361, 0.0215,
        0.0662, 0.0443, 0.0249, 0.4767], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,091][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.1968, 0.0875, 0.0598, 0.1077, 0.0675, 0.0532, 0.0483, 0.0489, 0.0532,
        0.0585, 0.0475, 0.0567, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,092][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1773, 0.0605, 0.0461, 0.1201, 0.0489, 0.0366, 0.0381, 0.0462, 0.0291,
        0.0419, 0.0515, 0.0280, 0.2756], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,092][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2686, 0.0734, 0.0622, 0.0596, 0.0286, 0.0763, 0.0518, 0.0510, 0.0488,
        0.0860, 0.0506, 0.0495, 0.0937], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,093][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1187, 0.0530, 0.0231, 0.0090, 0.0339, 0.0879, 0.0726, 0.1919, 0.1105,
        0.0628, 0.0714, 0.1262, 0.0268, 0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,094][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([5.5456e-02, 6.2910e-04, 5.2470e-05, 7.5203e-02, 1.5758e-03, 6.1685e-04,
        7.5570e-05, 2.6828e-04, 3.3991e-04, 5.4090e-04, 9.9822e-04, 3.3152e-04,
        9.3071e-04, 8.6298e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,094][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1933, 0.0688, 0.0495, 0.1293, 0.0759, 0.0232, 0.0593, 0.0433, 0.0392,
        0.0608, 0.0500, 0.0415, 0.1294, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,095][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.0327e-02, 5.5598e-04, 1.8978e-04, 8.8268e-04, 7.7357e-04, 1.4620e-03,
        1.3360e-03, 6.9476e-03, 4.0072e-03, 2.6717e-02, 3.9356e-02, 1.8470e-02,
        2.7426e-01, 6.1471e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,095][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0684, 0.0241, 0.0114, 0.0154, 0.0112, 0.0226, 0.0109, 0.0430, 0.0177,
        0.0876, 0.0816, 0.0392, 0.3444, 0.2226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,096][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2424, 0.1698, 0.0204, 0.1024, 0.0214, 0.0151, 0.0028, 0.0094, 0.0044,
        0.0075, 0.0067, 0.0036, 0.1080, 0.2859], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,097][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0360, 0.0823, 0.0722, 0.0017, 0.0304, 0.0477, 0.1088, 0.1206, 0.1416,
        0.0848, 0.1033, 0.1612, 0.0081, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,097][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0279, 0.0032, 0.0043, 0.0076, 0.0076, 0.0078, 0.0108, 0.0210, 0.0345,
        0.0615, 0.0589, 0.0995, 0.3213, 0.3340], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,098][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0320, 0.0061, 0.0078, 0.0554, 0.0104, 0.0231, 0.0051, 0.0258, 0.0086,
        0.0386, 0.0118, 0.0101, 0.3445, 0.4207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,098][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2133, 0.0698, 0.0517, 0.0984, 0.0611, 0.0412, 0.0386, 0.0457, 0.0436,
        0.0513, 0.0421, 0.0463, 0.0945, 0.1025], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,099][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3014, 0.0698, 0.0431, 0.0809, 0.0521, 0.0322, 0.0262, 0.0402, 0.0215,
        0.0372, 0.0384, 0.0207, 0.0695, 0.1669], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,100][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2058, 0.0688, 0.0761, 0.0377, 0.0235, 0.0725, 0.0467, 0.0535, 0.0570,
        0.0769, 0.0486, 0.0585, 0.0763, 0.0981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,100][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1031, 0.0711, 0.0270, 0.0167, 0.0285, 0.0588, 0.1160, 0.1381, 0.1326,
        0.0269, 0.0458, 0.1520, 0.0359, 0.0217, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,101][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ same] are: tensor([1.8082e-03, 1.3732e-02, 1.1511e-03, 1.0779e-03, 4.1301e-04, 1.3396e-03,
        2.4368e-04, 3.0300e-03, 7.6835e-05, 1.1669e-04, 5.8620e-04, 6.8874e-05,
        1.7218e-03, 1.2435e-03, 9.7339e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,101][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.1757, 0.0950, 0.0731, 0.0913, 0.0312, 0.0264, 0.0586, 0.0303, 0.0783,
        0.0449, 0.0384, 0.0810, 0.0720, 0.0579, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,102][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ same] are: tensor([8.8967e-04, 1.8280e-05, 6.2767e-06, 6.8885e-06, 2.7967e-05, 9.4842e-04,
        4.9636e-05, 1.8252e-03, 7.9017e-05, 1.3148e-03, 2.8364e-03, 3.4912e-04,
        7.8719e-04, 1.2641e-03, 9.8960e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,103][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0593, 0.0125, 0.0042, 0.0194, 0.0032, 0.0063, 0.0081, 0.0715, 0.0114,
        0.0400, 0.0668, 0.0231, 0.1284, 0.1237, 0.4222], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,103][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ same] are: tensor([1.7116e-02, 2.9655e-02, 3.2967e-04, 4.9665e-05, 3.0534e-04, 5.1293e-05,
        6.2239e-06, 1.0991e-04, 9.5665e-07, 4.3046e-05, 6.8255e-05, 6.4576e-07,
        1.0273e-05, 2.2959e-05, 9.5223e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,104][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0630, 0.1151, 0.1044, 0.0106, 0.0362, 0.0663, 0.0832, 0.0694, 0.0851,
        0.1247, 0.0766, 0.0907, 0.0208, 0.0064, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,105][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0458, 0.0035, 0.0025, 0.0074, 0.0058, 0.0018, 0.0071, 0.0236, 0.0257,
        0.0517, 0.0812, 0.0720, 0.2244, 0.3026, 0.1449], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,105][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.1046, 0.0408, 0.0543, 0.0555, 0.0232, 0.0311, 0.0253, 0.0348, 0.0407,
        0.0942, 0.0394, 0.0451, 0.1734, 0.2067, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,106][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1856, 0.0683, 0.0592, 0.0750, 0.0641, 0.0505, 0.0361, 0.0417, 0.0456,
        0.0462, 0.0577, 0.0486, 0.0854, 0.0831, 0.0527], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,106][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1891, 0.0882, 0.0505, 0.0725, 0.0386, 0.0369, 0.0231, 0.0368, 0.0206,
        0.0392, 0.0364, 0.0198, 0.0608, 0.0761, 0.2115], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,107][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.3156, 0.0679, 0.0640, 0.0382, 0.0263, 0.0611, 0.0326, 0.0359, 0.0412,
        0.0756, 0.0437, 0.0411, 0.0380, 0.0428, 0.0758], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,108][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0864, 0.0634, 0.0462, 0.0080, 0.0477, 0.0565, 0.0851, 0.1121, 0.0804,
        0.0994, 0.0845, 0.0916, 0.0482, 0.0261, 0.0486, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,108][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ as] are: tensor([2.3854e-03, 2.0895e-04, 2.5717e-04, 2.8681e-03, 3.5010e-04, 3.8690e-04,
        2.0201e-04, 2.2578e-04, 2.0202e-04, 2.0501e-04, 1.7903e-03, 1.9313e-04,
        1.9476e-04, 8.5942e-04, 7.8604e-04, 9.8889e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,109][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.1792, 0.0660, 0.0502, 0.0982, 0.0569, 0.0234, 0.0318, 0.0392, 0.0474,
        0.0406, 0.0338, 0.0509, 0.0833, 0.0367, 0.0603, 0.1021],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,109][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ as] are: tensor([5.1346e-04, 6.1543e-06, 4.4379e-06, 1.8338e-05, 4.5401e-06, 1.3863e-05,
        1.5373e-05, 1.1318e-04, 1.4519e-05, 1.5111e-04, 6.9566e-04, 7.1744e-05,
        5.8046e-03, 9.5863e-03, 8.0341e-01, 1.7958e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,110][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0219, 0.0037, 0.0035, 0.0052, 0.0030, 0.0036, 0.0018, 0.0170, 0.0050,
        0.0209, 0.0260, 0.0112, 0.2114, 0.0589, 0.1340, 0.4729],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,111][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ as] are: tensor([2.7994e-02, 6.6583e-03, 4.6749e-03, 4.2371e-03, 1.6441e-03, 4.1065e-04,
        3.0593e-04, 9.3266e-04, 1.3406e-04, 6.6179e-04, 1.5577e-03, 1.0409e-04,
        7.2350e-04, 1.4188e-03, 5.2119e-03, 9.4333e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,111][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0419, 0.0819, 0.0902, 0.0058, 0.0463, 0.0621, 0.0695, 0.1048, 0.0721,
        0.1508, 0.0772, 0.0808, 0.0232, 0.0098, 0.0676, 0.0160],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,112][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0233, 0.0017, 0.0023, 0.0033, 0.0024, 0.0027, 0.0071, 0.0129, 0.0260,
        0.0446, 0.0345, 0.0731, 0.1240, 0.1666, 0.1638, 0.3118],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,112][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0383, 0.0088, 0.0139, 0.0697, 0.0112, 0.0241, 0.0057, 0.0234, 0.0076,
        0.0357, 0.0135, 0.0087, 0.3306, 0.2743, 0.0189, 0.1156],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,113][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.1533, 0.0587, 0.0459, 0.0776, 0.0502, 0.0372, 0.0343, 0.0406, 0.0421,
        0.0474, 0.0489, 0.0451, 0.0847, 0.0851, 0.0574, 0.0913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,114][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1203, 0.0452, 0.0474, 0.0590, 0.0356, 0.0298, 0.0337, 0.0324, 0.0203,
        0.0293, 0.0414, 0.0208, 0.0882, 0.0823, 0.0667, 0.2474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,114][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.2140, 0.0688, 0.0537, 0.0550, 0.0268, 0.0528, 0.0414, 0.0405, 0.0389,
        0.0713, 0.0362, 0.0388, 0.0522, 0.0343, 0.0751, 0.1002],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,115][circuit_into_ebeddingspace.py][line:2169][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1166, 0.0495, 0.0222, 0.0082, 0.0323, 0.0856, 0.0671, 0.1789, 0.1055,
        0.0583, 0.0675, 0.1206, 0.0247, 0.0114, 0.0267, 0.0111, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,116][circuit_into_ebeddingspace.py][line:2172][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.7131e-02, 2.5542e-04, 2.0961e-05, 3.0705e-02, 6.9835e-04, 2.8151e-04,
        3.8641e-05, 1.1279e-04, 1.7476e-04, 2.5317e-04, 4.2775e-04, 1.7526e-04,
        4.4160e-04, 4.4301e-01, 3.9625e-04, 8.4876e-03, 4.8739e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,116][circuit_into_ebeddingspace.py][line:2175][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1437, 0.0520, 0.0372, 0.0960, 0.0583, 0.0181, 0.0453, 0.0334, 0.0294,
        0.0500, 0.0411, 0.0312, 0.1009, 0.0272, 0.0481, 0.1595, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,117][circuit_into_ebeddingspace.py][line:2178][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2250e-03, 1.9250e-04, 6.1471e-05, 2.4989e-04, 1.8313e-04, 3.1236e-04,
        3.3465e-04, 1.4383e-03, 8.9219e-04, 4.2014e-03, 6.1448e-03, 3.8249e-03,
        3.6687e-02, 8.4951e-02, 9.9260e-02, 1.6062e-01, 5.9442e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,117][circuit_into_ebeddingspace.py][line:2181][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0520, 0.0145, 0.0068, 0.0089, 0.0056, 0.0102, 0.0049, 0.0184, 0.0075,
        0.0337, 0.0293, 0.0156, 0.1236, 0.0799, 0.1484, 0.2184, 0.2223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,118][circuit_into_ebeddingspace.py][line:2184][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1642, 0.1197, 0.0144, 0.0747, 0.0162, 0.0110, 0.0019, 0.0069, 0.0031,
        0.0051, 0.0049, 0.0026, 0.0883, 0.2206, 0.0479, 0.0179, 0.2006],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,119][circuit_into_ebeddingspace.py][line:2187][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0320, 0.0744, 0.0640, 0.0014, 0.0275, 0.0443, 0.0976, 0.1133, 0.1324,
        0.0751, 0.0966, 0.1514, 0.0070, 0.0011, 0.0749, 0.0056, 0.0013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,119][circuit_into_ebeddingspace.py][line:2190][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0192, 0.0016, 0.0020, 0.0035, 0.0031, 0.0028, 0.0037, 0.0073, 0.0123,
        0.0173, 0.0162, 0.0329, 0.0784, 0.0839, 0.0832, 0.3199, 0.3127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,120][circuit_into_ebeddingspace.py][line:2193][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0202, 0.0038, 0.0051, 0.0343, 0.0067, 0.0150, 0.0033, 0.0169, 0.0057,
        0.0245, 0.0078, 0.0067, 0.2173, 0.2539, 0.0080, 0.0637, 0.3070],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,121][circuit_into_ebeddingspace.py][line:2196][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1614, 0.0528, 0.0403, 0.0739, 0.0477, 0.0318, 0.0309, 0.0350, 0.0342,
        0.0401, 0.0329, 0.0363, 0.0730, 0.0767, 0.0530, 0.0963, 0.0834],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,121][circuit_into_ebeddingspace.py][line:2199][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2087, 0.0473, 0.0313, 0.0565, 0.0391, 0.0250, 0.0208, 0.0314, 0.0179,
        0.0298, 0.0312, 0.0178, 0.0568, 0.1346, 0.0506, 0.0647, 0.1366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,122][circuit_into_ebeddingspace.py][line:2202][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1671, 0.0535, 0.0588, 0.0308, 0.0207, 0.0561, 0.0373, 0.0406, 0.0456,
        0.0567, 0.0356, 0.0459, 0.0560, 0.0735, 0.0735, 0.0735, 0.0749],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,127][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,127][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,128][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,129][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,129][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,130][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,130][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,131][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,131][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,132][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,133][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,133][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,134][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9802, 0.0198], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,134][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9767, 0.0233], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,135][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9867, 0.0133], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,135][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.6432, 0.3568], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,136][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.8878, 0.1122], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,136][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9232, 0.0768], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,137][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.8967, 0.1033], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,137][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9508, 0.0492], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,138][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9543, 0.0457], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,138][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9789, 0.0211], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,139][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ original] are: tensor([3.3137e-04, 9.9967e-01], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,139][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.3325, 0.6675], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,140][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5368, 0.3558, 0.1074], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,140][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8715, 0.0719, 0.0566], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,140][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.8856, 0.0287, 0.0858], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,141][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.5954, 0.1801, 0.2245], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,141][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.7879, 0.1004, 0.1117], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,142][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7162, 0.1383, 0.1455], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,142][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.8329, 0.0222, 0.1449], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,143][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8557, 0.0819, 0.0624], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,143][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.8934, 0.0564, 0.0502], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,144][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9589, 0.0251, 0.0160], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,144][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0006, 0.4557, 0.5436], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,145][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.2600, 0.0050, 0.7350], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,145][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5359, 0.1355, 0.2584, 0.0701], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,146][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.7028, 0.0987, 0.0947, 0.1039], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,146][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.6147, 0.0258, 0.0708, 0.2888], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,147][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.5594, 0.1268, 0.1650, 0.1487], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,147][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.7032, 0.1009, 0.1170, 0.0789], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,148][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.4506, 0.0116, 0.0073, 0.5305], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,148][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8037, 0.0739, 0.0588, 0.0636], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,149][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.6930, 0.0701, 0.1214, 0.1155], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,149][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.7986, 0.0557, 0.0506, 0.0951], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,150][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.8893, 0.0299, 0.0172, 0.0636], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,150][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0011, 0.3076, 0.3981, 0.2932], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,151][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.4321, 0.0046, 0.0024, 0.5608], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,151][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.5652, 0.0250, 0.0511, 0.1734, 0.1852], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,152][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.4993, 0.0849, 0.0997, 0.2038, 0.1122], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,152][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.4797, 0.0251, 0.0649, 0.2566, 0.1737], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,153][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.5205, 0.1052, 0.1359, 0.1171, 0.1213], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,153][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.6689, 0.0761, 0.0875, 0.0690, 0.0985], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,154][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.6630, 0.0276, 0.0145, 0.2310, 0.0639], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,154][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.7277, 0.0271, 0.1447, 0.0594, 0.0412], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,155][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.5431, 0.0928, 0.1660, 0.1144, 0.0838], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,155][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.6668, 0.0680, 0.0626, 0.1429, 0.0597], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,156][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.7918, 0.0464, 0.0428, 0.0898, 0.0293], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,157][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0008, 0.2261, 0.2875, 0.2167, 0.2688], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,157][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ De] are: tensor([4.2773e-02, 3.1417e-03, 3.1715e-04, 3.0576e-02, 9.2319e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,158][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.2215, 0.1121, 0.0908, 0.1433, 0.3080, 0.1244], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,158][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4741, 0.0653, 0.0618, 0.1879, 0.1789, 0.0320], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,159][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.3976, 0.0255, 0.0597, 0.2336, 0.1446, 0.1389], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,159][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.5188, 0.0754, 0.1060, 0.0980, 0.1015, 0.1003], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,160][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.6146, 0.0731, 0.0694, 0.0609, 0.0863, 0.0957], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,160][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.6854, 0.0709, 0.0199, 0.1218, 0.0864, 0.0157], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,161][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.5981, 0.0868, 0.0316, 0.0967, 0.1445, 0.0424], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,161][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.4483, 0.0941, 0.2399, 0.0977, 0.0955, 0.0246], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,162][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.6339, 0.0460, 0.0669, 0.1446, 0.0676, 0.0409], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,162][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.7222, 0.0455, 0.0526, 0.1213, 0.0372, 0.0213], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,163][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0010, 0.1640, 0.2270, 0.1740, 0.2207, 0.2133], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,163][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([2.2467e-02, 1.9179e-03, 1.2005e-04, 9.8701e-03, 1.0802e-03, 9.6454e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,164][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0417, 0.0200, 0.0132, 0.0379, 0.2125, 0.6565, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,164][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.4780, 0.0489, 0.0570, 0.1452, 0.1129, 0.0465, 0.1116],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,165][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.3124, 0.0234, 0.0556, 0.1894, 0.1288, 0.1275, 0.1630],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,165][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.5112, 0.0624, 0.0855, 0.0786, 0.0776, 0.0851, 0.0996],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,166][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.5435, 0.0649, 0.0708, 0.0563, 0.0828, 0.0958, 0.0859],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,166][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.6077, 0.0492, 0.0357, 0.0713, 0.0960, 0.1148, 0.0252],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,167][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.4413, 0.0989, 0.1374, 0.0424, 0.0516, 0.0315, 0.1969],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,167][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.4171, 0.0869, 0.1729, 0.0687, 0.0459, 0.1160, 0.0925],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,168][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.5628, 0.0601, 0.0600, 0.1465, 0.0669, 0.0567, 0.0472],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,169][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.7353, 0.0625, 0.0355, 0.0638, 0.0494, 0.0272, 0.0263],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,169][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0009, 0.1243, 0.1771, 0.1270, 0.1631, 0.1699, 0.2376],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,170][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.0874, 0.0033, 0.0104, 0.0453, 0.0033, 0.0009, 0.8493],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,170][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.1092, 0.0408, 0.0553, 0.0893, 0.1898, 0.1616, 0.2291, 0.1249],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,171][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.2856, 0.0552, 0.0614, 0.1114, 0.1180, 0.0726, 0.1803, 0.1154],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,171][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2418, 0.0211, 0.0454, 0.1660, 0.1111, 0.1082, 0.1264, 0.1801],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,172][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.4971, 0.0492, 0.0678, 0.0676, 0.0686, 0.0717, 0.0913, 0.0867],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,172][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.4949, 0.0555, 0.0583, 0.0543, 0.0674, 0.0923, 0.0768, 0.1005],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,173][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.5423, 0.0475, 0.0314, 0.0816, 0.1010, 0.0931, 0.0886, 0.0144],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,173][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.5985, 0.1296, 0.0121, 0.0787, 0.0288, 0.0123, 0.1218, 0.0182],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,174][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.3809, 0.0907, 0.1382, 0.0766, 0.0601, 0.0365, 0.1741, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,174][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.5375, 0.0452, 0.0550, 0.1250, 0.0615, 0.0478, 0.0734, 0.0545],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,175][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.6558, 0.0565, 0.0518, 0.0885, 0.0497, 0.0310, 0.0343, 0.0323],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,176][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0012, 0.0972, 0.1395, 0.1111, 0.1385, 0.1409, 0.1982, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,176][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([1.9925e-02, 1.3295e-03, 2.1208e-04, 1.6963e-02, 5.0615e-04, 8.5291e-03,
        4.8525e-04, 9.5205e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,177][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0090, 0.0030, 0.0034, 0.0069, 0.1361, 0.3465, 0.0282, 0.4314, 0.0355],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,177][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2901, 0.0362, 0.0501, 0.0846, 0.0884, 0.0424, 0.1298, 0.1635, 0.1148],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,178][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2045, 0.0182, 0.0445, 0.1405, 0.0987, 0.0949, 0.1253, 0.1544, 0.1190],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,178][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5031, 0.0425, 0.0586, 0.0570, 0.0560, 0.0555, 0.0737, 0.0713, 0.0823],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,179][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4735, 0.0492, 0.0547, 0.0473, 0.0600, 0.0690, 0.0695, 0.0797, 0.0970],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,179][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.3417, 0.0926, 0.0594, 0.1287, 0.1224, 0.0393, 0.1161, 0.0431, 0.0567],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,180][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.3420, 0.0121, 0.0525, 0.0313, 0.0696, 0.0064, 0.0694, 0.2084, 0.2084],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,181][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.3451, 0.0981, 0.1124, 0.0512, 0.0503, 0.0472, 0.1204, 0.1253, 0.0499],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,181][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4988, 0.0517, 0.0520, 0.1320, 0.0598, 0.0566, 0.0465, 0.0605, 0.0421],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,182][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6659, 0.0813, 0.0408, 0.0788, 0.0395, 0.0264, 0.0231, 0.0358, 0.0085],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,182][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0013, 0.0813, 0.1192, 0.0890, 0.1189, 0.1133, 0.1594, 0.1503, 0.1672],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,183][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([4.0507e-02, 2.8700e-03, 3.1883e-03, 2.4852e-02, 8.3621e-04, 2.0515e-03,
        1.0615e-02, 1.8154e-03, 9.1326e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,183][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0834, 0.0298, 0.0260, 0.0588, 0.0881, 0.0874, 0.3213, 0.0904, 0.1502,
        0.0646], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,184][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.2927, 0.0585, 0.0621, 0.0839, 0.0596, 0.0375, 0.0921, 0.0965, 0.1089,
        0.1082], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,184][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.1919, 0.0178, 0.0392, 0.1214, 0.0830, 0.0826, 0.1025, 0.1422, 0.0966,
        0.1228], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,185][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.4921, 0.0383, 0.0510, 0.0482, 0.0488, 0.0509, 0.0650, 0.0634, 0.0703,
        0.0719], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,186][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.3966, 0.0507, 0.0543, 0.0475, 0.0589, 0.0716, 0.0645, 0.0865, 0.0895,
        0.0798], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,186][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.3802, 0.0331, 0.0170, 0.0823, 0.0681, 0.0296, 0.0467, 0.0582, 0.1608,
        0.1240], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,187][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.5557, 0.0448, 0.0290, 0.0605, 0.0463, 0.0073, 0.1039, 0.0516, 0.0558,
        0.0451], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,187][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.3786, 0.0842, 0.0982, 0.0698, 0.0347, 0.0298, 0.0999, 0.0960, 0.0572,
        0.0517], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,188][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.4817, 0.0572, 0.0606, 0.0862, 0.0485, 0.0443, 0.0489, 0.0615, 0.0452,
        0.0660], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,188][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.6021, 0.0551, 0.0569, 0.0700, 0.0413, 0.0404, 0.0352, 0.0364, 0.0232,
        0.0394], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,189][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0013, 0.0682, 0.0973, 0.0797, 0.0973, 0.0988, 0.1333, 0.1244, 0.1473,
        0.1524], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,190][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ et] are: tensor([4.4566e-02, 3.5423e-03, 9.9786e-04, 2.5153e-02, 4.6912e-03, 4.7535e-04,
        1.0418e-03, 1.4763e-04, 1.2071e-03, 9.1818e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,190][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0494, 0.0108, 0.0243, 0.0198, 0.0181, 0.0701, 0.0875, 0.2175, 0.1099,
        0.2069, 0.1857], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,191][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.2021, 0.0289, 0.0286, 0.0615, 0.0388, 0.0285, 0.0723, 0.0933, 0.1083,
        0.2018, 0.1359], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,191][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.1488, 0.0187, 0.0367, 0.1092, 0.0775, 0.0741, 0.0888, 0.1235, 0.0885,
        0.1054, 0.1289], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,192][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.5168, 0.0264, 0.0375, 0.0389, 0.0394, 0.0410, 0.0546, 0.0513, 0.0591,
        0.0650, 0.0699], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,192][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.3564, 0.0416, 0.0455, 0.0444, 0.0568, 0.0650, 0.0566, 0.0794, 0.0883,
        0.0700, 0.0959], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,193][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.4104, 0.0531, 0.0168, 0.0704, 0.0517, 0.0757, 0.1370, 0.0692, 0.0581,
        0.0338, 0.0238], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,194][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.4591, 0.0469, 0.1551, 0.0480, 0.0177, 0.0059, 0.0527, 0.0170, 0.1215,
        0.0341, 0.0418], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,194][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.2848, 0.0627, 0.0930, 0.0566, 0.0474, 0.0340, 0.1334, 0.0997, 0.0755,
        0.0956, 0.0172], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,195][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.5119, 0.0423, 0.0299, 0.0783, 0.0387, 0.0316, 0.0484, 0.0510, 0.0404,
        0.0886, 0.0388], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,195][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.5934, 0.0432, 0.0365, 0.0967, 0.0450, 0.0281, 0.0361, 0.0320, 0.0226,
        0.0378, 0.0284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,196][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0024, 0.0542, 0.0795, 0.0635, 0.0797, 0.0820, 0.1152, 0.1064, 0.1311,
        0.1272, 0.1590], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,196][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([2.8940e-02, 1.4100e-03, 8.3856e-04, 7.1741e-03, 1.7185e-03, 4.1883e-04,
        3.5000e-04, 9.3285e-03, 1.0529e-03, 8.2416e-04, 9.4794e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,197][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0091, 0.0031, 0.0031, 0.0069, 0.0674, 0.2054, 0.0199, 0.2220, 0.0203,
        0.1333, 0.2727, 0.0368], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,198][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2000, 0.0191, 0.0271, 0.0463, 0.0382, 0.0187, 0.0587, 0.0717, 0.0546,
        0.1598, 0.1832, 0.1226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,198][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.1255, 0.0151, 0.0338, 0.0877, 0.0663, 0.0662, 0.0840, 0.1037, 0.0823,
        0.1044, 0.1218, 0.1094], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,199][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5194, 0.0243, 0.0348, 0.0349, 0.0337, 0.0336, 0.0436, 0.0440, 0.0502,
        0.0578, 0.0638, 0.0599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,199][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.3767, 0.0383, 0.0415, 0.0378, 0.0462, 0.0516, 0.0495, 0.0599, 0.0694,
        0.0625, 0.0817, 0.0850], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,200][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.2574, 0.0773, 0.0465, 0.0801, 0.0750, 0.0347, 0.1044, 0.0466, 0.0598,
        0.1228, 0.0406, 0.0546], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,200][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.2360, 0.0095, 0.0374, 0.0272, 0.0488, 0.0049, 0.0642, 0.1691, 0.1604,
        0.1101, 0.0262, 0.1062], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,201][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2563, 0.0755, 0.1050, 0.0419, 0.0412, 0.0460, 0.1193, 0.0955, 0.0432,
        0.0822, 0.0525, 0.0413], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,202][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4344, 0.0431, 0.0415, 0.1141, 0.0462, 0.0436, 0.0371, 0.0484, 0.0333,
        0.0748, 0.0456, 0.0378], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,202][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6141, 0.0606, 0.0361, 0.0805, 0.0447, 0.0225, 0.0200, 0.0334, 0.0081,
        0.0427, 0.0266, 0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,203][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0018, 0.0463, 0.0714, 0.0538, 0.0727, 0.0696, 0.0967, 0.0938, 0.1053,
        0.1126, 0.1408, 0.1353], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,203][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([1.2601e-02, 1.7846e-03, 1.5246e-03, 7.8711e-03, 7.2331e-04, 1.5676e-03,
        1.2430e-02, 1.6697e-03, 7.8429e-01, 1.4557e-03, 6.2924e-04, 1.7345e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,204][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1769, 0.0372, 0.0489, 0.0513, 0.0044, 0.0086, 0.0362, 0.0526, 0.0588,
        0.1048, 0.0481, 0.1177, 0.2544], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,205][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.3416, 0.0240, 0.0296, 0.0312, 0.0228, 0.0165, 0.0280, 0.0404, 0.0397,
        0.0734, 0.0837, 0.0732, 0.1959], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,205][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.1118, 0.0136, 0.0274, 0.0732, 0.0573, 0.0554, 0.0653, 0.0846, 0.0640,
        0.0821, 0.0983, 0.0843, 0.1829], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,206][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.5179, 0.0210, 0.0294, 0.0304, 0.0291, 0.0299, 0.0371, 0.0356, 0.0411,
        0.0467, 0.0525, 0.0484, 0.0810], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,206][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.3162, 0.0359, 0.0391, 0.0361, 0.0453, 0.0520, 0.0432, 0.0565, 0.0627,
        0.0604, 0.0795, 0.0784, 0.0948], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,207][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.4221, 0.0491, 0.0178, 0.0492, 0.0147, 0.0068, 0.0208, 0.0150, 0.0399,
        0.0403, 0.0065, 0.0327, 0.2851], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,207][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.4473, 0.1110, 0.1119, 0.0564, 0.0208, 0.0062, 0.0416, 0.0228, 0.0378,
        0.0587, 0.0125, 0.0240, 0.0489], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,208][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.2389, 0.0685, 0.0744, 0.0453, 0.0242, 0.0296, 0.0809, 0.0509, 0.0640,
        0.1221, 0.0383, 0.0544, 0.1084], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,209][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.3704, 0.0443, 0.0606, 0.0672, 0.0461, 0.0451, 0.0414, 0.0471, 0.0357,
        0.0466, 0.0441, 0.0386, 0.1128], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,209][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.5728, 0.0432, 0.0293, 0.0446, 0.0386, 0.0229, 0.0234, 0.0207, 0.0222,
        0.0308, 0.0281, 0.0250, 0.0984], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,210][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0019, 0.0381, 0.0587, 0.0468, 0.0596, 0.0614, 0.0808, 0.0774, 0.0897,
        0.0922, 0.1131, 0.1137, 0.1664], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,210][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ is] are: tensor([6.7531e-02, 1.1319e-03, 2.5476e-03, 1.4388e-02, 7.9826e-04, 4.3523e-04,
        3.5989e-03, 1.1874e-03, 2.5017e-03, 5.1122e-03, 1.0875e-03, 1.9595e-03,
        8.9772e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,211][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0664, 0.0122, 0.0131, 0.0216, 0.0046, 0.0045, 0.0081, 0.0099, 0.0196,
        0.0603, 0.0315, 0.0414, 0.4752, 0.2316], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,212][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1285, 0.0159, 0.0176, 0.0337, 0.0177, 0.0168, 0.0248, 0.0379, 0.0306,
        0.0576, 0.0719, 0.0557, 0.3327, 0.1585], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,212][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0959, 0.0140, 0.0270, 0.0558, 0.0473, 0.0497, 0.0577, 0.0764, 0.0558,
        0.0702, 0.0849, 0.0734, 0.1403, 0.1515], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,213][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5358, 0.0178, 0.0251, 0.0247, 0.0243, 0.0246, 0.0306, 0.0298, 0.0344,
        0.0381, 0.0452, 0.0408, 0.0646, 0.0641], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,213][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2762, 0.0395, 0.0421, 0.0312, 0.0433, 0.0514, 0.0453, 0.0585, 0.0575,
        0.0547, 0.0702, 0.0698, 0.0749, 0.0854], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,214][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3993, 0.0374, 0.0141, 0.1396, 0.0192, 0.0107, 0.0353, 0.0212, 0.0524,
        0.0469, 0.0087, 0.0333, 0.1161, 0.0659], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,215][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.3344, 0.1090, 0.0947, 0.0456, 0.0171, 0.0161, 0.0534, 0.0222, 0.0608,
        0.0591, 0.0105, 0.0384, 0.0827, 0.0560], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,215][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2725, 0.0527, 0.0734, 0.0442, 0.0272, 0.0275, 0.0769, 0.0480, 0.0540,
        0.0831, 0.0351, 0.0472, 0.0713, 0.0869], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,216][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3467, 0.0420, 0.0464, 0.0704, 0.0399, 0.0377, 0.0336, 0.0396, 0.0278,
        0.0435, 0.0376, 0.0303, 0.0865, 0.1180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,216][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4709, 0.0343, 0.0260, 0.0406, 0.0323, 0.0196, 0.0224, 0.0191, 0.0191,
        0.0239, 0.0275, 0.0206, 0.1060, 0.1378], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,217][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0024, 0.0336, 0.0528, 0.0388, 0.0498, 0.0517, 0.0702, 0.0696, 0.0785,
        0.0812, 0.0994, 0.1008, 0.1412, 0.1299], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,218][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([2.0451e-01, 1.2995e-03, 2.3778e-03, 5.1089e-02, 1.1417e-03, 1.1357e-03,
        7.4622e-04, 3.4443e-04, 1.5838e-03, 2.0216e-03, 4.6759e-04, 1.8235e-03,
        1.2299e-02, 7.1916e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,218][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0280, 0.0044, 0.0062, 0.0126, 0.0015, 0.0034, 0.0047, 0.0050, 0.0064,
        0.0374, 0.0190, 0.0161, 0.2568, 0.5112, 0.0872], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,219][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0675, 0.0044, 0.0065, 0.0161, 0.0075, 0.0109, 0.0107, 0.0168, 0.0175,
        0.0406, 0.0324, 0.0363, 0.3563, 0.2644, 0.1121], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,219][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0816, 0.0132, 0.0261, 0.0518, 0.0414, 0.0460, 0.0537, 0.0696, 0.0511,
        0.0672, 0.0774, 0.0667, 0.1330, 0.1293, 0.0921], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,220][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.5324, 0.0142, 0.0212, 0.0214, 0.0208, 0.0209, 0.0273, 0.0269, 0.0316,
        0.0362, 0.0423, 0.0380, 0.0596, 0.0534, 0.0539], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,221][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.2906, 0.0318, 0.0346, 0.0287, 0.0331, 0.0421, 0.0370, 0.0489, 0.0470,
        0.0485, 0.0634, 0.0592, 0.0706, 0.0752, 0.0891], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,221][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.3613, 0.1047, 0.0308, 0.0694, 0.0079, 0.0049, 0.0270, 0.0322, 0.0154,
        0.0286, 0.0079, 0.0106, 0.0923, 0.0800, 0.1270], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,222][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1699, 0.0991, 0.0665, 0.0446, 0.0348, 0.0199, 0.1856, 0.0165, 0.0228,
        0.0270, 0.0099, 0.0139, 0.0991, 0.0541, 0.1362], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,223][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.2215, 0.0585, 0.0506, 0.0394, 0.0264, 0.0258, 0.0567, 0.0647, 0.0338,
        0.0952, 0.0414, 0.0291, 0.1166, 0.0618, 0.0785], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,223][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.3226, 0.0327, 0.0402, 0.0557, 0.0348, 0.0321, 0.0394, 0.0431, 0.0293,
        0.0439, 0.0350, 0.0334, 0.0976, 0.1137, 0.0466], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,224][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.4423, 0.0242, 0.0285, 0.0530, 0.0299, 0.0148, 0.0197, 0.0139, 0.0224,
        0.0209, 0.0213, 0.0264, 0.0944, 0.1574, 0.0311], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,224][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0023, 0.0289, 0.0466, 0.0343, 0.0441, 0.0470, 0.0652, 0.0565, 0.0711,
        0.0708, 0.0876, 0.0911, 0.1233, 0.1032, 0.1279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,225][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ same] are: tensor([2.4253e-02, 5.8660e-03, 1.2620e-03, 1.3135e-02, 1.4381e-04, 1.0868e-04,
        2.5658e-04, 2.5889e-04, 7.9319e-05, 6.8759e-04, 8.5028e-04, 3.1011e-05,
        8.8392e-04, 8.5089e-04, 9.5133e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,226][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ as] are: tensor([2.2579e-02, 2.5241e-03, 2.7221e-03, 3.6300e-03, 6.6183e-04, 1.3145e-03,
        8.2964e-04, 2.0779e-03, 2.3253e-03, 1.2338e-02, 7.6555e-03, 4.8402e-03,
        9.3693e-02, 1.0718e-01, 7.1758e-01, 1.8047e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,226][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0905, 0.0089, 0.0085, 0.0136, 0.0091, 0.0108, 0.0169, 0.0179, 0.0232,
        0.0287, 0.0386, 0.0445, 0.2108, 0.1732, 0.1550, 0.1500],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,227][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0670, 0.0115, 0.0215, 0.0439, 0.0373, 0.0381, 0.0450, 0.0593, 0.0455,
        0.0568, 0.0665, 0.0607, 0.1114, 0.1185, 0.0765, 0.1405],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,227][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.5472, 0.0119, 0.0175, 0.0175, 0.0174, 0.0173, 0.0234, 0.0216, 0.0264,
        0.0300, 0.0338, 0.0322, 0.0517, 0.0486, 0.0478, 0.0556],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,228][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.2543, 0.0275, 0.0293, 0.0262, 0.0349, 0.0388, 0.0336, 0.0454, 0.0481,
        0.0471, 0.0561, 0.0613, 0.0650, 0.0732, 0.0714, 0.0878],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,229][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.2970, 0.0154, 0.0131, 0.0400, 0.0054, 0.0041, 0.0238, 0.0129, 0.0114,
        0.0146, 0.0071, 0.0079, 0.0544, 0.0558, 0.0319, 0.4052],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,229][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0809, 0.0364, 0.0380, 0.0186, 0.0074, 0.0084, 0.0053, 0.0113, 0.0166,
        0.0266, 0.0137, 0.0123, 0.0277, 0.0294, 0.6568, 0.0106],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,230][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.2534, 0.0520, 0.0490, 0.0366, 0.0250, 0.0234, 0.0626, 0.0360, 0.0454,
        0.0915, 0.0289, 0.0377, 0.0709, 0.0608, 0.0550, 0.0718],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,231][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3080, 0.0361, 0.0345, 0.0545, 0.0363, 0.0344, 0.0344, 0.0362, 0.0280,
        0.0382, 0.0344, 0.0308, 0.0870, 0.0868, 0.0478, 0.0727],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,231][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.4117, 0.0312, 0.0200, 0.0350, 0.0313, 0.0165, 0.0150, 0.0160, 0.0141,
        0.0192, 0.0235, 0.0157, 0.0965, 0.1370, 0.0443, 0.0730],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,232][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0027, 0.0222, 0.0367, 0.0275, 0.0364, 0.0366, 0.0508, 0.0485, 0.0604,
        0.0611, 0.0756, 0.0794, 0.1093, 0.0991, 0.1194, 0.1340],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,232][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ as] are: tensor([6.7876e-02, 1.8290e-03, 3.4354e-03, 3.4125e-02, 8.3434e-04, 4.1775e-04,
        3.8863e-03, 2.2117e-03, 4.1721e-04, 1.4925e-03, 6.5738e-04, 3.4533e-04,
        1.3955e-02, 5.2944e-03, 3.0400e-03, 8.6018e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,233][circuit_into_ebeddingspace.py][line:2169][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0467, 0.0050, 0.0053, 0.0121, 0.0027, 0.0021, 0.0039, 0.0039, 0.0103,
        0.0240, 0.0141, 0.0214, 0.0939, 0.0837, 0.1134, 0.2735, 0.2839],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,234][circuit_into_ebeddingspace.py][line:2172][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0710, 0.0072, 0.0087, 0.0161, 0.0092, 0.0098, 0.0132, 0.0179, 0.0171,
        0.0316, 0.0409, 0.0317, 0.1311, 0.0607, 0.1668, 0.2378, 0.1290],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,234][circuit_into_ebeddingspace.py][line:2175][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0601, 0.0119, 0.0218, 0.0357, 0.0321, 0.0351, 0.0406, 0.0520, 0.0411,
        0.0491, 0.0587, 0.0538, 0.0943, 0.1019, 0.0752, 0.1206, 0.1162],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,235][circuit_into_ebeddingspace.py][line:2178][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5589, 0.0108, 0.0157, 0.0154, 0.0148, 0.0148, 0.0191, 0.0187, 0.0219,
        0.0246, 0.0293, 0.0266, 0.0432, 0.0437, 0.0439, 0.0482, 0.0505],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,236][circuit_into_ebeddingspace.py][line:2181][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2260, 0.0302, 0.0324, 0.0232, 0.0338, 0.0395, 0.0336, 0.0439, 0.0440,
        0.0408, 0.0525, 0.0541, 0.0544, 0.0620, 0.0764, 0.0747, 0.0785],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,236][circuit_into_ebeddingspace.py][line:2184][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3099, 0.0257, 0.0107, 0.1083, 0.0142, 0.0080, 0.0308, 0.0166, 0.0399,
        0.0432, 0.0069, 0.0268, 0.0948, 0.0622, 0.0328, 0.1165, 0.0527],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,237][circuit_into_ebeddingspace.py][line:2187][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2726, 0.0894, 0.0746, 0.0356, 0.0153, 0.0155, 0.0518, 0.0212, 0.0546,
        0.0585, 0.0091, 0.0353, 0.0628, 0.0449, 0.0822, 0.0426, 0.0340],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,238][circuit_into_ebeddingspace.py][line:2190][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2531, 0.0387, 0.0561, 0.0334, 0.0257, 0.0230, 0.0655, 0.0419, 0.0443,
        0.0624, 0.0290, 0.0384, 0.0546, 0.0640, 0.0502, 0.0589, 0.0609],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,238][circuit_into_ebeddingspace.py][line:2193][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2894, 0.0344, 0.0328, 0.0589, 0.0308, 0.0277, 0.0293, 0.0304, 0.0245,
        0.0401, 0.0289, 0.0277, 0.0755, 0.0900, 0.0442, 0.0654, 0.0700],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,239][circuit_into_ebeddingspace.py][line:2196][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.3564, 0.0270, 0.0246, 0.0345, 0.0302, 0.0183, 0.0206, 0.0199, 0.0184,
        0.0225, 0.0285, 0.0198, 0.0813, 0.1100, 0.0408, 0.0575, 0.0895],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,239][circuit_into_ebeddingspace.py][line:2199][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0032, 0.0203, 0.0338, 0.0242, 0.0315, 0.0323, 0.0448, 0.0441, 0.0513,
        0.0525, 0.0659, 0.0673, 0.0952, 0.0895, 0.1081, 0.1218, 0.1141],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,240][circuit_into_ebeddingspace.py][line:2202][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.0195e-01, 8.0684e-04, 1.3415e-03, 2.9726e-02, 8.6288e-04, 8.8300e-04,
        4.9737e-04, 2.7058e-04, 1.0657e-03, 1.5409e-03, 3.2203e-04, 1.3946e-03,
        6.1262e-03, 4.1589e-01, 9.1680e-04, 1.0803e-02, 4.2560e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,245][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,246][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,246][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,247][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,248][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,248][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,249][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,249][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,250][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,250][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,251][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,251][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,252][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.8692, 0.1308], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,253][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9660, 0.0340], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,253][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.8972, 0.1028], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,254][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9594, 0.0406], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,254][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9570, 0.0430], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,255][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9651, 0.0349], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,255][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9599, 0.0401], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,256][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.7882, 0.2118], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,256][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.8688, 0.1312], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,257][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8320, 0.1680], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,257][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.8656, 0.1344], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,257][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9497, 0.0503], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,258][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5399, 0.4335, 0.0266], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,258][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8063, 0.0813, 0.1124], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,259][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.2372, 0.6954, 0.0674], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,259][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7874, 0.1893, 0.0233], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,260][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.2847, 0.6830, 0.0323], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,260][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7343, 0.2103, 0.0554], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,261][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9344, 0.0293, 0.0363], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,261][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4679, 0.1163, 0.4158], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,262][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.5416, 0.4163, 0.0421], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,262][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.2315, 0.6804, 0.0880], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,263][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6941, 0.1019, 0.2040], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,263][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8593, 0.0435, 0.0972], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,264][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.3858, 0.1567, 0.1149, 0.3426], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,264][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8729, 0.0286, 0.0415, 0.0570], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,265][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1666, 0.0905, 0.7259, 0.0171], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,265][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.8156, 0.1120, 0.0321, 0.0403], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,266][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2279, 0.0906, 0.6124, 0.0691], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,266][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.4871, 0.0893, 0.3008, 0.1228], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,267][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8358, 0.0225, 0.0380, 0.1036], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,267][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3953, 0.0638, 0.3006, 0.2403], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,268][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3709, 0.3178, 0.1754, 0.1359], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,268][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.2584, 0.1389, 0.5759, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,269][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5983, 0.0984, 0.2002, 0.1031], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,269][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.8154, 0.0323, 0.0814, 0.0709], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,270][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.3236, 0.0304, 0.0213, 0.3807, 0.2440], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,270][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.6276, 0.0643, 0.1251, 0.1001, 0.0829], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,271][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.3220, 0.0314, 0.0909, 0.5128, 0.0429], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,271][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.4996, 0.0581, 0.0498, 0.1911, 0.2014], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,272][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.6913, 0.0054, 0.0133, 0.2033, 0.0867], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,272][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.2926, 0.0622, 0.2369, 0.3723, 0.0361], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,273][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.8462, 0.0156, 0.0295, 0.0590, 0.0497], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,273][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2803, 0.0718, 0.2710, 0.1251, 0.2517], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,274][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.2498, 0.1113, 0.1867, 0.4184, 0.0338], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,274][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.3262, 0.0881, 0.2312, 0.1911, 0.1634], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,275][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.5563, 0.0851, 0.1729, 0.0578, 0.1280], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,275][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.7853, 0.0319, 0.0757, 0.0363, 0.0708], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,276][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.1993, 0.0236, 0.0219, 0.2405, 0.4467, 0.0681], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,277][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4687, 0.1003, 0.1477, 0.0944, 0.1423, 0.0465], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,277][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0793, 0.0142, 0.0040, 0.0373, 0.8343, 0.0310], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,278][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.1417, 0.0280, 0.0209, 0.0800, 0.5826, 0.1468], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,278][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.4331, 0.0239, 0.0427, 0.1938, 0.2958, 0.0107], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,279][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.3314, 0.0170, 0.1803, 0.2511, 0.1605, 0.0597], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,279][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.8404, 0.0150, 0.0212, 0.0644, 0.0421, 0.0169], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,280][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.3012, 0.0613, 0.1926, 0.0936, 0.1511, 0.2002], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,280][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.3148, 0.0908, 0.1008, 0.3037, 0.0745, 0.1153], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,281][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.2164, 0.0428, 0.1059, 0.0901, 0.4684, 0.0763], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,281][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.4726, 0.0742, 0.1567, 0.0503, 0.0937, 0.1526], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,282][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.7566, 0.0317, 0.0613, 0.0371, 0.0472, 0.0661], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,282][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.3164, 0.0118, 0.0084, 0.2238, 0.2370, 0.1700, 0.0326],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,283][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.5948, 0.0206, 0.0362, 0.1116, 0.1021, 0.0228, 0.1120],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,283][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0193, 0.0013, 0.0015, 0.0163, 0.2061, 0.7284, 0.0271],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,284][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.3106, 0.0210, 0.0188, 0.1133, 0.2858, 0.2063, 0.0442],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,284][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4164, 0.0190, 0.0048, 0.1233, 0.2607, 0.1624, 0.0134],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,285][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.2568, 0.0358, 0.0751, 0.2430, 0.0608, 0.2222, 0.1064],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,285][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.7667, 0.0143, 0.0205, 0.0626, 0.0459, 0.0215, 0.0685],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,286][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2405, 0.0336, 0.1339, 0.0741, 0.1186, 0.1305, 0.2688],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,287][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3053, 0.0378, 0.0538, 0.2674, 0.0258, 0.1017, 0.2082],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,287][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1828, 0.0382, 0.0817, 0.0925, 0.1041, 0.4037, 0.0971],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,288][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3635, 0.0542, 0.1156, 0.0493, 0.0812, 0.1129, 0.2234],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,288][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.7019, 0.0220, 0.0426, 0.0323, 0.0385, 0.0476, 0.1150],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,289][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.2233, 0.0167, 0.0109, 0.2458, 0.2989, 0.0954, 0.0489, 0.0601],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,289][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.4680, 0.0272, 0.0514, 0.0621, 0.0994, 0.0193, 0.2255, 0.0471],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,290][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2753, 0.0015, 0.0031, 0.0095, 0.2439, 0.0649, 0.3517, 0.0501],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,290][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.2834, 0.0263, 0.0150, 0.0610, 0.2374, 0.1212, 0.1578, 0.0980],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,291][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.4998, 0.0071, 0.0167, 0.0909, 0.0481, 0.0178, 0.3013, 0.0184],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,291][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.2481, 0.0094, 0.0739, 0.1188, 0.0394, 0.0553, 0.4013, 0.0538],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,292][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.8312, 0.0104, 0.0119, 0.0500, 0.0173, 0.0121, 0.0418, 0.0253],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,293][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1920, 0.0336, 0.1004, 0.0552, 0.0821, 0.0922, 0.2125, 0.2321],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,293][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2811, 0.0410, 0.0344, 0.2486, 0.0193, 0.0536, 0.2276, 0.0943],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,294][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.1430, 0.0130, 0.0466, 0.0283, 0.0570, 0.2093, 0.3831, 0.1198],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,294][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.3299, 0.0454, 0.0936, 0.0439, 0.0645, 0.0923, 0.1814, 0.1489],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,295][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.6733, 0.0227, 0.0394, 0.0295, 0.0287, 0.0456, 0.0882, 0.0728],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,295][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.3827, 0.0086, 0.0036, 0.1751, 0.1936, 0.0567, 0.0473, 0.1138, 0.0185],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,296][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4437, 0.0100, 0.0424, 0.0941, 0.1101, 0.0155, 0.1614, 0.0448, 0.0779],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,296][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([8.7231e-03, 4.3252e-05, 1.7074e-04, 2.2985e-03, 1.3743e-02, 9.6340e-02,
        5.3448e-02, 6.6141e-01, 1.6383e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,297][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2428, 0.0099, 0.0102, 0.0431, 0.0762, 0.1339, 0.0548, 0.3548, 0.0743],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,297][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.2605, 0.0009, 0.0013, 0.0221, 0.0170, 0.0269, 0.0664, 0.5437, 0.0612],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,298][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.2118, 0.0120, 0.0163, 0.0556, 0.0190, 0.0740, 0.1052, 0.2462, 0.2599],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,299][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8061, 0.0085, 0.0100, 0.0481, 0.0207, 0.0108, 0.0300, 0.0350, 0.0309],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,299][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1559, 0.0240, 0.0774, 0.0409, 0.0646, 0.0716, 0.1461, 0.1857, 0.2338],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,300][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2097, 0.0200, 0.0272, 0.1425, 0.0163, 0.0626, 0.1172, 0.2941, 0.1103],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,300][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0868, 0.0080, 0.0148, 0.0192, 0.0246, 0.0699, 0.1341, 0.4861, 0.1565],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,301][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2596, 0.0335, 0.0718, 0.0328, 0.0538, 0.0782, 0.1472, 0.1367, 0.1864],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,301][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.6588, 0.0157, 0.0309, 0.0215, 0.0206, 0.0315, 0.0685, 0.0667, 0.0857],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,302][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.3554, 0.0108, 0.0069, 0.1746, 0.0630, 0.0399, 0.0359, 0.1412, 0.0728,
        0.0995], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,302][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.5676, 0.0091, 0.0877, 0.1810, 0.0708, 0.0050, 0.0455, 0.0114, 0.0113,
        0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,303][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ et] are: tensor([1.8724e-01, 1.6701e-04, 8.4665e-04, 1.1765e-03, 1.0826e-03, 3.4516e-02,
        4.8274e-02, 3.1007e-01, 3.5593e-01, 6.0696e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,304][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.2810, 0.0228, 0.0091, 0.0336, 0.0783, 0.0768, 0.0746, 0.1726, 0.0762,
        0.1749], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,304][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.5351, 0.0011, 0.0016, 0.0573, 0.0014, 0.0537, 0.1254, 0.1325, 0.0706,
        0.0214], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,305][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.1712, 0.0038, 0.0185, 0.0425, 0.0144, 0.0250, 0.0799, 0.0979, 0.3619,
        0.1849], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,305][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.6513, 0.0128, 0.0166, 0.0465, 0.0239, 0.0131, 0.0423, 0.0372, 0.0368,
        0.1194], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,306][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1276, 0.0242, 0.0696, 0.0349, 0.0519, 0.0553, 0.1003, 0.1310, 0.1636,
        0.2416], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,306][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.3120, 0.0359, 0.0316, 0.1561, 0.0034, 0.0277, 0.1018, 0.1694, 0.1061,
        0.0561], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,307][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0856, 0.0048, 0.0130, 0.0099, 0.0196, 0.0607, 0.0734, 0.2361, 0.4515,
        0.0455], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,308][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.2577, 0.0398, 0.0668, 0.0303, 0.0510, 0.0794, 0.1061, 0.1142, 0.1325,
        0.1223], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,308][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.6508, 0.0125, 0.0218, 0.0149, 0.0213, 0.0363, 0.0507, 0.0667, 0.0669,
        0.0580], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,309][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.2273, 0.0139, 0.0028, 0.0937, 0.0390, 0.0775, 0.0263, 0.0930, 0.0292,
        0.3200, 0.0772], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,309][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.5133, 0.0084, 0.0567, 0.1724, 0.0634, 0.0087, 0.0719, 0.0187, 0.0497,
        0.0079, 0.0288], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,310][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([5.0759e-02, 2.0365e-04, 4.0755e-04, 7.0637e-04, 3.1488e-03, 1.1888e-02,
        1.1537e-02, 4.7276e-02, 1.4501e-01, 6.4669e-01, 8.2375e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,310][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.1489, 0.0181, 0.0064, 0.0306, 0.0493, 0.0332, 0.0667, 0.1572, 0.0782,
        0.2817, 0.1298], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,311][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.4471, 0.0014, 0.0101, 0.0186, 0.0012, 0.0408, 0.1128, 0.1251, 0.0928,
        0.1381, 0.0120], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,311][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.1546, 0.0017, 0.0071, 0.0135, 0.0049, 0.0122, 0.0581, 0.0608, 0.1838,
        0.4448, 0.0586], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,312][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.8283, 0.0066, 0.0062, 0.0343, 0.0101, 0.0058, 0.0160, 0.0159, 0.0133,
        0.0445, 0.0189], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,313][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.1361, 0.0211, 0.0545, 0.0233, 0.0363, 0.0428, 0.0908, 0.1059, 0.1288,
        0.2125, 0.1479], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,313][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.3123, 0.0134, 0.0095, 0.0854, 0.0082, 0.0103, 0.0581, 0.1486, 0.0524,
        0.2682, 0.0336], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,314][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.0782, 0.0024, 0.0177, 0.0048, 0.0282, 0.0504, 0.0592, 0.2190, 0.2525,
        0.1956, 0.0920], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,314][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2150, 0.0330, 0.0585, 0.0260, 0.0398, 0.0533, 0.1015, 0.0878, 0.1244,
        0.1332, 0.1275], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,315][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.6409, 0.0137, 0.0217, 0.0141, 0.0159, 0.0214, 0.0415, 0.0376, 0.0584,
        0.0721, 0.0626], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,316][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2675, 0.0051, 0.0015, 0.0838, 0.0442, 0.0187, 0.0168, 0.0399, 0.0075,
        0.2984, 0.1986, 0.0180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,316][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4770, 0.0062, 0.0486, 0.1218, 0.0900, 0.0078, 0.0960, 0.0223, 0.0406,
        0.0217, 0.0421, 0.0260], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,317][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([4.7861e-03, 3.4426e-06, 1.3387e-05, 1.3783e-04, 4.5878e-04, 3.1530e-03,
        2.5101e-03, 2.4754e-02, 7.9777e-03, 3.1255e-01, 4.6808e-01, 1.7557e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,317][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.1451, 0.0038, 0.0029, 0.0142, 0.0189, 0.0230, 0.0127, 0.0713, 0.0168,
        0.2162, 0.4003, 0.0748], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,318][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([3.7768e-01, 3.6746e-04, 8.5311e-04, 1.0904e-02, 3.4641e-03, 7.3399e-03,
        3.4002e-02, 1.2297e-01, 2.2733e-02, 2.1941e-01, 8.3878e-02, 1.1640e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,318][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.1317, 0.0017, 0.0026, 0.0096, 0.0023, 0.0089, 0.0135, 0.0385, 0.0389,
        0.3701, 0.1149, 0.2672], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,319][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.7333, 0.0066, 0.0073, 0.0449, 0.0126, 0.0062, 0.0164, 0.0210, 0.0175,
        0.0623, 0.0342, 0.0377], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,320][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0897, 0.0156, 0.0436, 0.0221, 0.0345, 0.0354, 0.0630, 0.0872, 0.0995,
        0.1722, 0.1233, 0.2138], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,320][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1759, 0.0082, 0.0071, 0.0505, 0.0035, 0.0093, 0.0302, 0.0532, 0.0289,
        0.4317, 0.0797, 0.1217], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,321][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0853, 0.0025, 0.0057, 0.0058, 0.0055, 0.0180, 0.0344, 0.1225, 0.0469,
        0.1485, 0.3177, 0.2073], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,321][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1499, 0.0237, 0.0481, 0.0248, 0.0356, 0.0480, 0.0825, 0.0844, 0.1062,
        0.1138, 0.1116, 0.1715], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,322][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.6192, 0.0120, 0.0197, 0.0146, 0.0138, 0.0192, 0.0342, 0.0380, 0.0429,
        0.0663, 0.0497, 0.0705], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,322][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.3344, 0.0395, 0.0084, 0.1553, 0.0215, 0.0093, 0.0284, 0.0412, 0.0288,
        0.1397, 0.1059, 0.0503, 0.0372], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,323][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.6425, 0.0164, 0.0295, 0.0576, 0.0309, 0.0083, 0.0249, 0.0291, 0.0173,
        0.0267, 0.0195, 0.0099, 0.0874], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,324][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ is] are: tensor([6.6025e-02, 3.8038e-05, 1.6858e-04, 2.0190e-04, 6.0846e-04, 7.4320e-04,
        2.5235e-03, 8.8530e-03, 2.3843e-02, 9.7253e-02, 1.7489e-01, 5.0999e-01,
        1.1486e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,324][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.3984, 0.0137, 0.0029, 0.0269, 0.0087, 0.0059, 0.0066, 0.0312, 0.0125,
        0.1052, 0.1854, 0.0734, 0.1291], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,325][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ is] are: tensor([8.0994e-01, 1.7873e-03, 1.1559e-03, 1.1742e-02, 5.1650e-05, 2.0903e-03,
        5.1572e-04, 2.7230e-03, 2.8463e-03, 1.8935e-02, 2.8004e-02, 1.0056e-02,
        1.1015e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,325][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1772, 0.0004, 0.0021, 0.0068, 0.0027, 0.0041, 0.0164, 0.0165, 0.0481,
        0.1372, 0.1381, 0.2655, 0.1849], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,326][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.6168, 0.0132, 0.0113, 0.0433, 0.0097, 0.0059, 0.0145, 0.0193, 0.0115,
        0.0816, 0.0346, 0.0267, 0.1116], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,327][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0818, 0.0153, 0.0355, 0.0255, 0.0324, 0.0320, 0.0541, 0.0762, 0.0709,
        0.1435, 0.1031, 0.1596, 0.1701], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,327][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.3940, 0.0286, 0.0151, 0.1124, 0.0009, 0.0026, 0.0119, 0.0073, 0.0122,
        0.0693, 0.0646, 0.0534, 0.2278], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,328][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0899, 0.0058, 0.0136, 0.0037, 0.0064, 0.0147, 0.0357, 0.0511, 0.0860,
        0.1345, 0.1940, 0.3141, 0.0503], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,328][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.2028, 0.0351, 0.0448, 0.0294, 0.0354, 0.0398, 0.0515, 0.0659, 0.0607,
        0.0907, 0.0926, 0.0979, 0.1534], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,329][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.6351, 0.0130, 0.0156, 0.0127, 0.0165, 0.0188, 0.0206, 0.0302, 0.0279,
        0.0474, 0.0440, 0.0429, 0.0752], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,330][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2541, 0.0284, 0.0174, 0.0836, 0.0240, 0.0127, 0.0304, 0.0591, 0.0497,
        0.0777, 0.1575, 0.0962, 0.0361, 0.0733], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,330][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.6511, 0.0199, 0.0273, 0.0505, 0.0254, 0.0086, 0.0162, 0.0201, 0.0125,
        0.0178, 0.0143, 0.0064, 0.0973, 0.0326], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,331][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.3792e-02, 4.5791e-06, 1.3275e-05, 1.9682e-04, 2.3293e-04, 6.9267e-04,
        1.2263e-03, 5.5554e-03, 6.2198e-03, 2.5874e-02, 6.1244e-02, 8.3447e-02,
        6.9959e-01, 9.1909e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,331][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1183, 0.0032, 0.0016, 0.0043, 0.0082, 0.0063, 0.0046, 0.0378, 0.0096,
        0.1181, 0.2241, 0.0485, 0.1253, 0.2903], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,332][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([3.3355e-01, 2.0520e-03, 1.1099e-03, 2.2220e-03, 8.1359e-05, 4.7948e-03,
        1.1729e-03, 1.4338e-02, 1.7615e-02, 1.0596e-02, 4.5447e-02, 6.9490e-02,
        2.4950e-01, 2.4803e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,333][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([1.3211e-01, 3.1890e-04, 1.3462e-03, 1.2515e-03, 1.1112e-03, 3.2693e-03,
        9.9724e-03, 8.1407e-03, 2.4503e-02, 7.4622e-02, 5.5857e-02, 1.2859e-01,
        4.5052e-01, 1.0839e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,333][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4984, 0.0120, 0.0112, 0.0459, 0.0129, 0.0061, 0.0173, 0.0173, 0.0129,
        0.0473, 0.0320, 0.0269, 0.0929, 0.1667], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,334][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0716, 0.0121, 0.0319, 0.0215, 0.0276, 0.0252, 0.0469, 0.0605, 0.0667,
        0.1131, 0.0875, 0.1499, 0.1541, 0.1314], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,334][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0933, 0.0014, 0.0018, 0.0079, 0.0007, 0.0034, 0.0036, 0.0226, 0.0060,
        0.0379, 0.0459, 0.0266, 0.6108, 0.1382], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,335][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1459, 0.0041, 0.0119, 0.0005, 0.0078, 0.0165, 0.0283, 0.0510, 0.0614,
        0.0866, 0.2412, 0.2032, 0.0734, 0.0682], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,336][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1758, 0.0381, 0.0469, 0.0294, 0.0296, 0.0368, 0.0466, 0.0506, 0.0501,
        0.0737, 0.0704, 0.0813, 0.1439, 0.1267], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,336][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.6126, 0.0116, 0.0145, 0.0113, 0.0140, 0.0157, 0.0203, 0.0239, 0.0251,
        0.0401, 0.0370, 0.0382, 0.0670, 0.0687], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,337][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1515, 0.0140, 0.0080, 0.0483, 0.0123, 0.0096, 0.0145, 0.0314, 0.0365,
        0.0931, 0.2192, 0.0753, 0.0575, 0.0580, 0.1707], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,337][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.5972, 0.0159, 0.0116, 0.0373, 0.0186, 0.0046, 0.0397, 0.0136, 0.0305,
        0.0285, 0.0110, 0.0169, 0.0951, 0.0537, 0.0258], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,338][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ same] are: tensor([8.3390e-02, 6.6536e-06, 1.1463e-05, 2.3631e-04, 7.2621e-05, 9.3150e-04,
        9.9531e-05, 1.7873e-03, 3.6720e-03, 2.9784e-02, 7.1717e-03, 4.2680e-02,
        5.7457e-01, 2.1678e-01, 3.8800e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,339][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0475, 0.0009, 0.0007, 0.0034, 0.0080, 0.0068, 0.0051, 0.0252, 0.0173,
        0.0785, 0.1036, 0.0747, 0.0612, 0.5226, 0.0446], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,339][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ same] are: tensor([1.7113e-01, 7.6884e-04, 1.6833e-04, 3.2963e-03, 2.7689e-04, 6.4299e-04,
        1.2928e-04, 7.0738e-03, 2.2022e-03, 1.0151e-03, 1.7220e-03, 8.0568e-03,
        2.5873e-01, 5.2341e-01, 2.1376e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,340][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ same] are: tensor([9.6931e-02, 2.9561e-04, 7.7864e-04, 1.3990e-03, 1.6534e-03, 1.7731e-03,
        9.7751e-03, 7.5441e-03, 2.4061e-02, 7.6232e-02, 3.9707e-02, 1.2848e-01,
        3.7647e-01, 1.9842e-01, 3.6476e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,340][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.4191, 0.0091, 0.0131, 0.0286, 0.0194, 0.0044, 0.0174, 0.0255, 0.0121,
        0.0396, 0.0419, 0.0246, 0.0939, 0.1369, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,341][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0960, 0.0167, 0.0301, 0.0236, 0.0247, 0.0221, 0.0434, 0.0494, 0.0552,
        0.0919, 0.0721, 0.1208, 0.1375, 0.1124, 0.1040], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,342][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ same] are: tensor([1.1245e-01, 4.1155e-04, 1.0324e-03, 7.4140e-03, 4.7570e-04, 2.1773e-03,
        1.7161e-03, 1.3000e-02, 5.1541e-03, 1.4580e-02, 1.5837e-02, 2.7434e-02,
        6.1049e-01, 1.3214e-01, 5.5690e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,342][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1131, 0.0024, 0.0170, 0.0014, 0.0092, 0.0084, 0.0205, 0.0305, 0.0352,
        0.1261, 0.0812, 0.1104, 0.0996, 0.1329, 0.2119], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,343][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1406, 0.0306, 0.0355, 0.0242, 0.0211, 0.0261, 0.0397, 0.0359, 0.0434,
        0.0570, 0.0591, 0.0726, 0.1274, 0.1070, 0.1799], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,343][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.5922, 0.0118, 0.0151, 0.0113, 0.0119, 0.0117, 0.0201, 0.0182, 0.0265,
        0.0277, 0.0343, 0.0395, 0.0589, 0.0593, 0.0614], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,344][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.1271, 0.0075, 0.0022, 0.0181, 0.0059, 0.0062, 0.0112, 0.0209, 0.0275,
        0.1059, 0.0293, 0.0595, 0.0429, 0.0501, 0.2493, 0.2363],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,345][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.7375, 0.0075, 0.0119, 0.0378, 0.0119, 0.0048, 0.0101, 0.0092, 0.0043,
        0.0039, 0.0042, 0.0016, 0.0438, 0.0380, 0.0200, 0.0535],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,345][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ as] are: tensor([2.1692e-02, 1.0030e-06, 1.7560e-06, 1.5977e-05, 1.7569e-05, 2.3510e-05,
        5.6029e-05, 2.2773e-04, 3.4433e-04, 2.5999e-03, 4.4522e-03, 6.2443e-03,
        1.7024e-01, 4.5047e-01, 2.8329e-01, 6.0325e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,346][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ as] are: tensor([6.7336e-02, 4.1353e-04, 1.4914e-04, 1.1776e-03, 2.0029e-03, 6.3911e-04,
        8.9338e-04, 5.5166e-03, 2.6994e-03, 1.8493e-02, 3.2468e-02, 1.4402e-02,
        1.0321e-01, 3.5800e-01, 3.3462e-01, 5.7977e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,347][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ as] are: tensor([4.5693e-02, 7.2851e-06, 9.2360e-06, 2.1690e-04, 5.5514e-05, 4.0494e-05,
        2.2409e-05, 1.5679e-04, 2.8242e-04, 1.8760e-03, 6.3451e-04, 9.8897e-04,
        4.3217e-02, 8.5629e-02, 7.9403e-01, 2.7141e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,347][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ as] are: tensor([7.8497e-02, 1.6718e-04, 2.2995e-04, 7.6687e-04, 3.9656e-04, 6.5109e-04,
        2.6104e-03, 2.8276e-03, 1.0055e-02, 2.5768e-02, 7.7458e-03, 6.2828e-02,
        2.8787e-01, 1.9163e-01, 8.2810e-02, 2.4515e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,348][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.4745, 0.0051, 0.0065, 0.0155, 0.0120, 0.0035, 0.0136, 0.0105, 0.0080,
        0.0281, 0.0196, 0.0174, 0.1041, 0.1203, 0.0629, 0.0984],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,348][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0686, 0.0098, 0.0233, 0.0142, 0.0203, 0.0175, 0.0342, 0.0432, 0.0492,
        0.0913, 0.0711, 0.1108, 0.1156, 0.0918, 0.1018, 0.1374],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,349][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ as] are: tensor([7.2766e-02, 3.3288e-04, 6.8747e-04, 2.3414e-03, 3.2274e-04, 3.3129e-04,
        6.2132e-04, 1.5944e-03, 1.1348e-03, 6.9780e-03, 8.2619e-03, 6.4283e-03,
        4.2702e-01, 2.0697e-01, 1.2716e-01, 1.3705e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,350][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ as] are: tensor([7.0027e-02, 9.5133e-04, 3.2065e-03, 2.3075e-04, 2.5097e-03, 4.9756e-03,
        3.3778e-03, 1.0255e-02, 1.8207e-02, 1.9757e-02, 6.0526e-02, 7.6221e-02,
        5.0835e-02, 1.1324e-01, 5.0717e-01, 5.8517e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,350][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1210, 0.0228, 0.0308, 0.0163, 0.0187, 0.0229, 0.0331, 0.0312, 0.0383,
        0.0507, 0.0497, 0.0617, 0.1033, 0.0824, 0.1610, 0.1560],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,351][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.5930, 0.0081, 0.0099, 0.0063, 0.0084, 0.0096, 0.0144, 0.0150, 0.0189,
        0.0235, 0.0238, 0.0289, 0.0504, 0.0490, 0.0698, 0.0711],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,351][circuit_into_ebeddingspace.py][line:2169][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1503, 0.0062, 0.0045, 0.0211, 0.0120, 0.0091, 0.0125, 0.0237, 0.0276,
        0.0420, 0.0580, 0.0525, 0.0174, 0.0216, 0.1595, 0.3185, 0.0637],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,352][circuit_into_ebeddingspace.py][line:2172][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5693, 0.0138, 0.0273, 0.0458, 0.0307, 0.0092, 0.0193, 0.0208, 0.0163,
        0.0185, 0.0156, 0.0077, 0.0770, 0.0270, 0.0278, 0.0567, 0.0172],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,353][circuit_into_ebeddingspace.py][line:2175][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.6797e-02, 3.6753e-07, 4.7387e-07, 7.4560e-06, 1.5254e-05, 3.6402e-05,
        4.6984e-05, 2.9258e-04, 4.0677e-04, 1.6349e-03, 3.3780e-03, 5.2346e-03,
        4.6068e-02, 3.3611e-03, 1.2445e-01, 6.2789e-01, 1.7038e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,353][circuit_into_ebeddingspace.py][line:2178][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([5.2603e-02, 4.4119e-04, 2.9418e-04, 4.3297e-04, 2.6128e-03, 1.8297e-03,
        1.5206e-03, 9.2463e-03, 3.3804e-03, 2.6552e-02, 6.4334e-02, 1.4853e-02,
        2.1892e-02, 4.0184e-02, 2.5583e-01, 1.8573e-01, 3.1826e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,354][circuit_into_ebeddingspace.py][line:2181][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([2.3808e-01, 3.5156e-04, 3.0191e-04, 3.8636e-04, 3.8251e-05, 1.0381e-03,
        4.9387e-04, 3.5892e-03, 8.1829e-03, 3.4521e-03, 1.2146e-02, 2.9054e-02,
        6.5381e-02, 5.3738e-02, 5.8596e-02, 2.8918e-01, 2.3599e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,355][circuit_into_ebeddingspace.py][line:2184][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([6.9368e-02, 9.1042e-05, 2.3724e-04, 1.4917e-04, 2.1574e-04, 7.0165e-04,
        1.6266e-03, 1.8157e-03, 5.0495e-03, 1.4712e-02, 9.8811e-03, 2.5636e-02,
        1.0779e-01, 1.2701e-02, 7.3618e-02, 5.7889e-01, 9.7511e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,355][circuit_into_ebeddingspace.py][line:2187][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4601, 0.0060, 0.0053, 0.0172, 0.0091, 0.0028, 0.0116, 0.0074, 0.0079,
        0.0229, 0.0144, 0.0158, 0.0847, 0.0751, 0.0601, 0.0862, 0.1132],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,356][circuit_into_ebeddingspace.py][line:2190][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0655, 0.0101, 0.0253, 0.0127, 0.0188, 0.0165, 0.0325, 0.0394, 0.0480,
        0.0733, 0.0562, 0.1039, 0.1007, 0.0790, 0.0971, 0.1137, 0.1073],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,356][circuit_into_ebeddingspace.py][line:2193][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([6.2079e-02, 3.3709e-04, 2.8203e-04, 1.0375e-03, 2.6474e-04, 9.8708e-04,
        1.2186e-03, 4.8248e-03, 2.1131e-03, 9.7537e-03, 1.2205e-02, 9.2011e-03,
        2.1632e-01, 1.7435e-02, 8.0418e-02, 4.9253e-01, 8.8990e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,357][circuit_into_ebeddingspace.py][line:2196][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.1850e-01, 1.0388e-03, 2.8494e-03, 1.3557e-04, 3.9144e-03, 6.5457e-03,
        8.2481e-03, 1.6808e-02, 2.6344e-02, 2.6371e-02, 6.5828e-02, 7.9343e-02,
        3.1589e-02, 1.4472e-02, 3.1772e-01, 1.4607e-01, 1.3422e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,358][circuit_into_ebeddingspace.py][line:2199][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1082, 0.0221, 0.0295, 0.0130, 0.0176, 0.0231, 0.0322, 0.0310, 0.0358,
        0.0440, 0.0442, 0.0564, 0.0778, 0.0651, 0.1671, 0.1269, 0.1059],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,358][circuit_into_ebeddingspace.py][line:2202][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5705, 0.0077, 0.0097, 0.0051, 0.0075, 0.0094, 0.0137, 0.0142, 0.0175,
        0.0217, 0.0224, 0.0259, 0.0404, 0.0343, 0.0764, 0.0623, 0.0613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 16:09:50,363][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,364][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,365][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,365][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,366][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,366][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,367][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,368][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,368][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,369][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,369][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,370][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 16:09:50,370][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9765, 0.0235], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,371][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9922, 0.0078], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,371][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9484, 0.0516], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,372][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9397, 0.0603], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,372][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ original] are: tensor([9.9919e-01, 8.1445e-04], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,373][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9764, 0.0236], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,373][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.7857, 0.2143], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,374][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9372, 0.0628], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,374][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9611, 0.0389], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,375][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9142, 0.0858], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,375][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9737, 0.0263], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,376][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.8427, 0.1573], device='cuda:0') for source tokens [The original]
[2024-06-27 16:09:50,376][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ language] are: tensor([9.7510e-01, 2.2277e-04, 2.4679e-02], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,377][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9469, 0.0213, 0.0318], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,377][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.2359, 0.7214, 0.0427], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,378][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.5373, 0.3994, 0.0633], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,378][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ language] are: tensor([9.9186e-01, 7.4456e-03, 6.9534e-04], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,379][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7755, 0.0961, 0.1285], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,379][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1855, 0.7448, 0.0697], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,380][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4976, 0.2854, 0.2170], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,380][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.5495, 0.3943, 0.0562], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,381][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.4703, 0.4203, 0.1094], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,381][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.8089, 0.0309, 0.1602], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,382][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.1847, 0.7320, 0.0832], device='cuda:0') for source tokens [The original language]
[2024-06-27 16:09:50,382][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ of] are: tensor([9.9583e-01, 2.1209e-03, 1.6761e-03, 3.6811e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,383][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8420, 0.0297, 0.0294, 0.0989], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,383][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.2535, 0.5535, 0.1068, 0.0862], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,384][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0596, 0.0126, 0.8865, 0.0412], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,384][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9767, 0.0044, 0.0115, 0.0073], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,385][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.7768, 0.0827, 0.0860, 0.0545], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,385][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2808, 0.3365, 0.2461, 0.1366], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,386][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0358, 0.0056, 0.9403, 0.0183], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,386][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.4582, 0.0533, 0.2232, 0.2654], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,387][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.5481, 0.1471, 0.1170, 0.1878], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,387][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.7359, 0.0338, 0.0449, 0.1854], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,388][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1079, 0.5682, 0.2877, 0.0363], device='cuda:0') for source tokens [The original language of]
[2024-06-27 16:09:50,388][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ De] are: tensor([9.3213e-01, 7.5009e-05, 8.9211e-04, 7.1079e-04, 6.6194e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,389][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.5776, 0.0568, 0.0913, 0.2368, 0.0375], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,389][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.5031, 0.0212, 0.0066, 0.2343, 0.2348], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,390][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.7874, 0.0032, 0.0073, 0.1608, 0.0413], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,390][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9719, 0.0058, 0.0047, 0.0118, 0.0058], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,391][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.7210, 0.0483, 0.0943, 0.0892, 0.0472], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,391][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.3911, 0.0229, 0.0229, 0.0847, 0.4785], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,392][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.4766, 0.0447, 0.1328, 0.2290, 0.1169], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,392][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.1845, 0.0117, 0.0632, 0.7138, 0.0269], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,393][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.7761, 0.0122, 0.0525, 0.0996, 0.0595], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,393][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.8255, 0.0068, 0.0211, 0.0265, 0.1200], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,394][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.4890, 0.0776, 0.3056, 0.1075, 0.0204], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 16:09:50,394][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([9.6789e-01, 3.2072e-04, 4.3012e-05, 1.1824e-04, 2.0239e-06, 3.1623e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,395][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4936, 0.0191, 0.0624, 0.1790, 0.1254, 0.1205], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,395][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([7.0641e-02, 9.4122e-04, 4.6462e-04, 1.6787e-02, 7.8002e-01, 1.3114e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,396][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([5.3485e-01, 2.8627e-04, 8.1561e-04, 1.9939e-02, 4.1453e-01, 2.9576e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,396][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.6025e-01, 4.4934e-03, 7.2835e-03, 1.0309e-02, 1.7563e-02, 9.7152e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,397][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.8438, 0.0244, 0.0306, 0.0253, 0.0482, 0.0275], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,397][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0946, 0.0023, 0.0018, 0.0177, 0.8343, 0.0494], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,398][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0892, 0.0014, 0.0030, 0.0088, 0.7944, 0.1033], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,398][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.1139, 0.0027, 0.0238, 0.1565, 0.6406, 0.0625], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,399][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.6251, 0.0044, 0.0126, 0.0368, 0.2458, 0.0754], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,399][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.5891, 0.0052, 0.0066, 0.0132, 0.1201, 0.2658], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,400][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.2994, 0.0238, 0.0730, 0.0597, 0.4530, 0.0910], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 16:09:50,400][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([9.8828e-01, 7.0337e-05, 3.3906e-04, 7.5860e-05, 5.0380e-06, 1.0800e-04,
        1.1124e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,401][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.6820, 0.0452, 0.0614, 0.1226, 0.0344, 0.0172, 0.0372],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,401][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0710, 0.0034, 0.0009, 0.1157, 0.5097, 0.2549, 0.0444],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,402][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([6.9898e-01, 4.8487e-04, 2.5982e-03, 3.0039e-02, 4.6600e-02, 2.0173e-01,
        1.9574e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,403][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([9.6246e-01, 1.9433e-03, 9.6235e-03, 5.8315e-03, 3.4617e-03, 1.6029e-02,
        6.5434e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,403][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.7254, 0.0202, 0.0597, 0.0646, 0.0649, 0.0365, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,404][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0999, 0.0081, 0.0063, 0.0402, 0.6533, 0.1409, 0.0512],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,404][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2143, 0.0127, 0.1007, 0.0914, 0.0948, 0.4430, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,405][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.1407, 0.0042, 0.0463, 0.4117, 0.1164, 0.1111, 0.1695],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,405][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.5573, 0.0123, 0.0247, 0.1303, 0.1213, 0.0636, 0.0906],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,406][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.6738, 0.0042, 0.0222, 0.0242, 0.0275, 0.0249, 0.2231],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,406][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.1448, 0.0546, 0.1123, 0.1363, 0.0133, 0.2644, 0.2742],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 16:09:50,407][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([9.6211e-01, 3.3778e-04, 2.5890e-04, 3.9362e-04, 1.3664e-06, 9.6896e-05,
        3.2090e-04, 3.6484e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,407][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.5290, 0.0417, 0.0629, 0.0986, 0.0760, 0.0316, 0.1017, 0.0584],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,408][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0841, 0.0053, 0.0019, 0.0459, 0.1021, 0.0547, 0.5950, 0.1110],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,408][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([3.8586e-01, 2.5487e-04, 1.5087e-03, 1.2252e-02, 2.0781e-02, 1.1621e-02,
        5.3920e-01, 2.8518e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,409][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.8765, 0.0112, 0.0330, 0.0284, 0.0075, 0.0045, 0.0380, 0.0009],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,410][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.7189, 0.0434, 0.0736, 0.0608, 0.0318, 0.0329, 0.0299, 0.0086],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,410][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.1023, 0.0041, 0.0101, 0.0223, 0.5076, 0.0987, 0.1918, 0.0631],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,411][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0932, 0.0013, 0.0257, 0.0126, 0.0361, 0.2991, 0.4789, 0.0531],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,411][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2383, 0.0056, 0.0456, 0.1100, 0.0926, 0.0268, 0.3794, 0.1016],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,412][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.7186, 0.0062, 0.0095, 0.0575, 0.0866, 0.0317, 0.0419, 0.0480],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,412][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.5715, 0.0027, 0.0096, 0.0253, 0.0556, 0.0722, 0.0929, 0.1703],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,413][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.3862, 0.0410, 0.0586, 0.0610, 0.0364, 0.1593, 0.1636, 0.0939],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 16:09:50,413][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([9.3346e-01, 1.0765e-04, 1.9532e-04, 1.1806e-04, 9.5376e-07, 1.7573e-04,
        3.9550e-04, 1.0961e-03, 6.4451e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,414][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.5299, 0.0167, 0.0450, 0.0853, 0.0638, 0.0254, 0.0922, 0.0817, 0.0599],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,414][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([3.8302e-02, 1.5254e-03, 2.2838e-04, 7.8310e-03, 1.0788e-01, 4.6999e-02,
        4.9913e-02, 7.2414e-01, 2.3182e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,415][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.4877e-01, 4.5746e-05, 5.9806e-04, 1.3505e-03, 3.4625e-03, 3.7074e-02,
        5.3145e-02, 7.1173e-01, 4.3827e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,415][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([9.3518e-01, 1.9952e-03, 5.4636e-03, 7.4518e-03, 2.8488e-03, 1.4446e-02,
        4.4965e-03, 2.7489e-02, 6.2924e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,416][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.7375, 0.0227, 0.0465, 0.0482, 0.0363, 0.0333, 0.0282, 0.0352, 0.0122],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,417][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0439, 0.0011, 0.0025, 0.0101, 0.4940, 0.1293, 0.0588, 0.2167, 0.0435],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,417][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0839, 0.0006, 0.0089, 0.0072, 0.0150, 0.0913, 0.3096, 0.2454, 0.2381],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,418][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1689, 0.0052, 0.0320, 0.1990, 0.0624, 0.0168, 0.2191, 0.1657, 0.1308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,418][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.4820, 0.0061, 0.0148, 0.0485, 0.0777, 0.0392, 0.0890, 0.1477, 0.0951],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,419][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.4506, 0.0022, 0.0107, 0.0123, 0.0322, 0.0341, 0.1997, 0.1128, 0.1453],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,419][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2471, 0.0289, 0.0318, 0.0801, 0.0063, 0.0774, 0.3289, 0.1267, 0.0728],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 16:09:50,420][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ et] are: tensor([9.8055e-01, 8.2223e-04, 9.7309e-05, 3.0338e-04, 4.0814e-05, 8.3123e-06,
        3.4644e-04, 1.1927e-06, 3.2201e-04, 1.7513e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,421][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.4562, 0.0492, 0.0558, 0.1193, 0.0241, 0.0143, 0.0852, 0.0330, 0.0664,
        0.0965], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,421][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0236, 0.0009, 0.0008, 0.0099, 0.0202, 0.0251, 0.0381, 0.6814, 0.0792,
        0.1208], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,422][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ et] are: tensor([7.8231e-01, 4.7585e-04, 1.3791e-03, 8.8141e-03, 3.9068e-04, 2.2461e-03,
        1.7809e-02, 3.9978e-02, 1.1749e-01, 2.9103e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,422][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.8591, 0.0039, 0.0197, 0.0076, 0.0059, 0.0051, 0.0492, 0.0108, 0.0374,
        0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,423][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.4356, 0.1034, 0.0815, 0.1244, 0.0416, 0.0803, 0.0593, 0.0259, 0.0326,
        0.0155], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,423][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0633, 0.0025, 0.0023, 0.0069, 0.2478, 0.0911, 0.0608, 0.3310, 0.1189,
        0.0753], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,424][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ et] are: tensor([2.2901e-02, 2.7273e-05, 1.4808e-03, 6.0977e-04, 7.4774e-04, 1.5895e-03,
        4.1444e-02, 2.0163e-01, 6.9901e-01, 3.0554e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,424][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.1459, 0.0023, 0.0090, 0.1717, 0.0158, 0.0052, 0.1344, 0.0468, 0.3236,
        0.1453], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,425][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.8059, 0.0014, 0.0029, 0.0410, 0.0118, 0.0133, 0.0379, 0.0380, 0.0292,
        0.0187], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,426][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.6258, 0.0036, 0.0092, 0.0283, 0.0224, 0.0274, 0.0400, 0.1133, 0.0633,
        0.0669], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,426][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2500, 0.0601, 0.0509, 0.0857, 0.0078, 0.0723, 0.1055, 0.1999, 0.0675,
        0.1002], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 16:09:50,427][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([8.8733e-01, 4.7694e-04, 2.9231e-03, 1.2465e-03, 1.7619e-05, 2.4089e-05,
        3.1172e-04, 1.1609e-05, 1.9102e-03, 2.1742e-05, 1.0573e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,427][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.4310, 0.0134, 0.0294, 0.0671, 0.0299, 0.0135, 0.0654, 0.0534, 0.0456,
        0.2151, 0.0362], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,428][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([3.2839e-02, 1.3853e-04, 8.7997e-05, 7.3818e-04, 5.4268e-03, 1.7436e-03,
        1.3901e-02, 3.8950e-02, 2.7542e-02, 8.2690e-01, 5.1733e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,428][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([4.8007e-01, 3.6494e-05, 5.0654e-04, 5.5460e-04, 6.9163e-04, 8.8314e-04,
        5.3961e-02, 3.3340e-02, 6.7112e-02, 3.3076e-01, 3.2080e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,429][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.8531, 0.0064, 0.0095, 0.0141, 0.0171, 0.0156, 0.0351, 0.0198, 0.0155,
        0.0108, 0.0031], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,430][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.5863, 0.0357, 0.0633, 0.0561, 0.0412, 0.0590, 0.0463, 0.0346, 0.0280,
        0.0365, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,430][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.3253, 0.0074, 0.0132, 0.0075, 0.0876, 0.0342, 0.0817, 0.0777, 0.0692,
        0.1496, 0.1467], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,431][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([3.6076e-02, 3.5940e-05, 4.9163e-04, 3.0725e-04, 8.0553e-04, 6.1335e-03,
        8.6748e-02, 7.4376e-02, 2.9293e-01, 4.6186e-01, 4.0235e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,431][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([1.2756e-01, 3.0079e-04, 1.4911e-03, 1.2073e-02, 4.0281e-03, 6.8116e-03,
        3.4057e-02, 9.2477e-02, 9.6337e-02, 5.7360e-01, 5.1269e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,432][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.6823, 0.0016, 0.0016, 0.0236, 0.0212, 0.0107, 0.0119, 0.0114, 0.0238,
        0.1846, 0.0273], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,432][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.5620, 0.0029, 0.0058, 0.0065, 0.0228, 0.0286, 0.0211, 0.0618, 0.0310,
        0.0925, 0.1650], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,433][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.5419, 0.0160, 0.0180, 0.0253, 0.0113, 0.0194, 0.0407, 0.0703, 0.0475,
        0.1751, 0.0345], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 16:09:50,434][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([5.2731e-01, 3.7976e-04, 1.2539e-03, 1.2793e-03, 1.7819e-05, 1.2835e-03,
        3.3309e-03, 4.0377e-03, 3.5325e-01, 2.2539e-04, 7.4390e-04, 1.0689e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,434][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4027, 0.0131, 0.0351, 0.0558, 0.0560, 0.0132, 0.0656, 0.0512, 0.0330,
        0.1636, 0.0524, 0.0584], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,435][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([6.9240e-03, 8.5876e-05, 3.4826e-05, 7.3521e-04, 7.9796e-03, 4.0180e-03,
        3.7425e-03, 7.9901e-02, 4.4076e-03, 2.5510e-01, 5.8887e-01, 4.8194e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,435][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.2043e-01, 1.0958e-05, 1.1695e-04, 1.4652e-04, 3.4354e-04, 5.2947e-03,
        8.0144e-03, 8.7002e-02, 1.2521e-02, 2.3815e-01, 3.3385e-01, 1.9412e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,436][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.8050, 0.0058, 0.0158, 0.0129, 0.0055, 0.0328, 0.0090, 0.0656, 0.0018,
        0.0287, 0.0143, 0.0027], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,436][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.6858, 0.0166, 0.0364, 0.0508, 0.0405, 0.0313, 0.0290, 0.0346, 0.0142,
        0.0295, 0.0237, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,437][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([1.9001e-02, 2.0721e-04, 4.5270e-04, 1.2699e-03, 8.1062e-02, 3.6012e-02,
        1.0372e-02, 4.8962e-02, 8.4998e-03, 4.6191e-02, 7.1262e-01, 3.5354e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,438][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([3.8715e-02, 7.3165e-05, 1.2288e-03, 5.3627e-04, 1.0896e-03, 1.0241e-02,
        4.0749e-02, 5.8443e-02, 4.2358e-02, 1.1648e-01, 3.3982e-01, 3.5026e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,438][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([6.1326e-02, 4.8832e-04, 2.1781e-03, 1.8935e-02, 5.1295e-03, 1.0518e-03,
        3.2404e-02, 2.0656e-02, 2.2221e-02, 5.2345e-01, 2.2731e-01, 8.4854e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,439][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.4765, 0.0025, 0.0040, 0.0245, 0.0378, 0.0171, 0.0253, 0.0365, 0.0403,
        0.1288, 0.1295, 0.0770], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,439][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.3341, 0.0014, 0.0060, 0.0081, 0.0122, 0.0168, 0.0805, 0.0606, 0.1085,
        0.0680, 0.0832, 0.2205], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,440][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2339, 0.0150, 0.0135, 0.0391, 0.0051, 0.0178, 0.0736, 0.0597, 0.0203,
        0.2825, 0.1764, 0.0632], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 16:09:50,440][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ is] are: tensor([8.9044e-01, 3.2334e-03, 3.3907e-04, 1.0407e-03, 3.0346e-05, 6.8620e-05,
        3.7747e-04, 4.2190e-05, 1.4641e-03, 7.4353e-04, 1.2474e-04, 7.3312e-04,
        1.0136e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,441][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.5763, 0.0069, 0.0231, 0.0108, 0.0067, 0.0018, 0.0115, 0.0431, 0.0107,
        0.1729, 0.0198, 0.0186, 0.0977], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,442][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0732, 0.0025, 0.0007, 0.0133, 0.0179, 0.0028, 0.0071, 0.0766, 0.0251,
        0.1746, 0.4395, 0.1191, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,442][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ is] are: tensor([8.2196e-01, 2.4695e-04, 8.2973e-04, 5.2134e-04, 7.9660e-05, 2.4075e-04,
        4.8805e-04, 1.0514e-02, 8.9655e-03, 1.9436e-02, 2.0113e-02, 6.1903e-02,
        5.4704e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,443][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.8061, 0.0026, 0.0067, 0.0066, 0.0061, 0.0041, 0.0096, 0.0222, 0.0186,
        0.0345, 0.0066, 0.0164, 0.0600], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,443][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.2765, 0.1945, 0.0754, 0.2177, 0.0352, 0.0284, 0.0350, 0.0296, 0.0230,
        0.0208, 0.0293, 0.0104, 0.0241], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,444][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0945, 0.0267, 0.0053, 0.0041, 0.1283, 0.0286, 0.0165, 0.0818, 0.0221,
        0.1379, 0.2654, 0.0766, 0.1122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,445][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ is] are: tensor([2.6535e-02, 3.5457e-05, 8.1629e-04, 2.2327e-04, 3.3475e-04, 6.7306e-04,
        7.4700e-03, 3.0070e-02, 8.7536e-02, 8.3039e-02, 1.5771e-01, 5.6474e-01,
        4.0819e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,445][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.2760, 0.0015, 0.0039, 0.0345, 0.0008, 0.0021, 0.0081, 0.0247, 0.0166,
        0.3415, 0.0808, 0.0452, 0.1642], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,446][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.8172, 0.0159, 0.0208, 0.0267, 0.0011, 0.0020, 0.0028, 0.0023, 0.0043,
        0.0141, 0.0104, 0.0054, 0.0770], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,446][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.6559, 0.0084, 0.0099, 0.0125, 0.0090, 0.0051, 0.0138, 0.0149, 0.0143,
        0.0384, 0.0426, 0.0307, 0.1444], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,447][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2282, 0.4051, 0.1497, 0.0395, 0.0006, 0.0029, 0.0117, 0.0073, 0.0078,
        0.0371, 0.0429, 0.0124, 0.0548], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 16:09:50,448][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.3435e-01, 2.3921e-03, 2.4884e-04, 5.6363e-04, 1.7548e-05, 5.2280e-05,
        3.0374e-04, 1.3344e-04, 1.1048e-03, 2.3675e-04, 2.3863e-05, 8.3308e-04,
        8.0203e-03, 5.1718e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,448][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4571, 0.0078, 0.0153, 0.0066, 0.0023, 0.0018, 0.0065, 0.0298, 0.0085,
        0.0828, 0.0266, 0.0122, 0.1144, 0.2283], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,449][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.3283e-01, 4.2495e-04, 4.9543e-04, 3.9637e-03, 1.8793e-03, 7.0458e-04,
        3.8099e-03, 1.5692e-02, 8.4596e-03, 2.4989e-02, 5.8180e-02, 3.8417e-02,
        4.3174e-01, 2.7841e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,449][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2410e-01, 2.9572e-04, 2.7289e-04, 2.3094e-05, 2.0153e-05, 1.7239e-05,
        3.4605e-04, 8.2848e-04, 1.4070e-03, 4.6997e-03, 2.4762e-03, 1.2266e-02,
        2.6305e-01, 9.0201e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,450][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.8485, 0.0015, 0.0163, 0.0048, 0.0028, 0.0021, 0.0054, 0.0097, 0.0063,
        0.0227, 0.0044, 0.0089, 0.0416, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,451][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2446, 0.1554, 0.1063, 0.2700, 0.0179, 0.0129, 0.0252, 0.0200, 0.0197,
        0.0256, 0.0209, 0.0114, 0.0421, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,451][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2154, 0.0221, 0.0271, 0.0070, 0.0550, 0.0047, 0.0144, 0.0390, 0.0222,
        0.0695, 0.0712, 0.0548, 0.2648, 0.1328], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,452][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2766, 0.0013, 0.0103, 0.0005, 0.0005, 0.0005, 0.0044, 0.0087, 0.0281,
        0.0302, 0.0542, 0.1301, 0.3280, 0.1268], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,452][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([2.1708e-01, 5.5757e-04, 3.2638e-03, 5.8079e-03, 3.3764e-04, 7.3118e-04,
        2.1930e-03, 9.2783e-03, 5.2619e-03, 7.7444e-02, 5.6606e-02, 1.9991e-02,
        5.2492e-01, 7.6528e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,453][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4319, 0.0406, 0.0321, 0.0144, 0.0009, 0.0015, 0.0036, 0.0057, 0.0023,
        0.0233, 0.0070, 0.0031, 0.3294, 0.1042], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,454][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.6592, 0.0052, 0.0086, 0.0037, 0.0063, 0.0032, 0.0069, 0.0123, 0.0063,
        0.0160, 0.0236, 0.0125, 0.0736, 0.1627], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,454][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([4.3234e-01, 1.5193e-01, 7.3190e-02, 3.2572e-02, 2.9498e-04, 3.0240e-03,
        4.6612e-03, 1.1915e-02, 3.7014e-03, 2.4846e-02, 2.6062e-02, 8.2365e-03,
        1.4552e-01, 8.1701e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 16:09:50,455][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ same] are: tensor([7.8949e-01, 7.6104e-04, 3.4155e-03, 7.2892e-04, 1.5375e-05, 2.9740e-05,
        5.8695e-05, 6.9176e-05, 3.9685e-04, 1.0009e-04, 7.7442e-05, 1.8932e-04,
        6.5756e-04, 1.8319e-03, 2.0218e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,455][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ same] are: tensor([5.3625e-01, 4.2116e-03, 2.9569e-03, 3.2077e-03, 1.0982e-03, 3.8543e-04,
        7.8399e-04, 7.9986e-03, 2.3388e-03, 1.9840e-02, 5.5269e-03, 4.9321e-03,
        1.2977e-01, 2.2969e-01, 5.1013e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,456][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ same] are: tensor([4.7943e-02, 2.9400e-04, 5.4311e-04, 6.9331e-03, 4.2278e-04, 2.7663e-04,
        4.1235e-04, 2.9064e-03, 1.1694e-03, 1.7153e-02, 1.4831e-02, 5.4848e-03,
        2.7040e-01, 5.3632e-01, 9.4910e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,457][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ same] are: tensor([5.4062e-01, 6.9656e-04, 1.3591e-04, 1.3071e-04, 1.3276e-04, 2.2307e-05,
        6.5236e-05, 6.2852e-04, 8.0121e-04, 3.0269e-03, 3.1007e-03, 5.0401e-03,
        2.1188e-01, 1.9940e-01, 3.4314e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,457][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.8875, 0.0042, 0.0177, 0.0043, 0.0026, 0.0009, 0.0067, 0.0098, 0.0054,
        0.0116, 0.0055, 0.0065, 0.0147, 0.0214, 0.0012], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,458][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.3266, 0.0755, 0.0546, 0.1972, 0.0293, 0.0095, 0.0363, 0.0194, 0.0288,
        0.0547, 0.0405, 0.0173, 0.0557, 0.0409, 0.0136], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,458][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1466, 0.0158, 0.0079, 0.0020, 0.0173, 0.0011, 0.0050, 0.0066, 0.0059,
        0.0243, 0.0186, 0.0080, 0.1525, 0.4019, 0.1862], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,459][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ same] are: tensor([2.9768e-01, 8.8475e-04, 5.4154e-03, 3.9427e-04, 1.1279e-03, 4.4642e-04,
        8.6512e-04, 2.4386e-03, 9.4366e-03, 2.2660e-02, 3.1803e-02, 4.9550e-02,
        4.1062e-01, 1.0130e-01, 6.5377e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,460][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ same] are: tensor([1.5572e-01, 3.4072e-04, 2.7576e-04, 4.6099e-03, 1.2043e-03, 6.7084e-04,
        2.4383e-03, 1.0993e-02, 8.1832e-03, 1.9444e-01, 6.3382e-02, 2.2646e-02,
        3.6991e-01, 1.0774e-01, 5.7452e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,460][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.2641, 0.0049, 0.0051, 0.0042, 0.0005, 0.0011, 0.0020, 0.0093, 0.0027,
        0.0204, 0.0061, 0.0054, 0.4597, 0.0953, 0.1191], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,461][circuit_into_ebeddingspace.py][line:2199][INFO] ##3-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.7090, 0.0056, 0.0088, 0.0026, 0.0028, 0.0008, 0.0052, 0.0059, 0.0055,
        0.0200, 0.0081, 0.0136, 0.0267, 0.0232, 0.1620], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,462][circuit_into_ebeddingspace.py][line:2202][INFO] ##3-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.4858, 0.0249, 0.0073, 0.0080, 0.0005, 0.0005, 0.0044, 0.0045, 0.0025,
        0.0040, 0.0075, 0.0061, 0.1285, 0.1065, 0.2087], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 16:09:50,462][circuit_into_ebeddingspace.py][line:2169][INFO] ##3-th layer ##Weight##: The head1 weight for token [ as] are: tensor([9.4613e-01, 1.0658e-03, 1.2531e-03, 3.3329e-04, 6.4079e-06, 3.7966e-05,
        1.1545e-04, 2.7893e-04, 7.2723e-05, 7.2437e-05, 7.6123e-05, 1.0441e-04,
        7.6355e-04, 4.6628e-03, 9.6943e-03, 3.5333e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,463][circuit_into_ebeddingspace.py][line:2172][INFO] ##3-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.4603, 0.0015, 0.0019, 0.0036, 0.0019, 0.0011, 0.0021, 0.0058, 0.0040,
        0.0189, 0.0083, 0.0065, 0.1097, 0.1989, 0.0791, 0.0965],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,463][circuit_into_ebeddingspace.py][line:2175][INFO] ##3-th layer ##Weight##: The head3 weight for token [ as] are: tensor([7.7078e-02, 4.2222e-05, 1.0498e-04, 3.8521e-04, 2.8412e-04, 4.2987e-04,
        1.3502e-04, 2.5543e-03, 4.7679e-04, 9.4670e-03, 1.8149e-02, 1.7407e-03,
        2.2925e-01, 5.1066e-01, 1.2440e-01, 2.4849e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,464][circuit_into_ebeddingspace.py][line:2178][INFO] ##3-th layer ##Weight##: The head4 weight for token [ as] are: tensor([1.8488e-01, 1.1052e-05, 2.7998e-05, 6.0490e-05, 4.8178e-05, 3.0264e-05,
        1.5707e-04, 1.2266e-04, 5.1336e-04, 4.8472e-04, 9.0465e-04, 1.8699e-03,
        4.3223e-02, 1.4485e-01, 4.7632e-01, 1.4650e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,465][circuit_into_ebeddingspace.py][line:2181][INFO] ##3-th layer ##Weight##: The head5 weight for token [ as] are: tensor([9.2334e-01, 2.3164e-03, 2.5732e-03, 3.3310e-03, 7.6685e-04, 1.0173e-03,
        2.9011e-03, 1.7745e-03, 2.0899e-03, 1.0582e-02, 9.3873e-04, 3.3101e-03,
        1.9705e-02, 1.4104e-02, 6.0650e-03, 5.1796e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,465][circuit_into_ebeddingspace.py][line:2184][INFO] ##3-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.4372, 0.0964, 0.0633, 0.1285, 0.0122, 0.0103, 0.0169, 0.0110, 0.0063,
        0.0288, 0.0115, 0.0062, 0.0373, 0.0607, 0.0293, 0.0441],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,466][circuit_into_ebeddingspace.py][line:2187][INFO] ##3-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.2457, 0.0083, 0.0029, 0.0022, 0.0229, 0.0022, 0.0064, 0.0074, 0.0109,
        0.0194, 0.0295, 0.0246, 0.2054, 0.2533, 0.0678, 0.0912],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,466][circuit_into_ebeddingspace.py][line:2190][INFO] ##3-th layer ##Weight##: The head8 weight for token [ as] are: tensor([9.2378e-02, 3.6425e-04, 1.6835e-03, 1.7772e-04, 3.6219e-04, 5.3170e-04,
        9.2484e-04, 2.6673e-03, 1.9045e-03, 4.4269e-03, 1.0069e-02, 6.8229e-03,
        7.0047e-02, 1.3961e-01, 5.9130e-01, 7.6727e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,467][circuit_into_ebeddingspace.py][line:2193][INFO] ##3-th layer ##Weight##: The head9 weight for token [ as] are: tensor([1.4727e-01, 1.8633e-04, 7.8120e-05, 8.4127e-04, 3.7767e-04, 2.8735e-04,
        1.2277e-03, 5.0883e-03, 2.6398e-03, 9.4990e-02, 5.7184e-03, 8.4249e-03,
        3.1487e-01, 1.1945e-01, 8.4013e-02, 2.1454e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 16:09:50,468][circuit_into_ebeddingspace.py][line:2196][INFO] ##3-th layer ##Weight##: The head10 weight for token [ as] are: tensor([1.5589e-01, 7.2915e-04, 1.2511e-03, 1.7187e-03, 1.7070e-04, 8.0204e-05,
        2.9666e-04, 1.4476e-03, 3.8253e-04, 8.6168e-04, 1.4419e-03, 6.8544e-04,
        1.7642e-01, 8.8135e-02, 5.4695e-01, 2.3545e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
