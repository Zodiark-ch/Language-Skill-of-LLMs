[2024-06-27 21:00:52,253][circuit_into_ebeddingspace.py][line:2016][INFO] max probability tokens are: originalwith ids tensor([2656], device='cuda:0')
[2024-06-27 21:00:52,262][circuit_into_ebeddingspace.py][line:2286][INFO] ################0-th layer#################
[2024-06-27 21:00:52,265][circuit_into_ebeddingspace.py][line:2288][INFO] ##0-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,266][circuit_into_ebeddingspace.py][line:2289][INFO] ##0-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,268][circuit_into_ebeddingspace.py][line:2290][INFO] ##0-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,269][circuit_into_ebeddingspace.py][line:2291][INFO] ##0-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [1, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,271][circuit_into_ebeddingspace.py][line:2292][INFO] ##0-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,272][circuit_into_ebeddingspace.py][line:2293][INFO] ##0-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,273][circuit_into_ebeddingspace.py][line:2294][INFO] ##0-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,275][circuit_into_ebeddingspace.py][line:2295][INFO] ##0-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[1, 1],
         [1, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,276][circuit_into_ebeddingspace.py][line:2296][INFO] ##0-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,278][circuit_into_ebeddingspace.py][line:2297][INFO] ##0-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[1, 1],
         [1, 1],
         [1, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,279][circuit_into_ebeddingspace.py][line:2298][INFO] ##0-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,280][circuit_into_ebeddingspace.py][line:2299][INFO] ##0-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:52,281][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,282][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,283][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,284][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,285][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,286][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,287][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,288][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,289][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,290][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,291][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,292][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,293][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.8760, 0.1240], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,294][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.0013, 0.9987], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,295][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.8840, 0.1160], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,296][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.0567, 0.9433], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,296][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.5982, 0.4018], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,297][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,298][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.8151, 0.1849], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,299][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9414, 0.0586], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,300][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9035, 0.0965], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,301][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8480, 0.1520], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,302][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.7101, 0.2899], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,303][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.7987, 0.2013], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,304][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5919, 0.3569, 0.0512], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,305][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0016, 0.0057, 0.9927], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,306][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.6998, 0.1853, 0.1149], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,307][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0228, 0.2130, 0.7642], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,308][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.3379, 0.2728, 0.3893], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,309][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0091, 0.0010, 0.9899], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,310][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.4141, 0.3172, 0.2686], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,311][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.5601, 0.2736, 0.1663], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,312][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.7863, 0.1338, 0.0799], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,313][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.6896, 0.2534, 0.0571], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,314][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.5777, 0.1519, 0.2704], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,315][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.6712, 0.1580, 0.1708], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,316][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5658, 0.2750, 0.0975, 0.0617], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,317][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ of] are: tensor([2.7660e-03, 7.1156e-04, 1.6157e-04, 9.9636e-01], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,318][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.5853, 0.1283, 0.1155, 0.1708], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,319][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0460, 0.3025, 0.1767, 0.4748], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,320][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2837, 0.4413, 0.1808, 0.0943], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,321][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.2564, 0.0711, 0.0179, 0.6546], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,322][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2911, 0.3001, 0.3820, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,323][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3290, 0.1464, 0.3082, 0.2164], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,324][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3944, 0.0866, 0.0580, 0.4609], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,325][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.5378, 0.1552, 0.0942, 0.2128], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,326][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5119, 0.1236, 0.0501, 0.3144], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,327][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.4950, 0.1927, 0.2060, 0.1064], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,328][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.4580, 0.1367, 0.1780, 0.1396, 0.0877], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,329][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ De] are: tensor([4.7577e-04, 1.8430e-03, 1.9662e-03, 7.1858e-04, 9.9500e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,330][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.4633, 0.1429, 0.1551, 0.1451, 0.0936], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,331][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ De] are: tensor([7.3997e-03, 1.5697e-03, 4.9343e-04, 1.9224e-03, 9.8861e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,332][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0848, 0.0304, 0.0194, 0.0221, 0.8434], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,333][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ De] are: tensor([2.5107e-02, 1.8511e-03, 3.3911e-05, 1.8053e-05, 9.7299e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,334][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.4389, 0.1573, 0.2067, 0.0502, 0.1469], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,335][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2565, 0.0871, 0.0704, 0.3043, 0.2817], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,336][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.4489, 0.0990, 0.0623, 0.2324, 0.1574], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,337][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.5173, 0.1428, 0.0882, 0.1776, 0.0742], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,338][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.4806, 0.1108, 0.0556, 0.1029, 0.2502], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,339][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.3248, 0.2346, 0.3069, 0.0896, 0.0441], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,340][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.3910, 0.0952, 0.1772, 0.1601, 0.1248, 0.0517], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,341][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([8.9033e-05, 4.7931e-03, 1.5460e-04, 1.4006e-03, 4.8277e-04, 9.9308e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,342][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.3253, 0.1499, 0.2778, 0.0976, 0.1197, 0.0297], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,343][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([1.3443e-03, 5.2498e-04, 2.4231e-05, 4.8777e-05, 9.8004e-04, 9.9708e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,344][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.6387e-03, 2.0815e-03, 8.9040e-04, 2.3722e-03, 1.8421e-02, 9.6660e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,345][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([4.1795e-03, 6.9115e-04, 3.2217e-05, 3.0980e-06, 3.2352e-04, 9.9477e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,346][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.3693, 0.1463, 0.2547, 0.0666, 0.0952, 0.0678], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,347][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.1704, 0.1249, 0.0505, 0.2912, 0.2461, 0.1170], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,348][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2367, 0.0718, 0.3045, 0.1910, 0.0930, 0.1031], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,349][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.3890, 0.1462, 0.1505, 0.1771, 0.1211, 0.0160], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,350][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.3530, 0.1560, 0.0485, 0.1120, 0.0644, 0.2661], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,351][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.3704, 0.1091, 0.1418, 0.1254, 0.0552, 0.1981], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,352][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.2608, 0.1815, 0.1922, 0.0788, 0.0981, 0.1201, 0.0684],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,353][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([1.0192e-06, 8.7755e-06, 3.8803e-04, 2.7981e-06, 3.3821e-05, 1.3176e-05,
        9.9955e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,354][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.3722, 0.1217, 0.1065, 0.0873, 0.2095, 0.0315, 0.0713],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,355][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([6.4776e-05, 1.0527e-04, 1.1847e-04, 9.7527e-06, 2.5299e-03, 2.3382e-03,
        9.9483e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,356][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([2.1835e-03, 9.6791e-04, 1.3731e-03, 5.5387e-04, 2.0302e-03, 4.1934e-01,
        5.7355e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,357][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([3.9244e-05, 4.0222e-06, 1.0783e-05, 3.0826e-09, 6.8012e-07, 4.1481e-06,
        9.9994e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,359][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.3578, 0.0460, 0.1112, 0.0472, 0.1037, 0.2075, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,360][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.1385, 0.0366, 0.0270, 0.0995, 0.1631, 0.5167, 0.0187],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,361][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3253, 0.0636, 0.0886, 0.1041, 0.2854, 0.0992, 0.0339],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,362][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.3516, 0.1608, 0.1300, 0.1073, 0.0936, 0.1321, 0.0246],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,363][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3575, 0.0932, 0.0864, 0.1007, 0.0738, 0.0333, 0.2552],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,364][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2338, 0.1569, 0.1996, 0.1117, 0.0276, 0.1409, 0.1294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,365][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.3167, 0.0847, 0.0889, 0.1721, 0.0527, 0.0848, 0.1027, 0.0974],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,366][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([1.8418e-05, 1.5845e-04, 3.4632e-05, 6.7318e-05, 1.0771e-04, 1.1345e-03,
        4.4688e-05, 9.9843e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,367][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2892, 0.1284, 0.1390, 0.1031, 0.1174, 0.0236, 0.1353, 0.0639],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,368][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([1.5037e-05, 2.4666e-06, 2.9078e-07, 2.2765e-07, 1.2378e-05, 3.5374e-04,
        3.4359e-06, 9.9961e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,369][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0191, 0.0041, 0.0011, 0.0074, 0.0135, 0.0872, 0.0397, 0.8278],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,370][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([1.0356e-03, 6.9173e-05, 2.1573e-06, 1.5790e-07, 1.1900e-05, 8.2333e-05,
        4.3949e-05, 9.9875e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,371][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.2490, 0.0573, 0.2070, 0.0563, 0.0739, 0.0955, 0.2027, 0.0582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,372][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1548, 0.0512, 0.0824, 0.1428, 0.1735, 0.1805, 0.1188, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,373][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.3282, 0.1681, 0.0635, 0.1119, 0.0505, 0.0399, 0.0711, 0.1668],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,374][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.3182, 0.1073, 0.0941, 0.1434, 0.1170, 0.1257, 0.0775, 0.0168],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,375][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.3652, 0.0983, 0.0357, 0.1019, 0.0610, 0.0540, 0.0257, 0.2582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,377][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.2492, 0.1058, 0.1344, 0.1007, 0.0442, 0.1888, 0.0806, 0.0963],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,378][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2609, 0.0855, 0.0964, 0.0838, 0.0518, 0.2071, 0.0757, 0.0482, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,379][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([2.1647e-05, 2.2618e-04, 7.5674e-04, 5.2729e-05, 4.7814e-04, 8.4970e-04,
        2.2010e-02, 4.4437e-03, 9.7116e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,380][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2813, 0.1034, 0.1891, 0.0709, 0.0713, 0.0681, 0.0694, 0.0395, 0.1068],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,381][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([3.1611e-04, 1.1498e-05, 6.2957e-06, 6.2300e-06, 1.8404e-04, 1.1954e-03,
        2.0652e-03, 3.0892e-03, 9.9313e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,382][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([3.4240e-03, 6.8455e-04, 1.7888e-03, 1.8692e-03, 5.7352e-03, 1.2699e-01,
        2.1376e-02, 1.0442e-01, 7.3372e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,383][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([2.7098e-03, 6.6089e-05, 4.2695e-05, 1.3363e-06, 1.5544e-05, 4.6581e-05,
        1.4676e-03, 3.3913e-05, 9.9562e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,384][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1964, 0.0446, 0.0970, 0.0334, 0.1440, 0.1716, 0.0924, 0.1504, 0.0703],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,385][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2108, 0.0727, 0.0266, 0.1216, 0.2963, 0.0979, 0.0108, 0.0621, 0.1013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,386][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.3112, 0.0957, 0.0654, 0.1124, 0.1428, 0.1106, 0.0355, 0.1010, 0.0254],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,387][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2871, 0.1045, 0.1239, 0.0954, 0.1186, 0.0917, 0.0766, 0.0803, 0.0218],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,388][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2810, 0.0887, 0.0638, 0.1092, 0.0574, 0.0646, 0.0869, 0.0316, 0.2168],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,389][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1898, 0.1720, 0.1684, 0.0619, 0.0262, 0.0930, 0.0876, 0.1385, 0.0625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,391][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.2441, 0.0991, 0.1640, 0.0518, 0.0870, 0.0434, 0.0966, 0.0902, 0.0689,
        0.0549], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,392][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ et] are: tensor([4.7677e-04, 9.6306e-04, 7.8710e-04, 1.2303e-03, 5.0300e-04, 2.9748e-04,
        5.6700e-04, 1.0168e-04, 1.6024e-03, 9.9347e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,393][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.2322, 0.0570, 0.1523, 0.1388, 0.1114, 0.0279, 0.1442, 0.0560, 0.0526,
        0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,394][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ et] are: tensor([2.3542e-04, 2.7035e-06, 2.9933e-06, 1.7182e-06, 1.9735e-05, 3.0432e-04,
        1.0826e-04, 1.1600e-03, 1.2660e-03, 9.9690e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,395][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0103, 0.0012, 0.0031, 0.0103, 0.0160, 0.0452, 0.0515, 0.0215, 0.0980,
        0.7429], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,396][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ et] are: tensor([6.6580e-04, 9.9814e-05, 1.2676e-05, 1.6858e-06, 2.2680e-05, 5.2646e-06,
        3.2962e-05, 2.5723e-06, 4.2750e-06, 9.9915e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,397][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.1125, 0.0502, 0.1162, 0.0187, 0.0562, 0.0757, 0.1488, 0.0737, 0.1571,
        0.1908], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,398][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1659, 0.0244, 0.0072, 0.0892, 0.0673, 0.0498, 0.0915, 0.0927, 0.2534,
        0.1588], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,399][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.2893, 0.1517, 0.0633, 0.2019, 0.0665, 0.0686, 0.0207, 0.0704, 0.0518,
        0.0158], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,400][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.2545, 0.1006, 0.1058, 0.1149, 0.0908, 0.0798, 0.0611, 0.0640, 0.0951,
        0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,402][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.2180, 0.1248, 0.0577, 0.1130, 0.0591, 0.0506, 0.0317, 0.0346, 0.0288,
        0.2817], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,403][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2041, 0.0920, 0.1192, 0.0695, 0.0307, 0.1229, 0.0598, 0.0869, 0.0556,
        0.1592], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,404][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.1812, 0.0553, 0.0874, 0.0538, 0.0374, 0.0381, 0.0958, 0.0798, 0.1372,
        0.1057, 0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,405][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([3.3097e-05, 2.2359e-04, 1.7588e-04, 4.3431e-05, 1.2347e-03, 1.5803e-04,
        4.7333e-04, 3.8590e-04, 1.7457e-04, 1.2392e-05, 9.9709e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,406][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.2732, 0.0515, 0.0984, 0.0803, 0.0509, 0.0455, 0.0462, 0.0600, 0.1690,
        0.0592, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,407][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([4.4071e-04, 1.7670e-05, 2.9868e-06, 1.0578e-06, 3.7284e-04, 6.4737e-04,
        1.1974e-04, 1.1023e-02, 2.9585e-03, 1.4759e-02, 9.6966e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,408][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0062, 0.0013, 0.0012, 0.0016, 0.0095, 0.0076, 0.0021, 0.0359, 0.0057,
        0.0430, 0.8860], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,409][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([1.7796e-04, 4.5544e-06, 1.0613e-06, 5.2177e-08, 1.2367e-05, 3.1378e-06,
        9.0654e-07, 5.1383e-05, 3.7395e-06, 3.3069e-07, 9.9974e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,410][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.1427, 0.0773, 0.1225, 0.0259, 0.0431, 0.0874, 0.1447, 0.0785, 0.1333,
        0.0724, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,411][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0949, 0.0229, 0.0258, 0.0527, 0.0250, 0.0513, 0.0445, 0.0423, 0.0832,
        0.3576, 0.1998], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,413][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.1804, 0.0613, 0.0599, 0.1166, 0.0567, 0.0692, 0.0987, 0.0650, 0.0953,
        0.0497, 0.1473], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,414][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2115, 0.0904, 0.1025, 0.1028, 0.0736, 0.1517, 0.0630, 0.0622, 0.0730,
        0.0602, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,415][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2339, 0.0713, 0.0406, 0.0957, 0.0639, 0.0431, 0.0316, 0.0534, 0.0286,
        0.0444, 0.2935], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,416][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.1835, 0.0823, 0.1299, 0.0683, 0.0266, 0.0865, 0.0563, 0.0819, 0.0699,
        0.0974, 0.1175], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,417][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2061, 0.0661, 0.0753, 0.0620, 0.0414, 0.1676, 0.0582, 0.0389, 0.0737,
        0.0778, 0.0566, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,418][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([8.6567e-06, 7.8134e-05, 2.7261e-04, 1.7985e-05, 1.8909e-04, 3.3536e-04,
        1.1171e-02, 1.7158e-03, 5.1216e-01, 4.4262e-05, 3.1099e-04, 4.7369e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,420][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2305, 0.0863, 0.1576, 0.0584, 0.0608, 0.0616, 0.0612, 0.0364, 0.0925,
        0.0278, 0.0359, 0.0911], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,421][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.0443e-04, 1.9482e-06, 9.9700e-07, 8.5760e-07, 2.4087e-05, 1.6049e-04,
        3.2396e-04, 4.3607e-04, 1.6716e-01, 6.0149e-03, 3.2648e-02, 7.9313e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,422][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([1.5673e-03, 2.4146e-04, 6.2082e-04, 6.6058e-04, 1.6916e-03, 4.0039e-02,
        7.2441e-03, 3.3730e-02, 2.5429e-01, 7.8436e-03, 1.3365e-01, 5.1842e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,423][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([1.2523e-03, 3.1623e-05, 2.0093e-05, 6.3776e-07, 7.9575e-06, 2.1615e-05,
        7.0141e-04, 1.6131e-05, 5.3550e-01, 6.5836e-06, 4.8697e-05, 4.6239e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,424][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1349, 0.0309, 0.0667, 0.0218, 0.1030, 0.1296, 0.0652, 0.1083, 0.0522,
        0.1469, 0.0874, 0.0530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,426][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1671, 0.0387, 0.0137, 0.0610, 0.1336, 0.0521, 0.0053, 0.0290, 0.0523,
        0.0668, 0.2698, 0.1105], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,427][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2079, 0.0639, 0.0469, 0.0733, 0.0967, 0.0797, 0.0248, 0.0714, 0.0177,
        0.1571, 0.1423, 0.0182], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,428][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2285, 0.0839, 0.1033, 0.0758, 0.0990, 0.0762, 0.0662, 0.0664, 0.0181,
        0.0816, 0.0824, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,429][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1796, 0.0584, 0.0463, 0.0744, 0.0429, 0.0502, 0.0706, 0.0247, 0.1896,
        0.0402, 0.0444, 0.1786], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,430][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1531, 0.1409, 0.1372, 0.0496, 0.0227, 0.0749, 0.0725, 0.1105, 0.0511,
        0.0794, 0.0538, 0.0543], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,431][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1593, 0.0595, 0.0350, 0.0117, 0.0350, 0.1127, 0.0594, 0.1245, 0.1007,
        0.0924, 0.0672, 0.1152, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,433][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ is] are: tensor([2.6377e-03, 5.3790e-04, 3.5144e-04, 4.8993e-03, 3.4439e-04, 1.7886e-04,
        1.4119e-04, 2.2006e-04, 3.7787e-04, 5.6773e-04, 4.3503e-04, 3.4606e-04,
        9.8896e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,434][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.2076, 0.0671, 0.0584, 0.1016, 0.0632, 0.0229, 0.1146, 0.0289, 0.0556,
        0.0608, 0.0346, 0.0583, 0.1264], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,435][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ is] are: tensor([8.9733e-03, 2.5728e-04, 6.3198e-05, 4.2710e-04, 1.2356e-04, 1.8010e-04,
        5.7753e-04, 1.3562e-03, 1.5530e-03, 1.7582e-02, 1.2350e-02, 8.0742e-03,
        9.4848e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,436][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0735, 0.0252, 0.0096, 0.0248, 0.0113, 0.0143, 0.0121, 0.0525, 0.0170,
        0.0705, 0.1032, 0.0380, 0.5479], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,437][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ is] are: tensor([2.4296e-02, 2.0065e-03, 2.4683e-03, 3.2399e-03, 9.1281e-04, 2.6412e-04,
        1.3690e-04, 2.7982e-04, 1.0724e-03, 3.4749e-04, 8.2915e-04, 8.5929e-04,
        9.6329e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,438][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0442, 0.0710, 0.0912, 0.0062, 0.0433, 0.0565, 0.1127, 0.1228, 0.0924,
        0.1439, 0.0943, 0.1050, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,439][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0382, 0.0055, 0.0063, 0.0105, 0.0089, 0.0068, 0.0116, 0.0223, 0.0553,
        0.1197, 0.0742, 0.1679, 0.4729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,441][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1005, 0.0181, 0.0222, 0.1145, 0.0270, 0.0361, 0.0120, 0.0361, 0.0215,
        0.0662, 0.0443, 0.0249, 0.4767], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,442][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.1968, 0.0875, 0.0598, 0.1077, 0.0675, 0.0532, 0.0483, 0.0489, 0.0532,
        0.0585, 0.0475, 0.0567, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,443][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1773, 0.0605, 0.0461, 0.1201, 0.0489, 0.0366, 0.0381, 0.0462, 0.0291,
        0.0419, 0.0515, 0.0280, 0.2756], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,444][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2686, 0.0734, 0.0622, 0.0596, 0.0286, 0.0763, 0.0518, 0.0510, 0.0488,
        0.0860, 0.0506, 0.0495, 0.0937], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,445][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1187, 0.0530, 0.0231, 0.0090, 0.0339, 0.0879, 0.0726, 0.1919, 0.1105,
        0.0628, 0.0714, 0.1262, 0.0268, 0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,446][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([5.5456e-02, 6.2910e-04, 5.2470e-05, 7.5203e-02, 1.5758e-03, 6.1685e-04,
        7.5570e-05, 2.6828e-04, 3.3991e-04, 5.4090e-04, 9.9822e-04, 3.3152e-04,
        9.3071e-04, 8.6298e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,448][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1933, 0.0688, 0.0495, 0.1293, 0.0759, 0.0232, 0.0593, 0.0433, 0.0392,
        0.0608, 0.0500, 0.0415, 0.1294, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,449][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.0327e-02, 5.5598e-04, 1.8978e-04, 8.8268e-04, 7.7357e-04, 1.4620e-03,
        1.3360e-03, 6.9476e-03, 4.0072e-03, 2.6717e-02, 3.9356e-02, 1.8470e-02,
        2.7426e-01, 6.1471e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,450][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0684, 0.0241, 0.0114, 0.0154, 0.0112, 0.0226, 0.0109, 0.0430, 0.0177,
        0.0876, 0.0816, 0.0392, 0.3444, 0.2226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,451][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2424, 0.1698, 0.0204, 0.1024, 0.0214, 0.0151, 0.0028, 0.0094, 0.0044,
        0.0075, 0.0067, 0.0036, 0.1080, 0.2859], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,452][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0360, 0.0823, 0.0722, 0.0017, 0.0304, 0.0477, 0.1088, 0.1206, 0.1416,
        0.0848, 0.1033, 0.1612, 0.0081, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,453][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0279, 0.0032, 0.0043, 0.0076, 0.0076, 0.0078, 0.0108, 0.0210, 0.0345,
        0.0615, 0.0589, 0.0995, 0.3213, 0.3340], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,455][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0320, 0.0061, 0.0078, 0.0554, 0.0104, 0.0231, 0.0051, 0.0258, 0.0086,
        0.0386, 0.0118, 0.0101, 0.3445, 0.4207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,456][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2133, 0.0698, 0.0517, 0.0984, 0.0611, 0.0412, 0.0386, 0.0457, 0.0436,
        0.0513, 0.0421, 0.0463, 0.0945, 0.1025], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,457][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3014, 0.0698, 0.0431, 0.0809, 0.0521, 0.0322, 0.0262, 0.0402, 0.0215,
        0.0372, 0.0384, 0.0207, 0.0695, 0.1669], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,458][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2058, 0.0688, 0.0761, 0.0377, 0.0235, 0.0725, 0.0467, 0.0535, 0.0570,
        0.0769, 0.0486, 0.0585, 0.0763, 0.0981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,459][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1031, 0.0711, 0.0270, 0.0167, 0.0285, 0.0588, 0.1160, 0.1381, 0.1326,
        0.0269, 0.0458, 0.1520, 0.0359, 0.0217, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,460][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ same] are: tensor([1.8082e-03, 1.3732e-02, 1.1511e-03, 1.0779e-03, 4.1301e-04, 1.3396e-03,
        2.4368e-04, 3.0300e-03, 7.6835e-05, 1.1669e-04, 5.8620e-04, 6.8874e-05,
        1.7218e-03, 1.2435e-03, 9.7339e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,462][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.1757, 0.0950, 0.0731, 0.0913, 0.0312, 0.0264, 0.0586, 0.0303, 0.0783,
        0.0449, 0.0384, 0.0810, 0.0720, 0.0579, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,463][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ same] are: tensor([8.8967e-04, 1.8280e-05, 6.2767e-06, 6.8885e-06, 2.7967e-05, 9.4842e-04,
        4.9636e-05, 1.8252e-03, 7.9017e-05, 1.3148e-03, 2.8364e-03, 3.4912e-04,
        7.8719e-04, 1.2641e-03, 9.8960e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,464][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0593, 0.0125, 0.0042, 0.0194, 0.0032, 0.0063, 0.0081, 0.0715, 0.0114,
        0.0400, 0.0668, 0.0231, 0.1284, 0.1237, 0.4222], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,465][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ same] are: tensor([1.7116e-02, 2.9655e-02, 3.2967e-04, 4.9665e-05, 3.0534e-04, 5.1293e-05,
        6.2239e-06, 1.0991e-04, 9.5665e-07, 4.3046e-05, 6.8255e-05, 6.4576e-07,
        1.0273e-05, 2.2959e-05, 9.5223e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,467][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0630, 0.1151, 0.1044, 0.0106, 0.0362, 0.0663, 0.0832, 0.0694, 0.0851,
        0.1247, 0.0766, 0.0907, 0.0208, 0.0064, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,468][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0458, 0.0035, 0.0025, 0.0074, 0.0058, 0.0018, 0.0071, 0.0236, 0.0257,
        0.0517, 0.0812, 0.0720, 0.2244, 0.3026, 0.1449], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,469][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.1046, 0.0408, 0.0543, 0.0555, 0.0232, 0.0311, 0.0253, 0.0348, 0.0407,
        0.0942, 0.0394, 0.0451, 0.1734, 0.2067, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,470][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1856, 0.0683, 0.0592, 0.0750, 0.0641, 0.0505, 0.0361, 0.0417, 0.0456,
        0.0462, 0.0577, 0.0486, 0.0854, 0.0831, 0.0527], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,471][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1891, 0.0882, 0.0505, 0.0725, 0.0386, 0.0369, 0.0231, 0.0368, 0.0206,
        0.0392, 0.0364, 0.0198, 0.0608, 0.0761, 0.2115], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,473][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.3156, 0.0679, 0.0640, 0.0382, 0.0263, 0.0611, 0.0326, 0.0359, 0.0412,
        0.0756, 0.0437, 0.0411, 0.0380, 0.0428, 0.0758], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,474][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0864, 0.0634, 0.0462, 0.0080, 0.0477, 0.0565, 0.0851, 0.1121, 0.0804,
        0.0994, 0.0845, 0.0916, 0.0482, 0.0261, 0.0486, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,475][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ as] are: tensor([2.3854e-03, 2.0895e-04, 2.5717e-04, 2.8681e-03, 3.5010e-04, 3.8690e-04,
        2.0201e-04, 2.2578e-04, 2.0202e-04, 2.0501e-04, 1.7903e-03, 1.9313e-04,
        1.9476e-04, 8.5942e-04, 7.8604e-04, 9.8889e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,476][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.1792, 0.0660, 0.0502, 0.0982, 0.0569, 0.0234, 0.0318, 0.0392, 0.0474,
        0.0406, 0.0338, 0.0509, 0.0833, 0.0367, 0.0603, 0.1021],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,477][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ as] are: tensor([5.1346e-04, 6.1543e-06, 4.4379e-06, 1.8338e-05, 4.5401e-06, 1.3863e-05,
        1.5373e-05, 1.1318e-04, 1.4519e-05, 1.5111e-04, 6.9566e-04, 7.1744e-05,
        5.8046e-03, 9.5863e-03, 8.0341e-01, 1.7958e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,479][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0219, 0.0037, 0.0035, 0.0052, 0.0030, 0.0036, 0.0018, 0.0170, 0.0050,
        0.0209, 0.0260, 0.0112, 0.2114, 0.0589, 0.1340, 0.4729],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,480][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ as] are: tensor([2.7994e-02, 6.6583e-03, 4.6749e-03, 4.2371e-03, 1.6441e-03, 4.1065e-04,
        3.0593e-04, 9.3266e-04, 1.3406e-04, 6.6179e-04, 1.5577e-03, 1.0409e-04,
        7.2350e-04, 1.4188e-03, 5.2119e-03, 9.4333e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,481][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0419, 0.0819, 0.0902, 0.0058, 0.0463, 0.0621, 0.0695, 0.1048, 0.0721,
        0.1508, 0.0772, 0.0808, 0.0232, 0.0098, 0.0676, 0.0160],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,482][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0233, 0.0017, 0.0023, 0.0033, 0.0024, 0.0027, 0.0071, 0.0129, 0.0260,
        0.0446, 0.0345, 0.0731, 0.1240, 0.1666, 0.1638, 0.3118],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,484][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0383, 0.0088, 0.0139, 0.0697, 0.0112, 0.0241, 0.0057, 0.0234, 0.0076,
        0.0357, 0.0135, 0.0087, 0.3306, 0.2743, 0.0189, 0.1156],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,485][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.1533, 0.0587, 0.0459, 0.0776, 0.0502, 0.0372, 0.0343, 0.0406, 0.0421,
        0.0474, 0.0489, 0.0451, 0.0847, 0.0851, 0.0574, 0.0913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,486][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1203, 0.0452, 0.0474, 0.0590, 0.0356, 0.0298, 0.0337, 0.0324, 0.0203,
        0.0293, 0.0414, 0.0208, 0.0882, 0.0823, 0.0667, 0.2474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,487][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.2140, 0.0688, 0.0537, 0.0550, 0.0268, 0.0528, 0.0414, 0.0405, 0.0389,
        0.0713, 0.0362, 0.0388, 0.0522, 0.0343, 0.0751, 0.1002],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,488][circuit_into_ebeddingspace.py][line:2310][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1166, 0.0495, 0.0222, 0.0082, 0.0323, 0.0856, 0.0671, 0.1789, 0.1055,
        0.0583, 0.0675, 0.1206, 0.0247, 0.0114, 0.0267, 0.0111, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,490][circuit_into_ebeddingspace.py][line:2313][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.7131e-02, 2.5542e-04, 2.0961e-05, 3.0705e-02, 6.9835e-04, 2.8151e-04,
        3.8641e-05, 1.1279e-04, 1.7476e-04, 2.5317e-04, 4.2775e-04, 1.7526e-04,
        4.4160e-04, 4.4301e-01, 3.9625e-04, 8.4876e-03, 4.8739e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,491][circuit_into_ebeddingspace.py][line:2316][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1437, 0.0520, 0.0372, 0.0960, 0.0583, 0.0181, 0.0453, 0.0334, 0.0294,
        0.0500, 0.0411, 0.0312, 0.1009, 0.0272, 0.0481, 0.1595, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,492][circuit_into_ebeddingspace.py][line:2319][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2250e-03, 1.9250e-04, 6.1471e-05, 2.4989e-04, 1.8313e-04, 3.1236e-04,
        3.3465e-04, 1.4383e-03, 8.9219e-04, 4.2014e-03, 6.1448e-03, 3.8249e-03,
        3.6687e-02, 8.4951e-02, 9.9260e-02, 1.6062e-01, 5.9442e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,493][circuit_into_ebeddingspace.py][line:2322][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0520, 0.0145, 0.0068, 0.0089, 0.0056, 0.0102, 0.0049, 0.0184, 0.0075,
        0.0337, 0.0293, 0.0156, 0.1236, 0.0799, 0.1484, 0.2184, 0.2223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,495][circuit_into_ebeddingspace.py][line:2325][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1642, 0.1197, 0.0144, 0.0747, 0.0162, 0.0110, 0.0019, 0.0069, 0.0031,
        0.0051, 0.0049, 0.0026, 0.0883, 0.2206, 0.0479, 0.0179, 0.2006],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,496][circuit_into_ebeddingspace.py][line:2328][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0320, 0.0744, 0.0640, 0.0014, 0.0275, 0.0443, 0.0976, 0.1133, 0.1324,
        0.0751, 0.0966, 0.1514, 0.0070, 0.0011, 0.0749, 0.0056, 0.0013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,497][circuit_into_ebeddingspace.py][line:2331][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0192, 0.0016, 0.0020, 0.0035, 0.0031, 0.0028, 0.0037, 0.0073, 0.0123,
        0.0173, 0.0162, 0.0329, 0.0784, 0.0839, 0.0832, 0.3199, 0.3127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,498][circuit_into_ebeddingspace.py][line:2334][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0202, 0.0038, 0.0051, 0.0343, 0.0067, 0.0150, 0.0033, 0.0169, 0.0057,
        0.0245, 0.0078, 0.0067, 0.2173, 0.2539, 0.0080, 0.0637, 0.3070],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,499][circuit_into_ebeddingspace.py][line:2337][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1614, 0.0528, 0.0403, 0.0739, 0.0477, 0.0318, 0.0309, 0.0350, 0.0342,
        0.0401, 0.0329, 0.0363, 0.0730, 0.0767, 0.0530, 0.0963, 0.0834],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,501][circuit_into_ebeddingspace.py][line:2340][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2087, 0.0473, 0.0313, 0.0565, 0.0391, 0.0250, 0.0208, 0.0314, 0.0179,
        0.0298, 0.0312, 0.0178, 0.0568, 0.1346, 0.0506, 0.0647, 0.1366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,502][circuit_into_ebeddingspace.py][line:2343][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1671, 0.0535, 0.0588, 0.0308, 0.0207, 0.0561, 0.0373, 0.0406, 0.0456,
        0.0567, 0.0356, 0.0459, 0.0560, 0.0735, 0.0735, 0.0735, 0.0749],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,510][circuit_into_ebeddingspace.py][line:2286][INFO] ################1-th layer#################
[2024-06-27 21:00:52,513][circuit_into_ebeddingspace.py][line:2288][INFO] ##1-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0]]], device='cuda:0')
[2024-06-27 21:00:52,514][circuit_into_ebeddingspace.py][line:2289][INFO] ##1-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,516][circuit_into_ebeddingspace.py][line:2290][INFO] ##1-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,517][circuit_into_ebeddingspace.py][line:2291][INFO] ##1-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,519][circuit_into_ebeddingspace.py][line:2292][INFO] ##1-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,520][circuit_into_ebeddingspace.py][line:2293][INFO] ##1-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,521][circuit_into_ebeddingspace.py][line:2294][INFO] ##1-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,523][circuit_into_ebeddingspace.py][line:2295][INFO] ##1-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,524][circuit_into_ebeddingspace.py][line:2296][INFO] ##1-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,525][circuit_into_ebeddingspace.py][line:2297][INFO] ##1-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,527][circuit_into_ebeddingspace.py][line:2298][INFO] ##1-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,528][circuit_into_ebeddingspace.py][line:2299][INFO] ##1-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [1, 1],
         [0, 0],
         [1, 1],
         [0, 1],
         [1, 0],
         [1, 1],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,529][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,530][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,531][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,532][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,533][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,534][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,535][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,536][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,537][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,537][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,538][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,539][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,540][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9802, 0.0198], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,541][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9767, 0.0233], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,542][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9867, 0.0133], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,543][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.6432, 0.3568], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,544][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.8878, 0.1122], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,545][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9232, 0.0768], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,546][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.8967, 0.1033], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,547][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9508, 0.0492], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,548][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9543, 0.0457], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,549][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9789, 0.0211], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,550][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ original] are: tensor([3.3137e-04, 9.9967e-01], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,551][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.3325, 0.6675], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,552][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5368, 0.3558, 0.1074], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,553][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8715, 0.0719, 0.0566], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,554][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.8856, 0.0287, 0.0858], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,555][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.5954, 0.1801, 0.2245], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,555][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.7879, 0.1004, 0.1117], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,556][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7162, 0.1383, 0.1455], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,557][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.8329, 0.0222, 0.1449], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,558][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8557, 0.0819, 0.0624], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,559][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.8934, 0.0564, 0.0502], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,560][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9589, 0.0251, 0.0160], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,561][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0006, 0.4557, 0.5436], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,562][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.2600, 0.0050, 0.7350], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,563][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5359, 0.1355, 0.2584, 0.0701], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,564][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.7028, 0.0987, 0.0947, 0.1039], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,565][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.6147, 0.0258, 0.0708, 0.2888], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,566][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.5594, 0.1268, 0.1650, 0.1487], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,567][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.7032, 0.1009, 0.1170, 0.0789], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,568][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.4506, 0.0116, 0.0073, 0.5305], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,569][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8037, 0.0739, 0.0588, 0.0636], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,570][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.6930, 0.0701, 0.1214, 0.1155], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,571][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.7986, 0.0557, 0.0506, 0.0951], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,572][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.8893, 0.0299, 0.0172, 0.0636], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,573][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0011, 0.3076, 0.3981, 0.2932], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,574][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.4321, 0.0046, 0.0024, 0.5608], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,575][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.5652, 0.0250, 0.0511, 0.1734, 0.1852], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,576][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.4993, 0.0849, 0.0997, 0.2038, 0.1122], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,577][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.4797, 0.0251, 0.0649, 0.2566, 0.1737], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,578][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.5205, 0.1052, 0.1359, 0.1171, 0.1213], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,579][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.6689, 0.0761, 0.0875, 0.0690, 0.0985], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,580][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.6630, 0.0276, 0.0145, 0.2310, 0.0639], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,581][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.7277, 0.0271, 0.1447, 0.0594, 0.0412], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,582][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.5431, 0.0928, 0.1660, 0.1144, 0.0838], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,583][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.6668, 0.0680, 0.0626, 0.1429, 0.0597], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,584][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.7918, 0.0464, 0.0428, 0.0898, 0.0293], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,585][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0008, 0.2261, 0.2875, 0.2167, 0.2688], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,586][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ De] are: tensor([4.2773e-02, 3.1417e-03, 3.1715e-04, 3.0576e-02, 9.2319e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,587][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.2215, 0.1121, 0.0908, 0.1433, 0.3080, 0.1244], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,588][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4741, 0.0653, 0.0618, 0.1879, 0.1789, 0.0320], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,589][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.3976, 0.0255, 0.0597, 0.2336, 0.1446, 0.1389], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,590][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.5188, 0.0754, 0.1060, 0.0980, 0.1015, 0.1003], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,591][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.6146, 0.0731, 0.0694, 0.0609, 0.0863, 0.0957], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,592][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.6854, 0.0709, 0.0199, 0.1218, 0.0864, 0.0157], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,593][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.5981, 0.0868, 0.0316, 0.0967, 0.1445, 0.0424], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,594][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.4483, 0.0941, 0.2399, 0.0977, 0.0955, 0.0246], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,595][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.6339, 0.0460, 0.0669, 0.1446, 0.0676, 0.0409], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,596][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.7222, 0.0455, 0.0526, 0.1213, 0.0372, 0.0213], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,597][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0010, 0.1640, 0.2270, 0.1740, 0.2207, 0.2133], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,598][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([2.2467e-02, 1.9179e-03, 1.2005e-04, 9.8701e-03, 1.0802e-03, 9.6454e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,599][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0417, 0.0200, 0.0132, 0.0379, 0.2125, 0.6565, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,600][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.4780, 0.0489, 0.0570, 0.1452, 0.1129, 0.0465, 0.1116],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,602][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.3124, 0.0234, 0.0556, 0.1894, 0.1288, 0.1275, 0.1630],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,603][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.5112, 0.0624, 0.0855, 0.0786, 0.0776, 0.0851, 0.0996],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,604][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.5435, 0.0649, 0.0708, 0.0563, 0.0828, 0.0958, 0.0859],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,605][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.6077, 0.0492, 0.0357, 0.0713, 0.0960, 0.1148, 0.0252],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,606][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.4413, 0.0989, 0.1374, 0.0424, 0.0516, 0.0315, 0.1969],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,607][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.4171, 0.0869, 0.1729, 0.0687, 0.0459, 0.1160, 0.0925],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,608][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.5628, 0.0601, 0.0600, 0.1465, 0.0669, 0.0567, 0.0472],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,609][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.7353, 0.0625, 0.0355, 0.0638, 0.0494, 0.0272, 0.0263],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,610][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0009, 0.1243, 0.1771, 0.1270, 0.1631, 0.1699, 0.2376],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,611][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.0874, 0.0033, 0.0104, 0.0453, 0.0033, 0.0009, 0.8493],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,612][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.1092, 0.0408, 0.0553, 0.0893, 0.1898, 0.1616, 0.2291, 0.1249],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,613][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.2856, 0.0552, 0.0614, 0.1114, 0.1180, 0.0726, 0.1803, 0.1154],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,614][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2418, 0.0211, 0.0454, 0.1660, 0.1111, 0.1082, 0.1264, 0.1801],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,615][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.4971, 0.0492, 0.0678, 0.0676, 0.0686, 0.0717, 0.0913, 0.0867],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,616][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.4949, 0.0555, 0.0583, 0.0543, 0.0674, 0.0923, 0.0768, 0.1005],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,617][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.5423, 0.0475, 0.0314, 0.0816, 0.1010, 0.0931, 0.0886, 0.0144],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,618][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.5985, 0.1296, 0.0121, 0.0787, 0.0288, 0.0123, 0.1218, 0.0182],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,619][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.3809, 0.0907, 0.1382, 0.0766, 0.0601, 0.0365, 0.1741, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,621][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.5375, 0.0452, 0.0550, 0.1250, 0.0615, 0.0478, 0.0734, 0.0545],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,622][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.6558, 0.0565, 0.0518, 0.0885, 0.0497, 0.0310, 0.0343, 0.0323],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,623][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0012, 0.0972, 0.1395, 0.1111, 0.1385, 0.1409, 0.1982, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,624][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([1.9925e-02, 1.3295e-03, 2.1208e-04, 1.6963e-02, 5.0615e-04, 8.5291e-03,
        4.8525e-04, 9.5205e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,625][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0090, 0.0030, 0.0034, 0.0069, 0.1361, 0.3465, 0.0282, 0.4314, 0.0355],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,626][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2901, 0.0362, 0.0501, 0.0846, 0.0884, 0.0424, 0.1298, 0.1635, 0.1148],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,627][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2045, 0.0182, 0.0445, 0.1405, 0.0987, 0.0949, 0.1253, 0.1544, 0.1190],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,628][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5031, 0.0425, 0.0586, 0.0570, 0.0560, 0.0555, 0.0737, 0.0713, 0.0823],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,629][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4735, 0.0492, 0.0547, 0.0473, 0.0600, 0.0690, 0.0695, 0.0797, 0.0970],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,630][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.3417, 0.0926, 0.0594, 0.1287, 0.1224, 0.0393, 0.1161, 0.0431, 0.0567],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,631][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.3420, 0.0121, 0.0525, 0.0313, 0.0696, 0.0064, 0.0694, 0.2084, 0.2084],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,632][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.3451, 0.0981, 0.1124, 0.0512, 0.0503, 0.0472, 0.1204, 0.1253, 0.0499],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,633][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4988, 0.0517, 0.0520, 0.1320, 0.0598, 0.0566, 0.0465, 0.0605, 0.0421],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,634][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6659, 0.0813, 0.0408, 0.0788, 0.0395, 0.0264, 0.0231, 0.0358, 0.0085],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,636][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0013, 0.0813, 0.1192, 0.0890, 0.1189, 0.1133, 0.1594, 0.1503, 0.1672],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,637][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([4.0507e-02, 2.8700e-03, 3.1883e-03, 2.4852e-02, 8.3621e-04, 2.0515e-03,
        1.0615e-02, 1.8154e-03, 9.1326e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,638][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0834, 0.0298, 0.0260, 0.0588, 0.0881, 0.0874, 0.3213, 0.0904, 0.1502,
        0.0646], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,639][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.2927, 0.0585, 0.0621, 0.0839, 0.0596, 0.0375, 0.0921, 0.0965, 0.1089,
        0.1082], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,640][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.1919, 0.0178, 0.0392, 0.1214, 0.0830, 0.0826, 0.1025, 0.1422, 0.0966,
        0.1228], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,641][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.4921, 0.0383, 0.0510, 0.0482, 0.0488, 0.0509, 0.0650, 0.0634, 0.0703,
        0.0719], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,642][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.3966, 0.0507, 0.0543, 0.0475, 0.0589, 0.0716, 0.0645, 0.0865, 0.0895,
        0.0798], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,643][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.3802, 0.0331, 0.0170, 0.0823, 0.0681, 0.0296, 0.0467, 0.0582, 0.1608,
        0.1240], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,644][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.5557, 0.0448, 0.0290, 0.0605, 0.0463, 0.0073, 0.1039, 0.0516, 0.0558,
        0.0451], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,645][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.3786, 0.0842, 0.0982, 0.0698, 0.0347, 0.0298, 0.0999, 0.0960, 0.0572,
        0.0517], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,646][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.4817, 0.0572, 0.0606, 0.0862, 0.0485, 0.0443, 0.0489, 0.0615, 0.0452,
        0.0660], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,648][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.6021, 0.0551, 0.0569, 0.0700, 0.0413, 0.0404, 0.0352, 0.0364, 0.0232,
        0.0394], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,649][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0013, 0.0682, 0.0973, 0.0797, 0.0973, 0.0988, 0.1333, 0.1244, 0.1473,
        0.1524], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,650][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ et] are: tensor([4.4566e-02, 3.5423e-03, 9.9786e-04, 2.5153e-02, 4.6912e-03, 4.7535e-04,
        1.0418e-03, 1.4763e-04, 1.2071e-03, 9.1818e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,651][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0494, 0.0108, 0.0243, 0.0198, 0.0181, 0.0701, 0.0875, 0.2175, 0.1099,
        0.2069, 0.1857], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,652][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.2021, 0.0289, 0.0286, 0.0615, 0.0388, 0.0285, 0.0723, 0.0933, 0.1083,
        0.2018, 0.1359], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,653][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.1488, 0.0187, 0.0367, 0.1092, 0.0775, 0.0741, 0.0888, 0.1235, 0.0885,
        0.1054, 0.1289], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,654][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.5168, 0.0264, 0.0375, 0.0389, 0.0394, 0.0410, 0.0546, 0.0513, 0.0591,
        0.0650, 0.0699], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,655][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.3564, 0.0416, 0.0455, 0.0444, 0.0568, 0.0650, 0.0566, 0.0794, 0.0883,
        0.0700, 0.0959], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,656][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.4104, 0.0531, 0.0168, 0.0704, 0.0517, 0.0757, 0.1370, 0.0692, 0.0581,
        0.0338, 0.0238], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,658][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.4591, 0.0469, 0.1551, 0.0480, 0.0177, 0.0059, 0.0527, 0.0170, 0.1215,
        0.0341, 0.0418], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,659][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.2848, 0.0627, 0.0930, 0.0566, 0.0474, 0.0340, 0.1334, 0.0997, 0.0755,
        0.0956, 0.0172], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,660][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.5119, 0.0423, 0.0299, 0.0783, 0.0387, 0.0316, 0.0484, 0.0510, 0.0404,
        0.0886, 0.0388], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,661][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.5934, 0.0432, 0.0365, 0.0967, 0.0450, 0.0281, 0.0361, 0.0320, 0.0226,
        0.0378, 0.0284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,662][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0024, 0.0542, 0.0795, 0.0635, 0.0797, 0.0820, 0.1152, 0.1064, 0.1311,
        0.1272, 0.1590], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,663][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([2.8940e-02, 1.4100e-03, 8.3856e-04, 7.1741e-03, 1.7185e-03, 4.1883e-04,
        3.5000e-04, 9.3285e-03, 1.0529e-03, 8.2416e-04, 9.4794e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,664][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0091, 0.0031, 0.0031, 0.0069, 0.0674, 0.2054, 0.0199, 0.2220, 0.0203,
        0.1333, 0.2727, 0.0368], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,665][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2000, 0.0191, 0.0271, 0.0463, 0.0382, 0.0187, 0.0587, 0.0717, 0.0546,
        0.1598, 0.1832, 0.1226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,667][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.1255, 0.0151, 0.0338, 0.0877, 0.0663, 0.0662, 0.0840, 0.1037, 0.0823,
        0.1044, 0.1218, 0.1094], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,668][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5194, 0.0243, 0.0348, 0.0349, 0.0337, 0.0336, 0.0436, 0.0440, 0.0502,
        0.0578, 0.0638, 0.0599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,669][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.3767, 0.0383, 0.0415, 0.0378, 0.0462, 0.0516, 0.0495, 0.0599, 0.0694,
        0.0625, 0.0817, 0.0850], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,670][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.2574, 0.0773, 0.0465, 0.0801, 0.0750, 0.0347, 0.1044, 0.0466, 0.0598,
        0.1228, 0.0406, 0.0546], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,671][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.2360, 0.0095, 0.0374, 0.0272, 0.0488, 0.0049, 0.0642, 0.1691, 0.1604,
        0.1101, 0.0262, 0.1062], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,672][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2563, 0.0755, 0.1050, 0.0419, 0.0412, 0.0460, 0.1193, 0.0955, 0.0432,
        0.0822, 0.0525, 0.0413], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,673][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4344, 0.0431, 0.0415, 0.1141, 0.0462, 0.0436, 0.0371, 0.0484, 0.0333,
        0.0748, 0.0456, 0.0378], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,674][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6141, 0.0606, 0.0361, 0.0805, 0.0447, 0.0225, 0.0200, 0.0334, 0.0081,
        0.0427, 0.0266, 0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,676][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0018, 0.0463, 0.0714, 0.0538, 0.0727, 0.0696, 0.0967, 0.0938, 0.1053,
        0.1126, 0.1408, 0.1353], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,677][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([1.2601e-02, 1.7846e-03, 1.5246e-03, 7.8711e-03, 7.2331e-04, 1.5676e-03,
        1.2430e-02, 1.6697e-03, 7.8429e-01, 1.4557e-03, 6.2924e-04, 1.7345e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,678][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1769, 0.0372, 0.0489, 0.0513, 0.0044, 0.0086, 0.0362, 0.0526, 0.0588,
        0.1048, 0.0481, 0.1177, 0.2544], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,679][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.3416, 0.0240, 0.0296, 0.0312, 0.0228, 0.0165, 0.0280, 0.0404, 0.0397,
        0.0734, 0.0837, 0.0732, 0.1959], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,680][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.1118, 0.0136, 0.0274, 0.0732, 0.0573, 0.0554, 0.0653, 0.0846, 0.0640,
        0.0821, 0.0983, 0.0843, 0.1829], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,681][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.5179, 0.0210, 0.0294, 0.0304, 0.0291, 0.0299, 0.0371, 0.0356, 0.0411,
        0.0467, 0.0525, 0.0484, 0.0810], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,682][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.3162, 0.0359, 0.0391, 0.0361, 0.0453, 0.0520, 0.0432, 0.0565, 0.0627,
        0.0604, 0.0795, 0.0784, 0.0948], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,684][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.4221, 0.0491, 0.0178, 0.0492, 0.0147, 0.0068, 0.0208, 0.0150, 0.0399,
        0.0403, 0.0065, 0.0327, 0.2851], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,685][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.4473, 0.1110, 0.1119, 0.0564, 0.0208, 0.0062, 0.0416, 0.0228, 0.0378,
        0.0587, 0.0125, 0.0240, 0.0489], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,686][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.2389, 0.0685, 0.0744, 0.0453, 0.0242, 0.0296, 0.0809, 0.0509, 0.0640,
        0.1221, 0.0383, 0.0544, 0.1084], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,687][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.3704, 0.0443, 0.0606, 0.0672, 0.0461, 0.0451, 0.0414, 0.0471, 0.0357,
        0.0466, 0.0441, 0.0386, 0.1128], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,688][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.5728, 0.0432, 0.0293, 0.0446, 0.0386, 0.0229, 0.0234, 0.0207, 0.0222,
        0.0308, 0.0281, 0.0250, 0.0984], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,689][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0019, 0.0381, 0.0587, 0.0468, 0.0596, 0.0614, 0.0808, 0.0774, 0.0897,
        0.0922, 0.1131, 0.1137, 0.1664], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,690][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ is] are: tensor([6.7531e-02, 1.1319e-03, 2.5476e-03, 1.4388e-02, 7.9826e-04, 4.3523e-04,
        3.5989e-03, 1.1874e-03, 2.5017e-03, 5.1122e-03, 1.0875e-03, 1.9595e-03,
        8.9772e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,692][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0664, 0.0122, 0.0131, 0.0216, 0.0046, 0.0045, 0.0081, 0.0099, 0.0196,
        0.0603, 0.0315, 0.0414, 0.4752, 0.2316], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,693][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1285, 0.0159, 0.0176, 0.0337, 0.0177, 0.0168, 0.0248, 0.0379, 0.0306,
        0.0576, 0.0719, 0.0557, 0.3327, 0.1585], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,694][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0959, 0.0140, 0.0270, 0.0558, 0.0473, 0.0497, 0.0577, 0.0764, 0.0558,
        0.0702, 0.0849, 0.0734, 0.1403, 0.1515], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,695][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5358, 0.0178, 0.0251, 0.0247, 0.0243, 0.0246, 0.0306, 0.0298, 0.0344,
        0.0381, 0.0452, 0.0408, 0.0646, 0.0641], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,696][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2762, 0.0395, 0.0421, 0.0312, 0.0433, 0.0514, 0.0453, 0.0585, 0.0575,
        0.0547, 0.0702, 0.0698, 0.0749, 0.0854], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,697][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3993, 0.0374, 0.0141, 0.1396, 0.0192, 0.0107, 0.0353, 0.0212, 0.0524,
        0.0469, 0.0087, 0.0333, 0.1161, 0.0659], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,699][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.3344, 0.1090, 0.0947, 0.0456, 0.0171, 0.0161, 0.0534, 0.0222, 0.0608,
        0.0591, 0.0105, 0.0384, 0.0827, 0.0560], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,700][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2725, 0.0527, 0.0734, 0.0442, 0.0272, 0.0275, 0.0769, 0.0480, 0.0540,
        0.0831, 0.0351, 0.0472, 0.0713, 0.0869], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,701][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3467, 0.0420, 0.0464, 0.0704, 0.0399, 0.0377, 0.0336, 0.0396, 0.0278,
        0.0435, 0.0376, 0.0303, 0.0865, 0.1180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,702][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4709, 0.0343, 0.0260, 0.0406, 0.0323, 0.0196, 0.0224, 0.0191, 0.0191,
        0.0239, 0.0275, 0.0206, 0.1060, 0.1378], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,703][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0024, 0.0336, 0.0528, 0.0388, 0.0498, 0.0517, 0.0702, 0.0696, 0.0785,
        0.0812, 0.0994, 0.1008, 0.1412, 0.1299], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,704][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([2.0451e-01, 1.2995e-03, 2.3778e-03, 5.1089e-02, 1.1417e-03, 1.1357e-03,
        7.4622e-04, 3.4443e-04, 1.5838e-03, 2.0216e-03, 4.6759e-04, 1.8235e-03,
        1.2299e-02, 7.1916e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,706][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0280, 0.0044, 0.0062, 0.0126, 0.0015, 0.0034, 0.0047, 0.0050, 0.0064,
        0.0374, 0.0190, 0.0161, 0.2568, 0.5112, 0.0872], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,707][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0675, 0.0044, 0.0065, 0.0161, 0.0075, 0.0109, 0.0107, 0.0168, 0.0175,
        0.0406, 0.0324, 0.0363, 0.3563, 0.2644, 0.1121], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,708][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0816, 0.0132, 0.0261, 0.0518, 0.0414, 0.0460, 0.0537, 0.0696, 0.0511,
        0.0672, 0.0774, 0.0667, 0.1330, 0.1293, 0.0921], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,709][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.5324, 0.0142, 0.0212, 0.0214, 0.0208, 0.0209, 0.0273, 0.0269, 0.0316,
        0.0362, 0.0423, 0.0380, 0.0596, 0.0534, 0.0539], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,710][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.2906, 0.0318, 0.0346, 0.0287, 0.0331, 0.0421, 0.0370, 0.0489, 0.0470,
        0.0485, 0.0634, 0.0592, 0.0706, 0.0752, 0.0891], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,712][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.3613, 0.1047, 0.0308, 0.0694, 0.0079, 0.0049, 0.0270, 0.0322, 0.0154,
        0.0286, 0.0079, 0.0106, 0.0923, 0.0800, 0.1270], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,713][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1699, 0.0991, 0.0665, 0.0446, 0.0348, 0.0199, 0.1856, 0.0165, 0.0228,
        0.0270, 0.0099, 0.0139, 0.0991, 0.0541, 0.1362], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,714][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.2215, 0.0585, 0.0506, 0.0394, 0.0264, 0.0258, 0.0567, 0.0647, 0.0338,
        0.0952, 0.0414, 0.0291, 0.1166, 0.0618, 0.0785], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,715][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.3226, 0.0327, 0.0402, 0.0557, 0.0348, 0.0321, 0.0394, 0.0431, 0.0293,
        0.0439, 0.0350, 0.0334, 0.0976, 0.1137, 0.0466], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,716][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.4423, 0.0242, 0.0285, 0.0530, 0.0299, 0.0148, 0.0197, 0.0139, 0.0224,
        0.0209, 0.0213, 0.0264, 0.0944, 0.1574, 0.0311], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,718][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0023, 0.0289, 0.0466, 0.0343, 0.0441, 0.0470, 0.0652, 0.0565, 0.0711,
        0.0708, 0.0876, 0.0911, 0.1233, 0.1032, 0.1279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,719][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ same] are: tensor([2.4253e-02, 5.8660e-03, 1.2620e-03, 1.3135e-02, 1.4381e-04, 1.0868e-04,
        2.5658e-04, 2.5889e-04, 7.9319e-05, 6.8759e-04, 8.5028e-04, 3.1011e-05,
        8.8392e-04, 8.5089e-04, 9.5133e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,720][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ as] are: tensor([2.2579e-02, 2.5241e-03, 2.7221e-03, 3.6300e-03, 6.6183e-04, 1.3145e-03,
        8.2964e-04, 2.0779e-03, 2.3253e-03, 1.2338e-02, 7.6555e-03, 4.8402e-03,
        9.3693e-02, 1.0718e-01, 7.1758e-01, 1.8047e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,721][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0905, 0.0089, 0.0085, 0.0136, 0.0091, 0.0108, 0.0169, 0.0179, 0.0232,
        0.0287, 0.0386, 0.0445, 0.2108, 0.1732, 0.1550, 0.1500],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,722][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0670, 0.0115, 0.0215, 0.0439, 0.0373, 0.0381, 0.0450, 0.0593, 0.0455,
        0.0568, 0.0665, 0.0607, 0.1114, 0.1185, 0.0765, 0.1405],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,723][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.5472, 0.0119, 0.0175, 0.0175, 0.0174, 0.0173, 0.0234, 0.0216, 0.0264,
        0.0300, 0.0338, 0.0322, 0.0517, 0.0486, 0.0478, 0.0556],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,725][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.2543, 0.0275, 0.0293, 0.0262, 0.0349, 0.0388, 0.0336, 0.0454, 0.0481,
        0.0471, 0.0561, 0.0613, 0.0650, 0.0732, 0.0714, 0.0878],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,726][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.2970, 0.0154, 0.0131, 0.0400, 0.0054, 0.0041, 0.0238, 0.0129, 0.0114,
        0.0146, 0.0071, 0.0079, 0.0544, 0.0558, 0.0319, 0.4052],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,727][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0809, 0.0364, 0.0380, 0.0186, 0.0074, 0.0084, 0.0053, 0.0113, 0.0166,
        0.0266, 0.0137, 0.0123, 0.0277, 0.0294, 0.6568, 0.0106],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,728][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.2534, 0.0520, 0.0490, 0.0366, 0.0250, 0.0234, 0.0626, 0.0360, 0.0454,
        0.0915, 0.0289, 0.0377, 0.0709, 0.0608, 0.0550, 0.0718],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,730][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3080, 0.0361, 0.0345, 0.0545, 0.0363, 0.0344, 0.0344, 0.0362, 0.0280,
        0.0382, 0.0344, 0.0308, 0.0870, 0.0868, 0.0478, 0.0727],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,731][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.4117, 0.0312, 0.0200, 0.0350, 0.0313, 0.0165, 0.0150, 0.0160, 0.0141,
        0.0192, 0.0235, 0.0157, 0.0965, 0.1370, 0.0443, 0.0730],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,732][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0027, 0.0222, 0.0367, 0.0275, 0.0364, 0.0366, 0.0508, 0.0485, 0.0604,
        0.0611, 0.0756, 0.0794, 0.1093, 0.0991, 0.1194, 0.1340],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,733][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ as] are: tensor([6.7876e-02, 1.8290e-03, 3.4354e-03, 3.4125e-02, 8.3434e-04, 4.1775e-04,
        3.8863e-03, 2.2117e-03, 4.1721e-04, 1.4925e-03, 6.5738e-04, 3.4533e-04,
        1.3955e-02, 5.2944e-03, 3.0400e-03, 8.6018e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,734][circuit_into_ebeddingspace.py][line:2310][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0467, 0.0050, 0.0053, 0.0121, 0.0027, 0.0021, 0.0039, 0.0039, 0.0103,
        0.0240, 0.0141, 0.0214, 0.0939, 0.0837, 0.1134, 0.2735, 0.2839],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,736][circuit_into_ebeddingspace.py][line:2313][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0710, 0.0072, 0.0087, 0.0161, 0.0092, 0.0098, 0.0132, 0.0179, 0.0171,
        0.0316, 0.0409, 0.0317, 0.1311, 0.0607, 0.1668, 0.2378, 0.1290],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,737][circuit_into_ebeddingspace.py][line:2316][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0601, 0.0119, 0.0218, 0.0357, 0.0321, 0.0351, 0.0406, 0.0520, 0.0411,
        0.0491, 0.0587, 0.0538, 0.0943, 0.1019, 0.0752, 0.1206, 0.1162],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,738][circuit_into_ebeddingspace.py][line:2319][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5589, 0.0108, 0.0157, 0.0154, 0.0148, 0.0148, 0.0191, 0.0187, 0.0219,
        0.0246, 0.0293, 0.0266, 0.0432, 0.0437, 0.0439, 0.0482, 0.0505],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,739][circuit_into_ebeddingspace.py][line:2322][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2260, 0.0302, 0.0324, 0.0232, 0.0338, 0.0395, 0.0336, 0.0439, 0.0440,
        0.0408, 0.0525, 0.0541, 0.0544, 0.0620, 0.0764, 0.0747, 0.0785],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,740][circuit_into_ebeddingspace.py][line:2325][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3099, 0.0257, 0.0107, 0.1083, 0.0142, 0.0080, 0.0308, 0.0166, 0.0399,
        0.0432, 0.0069, 0.0268, 0.0948, 0.0622, 0.0328, 0.1165, 0.0527],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,742][circuit_into_ebeddingspace.py][line:2328][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2726, 0.0894, 0.0746, 0.0356, 0.0153, 0.0155, 0.0518, 0.0212, 0.0546,
        0.0585, 0.0091, 0.0353, 0.0628, 0.0449, 0.0822, 0.0426, 0.0340],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,743][circuit_into_ebeddingspace.py][line:2331][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2531, 0.0387, 0.0561, 0.0334, 0.0257, 0.0230, 0.0655, 0.0419, 0.0443,
        0.0624, 0.0290, 0.0384, 0.0546, 0.0640, 0.0502, 0.0589, 0.0609],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,744][circuit_into_ebeddingspace.py][line:2334][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2894, 0.0344, 0.0328, 0.0589, 0.0308, 0.0277, 0.0293, 0.0304, 0.0245,
        0.0401, 0.0289, 0.0277, 0.0755, 0.0900, 0.0442, 0.0654, 0.0700],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,745][circuit_into_ebeddingspace.py][line:2337][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.3564, 0.0270, 0.0246, 0.0345, 0.0302, 0.0183, 0.0206, 0.0199, 0.0184,
        0.0225, 0.0285, 0.0198, 0.0813, 0.1100, 0.0408, 0.0575, 0.0895],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,747][circuit_into_ebeddingspace.py][line:2340][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0032, 0.0203, 0.0338, 0.0242, 0.0315, 0.0323, 0.0448, 0.0441, 0.0513,
        0.0525, 0.0659, 0.0673, 0.0952, 0.0895, 0.1081, 0.1218, 0.1141],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,748][circuit_into_ebeddingspace.py][line:2343][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.0195e-01, 8.0684e-04, 1.3415e-03, 2.9726e-02, 8.6288e-04, 8.8300e-04,
        4.9737e-04, 2.7058e-04, 1.0657e-03, 1.5409e-03, 3.2203e-04, 1.3946e-03,
        6.1262e-03, 4.1589e-01, 9.1680e-04, 1.0803e-02, 4.2560e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,756][circuit_into_ebeddingspace.py][line:2286][INFO] ################2-th layer#################
[2024-06-27 21:00:52,759][circuit_into_ebeddingspace.py][line:2288][INFO] ##2-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,760][circuit_into_ebeddingspace.py][line:2289][INFO] ##2-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,762][circuit_into_ebeddingspace.py][line:2290][INFO] ##2-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [1, 1],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,763][circuit_into_ebeddingspace.py][line:2291][INFO] ##2-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,764][circuit_into_ebeddingspace.py][line:2292][INFO] ##2-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [1, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,766][circuit_into_ebeddingspace.py][line:2293][INFO] ##2-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,767][circuit_into_ebeddingspace.py][line:2294][INFO] ##2-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,769][circuit_into_ebeddingspace.py][line:2295][INFO] ##2-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,770][circuit_into_ebeddingspace.py][line:2296][INFO] ##2-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,771][circuit_into_ebeddingspace.py][line:2297][INFO] ##2-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,773][circuit_into_ebeddingspace.py][line:2298][INFO] ##2-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,774][circuit_into_ebeddingspace.py][line:2299][INFO] ##2-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:52,775][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,776][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,777][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,778][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,779][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,780][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,781][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,781][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,782][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,783][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,784][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,785][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:52,786][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.8692, 0.1308], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,787][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9660, 0.0340], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,788][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.8972, 0.1028], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,789][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9594, 0.0406], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,790][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9570, 0.0430], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,791][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9651, 0.0349], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,792][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9599, 0.0401], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,793][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.7882, 0.2118], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,794][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.8688, 0.1312], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,795][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8320, 0.1680], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,796][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.8656, 0.1344], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,796][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9497, 0.0503], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:52,797][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5399, 0.4335, 0.0266], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,798][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8063, 0.0813, 0.1124], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,799][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.2372, 0.6954, 0.0674], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,800][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7874, 0.1893, 0.0233], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,801][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.2847, 0.6830, 0.0323], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,802][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7343, 0.2103, 0.0554], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,803][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9344, 0.0293, 0.0363], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,804][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4679, 0.1163, 0.4158], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,805][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.5416, 0.4163, 0.0421], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,806][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.2315, 0.6804, 0.0880], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,807][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6941, 0.1019, 0.2040], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,808][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8593, 0.0435, 0.0972], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:52,809][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.3858, 0.1567, 0.1149, 0.3426], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,810][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8729, 0.0286, 0.0415, 0.0570], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,811][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1666, 0.0905, 0.7259, 0.0171], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,812][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.8156, 0.1120, 0.0321, 0.0403], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,813][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2279, 0.0906, 0.6124, 0.0691], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,814][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.4871, 0.0893, 0.3008, 0.1228], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,815][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8358, 0.0225, 0.0380, 0.1036], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,816][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3953, 0.0638, 0.3006, 0.2403], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,817][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3709, 0.3178, 0.1754, 0.1359], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,818][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.2584, 0.1389, 0.5759, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,819][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5983, 0.0984, 0.2002, 0.1031], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,820][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.8154, 0.0323, 0.0814, 0.0709], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:52,821][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.3236, 0.0304, 0.0213, 0.3807, 0.2440], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,822][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.6276, 0.0643, 0.1251, 0.1001, 0.0829], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,823][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.3220, 0.0314, 0.0909, 0.5128, 0.0429], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,824][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.4996, 0.0581, 0.0498, 0.1911, 0.2014], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,825][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.6913, 0.0054, 0.0133, 0.2033, 0.0867], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,826][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.2926, 0.0622, 0.2369, 0.3723, 0.0361], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,827][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.8462, 0.0156, 0.0295, 0.0590, 0.0497], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,828][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2803, 0.0718, 0.2710, 0.1251, 0.2517], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,829][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.2498, 0.1113, 0.1867, 0.4184, 0.0338], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,830][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.3262, 0.0881, 0.2312, 0.1911, 0.1634], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,831][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.5563, 0.0851, 0.1729, 0.0578, 0.1280], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,832][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.7853, 0.0319, 0.0757, 0.0363, 0.0708], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:52,833][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.1993, 0.0236, 0.0219, 0.2405, 0.4467, 0.0681], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,834][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4687, 0.1003, 0.1477, 0.0944, 0.1423, 0.0465], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,835][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0793, 0.0142, 0.0040, 0.0373, 0.8343, 0.0310], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,836][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.1417, 0.0280, 0.0209, 0.0800, 0.5826, 0.1468], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,837][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.4331, 0.0239, 0.0427, 0.1938, 0.2958, 0.0107], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,838][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.3314, 0.0170, 0.1803, 0.2511, 0.1605, 0.0597], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,839][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.8404, 0.0150, 0.0212, 0.0644, 0.0421, 0.0169], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,840][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.3012, 0.0613, 0.1926, 0.0936, 0.1511, 0.2002], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,841][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.3148, 0.0908, 0.1008, 0.3037, 0.0745, 0.1153], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,842][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.2164, 0.0428, 0.1059, 0.0901, 0.4684, 0.0763], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,843][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.4726, 0.0742, 0.1567, 0.0503, 0.0937, 0.1526], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,844][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.7566, 0.0317, 0.0613, 0.0371, 0.0472, 0.0661], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:52,845][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.3164, 0.0118, 0.0084, 0.2238, 0.2370, 0.1700, 0.0326],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,846][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.5948, 0.0206, 0.0362, 0.1116, 0.1021, 0.0228, 0.1120],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,847][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0193, 0.0013, 0.0015, 0.0163, 0.2061, 0.7284, 0.0271],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,848][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.3106, 0.0210, 0.0188, 0.1133, 0.2858, 0.2063, 0.0442],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,849][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4164, 0.0190, 0.0048, 0.1233, 0.2607, 0.1624, 0.0134],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,850][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.2568, 0.0358, 0.0751, 0.2430, 0.0608, 0.2222, 0.1064],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,851][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.7667, 0.0143, 0.0205, 0.0626, 0.0459, 0.0215, 0.0685],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,853][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2405, 0.0336, 0.1339, 0.0741, 0.1186, 0.1305, 0.2688],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,854][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3053, 0.0378, 0.0538, 0.2674, 0.0258, 0.1017, 0.2082],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,855][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1828, 0.0382, 0.0817, 0.0925, 0.1041, 0.4037, 0.0971],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,856][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3635, 0.0542, 0.1156, 0.0493, 0.0812, 0.1129, 0.2234],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,857][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.7019, 0.0220, 0.0426, 0.0323, 0.0385, 0.0476, 0.1150],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:52,858][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.2233, 0.0167, 0.0109, 0.2458, 0.2989, 0.0954, 0.0489, 0.0601],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,859][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.4680, 0.0272, 0.0514, 0.0621, 0.0994, 0.0193, 0.2255, 0.0471],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,860][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2753, 0.0015, 0.0031, 0.0095, 0.2439, 0.0649, 0.3517, 0.0501],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,861][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.2834, 0.0263, 0.0150, 0.0610, 0.2374, 0.1212, 0.1578, 0.0980],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,862][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.4998, 0.0071, 0.0167, 0.0909, 0.0481, 0.0178, 0.3013, 0.0184],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,863][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.2481, 0.0094, 0.0739, 0.1188, 0.0394, 0.0553, 0.4013, 0.0538],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,864][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.8312, 0.0104, 0.0119, 0.0500, 0.0173, 0.0121, 0.0418, 0.0253],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,865][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1920, 0.0336, 0.1004, 0.0552, 0.0821, 0.0922, 0.2125, 0.2321],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,866][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2811, 0.0410, 0.0344, 0.2486, 0.0193, 0.0536, 0.2276, 0.0943],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,867][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.1430, 0.0130, 0.0466, 0.0283, 0.0570, 0.2093, 0.3831, 0.1198],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,868][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.3299, 0.0454, 0.0936, 0.0439, 0.0645, 0.0923, 0.1814, 0.1489],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,869][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.6733, 0.0227, 0.0394, 0.0295, 0.0287, 0.0456, 0.0882, 0.0728],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:52,870][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.3827, 0.0086, 0.0036, 0.1751, 0.1936, 0.0567, 0.0473, 0.1138, 0.0185],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,872][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4437, 0.0100, 0.0424, 0.0941, 0.1101, 0.0155, 0.1614, 0.0448, 0.0779],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,873][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([8.7231e-03, 4.3252e-05, 1.7074e-04, 2.2985e-03, 1.3743e-02, 9.6340e-02,
        5.3448e-02, 6.6141e-01, 1.6383e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,874][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2428, 0.0099, 0.0102, 0.0431, 0.0762, 0.1339, 0.0548, 0.3548, 0.0743],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,875][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.2605, 0.0009, 0.0013, 0.0221, 0.0170, 0.0269, 0.0664, 0.5437, 0.0612],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,876][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.2118, 0.0120, 0.0163, 0.0556, 0.0190, 0.0740, 0.1052, 0.2462, 0.2599],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,877][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8061, 0.0085, 0.0100, 0.0481, 0.0207, 0.0108, 0.0300, 0.0350, 0.0309],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,878][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1559, 0.0240, 0.0774, 0.0409, 0.0646, 0.0716, 0.1461, 0.1857, 0.2338],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,879][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2097, 0.0200, 0.0272, 0.1425, 0.0163, 0.0626, 0.1172, 0.2941, 0.1103],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,880][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0868, 0.0080, 0.0148, 0.0192, 0.0246, 0.0699, 0.1341, 0.4861, 0.1565],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,881][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2596, 0.0335, 0.0718, 0.0328, 0.0538, 0.0782, 0.1472, 0.1367, 0.1864],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,882][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.6588, 0.0157, 0.0309, 0.0215, 0.0206, 0.0315, 0.0685, 0.0667, 0.0857],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:52,883][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.3554, 0.0108, 0.0069, 0.1746, 0.0630, 0.0399, 0.0359, 0.1412, 0.0728,
        0.0995], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,885][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.5676, 0.0091, 0.0877, 0.1810, 0.0708, 0.0050, 0.0455, 0.0114, 0.0113,
        0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,886][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ et] are: tensor([1.8724e-01, 1.6701e-04, 8.4665e-04, 1.1765e-03, 1.0826e-03, 3.4516e-02,
        4.8274e-02, 3.1007e-01, 3.5593e-01, 6.0696e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,887][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.2810, 0.0228, 0.0091, 0.0336, 0.0783, 0.0768, 0.0746, 0.1726, 0.0762,
        0.1749], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,888][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.5351, 0.0011, 0.0016, 0.0573, 0.0014, 0.0537, 0.1254, 0.1325, 0.0706,
        0.0214], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,889][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.1712, 0.0038, 0.0185, 0.0425, 0.0144, 0.0250, 0.0799, 0.0979, 0.3619,
        0.1849], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,890][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.6513, 0.0128, 0.0166, 0.0465, 0.0239, 0.0131, 0.0423, 0.0372, 0.0368,
        0.1194], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,891][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1276, 0.0242, 0.0696, 0.0349, 0.0519, 0.0553, 0.1003, 0.1310, 0.1636,
        0.2416], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,892][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.3120, 0.0359, 0.0316, 0.1561, 0.0034, 0.0277, 0.1018, 0.1694, 0.1061,
        0.0561], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,893][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0856, 0.0048, 0.0130, 0.0099, 0.0196, 0.0607, 0.0734, 0.2361, 0.4515,
        0.0455], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,894][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.2577, 0.0398, 0.0668, 0.0303, 0.0510, 0.0794, 0.1061, 0.1142, 0.1325,
        0.1223], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,895][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.6508, 0.0125, 0.0218, 0.0149, 0.0213, 0.0363, 0.0507, 0.0667, 0.0669,
        0.0580], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:52,897][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.2273, 0.0139, 0.0028, 0.0937, 0.0390, 0.0775, 0.0263, 0.0930, 0.0292,
        0.3200, 0.0772], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,898][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.5133, 0.0084, 0.0567, 0.1724, 0.0634, 0.0087, 0.0719, 0.0187, 0.0497,
        0.0079, 0.0288], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,899][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([5.0759e-02, 2.0365e-04, 4.0755e-04, 7.0637e-04, 3.1488e-03, 1.1888e-02,
        1.1537e-02, 4.7276e-02, 1.4501e-01, 6.4669e-01, 8.2375e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,900][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.1489, 0.0181, 0.0064, 0.0306, 0.0493, 0.0332, 0.0667, 0.1572, 0.0782,
        0.2817, 0.1298], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,901][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.4471, 0.0014, 0.0101, 0.0186, 0.0012, 0.0408, 0.1128, 0.1251, 0.0928,
        0.1381, 0.0120], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,902][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.1546, 0.0017, 0.0071, 0.0135, 0.0049, 0.0122, 0.0581, 0.0608, 0.1838,
        0.4448, 0.0586], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,903][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.8283, 0.0066, 0.0062, 0.0343, 0.0101, 0.0058, 0.0160, 0.0159, 0.0133,
        0.0445, 0.0189], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,904][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.1361, 0.0211, 0.0545, 0.0233, 0.0363, 0.0428, 0.0908, 0.1059, 0.1288,
        0.2125, 0.1479], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,905][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.3123, 0.0134, 0.0095, 0.0854, 0.0082, 0.0103, 0.0581, 0.1486, 0.0524,
        0.2682, 0.0336], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,907][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.0782, 0.0024, 0.0177, 0.0048, 0.0282, 0.0504, 0.0592, 0.2190, 0.2525,
        0.1956, 0.0920], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,908][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2150, 0.0330, 0.0585, 0.0260, 0.0398, 0.0533, 0.1015, 0.0878, 0.1244,
        0.1332, 0.1275], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,909][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.6409, 0.0137, 0.0217, 0.0141, 0.0159, 0.0214, 0.0415, 0.0376, 0.0584,
        0.0721, 0.0626], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:52,910][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2675, 0.0051, 0.0015, 0.0838, 0.0442, 0.0187, 0.0168, 0.0399, 0.0075,
        0.2984, 0.1986, 0.0180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,911][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4770, 0.0062, 0.0486, 0.1218, 0.0900, 0.0078, 0.0960, 0.0223, 0.0406,
        0.0217, 0.0421, 0.0260], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,912][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([4.7861e-03, 3.4426e-06, 1.3387e-05, 1.3783e-04, 4.5878e-04, 3.1530e-03,
        2.5101e-03, 2.4754e-02, 7.9777e-03, 3.1255e-01, 4.6808e-01, 1.7557e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,913][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.1451, 0.0038, 0.0029, 0.0142, 0.0189, 0.0230, 0.0127, 0.0713, 0.0168,
        0.2162, 0.4003, 0.0748], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,914][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([3.7768e-01, 3.6746e-04, 8.5311e-04, 1.0904e-02, 3.4641e-03, 7.3399e-03,
        3.4002e-02, 1.2297e-01, 2.2733e-02, 2.1941e-01, 8.3878e-02, 1.1640e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,916][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.1317, 0.0017, 0.0026, 0.0096, 0.0023, 0.0089, 0.0135, 0.0385, 0.0389,
        0.3701, 0.1149, 0.2672], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,917][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.7333, 0.0066, 0.0073, 0.0449, 0.0126, 0.0062, 0.0164, 0.0210, 0.0175,
        0.0623, 0.0342, 0.0377], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,918][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0897, 0.0156, 0.0436, 0.0221, 0.0345, 0.0354, 0.0630, 0.0872, 0.0995,
        0.1722, 0.1233, 0.2138], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,919][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1759, 0.0082, 0.0071, 0.0505, 0.0035, 0.0093, 0.0302, 0.0532, 0.0289,
        0.4317, 0.0797, 0.1217], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,920][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0853, 0.0025, 0.0057, 0.0058, 0.0055, 0.0180, 0.0344, 0.1225, 0.0469,
        0.1485, 0.3177, 0.2073], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,921][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1499, 0.0237, 0.0481, 0.0248, 0.0356, 0.0480, 0.0825, 0.0844, 0.1062,
        0.1138, 0.1116, 0.1715], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,922][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.6192, 0.0120, 0.0197, 0.0146, 0.0138, 0.0192, 0.0342, 0.0380, 0.0429,
        0.0663, 0.0497, 0.0705], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:52,924][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.3344, 0.0395, 0.0084, 0.1553, 0.0215, 0.0093, 0.0284, 0.0412, 0.0288,
        0.1397, 0.1059, 0.0503, 0.0372], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,925][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.6425, 0.0164, 0.0295, 0.0576, 0.0309, 0.0083, 0.0249, 0.0291, 0.0173,
        0.0267, 0.0195, 0.0099, 0.0874], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,926][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ is] are: tensor([6.6025e-02, 3.8038e-05, 1.6858e-04, 2.0190e-04, 6.0846e-04, 7.4320e-04,
        2.5235e-03, 8.8530e-03, 2.3843e-02, 9.7253e-02, 1.7489e-01, 5.0999e-01,
        1.1486e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,927][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.3984, 0.0137, 0.0029, 0.0269, 0.0087, 0.0059, 0.0066, 0.0312, 0.0125,
        0.1052, 0.1854, 0.0734, 0.1291], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,928][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ is] are: tensor([8.0994e-01, 1.7873e-03, 1.1559e-03, 1.1742e-02, 5.1650e-05, 2.0903e-03,
        5.1572e-04, 2.7230e-03, 2.8463e-03, 1.8935e-02, 2.8004e-02, 1.0056e-02,
        1.1015e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,929][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1772, 0.0004, 0.0021, 0.0068, 0.0027, 0.0041, 0.0164, 0.0165, 0.0481,
        0.1372, 0.1381, 0.2655, 0.1849], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,930][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.6168, 0.0132, 0.0113, 0.0433, 0.0097, 0.0059, 0.0145, 0.0193, 0.0115,
        0.0816, 0.0346, 0.0267, 0.1116], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,932][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0818, 0.0153, 0.0355, 0.0255, 0.0324, 0.0320, 0.0541, 0.0762, 0.0709,
        0.1435, 0.1031, 0.1596, 0.1701], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,933][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.3940, 0.0286, 0.0151, 0.1124, 0.0009, 0.0026, 0.0119, 0.0073, 0.0122,
        0.0693, 0.0646, 0.0534, 0.2278], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,934][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0899, 0.0058, 0.0136, 0.0037, 0.0064, 0.0147, 0.0357, 0.0511, 0.0860,
        0.1345, 0.1940, 0.3141, 0.0503], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,935][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.2028, 0.0351, 0.0448, 0.0294, 0.0354, 0.0398, 0.0515, 0.0659, 0.0607,
        0.0907, 0.0926, 0.0979, 0.1534], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,936][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.6351, 0.0130, 0.0156, 0.0127, 0.0165, 0.0188, 0.0206, 0.0302, 0.0279,
        0.0474, 0.0440, 0.0429, 0.0752], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:52,937][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2541, 0.0284, 0.0174, 0.0836, 0.0240, 0.0127, 0.0304, 0.0591, 0.0497,
        0.0777, 0.1575, 0.0962, 0.0361, 0.0733], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,938][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.6511, 0.0199, 0.0273, 0.0505, 0.0254, 0.0086, 0.0162, 0.0201, 0.0125,
        0.0178, 0.0143, 0.0064, 0.0973, 0.0326], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,940][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.3792e-02, 4.5791e-06, 1.3275e-05, 1.9682e-04, 2.3293e-04, 6.9267e-04,
        1.2263e-03, 5.5554e-03, 6.2198e-03, 2.5874e-02, 6.1244e-02, 8.3447e-02,
        6.9959e-01, 9.1909e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,941][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1183, 0.0032, 0.0016, 0.0043, 0.0082, 0.0063, 0.0046, 0.0378, 0.0096,
        0.1181, 0.2241, 0.0485, 0.1253, 0.2903], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,942][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([3.3355e-01, 2.0520e-03, 1.1099e-03, 2.2220e-03, 8.1359e-05, 4.7948e-03,
        1.1729e-03, 1.4338e-02, 1.7615e-02, 1.0596e-02, 4.5447e-02, 6.9490e-02,
        2.4950e-01, 2.4803e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,943][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([1.3211e-01, 3.1890e-04, 1.3462e-03, 1.2515e-03, 1.1112e-03, 3.2693e-03,
        9.9724e-03, 8.1407e-03, 2.4503e-02, 7.4622e-02, 5.5857e-02, 1.2859e-01,
        4.5052e-01, 1.0839e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,944][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4984, 0.0120, 0.0112, 0.0459, 0.0129, 0.0061, 0.0173, 0.0173, 0.0129,
        0.0473, 0.0320, 0.0269, 0.0929, 0.1667], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,945][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0716, 0.0121, 0.0319, 0.0215, 0.0276, 0.0252, 0.0469, 0.0605, 0.0667,
        0.1131, 0.0875, 0.1499, 0.1541, 0.1314], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,947][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0933, 0.0014, 0.0018, 0.0079, 0.0007, 0.0034, 0.0036, 0.0226, 0.0060,
        0.0379, 0.0459, 0.0266, 0.6108, 0.1382], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,948][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1459, 0.0041, 0.0119, 0.0005, 0.0078, 0.0165, 0.0283, 0.0510, 0.0614,
        0.0866, 0.2412, 0.2032, 0.0734, 0.0682], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,949][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1758, 0.0381, 0.0469, 0.0294, 0.0296, 0.0368, 0.0466, 0.0506, 0.0501,
        0.0737, 0.0704, 0.0813, 0.1439, 0.1267], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,950][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.6126, 0.0116, 0.0145, 0.0113, 0.0140, 0.0157, 0.0203, 0.0239, 0.0251,
        0.0401, 0.0370, 0.0382, 0.0670, 0.0687], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:52,951][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1515, 0.0140, 0.0080, 0.0483, 0.0123, 0.0096, 0.0145, 0.0314, 0.0365,
        0.0931, 0.2192, 0.0753, 0.0575, 0.0580, 0.1707], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,952][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.5972, 0.0159, 0.0116, 0.0373, 0.0186, 0.0046, 0.0397, 0.0136, 0.0305,
        0.0285, 0.0110, 0.0169, 0.0951, 0.0537, 0.0258], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,954][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ same] are: tensor([8.3390e-02, 6.6536e-06, 1.1463e-05, 2.3631e-04, 7.2621e-05, 9.3150e-04,
        9.9531e-05, 1.7873e-03, 3.6720e-03, 2.9784e-02, 7.1717e-03, 4.2680e-02,
        5.7457e-01, 2.1678e-01, 3.8800e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,955][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0475, 0.0009, 0.0007, 0.0034, 0.0080, 0.0068, 0.0051, 0.0252, 0.0173,
        0.0785, 0.1036, 0.0747, 0.0612, 0.5226, 0.0446], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,956][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ same] are: tensor([1.7113e-01, 7.6884e-04, 1.6833e-04, 3.2963e-03, 2.7689e-04, 6.4299e-04,
        1.2928e-04, 7.0738e-03, 2.2022e-03, 1.0151e-03, 1.7220e-03, 8.0568e-03,
        2.5873e-01, 5.2341e-01, 2.1376e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,957][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ same] are: tensor([9.6931e-02, 2.9561e-04, 7.7864e-04, 1.3990e-03, 1.6534e-03, 1.7731e-03,
        9.7751e-03, 7.5441e-03, 2.4061e-02, 7.6232e-02, 3.9707e-02, 1.2848e-01,
        3.7647e-01, 1.9842e-01, 3.6476e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,958][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.4191, 0.0091, 0.0131, 0.0286, 0.0194, 0.0044, 0.0174, 0.0255, 0.0121,
        0.0396, 0.0419, 0.0246, 0.0939, 0.1369, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,960][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0960, 0.0167, 0.0301, 0.0236, 0.0247, 0.0221, 0.0434, 0.0494, 0.0552,
        0.0919, 0.0721, 0.1208, 0.1375, 0.1124, 0.1040], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,961][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ same] are: tensor([1.1245e-01, 4.1155e-04, 1.0324e-03, 7.4140e-03, 4.7570e-04, 2.1773e-03,
        1.7161e-03, 1.3000e-02, 5.1541e-03, 1.4580e-02, 1.5837e-02, 2.7434e-02,
        6.1049e-01, 1.3214e-01, 5.5690e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,962][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1131, 0.0024, 0.0170, 0.0014, 0.0092, 0.0084, 0.0205, 0.0305, 0.0352,
        0.1261, 0.0812, 0.1104, 0.0996, 0.1329, 0.2119], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,963][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1406, 0.0306, 0.0355, 0.0242, 0.0211, 0.0261, 0.0397, 0.0359, 0.0434,
        0.0570, 0.0591, 0.0726, 0.1274, 0.1070, 0.1799], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,964][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.5922, 0.0118, 0.0151, 0.0113, 0.0119, 0.0117, 0.0201, 0.0182, 0.0265,
        0.0277, 0.0343, 0.0395, 0.0589, 0.0593, 0.0614], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:52,965][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.1271, 0.0075, 0.0022, 0.0181, 0.0059, 0.0062, 0.0112, 0.0209, 0.0275,
        0.1059, 0.0293, 0.0595, 0.0429, 0.0501, 0.2493, 0.2363],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,967][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.7375, 0.0075, 0.0119, 0.0378, 0.0119, 0.0048, 0.0101, 0.0092, 0.0043,
        0.0039, 0.0042, 0.0016, 0.0438, 0.0380, 0.0200, 0.0535],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,968][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ as] are: tensor([2.1692e-02, 1.0030e-06, 1.7560e-06, 1.5977e-05, 1.7569e-05, 2.3510e-05,
        5.6029e-05, 2.2773e-04, 3.4433e-04, 2.5999e-03, 4.4522e-03, 6.2443e-03,
        1.7024e-01, 4.5047e-01, 2.8329e-01, 6.0325e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,969][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ as] are: tensor([6.7336e-02, 4.1353e-04, 1.4914e-04, 1.1776e-03, 2.0029e-03, 6.3911e-04,
        8.9338e-04, 5.5166e-03, 2.6994e-03, 1.8493e-02, 3.2468e-02, 1.4402e-02,
        1.0321e-01, 3.5800e-01, 3.3462e-01, 5.7977e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,970][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ as] are: tensor([4.5693e-02, 7.2851e-06, 9.2360e-06, 2.1690e-04, 5.5514e-05, 4.0494e-05,
        2.2409e-05, 1.5679e-04, 2.8242e-04, 1.8760e-03, 6.3451e-04, 9.8897e-04,
        4.3217e-02, 8.5629e-02, 7.9403e-01, 2.7141e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,971][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ as] are: tensor([7.8497e-02, 1.6718e-04, 2.2995e-04, 7.6687e-04, 3.9656e-04, 6.5109e-04,
        2.6104e-03, 2.8276e-03, 1.0055e-02, 2.5768e-02, 7.7458e-03, 6.2828e-02,
        2.8787e-01, 1.9163e-01, 8.2810e-02, 2.4515e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,973][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.4745, 0.0051, 0.0065, 0.0155, 0.0120, 0.0035, 0.0136, 0.0105, 0.0080,
        0.0281, 0.0196, 0.0174, 0.1041, 0.1203, 0.0629, 0.0984],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,974][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0686, 0.0098, 0.0233, 0.0142, 0.0203, 0.0175, 0.0342, 0.0432, 0.0492,
        0.0913, 0.0711, 0.1108, 0.1156, 0.0918, 0.1018, 0.1374],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,975][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ as] are: tensor([7.2766e-02, 3.3288e-04, 6.8747e-04, 2.3414e-03, 3.2274e-04, 3.3129e-04,
        6.2132e-04, 1.5944e-03, 1.1348e-03, 6.9780e-03, 8.2619e-03, 6.4283e-03,
        4.2702e-01, 2.0697e-01, 1.2716e-01, 1.3705e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,976][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ as] are: tensor([7.0027e-02, 9.5133e-04, 3.2065e-03, 2.3075e-04, 2.5097e-03, 4.9756e-03,
        3.3778e-03, 1.0255e-02, 1.8207e-02, 1.9757e-02, 6.0526e-02, 7.6221e-02,
        5.0835e-02, 1.1324e-01, 5.0717e-01, 5.8517e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,977][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1210, 0.0228, 0.0308, 0.0163, 0.0187, 0.0229, 0.0331, 0.0312, 0.0383,
        0.0507, 0.0497, 0.0617, 0.1033, 0.0824, 0.1610, 0.1560],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,979][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.5930, 0.0081, 0.0099, 0.0063, 0.0084, 0.0096, 0.0144, 0.0150, 0.0189,
        0.0235, 0.0238, 0.0289, 0.0504, 0.0490, 0.0698, 0.0711],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:52,980][circuit_into_ebeddingspace.py][line:2310][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1503, 0.0062, 0.0045, 0.0211, 0.0120, 0.0091, 0.0125, 0.0237, 0.0276,
        0.0420, 0.0580, 0.0525, 0.0174, 0.0216, 0.1595, 0.3185, 0.0637],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,981][circuit_into_ebeddingspace.py][line:2313][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5693, 0.0138, 0.0273, 0.0458, 0.0307, 0.0092, 0.0193, 0.0208, 0.0163,
        0.0185, 0.0156, 0.0077, 0.0770, 0.0270, 0.0278, 0.0567, 0.0172],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,982][circuit_into_ebeddingspace.py][line:2316][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.6797e-02, 3.6753e-07, 4.7387e-07, 7.4560e-06, 1.5254e-05, 3.6402e-05,
        4.6984e-05, 2.9258e-04, 4.0677e-04, 1.6349e-03, 3.3780e-03, 5.2346e-03,
        4.6068e-02, 3.3611e-03, 1.2445e-01, 6.2789e-01, 1.7038e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,983][circuit_into_ebeddingspace.py][line:2319][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([5.2603e-02, 4.4119e-04, 2.9418e-04, 4.3297e-04, 2.6128e-03, 1.8297e-03,
        1.5206e-03, 9.2463e-03, 3.3804e-03, 2.6552e-02, 6.4334e-02, 1.4853e-02,
        2.1892e-02, 4.0184e-02, 2.5583e-01, 1.8573e-01, 3.1826e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,985][circuit_into_ebeddingspace.py][line:2322][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([2.3808e-01, 3.5156e-04, 3.0191e-04, 3.8636e-04, 3.8251e-05, 1.0381e-03,
        4.9387e-04, 3.5892e-03, 8.1829e-03, 3.4521e-03, 1.2146e-02, 2.9054e-02,
        6.5381e-02, 5.3738e-02, 5.8596e-02, 2.8918e-01, 2.3599e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,986][circuit_into_ebeddingspace.py][line:2325][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([6.9368e-02, 9.1042e-05, 2.3724e-04, 1.4917e-04, 2.1574e-04, 7.0165e-04,
        1.6266e-03, 1.8157e-03, 5.0495e-03, 1.4712e-02, 9.8811e-03, 2.5636e-02,
        1.0779e-01, 1.2701e-02, 7.3618e-02, 5.7889e-01, 9.7511e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,987][circuit_into_ebeddingspace.py][line:2328][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.4601, 0.0060, 0.0053, 0.0172, 0.0091, 0.0028, 0.0116, 0.0074, 0.0079,
        0.0229, 0.0144, 0.0158, 0.0847, 0.0751, 0.0601, 0.0862, 0.1132],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,988][circuit_into_ebeddingspace.py][line:2331][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0655, 0.0101, 0.0253, 0.0127, 0.0188, 0.0165, 0.0325, 0.0394, 0.0480,
        0.0733, 0.0562, 0.1039, 0.1007, 0.0790, 0.0971, 0.1137, 0.1073],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,989][circuit_into_ebeddingspace.py][line:2334][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([6.2079e-02, 3.3709e-04, 2.8203e-04, 1.0375e-03, 2.6474e-04, 9.8708e-04,
        1.2186e-03, 4.8248e-03, 2.1131e-03, 9.7537e-03, 1.2205e-02, 9.2011e-03,
        2.1632e-01, 1.7435e-02, 8.0418e-02, 4.9253e-01, 8.8990e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,991][circuit_into_ebeddingspace.py][line:2337][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.1850e-01, 1.0388e-03, 2.8494e-03, 1.3557e-04, 3.9144e-03, 6.5457e-03,
        8.2481e-03, 1.6808e-02, 2.6344e-02, 2.6371e-02, 6.5828e-02, 7.9343e-02,
        3.1589e-02, 1.4472e-02, 3.1772e-01, 1.4607e-01, 1.3422e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,992][circuit_into_ebeddingspace.py][line:2340][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1082, 0.0221, 0.0295, 0.0130, 0.0176, 0.0231, 0.0322, 0.0310, 0.0358,
        0.0440, 0.0442, 0.0564, 0.0778, 0.0651, 0.1671, 0.1269, 0.1059],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:52,993][circuit_into_ebeddingspace.py][line:2343][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5705, 0.0077, 0.0097, 0.0051, 0.0075, 0.0094, 0.0137, 0.0142, 0.0175,
        0.0217, 0.0224, 0.0259, 0.0404, 0.0343, 0.0764, 0.0623, 0.0613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,001][circuit_into_ebeddingspace.py][line:2286][INFO] ################3-th layer#################
[2024-06-27 21:00:53,004][circuit_into_ebeddingspace.py][line:2288][INFO] ##3-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,005][circuit_into_ebeddingspace.py][line:2289][INFO] ##3-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,007][circuit_into_ebeddingspace.py][line:2290][INFO] ##3-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,008][circuit_into_ebeddingspace.py][line:2291][INFO] ##3-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,010][circuit_into_ebeddingspace.py][line:2292][INFO] ##3-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,011][circuit_into_ebeddingspace.py][line:2293][INFO] ##3-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 1],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:53,012][circuit_into_ebeddingspace.py][line:2294][INFO] ##3-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,014][circuit_into_ebeddingspace.py][line:2295][INFO] ##3-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,015][circuit_into_ebeddingspace.py][line:2296][INFO] ##3-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,016][circuit_into_ebeddingspace.py][line:2297][INFO] ##3-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,018][circuit_into_ebeddingspace.py][line:2298][INFO] ##3-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,019][circuit_into_ebeddingspace.py][line:2299][INFO] ##3-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,020][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,021][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,022][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,023][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,024][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,025][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,026][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,027][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,028][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,028][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,029][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,030][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,031][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9765, 0.0235], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,032][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9922, 0.0078], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,033][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9484, 0.0516], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,034][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9397, 0.0603], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,035][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ original] are: tensor([9.9919e-01, 8.1445e-04], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,036][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9764, 0.0236], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,037][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.7857, 0.2143], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,038][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9372, 0.0628], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,039][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9611, 0.0389], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,040][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9142, 0.0858], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,041][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9737, 0.0263], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,042][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.8427, 0.1573], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,043][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ language] are: tensor([9.7510e-01, 2.2277e-04, 2.4679e-02], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,044][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9469, 0.0213, 0.0318], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,045][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.2359, 0.7214, 0.0427], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,045][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.5373, 0.3994, 0.0633], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,046][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ language] are: tensor([9.9186e-01, 7.4456e-03, 6.9534e-04], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,047][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7755, 0.0961, 0.1285], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,048][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1855, 0.7448, 0.0697], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,049][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4976, 0.2854, 0.2170], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,050][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.5495, 0.3943, 0.0562], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,051][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.4703, 0.4203, 0.1094], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,052][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.8089, 0.0309, 0.1602], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,053][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.1847, 0.7320, 0.0832], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,054][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ of] are: tensor([9.9583e-01, 2.1209e-03, 1.6761e-03, 3.6811e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,055][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8420, 0.0297, 0.0294, 0.0989], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,056][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.2535, 0.5535, 0.1068, 0.0862], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,057][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0596, 0.0126, 0.8865, 0.0412], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,058][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9767, 0.0044, 0.0115, 0.0073], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,059][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.7768, 0.0827, 0.0860, 0.0545], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,060][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2808, 0.3365, 0.2461, 0.1366], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,061][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0358, 0.0056, 0.9403, 0.0183], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,062][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.4582, 0.0533, 0.2232, 0.2654], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,063][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.5481, 0.1471, 0.1170, 0.1878], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,064][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.7359, 0.0338, 0.0449, 0.1854], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,065][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1079, 0.5682, 0.2877, 0.0363], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,066][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ De] are: tensor([9.3213e-01, 7.5009e-05, 8.9211e-04, 7.1079e-04, 6.6194e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,067][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.5776, 0.0568, 0.0913, 0.2368, 0.0375], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,068][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.5031, 0.0212, 0.0066, 0.2343, 0.2348], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,069][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.7874, 0.0032, 0.0073, 0.1608, 0.0413], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,070][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9719, 0.0058, 0.0047, 0.0118, 0.0058], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,071][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.7210, 0.0483, 0.0943, 0.0892, 0.0472], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,072][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.3911, 0.0229, 0.0229, 0.0847, 0.4785], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,073][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.4766, 0.0447, 0.1328, 0.2290, 0.1169], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,074][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.1845, 0.0117, 0.0632, 0.7138, 0.0269], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,075][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.7761, 0.0122, 0.0525, 0.0996, 0.0595], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,076][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.8255, 0.0068, 0.0211, 0.0265, 0.1200], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,077][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.4890, 0.0776, 0.3056, 0.1075, 0.0204], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,078][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([9.6789e-01, 3.2072e-04, 4.3012e-05, 1.1824e-04, 2.0239e-06, 3.1623e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,079][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4936, 0.0191, 0.0624, 0.1790, 0.1254, 0.1205], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,080][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([7.0641e-02, 9.4122e-04, 4.6462e-04, 1.6787e-02, 7.8002e-01, 1.3114e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,081][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([5.3485e-01, 2.8627e-04, 8.1561e-04, 1.9939e-02, 4.1453e-01, 2.9576e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,082][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.6025e-01, 4.4934e-03, 7.2835e-03, 1.0309e-02, 1.7563e-02, 9.7152e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,083][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.8438, 0.0244, 0.0306, 0.0253, 0.0482, 0.0275], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,084][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0946, 0.0023, 0.0018, 0.0177, 0.8343, 0.0494], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,085][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0892, 0.0014, 0.0030, 0.0088, 0.7944, 0.1033], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,086][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.1139, 0.0027, 0.0238, 0.1565, 0.6406, 0.0625], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,087][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.6251, 0.0044, 0.0126, 0.0368, 0.2458, 0.0754], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,088][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.5891, 0.0052, 0.0066, 0.0132, 0.1201, 0.2658], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,089][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.2994, 0.0238, 0.0730, 0.0597, 0.4530, 0.0910], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,090][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([9.8828e-01, 7.0337e-05, 3.3906e-04, 7.5860e-05, 5.0380e-06, 1.0800e-04,
        1.1124e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,091][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.6820, 0.0452, 0.0614, 0.1226, 0.0344, 0.0172, 0.0372],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,092][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0710, 0.0034, 0.0009, 0.1157, 0.5097, 0.2549, 0.0444],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,093][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([6.9898e-01, 4.8487e-04, 2.5982e-03, 3.0039e-02, 4.6600e-02, 2.0173e-01,
        1.9574e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,094][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([9.6246e-01, 1.9433e-03, 9.6235e-03, 5.8315e-03, 3.4617e-03, 1.6029e-02,
        6.5434e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,095][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.7254, 0.0202, 0.0597, 0.0646, 0.0649, 0.0365, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,096][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0999, 0.0081, 0.0063, 0.0402, 0.6533, 0.1409, 0.0512],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,097][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2143, 0.0127, 0.1007, 0.0914, 0.0948, 0.4430, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,098][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.1407, 0.0042, 0.0463, 0.4117, 0.1164, 0.1111, 0.1695],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,099][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.5573, 0.0123, 0.0247, 0.1303, 0.1213, 0.0636, 0.0906],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,100][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.6738, 0.0042, 0.0222, 0.0242, 0.0275, 0.0249, 0.2231],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,101][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.1448, 0.0546, 0.1123, 0.1363, 0.0133, 0.2644, 0.2742],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,102][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([9.6211e-01, 3.3778e-04, 2.5890e-04, 3.9362e-04, 1.3664e-06, 9.6896e-05,
        3.2090e-04, 3.6484e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,103][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.5290, 0.0417, 0.0629, 0.0986, 0.0760, 0.0316, 0.1017, 0.0584],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,105][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0841, 0.0053, 0.0019, 0.0459, 0.1021, 0.0547, 0.5950, 0.1110],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,106][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([3.8586e-01, 2.5487e-04, 1.5087e-03, 1.2252e-02, 2.0781e-02, 1.1621e-02,
        5.3920e-01, 2.8518e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,107][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.8765, 0.0112, 0.0330, 0.0284, 0.0075, 0.0045, 0.0380, 0.0009],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,108][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.7189, 0.0434, 0.0736, 0.0608, 0.0318, 0.0329, 0.0299, 0.0086],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,109][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.1023, 0.0041, 0.0101, 0.0223, 0.5076, 0.0987, 0.1918, 0.0631],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,110][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0932, 0.0013, 0.0257, 0.0126, 0.0361, 0.2991, 0.4789, 0.0531],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,111][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2383, 0.0056, 0.0456, 0.1100, 0.0926, 0.0268, 0.3794, 0.1016],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,112][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.7186, 0.0062, 0.0095, 0.0575, 0.0866, 0.0317, 0.0419, 0.0480],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,113][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.5715, 0.0027, 0.0096, 0.0253, 0.0556, 0.0722, 0.0929, 0.1703],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,114][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.3862, 0.0410, 0.0586, 0.0610, 0.0364, 0.1593, 0.1636, 0.0939],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,115][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([9.3346e-01, 1.0765e-04, 1.9532e-04, 1.1806e-04, 9.5376e-07, 1.7573e-04,
        3.9550e-04, 1.0961e-03, 6.4451e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,116][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.5299, 0.0167, 0.0450, 0.0853, 0.0638, 0.0254, 0.0922, 0.0817, 0.0599],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,117][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([3.8302e-02, 1.5254e-03, 2.2838e-04, 7.8310e-03, 1.0788e-01, 4.6999e-02,
        4.9913e-02, 7.2414e-01, 2.3182e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,118][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.4877e-01, 4.5746e-05, 5.9806e-04, 1.3505e-03, 3.4625e-03, 3.7074e-02,
        5.3145e-02, 7.1173e-01, 4.3827e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,119][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([9.3518e-01, 1.9952e-03, 5.4636e-03, 7.4518e-03, 2.8488e-03, 1.4446e-02,
        4.4965e-03, 2.7489e-02, 6.2924e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,120][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.7375, 0.0227, 0.0465, 0.0482, 0.0363, 0.0333, 0.0282, 0.0352, 0.0122],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,121][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0439, 0.0011, 0.0025, 0.0101, 0.4940, 0.1293, 0.0588, 0.2167, 0.0435],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,123][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0839, 0.0006, 0.0089, 0.0072, 0.0150, 0.0913, 0.3096, 0.2454, 0.2381],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,124][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1689, 0.0052, 0.0320, 0.1990, 0.0624, 0.0168, 0.2191, 0.1657, 0.1308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,125][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.4820, 0.0061, 0.0148, 0.0485, 0.0777, 0.0392, 0.0890, 0.1477, 0.0951],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,126][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.4506, 0.0022, 0.0107, 0.0123, 0.0322, 0.0341, 0.1997, 0.1128, 0.1453],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,127][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2471, 0.0289, 0.0318, 0.0801, 0.0063, 0.0774, 0.3289, 0.1267, 0.0728],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,128][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ et] are: tensor([9.8055e-01, 8.2223e-04, 9.7309e-05, 3.0338e-04, 4.0814e-05, 8.3123e-06,
        3.4644e-04, 1.1927e-06, 3.2201e-04, 1.7513e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,129][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.4562, 0.0492, 0.0558, 0.1193, 0.0241, 0.0143, 0.0852, 0.0330, 0.0664,
        0.0965], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,130][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0236, 0.0009, 0.0008, 0.0099, 0.0202, 0.0251, 0.0381, 0.6814, 0.0792,
        0.1208], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,131][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ et] are: tensor([7.8231e-01, 4.7585e-04, 1.3791e-03, 8.8141e-03, 3.9068e-04, 2.2461e-03,
        1.7809e-02, 3.9978e-02, 1.1749e-01, 2.9103e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,132][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.8591, 0.0039, 0.0197, 0.0076, 0.0059, 0.0051, 0.0492, 0.0108, 0.0374,
        0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,133][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.4356, 0.1034, 0.0815, 0.1244, 0.0416, 0.0803, 0.0593, 0.0259, 0.0326,
        0.0155], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,134][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0633, 0.0025, 0.0023, 0.0069, 0.2478, 0.0911, 0.0608, 0.3310, 0.1189,
        0.0753], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,135][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ et] are: tensor([2.2901e-02, 2.7273e-05, 1.4808e-03, 6.0977e-04, 7.4774e-04, 1.5895e-03,
        4.1444e-02, 2.0163e-01, 6.9901e-01, 3.0554e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,137][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.1459, 0.0023, 0.0090, 0.1717, 0.0158, 0.0052, 0.1344, 0.0468, 0.3236,
        0.1453], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,138][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.8059, 0.0014, 0.0029, 0.0410, 0.0118, 0.0133, 0.0379, 0.0380, 0.0292,
        0.0187], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,139][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.6258, 0.0036, 0.0092, 0.0283, 0.0224, 0.0274, 0.0400, 0.1133, 0.0633,
        0.0669], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,140][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2500, 0.0601, 0.0509, 0.0857, 0.0078, 0.0723, 0.1055, 0.1999, 0.0675,
        0.1002], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,141][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([8.8733e-01, 4.7694e-04, 2.9231e-03, 1.2465e-03, 1.7619e-05, 2.4089e-05,
        3.1172e-04, 1.1609e-05, 1.9102e-03, 2.1742e-05, 1.0573e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,142][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.4310, 0.0134, 0.0294, 0.0671, 0.0299, 0.0135, 0.0654, 0.0534, 0.0456,
        0.2151, 0.0362], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,143][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([3.2839e-02, 1.3853e-04, 8.7997e-05, 7.3818e-04, 5.4268e-03, 1.7436e-03,
        1.3901e-02, 3.8950e-02, 2.7542e-02, 8.2690e-01, 5.1733e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,144][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([4.8007e-01, 3.6494e-05, 5.0654e-04, 5.5460e-04, 6.9163e-04, 8.8314e-04,
        5.3961e-02, 3.3340e-02, 6.7112e-02, 3.3076e-01, 3.2080e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,145][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.8531, 0.0064, 0.0095, 0.0141, 0.0171, 0.0156, 0.0351, 0.0198, 0.0155,
        0.0108, 0.0031], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,146][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.5863, 0.0357, 0.0633, 0.0561, 0.0412, 0.0590, 0.0463, 0.0346, 0.0280,
        0.0365, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,148][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.3253, 0.0074, 0.0132, 0.0075, 0.0876, 0.0342, 0.0817, 0.0777, 0.0692,
        0.1496, 0.1467], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,149][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([3.6076e-02, 3.5940e-05, 4.9163e-04, 3.0725e-04, 8.0553e-04, 6.1335e-03,
        8.6748e-02, 7.4376e-02, 2.9293e-01, 4.6186e-01, 4.0235e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,150][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([1.2756e-01, 3.0079e-04, 1.4911e-03, 1.2073e-02, 4.0281e-03, 6.8116e-03,
        3.4057e-02, 9.2477e-02, 9.6337e-02, 5.7360e-01, 5.1269e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,151][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.6823, 0.0016, 0.0016, 0.0236, 0.0212, 0.0107, 0.0119, 0.0114, 0.0238,
        0.1846, 0.0273], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,152][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.5620, 0.0029, 0.0058, 0.0065, 0.0228, 0.0286, 0.0211, 0.0618, 0.0310,
        0.0925, 0.1650], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,153][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.5419, 0.0160, 0.0180, 0.0253, 0.0113, 0.0194, 0.0407, 0.0703, 0.0475,
        0.1751, 0.0345], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,154][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([5.2731e-01, 3.7976e-04, 1.2539e-03, 1.2793e-03, 1.7819e-05, 1.2835e-03,
        3.3309e-03, 4.0377e-03, 3.5325e-01, 2.2539e-04, 7.4390e-04, 1.0689e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,155][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4027, 0.0131, 0.0351, 0.0558, 0.0560, 0.0132, 0.0656, 0.0512, 0.0330,
        0.1636, 0.0524, 0.0584], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,156][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([6.9240e-03, 8.5876e-05, 3.4826e-05, 7.3521e-04, 7.9796e-03, 4.0180e-03,
        3.7425e-03, 7.9901e-02, 4.4076e-03, 2.5510e-01, 5.8887e-01, 4.8194e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,157][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.2043e-01, 1.0958e-05, 1.1695e-04, 1.4652e-04, 3.4354e-04, 5.2947e-03,
        8.0144e-03, 8.7002e-02, 1.2521e-02, 2.3815e-01, 3.3385e-01, 1.9412e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,159][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.8050, 0.0058, 0.0158, 0.0129, 0.0055, 0.0328, 0.0090, 0.0656, 0.0018,
        0.0287, 0.0143, 0.0027], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,160][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.6858, 0.0166, 0.0364, 0.0508, 0.0405, 0.0313, 0.0290, 0.0346, 0.0142,
        0.0295, 0.0237, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,161][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([1.9001e-02, 2.0721e-04, 4.5270e-04, 1.2699e-03, 8.1062e-02, 3.6012e-02,
        1.0372e-02, 4.8962e-02, 8.4998e-03, 4.6191e-02, 7.1262e-01, 3.5354e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,162][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([3.8715e-02, 7.3165e-05, 1.2288e-03, 5.3627e-04, 1.0896e-03, 1.0241e-02,
        4.0749e-02, 5.8443e-02, 4.2358e-02, 1.1648e-01, 3.3982e-01, 3.5026e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,163][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([6.1326e-02, 4.8832e-04, 2.1781e-03, 1.8935e-02, 5.1295e-03, 1.0518e-03,
        3.2404e-02, 2.0656e-02, 2.2221e-02, 5.2345e-01, 2.2731e-01, 8.4854e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,164][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.4765, 0.0025, 0.0040, 0.0245, 0.0378, 0.0171, 0.0253, 0.0365, 0.0403,
        0.1288, 0.1295, 0.0770], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,165][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.3341, 0.0014, 0.0060, 0.0081, 0.0122, 0.0168, 0.0805, 0.0606, 0.1085,
        0.0680, 0.0832, 0.2205], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,166][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2339, 0.0150, 0.0135, 0.0391, 0.0051, 0.0178, 0.0736, 0.0597, 0.0203,
        0.2825, 0.1764, 0.0632], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,168][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ is] are: tensor([8.9044e-01, 3.2334e-03, 3.3907e-04, 1.0407e-03, 3.0346e-05, 6.8620e-05,
        3.7747e-04, 4.2190e-05, 1.4641e-03, 7.4353e-04, 1.2474e-04, 7.3312e-04,
        1.0136e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,169][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.5763, 0.0069, 0.0231, 0.0108, 0.0067, 0.0018, 0.0115, 0.0431, 0.0107,
        0.1729, 0.0198, 0.0186, 0.0977], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,170][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0732, 0.0025, 0.0007, 0.0133, 0.0179, 0.0028, 0.0071, 0.0766, 0.0251,
        0.1746, 0.4395, 0.1191, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,171][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ is] are: tensor([8.2196e-01, 2.4695e-04, 8.2973e-04, 5.2134e-04, 7.9660e-05, 2.4075e-04,
        4.8805e-04, 1.0514e-02, 8.9655e-03, 1.9436e-02, 2.0113e-02, 6.1903e-02,
        5.4704e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,172][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.8061, 0.0026, 0.0067, 0.0066, 0.0061, 0.0041, 0.0096, 0.0222, 0.0186,
        0.0345, 0.0066, 0.0164, 0.0600], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,173][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.2765, 0.1945, 0.0754, 0.2177, 0.0352, 0.0284, 0.0350, 0.0296, 0.0230,
        0.0208, 0.0293, 0.0104, 0.0241], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,174][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0945, 0.0267, 0.0053, 0.0041, 0.1283, 0.0286, 0.0165, 0.0818, 0.0221,
        0.1379, 0.2654, 0.0766, 0.1122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,175][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ is] are: tensor([2.6535e-02, 3.5457e-05, 8.1629e-04, 2.2327e-04, 3.3475e-04, 6.7306e-04,
        7.4700e-03, 3.0070e-02, 8.7536e-02, 8.3039e-02, 1.5771e-01, 5.6474e-01,
        4.0819e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,177][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.2760, 0.0015, 0.0039, 0.0345, 0.0008, 0.0021, 0.0081, 0.0247, 0.0166,
        0.3415, 0.0808, 0.0452, 0.1642], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,178][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.8172, 0.0159, 0.0208, 0.0267, 0.0011, 0.0020, 0.0028, 0.0023, 0.0043,
        0.0141, 0.0104, 0.0054, 0.0770], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,179][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.6559, 0.0084, 0.0099, 0.0125, 0.0090, 0.0051, 0.0138, 0.0149, 0.0143,
        0.0384, 0.0426, 0.0307, 0.1444], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,180][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2282, 0.4051, 0.1497, 0.0395, 0.0006, 0.0029, 0.0117, 0.0073, 0.0078,
        0.0371, 0.0429, 0.0124, 0.0548], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,181][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.3435e-01, 2.3921e-03, 2.4884e-04, 5.6363e-04, 1.7548e-05, 5.2280e-05,
        3.0374e-04, 1.3344e-04, 1.1048e-03, 2.3675e-04, 2.3863e-05, 8.3308e-04,
        8.0203e-03, 5.1718e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,182][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4571, 0.0078, 0.0153, 0.0066, 0.0023, 0.0018, 0.0065, 0.0298, 0.0085,
        0.0828, 0.0266, 0.0122, 0.1144, 0.2283], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,184][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.3283e-01, 4.2495e-04, 4.9543e-04, 3.9637e-03, 1.8793e-03, 7.0458e-04,
        3.8099e-03, 1.5692e-02, 8.4596e-03, 2.4989e-02, 5.8180e-02, 3.8417e-02,
        4.3174e-01, 2.7841e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,185][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2410e-01, 2.9572e-04, 2.7289e-04, 2.3094e-05, 2.0153e-05, 1.7239e-05,
        3.4605e-04, 8.2848e-04, 1.4070e-03, 4.6997e-03, 2.4762e-03, 1.2266e-02,
        2.6305e-01, 9.0201e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,186][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.8485, 0.0015, 0.0163, 0.0048, 0.0028, 0.0021, 0.0054, 0.0097, 0.0063,
        0.0227, 0.0044, 0.0089, 0.0416, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,187][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2446, 0.1554, 0.1063, 0.2700, 0.0179, 0.0129, 0.0252, 0.0200, 0.0197,
        0.0256, 0.0209, 0.0114, 0.0421, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,188][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2154, 0.0221, 0.0271, 0.0070, 0.0550, 0.0047, 0.0144, 0.0390, 0.0222,
        0.0695, 0.0712, 0.0548, 0.2648, 0.1328], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,189][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2766, 0.0013, 0.0103, 0.0005, 0.0005, 0.0005, 0.0044, 0.0087, 0.0281,
        0.0302, 0.0542, 0.1301, 0.3280, 0.1268], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,190][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([2.1708e-01, 5.5757e-04, 3.2638e-03, 5.8079e-03, 3.3764e-04, 7.3118e-04,
        2.1930e-03, 9.2783e-03, 5.2619e-03, 7.7444e-02, 5.6606e-02, 1.9991e-02,
        5.2492e-01, 7.6528e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,192][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4319, 0.0406, 0.0321, 0.0144, 0.0009, 0.0015, 0.0036, 0.0057, 0.0023,
        0.0233, 0.0070, 0.0031, 0.3294, 0.1042], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,193][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.6592, 0.0052, 0.0086, 0.0037, 0.0063, 0.0032, 0.0069, 0.0123, 0.0063,
        0.0160, 0.0236, 0.0125, 0.0736, 0.1627], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,194][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([4.3234e-01, 1.5193e-01, 7.3190e-02, 3.2572e-02, 2.9498e-04, 3.0240e-03,
        4.6612e-03, 1.1915e-02, 3.7014e-03, 2.4846e-02, 2.6062e-02, 8.2365e-03,
        1.4552e-01, 8.1701e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,195][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ same] are: tensor([7.8949e-01, 7.6104e-04, 3.4155e-03, 7.2892e-04, 1.5375e-05, 2.9740e-05,
        5.8695e-05, 6.9176e-05, 3.9685e-04, 1.0009e-04, 7.7442e-05, 1.8932e-04,
        6.5756e-04, 1.8319e-03, 2.0218e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,196][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ same] are: tensor([5.3625e-01, 4.2116e-03, 2.9569e-03, 3.2077e-03, 1.0982e-03, 3.8543e-04,
        7.8399e-04, 7.9986e-03, 2.3388e-03, 1.9840e-02, 5.5269e-03, 4.9321e-03,
        1.2977e-01, 2.2969e-01, 5.1013e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,197][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ same] are: tensor([4.7943e-02, 2.9400e-04, 5.4311e-04, 6.9331e-03, 4.2278e-04, 2.7663e-04,
        4.1235e-04, 2.9064e-03, 1.1694e-03, 1.7153e-02, 1.4831e-02, 5.4848e-03,
        2.7040e-01, 5.3632e-01, 9.4910e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,199][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ same] are: tensor([5.4062e-01, 6.9656e-04, 1.3591e-04, 1.3071e-04, 1.3276e-04, 2.2307e-05,
        6.5236e-05, 6.2852e-04, 8.0121e-04, 3.0269e-03, 3.1007e-03, 5.0401e-03,
        2.1188e-01, 1.9940e-01, 3.4314e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,200][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.8875, 0.0042, 0.0177, 0.0043, 0.0026, 0.0009, 0.0067, 0.0098, 0.0054,
        0.0116, 0.0055, 0.0065, 0.0147, 0.0214, 0.0012], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,201][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.3266, 0.0755, 0.0546, 0.1972, 0.0293, 0.0095, 0.0363, 0.0194, 0.0288,
        0.0547, 0.0405, 0.0173, 0.0557, 0.0409, 0.0136], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,202][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1466, 0.0158, 0.0079, 0.0020, 0.0173, 0.0011, 0.0050, 0.0066, 0.0059,
        0.0243, 0.0186, 0.0080, 0.1525, 0.4019, 0.1862], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,203][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ same] are: tensor([2.9768e-01, 8.8475e-04, 5.4154e-03, 3.9427e-04, 1.1279e-03, 4.4642e-04,
        8.6512e-04, 2.4386e-03, 9.4366e-03, 2.2660e-02, 3.1803e-02, 4.9550e-02,
        4.1062e-01, 1.0130e-01, 6.5377e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,204][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ same] are: tensor([1.5572e-01, 3.4072e-04, 2.7576e-04, 4.6099e-03, 1.2043e-03, 6.7084e-04,
        2.4383e-03, 1.0993e-02, 8.1832e-03, 1.9444e-01, 6.3382e-02, 2.2646e-02,
        3.6991e-01, 1.0774e-01, 5.7452e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,206][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.2641, 0.0049, 0.0051, 0.0042, 0.0005, 0.0011, 0.0020, 0.0093, 0.0027,
        0.0204, 0.0061, 0.0054, 0.4597, 0.0953, 0.1191], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,207][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.7090, 0.0056, 0.0088, 0.0026, 0.0028, 0.0008, 0.0052, 0.0059, 0.0055,
        0.0200, 0.0081, 0.0136, 0.0267, 0.0232, 0.1620], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,208][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.4858, 0.0249, 0.0073, 0.0080, 0.0005, 0.0005, 0.0044, 0.0045, 0.0025,
        0.0040, 0.0075, 0.0061, 0.1285, 0.1065, 0.2087], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,209][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ as] are: tensor([9.4613e-01, 1.0658e-03, 1.2531e-03, 3.3329e-04, 6.4079e-06, 3.7966e-05,
        1.1545e-04, 2.7893e-04, 7.2723e-05, 7.2437e-05, 7.6123e-05, 1.0441e-04,
        7.6355e-04, 4.6628e-03, 9.6943e-03, 3.5333e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,210][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.4603, 0.0015, 0.0019, 0.0036, 0.0019, 0.0011, 0.0021, 0.0058, 0.0040,
        0.0189, 0.0083, 0.0065, 0.1097, 0.1989, 0.0791, 0.0965],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,212][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ as] are: tensor([7.7078e-02, 4.2222e-05, 1.0498e-04, 3.8521e-04, 2.8412e-04, 4.2987e-04,
        1.3502e-04, 2.5543e-03, 4.7679e-04, 9.4670e-03, 1.8149e-02, 1.7407e-03,
        2.2925e-01, 5.1066e-01, 1.2440e-01, 2.4849e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,213][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ as] are: tensor([1.8488e-01, 1.1052e-05, 2.7998e-05, 6.0490e-05, 4.8178e-05, 3.0264e-05,
        1.5707e-04, 1.2266e-04, 5.1336e-04, 4.8472e-04, 9.0465e-04, 1.8699e-03,
        4.3223e-02, 1.4485e-01, 4.7632e-01, 1.4650e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,214][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ as] are: tensor([9.2334e-01, 2.3164e-03, 2.5732e-03, 3.3310e-03, 7.6685e-04, 1.0173e-03,
        2.9011e-03, 1.7745e-03, 2.0899e-03, 1.0582e-02, 9.3873e-04, 3.3101e-03,
        1.9705e-02, 1.4104e-02, 6.0650e-03, 5.1796e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,215][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.4372, 0.0964, 0.0633, 0.1285, 0.0122, 0.0103, 0.0169, 0.0110, 0.0063,
        0.0288, 0.0115, 0.0062, 0.0373, 0.0607, 0.0293, 0.0441],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,216][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.2457, 0.0083, 0.0029, 0.0022, 0.0229, 0.0022, 0.0064, 0.0074, 0.0109,
        0.0194, 0.0295, 0.0246, 0.2054, 0.2533, 0.0678, 0.0912],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,218][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ as] are: tensor([9.2378e-02, 3.6425e-04, 1.6835e-03, 1.7772e-04, 3.6219e-04, 5.3170e-04,
        9.2484e-04, 2.6673e-03, 1.9045e-03, 4.4269e-03, 1.0069e-02, 6.8229e-03,
        7.0047e-02, 1.3961e-01, 5.9130e-01, 7.6727e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,219][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ as] are: tensor([1.4727e-01, 1.8633e-04, 7.8120e-05, 8.4127e-04, 3.7767e-04, 2.8735e-04,
        1.2277e-03, 5.0883e-03, 2.6398e-03, 9.4990e-02, 5.7184e-03, 8.4249e-03,
        3.1487e-01, 1.1945e-01, 8.4013e-02, 2.1454e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,220][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ as] are: tensor([1.5589e-01, 7.2915e-04, 1.2511e-03, 1.7187e-03, 1.7070e-04, 8.0204e-05,
        2.9666e-04, 1.4476e-03, 3.8253e-04, 8.6168e-04, 1.4419e-03, 6.8544e-04,
        1.7642e-01, 8.8135e-02, 5.4695e-01, 2.3545e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,221][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.6291, 0.0070, 0.0047, 0.0025, 0.0035, 0.0014, 0.0057, 0.0051, 0.0079,
        0.0093, 0.0066, 0.0107, 0.0233, 0.0361, 0.0497, 0.1975],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,222][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.2883, 0.0063, 0.0009, 0.0035, 0.0016, 0.0005, 0.0015, 0.0036, 0.0048,
        0.0067, 0.0167, 0.0119, 0.0811, 0.1678, 0.3262, 0.0787],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,224][circuit_into_ebeddingspace.py][line:2310][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.9149e-01, 3.1764e-03, 4.9248e-04, 1.0256e-03, 2.6919e-05, 1.5991e-04,
        4.1111e-04, 2.9701e-04, 1.8516e-03, 2.3837e-04, 2.9916e-05, 1.3370e-03,
        5.3489e-03, 6.0130e-02, 4.9799e-04, 2.5815e-03, 3.0905e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,225][circuit_into_ebeddingspace.py][line:2313][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.3875, 0.0021, 0.0022, 0.0016, 0.0021, 0.0006, 0.0022, 0.0040, 0.0028,
        0.0134, 0.0068, 0.0043, 0.0583, 0.0620, 0.0601, 0.1689, 0.2212],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,226][circuit_into_ebeddingspace.py][line:2316][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.3770e-01, 2.6853e-05, 2.6637e-05, 4.4227e-04, 6.5779e-04, 4.4851e-04,
        6.9212e-04, 2.0675e-03, 6.7667e-04, 5.7956e-03, 5.3158e-03, 2.8294e-03,
        1.0339e-01, 1.2466e-02, 6.1624e-02, 2.3881e-01, 3.2703e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,227][circuit_into_ebeddingspace.py][line:2319][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([4.1933e-01, 2.5385e-05, 2.2721e-05, 3.8960e-06, 2.4880e-05, 1.9091e-05,
        7.0038e-05, 1.5863e-04, 1.4829e-04, 5.3519e-04, 7.5389e-04, 1.2329e-03,
        1.0892e-02, 3.4136e-03, 6.0110e-02, 4.3218e-01, 7.1078e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,228][circuit_into_ebeddingspace.py][line:2322][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.8340, 0.0023, 0.0231, 0.0052, 0.0037, 0.0020, 0.0050, 0.0059, 0.0042,
        0.0138, 0.0042, 0.0065, 0.0250, 0.0268, 0.0055, 0.0082, 0.0247],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,230][circuit_into_ebeddingspace.py][line:2325][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.3393, 0.0670, 0.1164, 0.0778, 0.0244, 0.0201, 0.0412, 0.0209, 0.0297,
        0.0240, 0.0332, 0.0178, 0.0489, 0.0315, 0.0232, 0.0559, 0.0286],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,231][circuit_into_ebeddingspace.py][line:2328][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.3269, 0.0029, 0.0030, 0.0010, 0.0254, 0.0026, 0.0053, 0.0091, 0.0062,
        0.0175, 0.0169, 0.0121, 0.0679, 0.0176, 0.0915, 0.1878, 0.2063],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,232][circuit_into_ebeddingspace.py][line:2331][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([2.4178e-01, 8.1074e-05, 5.0060e-04, 4.7957e-05, 3.2729e-04, 3.6638e-04,
        1.0270e-03, 2.3410e-03, 3.2817e-03, 3.9697e-03, 9.6010e-03, 1.2055e-02,
        6.0567e-02, 8.9324e-03, 2.7550e-01, 2.3284e-01, 1.4678e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,233][circuit_into_ebeddingspace.py][line:2334][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.4667e-01, 1.5916e-05, 6.8143e-05, 3.2916e-04, 1.0372e-04, 1.4732e-04,
        2.4078e-04, 8.2518e-04, 4.7145e-04, 8.2070e-03, 4.4489e-03, 2.0148e-03,
        5.1758e-02, 3.2031e-03, 4.1142e-02, 6.9692e-01, 4.3437e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,234][circuit_into_ebeddingspace.py][line:2337][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.7437e-01, 1.9465e-03, 2.0309e-03, 1.8146e-03, 6.6493e-04, 2.4640e-04,
        8.2313e-04, 1.8486e-03, 5.9302e-04, 3.4408e-03, 1.9579e-03, 8.9258e-04,
        7.3799e-02, 2.2659e-02, 5.4495e-01, 1.0199e-01, 6.5971e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,236][circuit_into_ebeddingspace.py][line:2340][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.6032, 0.0018, 0.0029, 0.0018, 0.0055, 0.0022, 0.0030, 0.0059, 0.0031,
        0.0060, 0.0106, 0.0050, 0.0146, 0.0423, 0.0548, 0.0682, 0.1689],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,237][circuit_into_ebeddingspace.py][line:2343][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.8853e-01, 3.2482e-03, 1.7310e-03, 1.3394e-03, 1.9240e-04, 2.5268e-04,
        7.6977e-04, 1.0775e-03, 7.5519e-04, 2.8735e-03, 2.3296e-03, 1.7133e-03,
        1.2950e-02, 6.6209e-03, 4.2036e-01, 3.1830e-01, 3.6956e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,245][circuit_into_ebeddingspace.py][line:2286][INFO] ################4-th layer#################
[2024-06-27 21:00:53,248][circuit_into_ebeddingspace.py][line:2288][INFO] ##4-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1]]], device='cuda:0')
[2024-06-27 21:00:53,249][circuit_into_ebeddingspace.py][line:2289][INFO] ##4-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,250][circuit_into_ebeddingspace.py][line:2290][INFO] ##4-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,252][circuit_into_ebeddingspace.py][line:2291][INFO] ##4-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,253][circuit_into_ebeddingspace.py][line:2292][INFO] ##4-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,255][circuit_into_ebeddingspace.py][line:2293][INFO] ##4-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [0, 1]]], device='cuda:0')
[2024-06-27 21:00:53,256][circuit_into_ebeddingspace.py][line:2294][INFO] ##4-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,257][circuit_into_ebeddingspace.py][line:2295][INFO] ##4-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,259][circuit_into_ebeddingspace.py][line:2296][INFO] ##4-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [1, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,260][circuit_into_ebeddingspace.py][line:2297][INFO] ##4-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,261][circuit_into_ebeddingspace.py][line:2298][INFO] ##4-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,263][circuit_into_ebeddingspace.py][line:2299][INFO] ##4-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [0, 0],
         [1, 1],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,264][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,265][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,266][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,267][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,268][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,268][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,269][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,270][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,271][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,272][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,273][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,274][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,275][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9863, 0.0137], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,276][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9548, 0.0452], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,277][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9543, 0.0457], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,278][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9834, 0.0166], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,279][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9443, 0.0557], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,280][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.8612, 0.1388], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,280][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9649, 0.0351], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,281][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.8738, 0.1262], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,282][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9799, 0.0201], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,283][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9474, 0.0526], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,284][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9382, 0.0618], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,285][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9131, 0.0869], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,286][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.8367, 0.1149, 0.0484], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,287][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8965, 0.0782, 0.0253], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,288][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.8809, 0.0244, 0.0947], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,289][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7417, 0.0674, 0.1910], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,290][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8604, 0.1102, 0.0294], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,291][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.5448, 0.4202, 0.0350], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,292][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.8707, 0.1007, 0.0286], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,293][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.3413, 0.0938, 0.5649], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,294][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9677, 0.0112, 0.0211], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,295][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.7817, 0.1594, 0.0589], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,296][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.9519, 0.0262, 0.0219], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,297][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ language] are: tensor([5.2296e-07, 1.0000e+00, 1.8939e-06], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,298][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.6017, 0.2279, 0.1443, 0.0261], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,299][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.2838, 0.1848, 0.4822, 0.0492], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,300][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.8667, 0.0450, 0.0653, 0.0230], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,301][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1735, 0.0251, 0.7745, 0.0269], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,302][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.8386, 0.0246, 0.0852, 0.0515], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,303][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.4232, 0.4029, 0.1360, 0.0379], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,304][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.7103, 0.0580, 0.1633, 0.0683], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,305][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3722, 0.0239, 0.1034, 0.5005], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,306][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.8805, 0.0190, 0.0562, 0.0443], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,307][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.7693, 0.1769, 0.0335, 0.0204], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,308][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.9505, 0.0048, 0.0089, 0.0358], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,309][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ of] are: tensor([3.2939e-05, 4.4127e-10, 9.9995e-01, 1.2665e-05], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,310][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.2088, 0.0175, 0.1868, 0.5750, 0.0119], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,311][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.3795, 0.2269, 0.2539, 0.1172, 0.0225], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,312][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.8736, 0.0495, 0.0388, 0.0296, 0.0084], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,313][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.7013, 0.0054, 0.0980, 0.0615, 0.1338], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,314][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9382, 0.0083, 0.0042, 0.0224, 0.0269], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,315][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.5998, 0.0112, 0.0632, 0.2688, 0.0571], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,316][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.7690, 0.0368, 0.0630, 0.1006, 0.0307], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,317][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2794, 0.0019, 0.0049, 0.0901, 0.6238], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,318][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.9694, 0.0035, 0.0038, 0.0152, 0.0081], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,319][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.7138, 0.0391, 0.1589, 0.0655, 0.0227], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,320][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.9568, 0.0094, 0.0106, 0.0196, 0.0035], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,321][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ De] are: tensor([2.6435e-05, 3.1279e-09, 1.0488e-09, 9.9996e-01, 1.1350e-05],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,322][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.4042, 0.0012, 0.0014, 0.0354, 0.5429, 0.0148], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,323][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4580, 0.1196, 0.0357, 0.0527, 0.2248, 0.1092], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,324][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.8804, 0.0133, 0.0227, 0.0428, 0.0160, 0.0249], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,325][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.5269, 0.0020, 0.0029, 0.0031, 0.4306, 0.0346], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,326][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.8638, 0.0011, 0.0012, 0.0062, 0.0474, 0.0803], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,327][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.1933, 0.0046, 0.0075, 0.0498, 0.7170, 0.0279], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,328][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.6734, 0.0052, 0.0209, 0.0248, 0.2363, 0.0395], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,329][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.3694, 0.0159, 0.0048, 0.0110, 0.3457, 0.2532], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,330][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.9674, 0.0012, 0.0127, 0.0091, 0.0047, 0.0049], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,331][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.7804, 0.0144, 0.0934, 0.0338, 0.0588, 0.0192], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,332][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.9715, 0.0051, 0.0042, 0.0120, 0.0017, 0.0055], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,333][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([6.7643e-11, 1.5502e-11, 2.0730e-18, 2.9694e-16, 9.9936e-01, 6.3741e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,334][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.3890, 0.0128, 0.0372, 0.2940, 0.0314, 0.1914, 0.0442],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,335][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.5069, 0.0284, 0.1458, 0.0970, 0.0875, 0.0300, 0.1044],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,336][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.8627, 0.0110, 0.0265, 0.0257, 0.0222, 0.0377, 0.0142],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,337][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.4922, 0.0058, 0.0294, 0.0145, 0.2598, 0.0922, 0.1061],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,338][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.8653, 0.0040, 0.0103, 0.0252, 0.0394, 0.0461, 0.0098],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,339][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.2388, 0.0051, 0.0140, 0.3292, 0.3211, 0.0116, 0.0803],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,340][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.6976, 0.0135, 0.0386, 0.0492, 0.0599, 0.1173, 0.0238],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,341][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2443, 0.0035, 0.0085, 0.0155, 0.0552, 0.2292, 0.4438],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,342][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.9122, 0.0021, 0.0068, 0.0165, 0.0140, 0.0186, 0.0299],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,343][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.7137, 0.0209, 0.0368, 0.0755, 0.0556, 0.0782, 0.0194],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,344][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([9.7616e-01, 3.0071e-03, 4.7207e-03, 1.1175e-02, 5.4247e-04, 4.4947e-04,
        3.9452e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,345][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([3.6417e-15, 1.2013e-21, 2.7439e-21, 8.4629e-21, 9.2170e-10, 1.0000e+00,
        2.0793e-09], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,346][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.6474, 0.0074, 0.0335, 0.1689, 0.0415, 0.0159, 0.0631, 0.0223],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,347][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.5508, 0.0396, 0.0201, 0.0489, 0.0489, 0.0135, 0.1242, 0.1539],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,348][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.8139, 0.0097, 0.0515, 0.0412, 0.0176, 0.0075, 0.0416, 0.0170],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,349][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.3607, 0.0081, 0.0269, 0.0233, 0.2633, 0.0616, 0.2045, 0.0516],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,350][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.8667, 0.0043, 0.0079, 0.0243, 0.0099, 0.0441, 0.0058, 0.0368],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,352][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.2815, 0.0064, 0.0492, 0.1612, 0.1857, 0.0401, 0.2255, 0.0505],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,353][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.5444, 0.0271, 0.0434, 0.0580, 0.0774, 0.0867, 0.1368, 0.0262],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,354][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.3000, 0.0042, 0.0127, 0.0860, 0.0913, 0.0499, 0.0686, 0.3872],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,355][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.8384, 0.0028, 0.0087, 0.0136, 0.0062, 0.0163, 0.0952, 0.0187],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,356][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.6364, 0.0238, 0.0953, 0.1051, 0.0284, 0.0266, 0.0483, 0.0361],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,357][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.9655, 0.0028, 0.0015, 0.0066, 0.0045, 0.0037, 0.0103, 0.0052],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,358][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([4.1695e-07, 4.1903e-13, 6.8848e-08, 2.0639e-09, 3.8362e-09, 1.9804e-08,
        9.9970e-01, 2.9470e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,359][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.4736, 0.0114, 0.0371, 0.1783, 0.0124, 0.0355, 0.0994, 0.0891, 0.0633],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,360][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4232, 0.0133, 0.0553, 0.1547, 0.0175, 0.0129, 0.0931, 0.1173, 0.1127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,361][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.7509, 0.0299, 0.0623, 0.0342, 0.0152, 0.0109, 0.0432, 0.0247, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,362][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2179, 0.0051, 0.0077, 0.0071, 0.3099, 0.0770, 0.1807, 0.1364, 0.0583],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,363][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.8511, 0.0047, 0.0036, 0.0109, 0.0154, 0.0255, 0.0096, 0.0529, 0.0264],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,364][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.4297, 0.0079, 0.0258, 0.2093, 0.1318, 0.0100, 0.0845, 0.0398, 0.0611],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,365][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.5119, 0.0220, 0.0272, 0.0396, 0.0477, 0.0916, 0.0561, 0.1754, 0.0285],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,366][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1346, 0.0036, 0.0057, 0.0128, 0.0217, 0.0203, 0.1429, 0.0797, 0.5787],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,368][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8687, 0.0020, 0.0091, 0.0122, 0.0036, 0.0107, 0.0528, 0.0209, 0.0200],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,369][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.7289, 0.0206, 0.0300, 0.0881, 0.0183, 0.0341, 0.0134, 0.0411, 0.0256],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,370][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.9519, 0.0019, 0.0014, 0.0085, 0.0012, 0.0025, 0.0154, 0.0080, 0.0091],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,371][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [orum] are: tensor([1.3698e-14, 4.0893e-21, 2.3925e-17, 1.0524e-21, 2.4669e-16, 4.9110e-07,
        1.1521e-07, 1.0000e+00, 1.1605e-07], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,372][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.5500, 0.0045, 0.0104, 0.1677, 0.0067, 0.0089, 0.0535, 0.0197, 0.1221,
        0.0566], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,373][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.6028, 0.0196, 0.0229, 0.1080, 0.0192, 0.0153, 0.0350, 0.0461, 0.0513,
        0.0799], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,374][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.6488, 0.0404, 0.0951, 0.0463, 0.0147, 0.0135, 0.0215, 0.0269, 0.0615,
        0.0312], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,375][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.1663, 0.0072, 0.0248, 0.0307, 0.0904, 0.0805, 0.0926, 0.1542, 0.2901,
        0.0633], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,376][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.6181, 0.0165, 0.0139, 0.0068, 0.0261, 0.0070, 0.0249, 0.0212, 0.0270,
        0.2384], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,377][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.4220, 0.0073, 0.0229, 0.3425, 0.0399, 0.0039, 0.0682, 0.0164, 0.0496,
        0.0273], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,378][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.3363, 0.0305, 0.0949, 0.0622, 0.0374, 0.0375, 0.0563, 0.1327, 0.1728,
        0.0395], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,379][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1381, 0.0021, 0.0023, 0.0191, 0.0108, 0.0044, 0.0206, 0.1594, 0.1560,
        0.4871], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,381][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.4488, 0.0041, 0.0028, 0.0072, 0.0025, 0.0174, 0.1625, 0.0847, 0.2445,
        0.0255], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,382][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.6370, 0.0458, 0.0539, 0.1060, 0.0367, 0.0134, 0.0158, 0.0327, 0.0433,
        0.0155], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,383][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ et] are: tensor([9.4965e-01, 6.2302e-03, 9.4193e-03, 7.9837e-03, 1.8973e-03, 4.3652e-04,
        2.4405e-03, 3.3638e-03, 3.7479e-03, 1.4831e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,384][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ et] are: tensor([3.5279e-12, 4.1914e-21, 4.2342e-15, 5.3995e-18, 1.2997e-22, 1.6804e-19,
        2.0397e-08, 4.5937e-07, 1.0000e+00, 3.6691e-07], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,385][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.6868, 0.0034, 0.0040, 0.0614, 0.0072, 0.0035, 0.0240, 0.0188, 0.0296,
        0.1495, 0.0118], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,386][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.1948, 0.0151, 0.0035, 0.0175, 0.0068, 0.0041, 0.0260, 0.0292, 0.0456,
        0.6367, 0.0205], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,387][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.8299, 0.0123, 0.0183, 0.0290, 0.0171, 0.0027, 0.0103, 0.0191, 0.0199,
        0.0327, 0.0087], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,388][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.2561, 0.0051, 0.0290, 0.0073, 0.0224, 0.0410, 0.1278, 0.1209, 0.1741,
        0.1207, 0.0957], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,389][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.7596, 0.0023, 0.0026, 0.0106, 0.0256, 0.0471, 0.0058, 0.0275, 0.0100,
        0.0319, 0.0771], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,390][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.5806, 0.0053, 0.0170, 0.0373, 0.0234, 0.0073, 0.0338, 0.0291, 0.0576,
        0.1412, 0.0673], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,392][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.5637, 0.0168, 0.0256, 0.0291, 0.0107, 0.0402, 0.0311, 0.0833, 0.0577,
        0.1197, 0.0222], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,393][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.1281, 0.0025, 0.0022, 0.0026, 0.0207, 0.0115, 0.0048, 0.0944, 0.0108,
        0.1019, 0.6205], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,394][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.7814, 0.0011, 0.0021, 0.0101, 0.0021, 0.0295, 0.0648, 0.0196, 0.0324,
        0.0554, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,395][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.6382, 0.0051, 0.0275, 0.0627, 0.0225, 0.0263, 0.0225, 0.0477, 0.0469,
        0.0778, 0.0229], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,396][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.6525e-01, 6.2038e-03, 3.3204e-03, 7.6911e-03, 3.8723e-04, 2.2887e-04,
        8.9447e-03, 1.2109e-03, 3.3917e-03, 2.0016e-03, 1.3721e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,397][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([3.5707e-10, 1.4422e-19, 2.3814e-20, 1.0867e-17, 9.2919e-18, 2.1819e-18,
        6.8622e-15, 3.3074e-12, 9.3627e-11, 1.0000e+00, 6.3465e-08],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,398][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.6926, 0.0077, 0.0067, 0.0420, 0.0030, 0.0054, 0.0127, 0.0101, 0.0065,
        0.1513, 0.0440, 0.0180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,399][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.3323, 0.0036, 0.0216, 0.0358, 0.0036, 0.0028, 0.0153, 0.0379, 0.0360,
        0.2403, 0.1319, 0.1390], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,400][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.6230, 0.0180, 0.0669, 0.0504, 0.0317, 0.0060, 0.0320, 0.0211, 0.0290,
        0.0658, 0.0282, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,402][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2463, 0.0062, 0.0184, 0.0067, 0.0667, 0.0453, 0.1161, 0.0920, 0.0358,
        0.0677, 0.2296, 0.0691], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,403][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.6161, 0.0019, 0.0058, 0.0081, 0.0202, 0.0171, 0.0128, 0.0238, 0.0312,
        0.0903, 0.1268, 0.0459], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,404][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.7139, 0.0111, 0.0161, 0.0902, 0.0269, 0.0013, 0.0131, 0.0046, 0.0148,
        0.0463, 0.0262, 0.0356], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,405][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.3952, 0.0175, 0.0160, 0.0229, 0.0217, 0.0415, 0.0299, 0.1103, 0.0278,
        0.1726, 0.0981, 0.0465], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,406][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1071, 0.0014, 0.0028, 0.0034, 0.0067, 0.0191, 0.0318, 0.0633, 0.0877,
        0.0872, 0.2824, 0.3072], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,407][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8359, 0.0021, 0.0041, 0.0129, 0.0032, 0.0047, 0.0234, 0.0096, 0.0116,
        0.0649, 0.0045, 0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,408][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.7678, 0.0076, 0.0116, 0.0644, 0.0120, 0.0187, 0.0064, 0.0147, 0.0129,
        0.0557, 0.0182, 0.0099], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,409][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.9390, 0.0030, 0.0021, 0.0079, 0.0016, 0.0017, 0.0077, 0.0083, 0.0073,
        0.0057, 0.0063, 0.0094], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,411][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [orum] are: tensor([8.5929e-13, 5.5132e-22, 8.4509e-18, 8.8163e-22, 8.9516e-17, 1.3925e-09,
        7.9726e-10, 2.8452e-05, 1.0790e-10, 1.0594e-05, 9.9996e-01, 1.5459e-06],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,412][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.3330, 0.0082, 0.0108, 0.5832, 0.0029, 0.0006, 0.0028, 0.0024, 0.0064,
        0.0165, 0.0083, 0.0047, 0.0202], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,413][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.6210, 0.0079, 0.0045, 0.0129, 0.0019, 0.0009, 0.0058, 0.0213, 0.0067,
        0.1840, 0.0040, 0.0163, 0.1126], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,414][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7902, 0.0201, 0.0181, 0.0194, 0.0074, 0.0053, 0.0131, 0.0144, 0.0148,
        0.0357, 0.0188, 0.0096, 0.0332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,415][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0319, 0.1304, 0.4193, 0.2349, 0.0041, 0.0076, 0.0094, 0.0185, 0.0260,
        0.0154, 0.0391, 0.0425, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,416][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.7690, 0.0177, 0.0277, 0.0151, 0.0094, 0.0058, 0.0250, 0.0063, 0.0226,
        0.0111, 0.0050, 0.0278, 0.0574], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,417][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.2014, 0.2583, 0.0252, 0.4133, 0.0127, 0.0034, 0.0132, 0.0031, 0.0099,
        0.0051, 0.0159, 0.0106, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,419][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.2214, 0.0658, 0.0338, 0.0595, 0.0475, 0.0449, 0.0396, 0.1029, 0.0705,
        0.1401, 0.0676, 0.0592, 0.0471], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,420][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0727, 0.0024, 0.0173, 0.0565, 0.0022, 0.0022, 0.0108, 0.0420, 0.0960,
        0.1048, 0.0402, 0.1022, 0.4507], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,421][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.6904, 0.0072, 0.0190, 0.0325, 0.0024, 0.0116, 0.0566, 0.0171, 0.0222,
        0.0763, 0.0149, 0.0226, 0.0271], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,422][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.4779, 0.2085, 0.0451, 0.2247, 0.0030, 0.0055, 0.0029, 0.0039, 0.0042,
        0.0061, 0.0074, 0.0020, 0.0087], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,423][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ is] are: tensor([9.2902e-01, 6.5169e-03, 7.6874e-03, 1.4994e-02, 3.9694e-03, 6.9887e-04,
        3.7371e-03, 3.5454e-03, 3.5136e-03, 4.9788e-03, 2.5967e-03, 3.7066e-03,
        1.5038e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,424][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ is] are: tensor([1.9727e-15, 4.6530e-26, 2.6148e-20, 2.6687e-21, 3.9815e-23, 9.9442e-21,
        3.9758e-12, 2.4301e-11, 2.3702e-05, 4.4699e-06, 1.4116e-08, 9.9997e-01,
        1.2032e-08], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,425][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([6.0595e-01, 5.3257e-03, 3.0428e-03, 6.8985e-02, 5.0335e-04, 5.2922e-04,
        1.0366e-03, 1.9217e-03, 3.0734e-03, 1.3082e-02, 4.6748e-03, 2.6534e-03,
        1.6632e-01, 1.2290e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,427][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([3.4198e-01, 5.9997e-03, 3.2860e-03, 3.5653e-03, 8.0868e-04, 2.2550e-04,
        1.6121e-03, 1.9618e-02, 1.6569e-03, 2.5620e-02, 2.9604e-03, 1.6756e-03,
        3.8189e-01, 2.0911e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,428][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7187, 0.0286, 0.0206, 0.0136, 0.0064, 0.0043, 0.0180, 0.0092, 0.0121,
        0.0453, 0.0142, 0.0073, 0.0369, 0.0649], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,429][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0321, 0.2932, 0.4532, 0.1432, 0.0016, 0.0026, 0.0023, 0.0045, 0.0112,
        0.0103, 0.0100, 0.0122, 0.0195, 0.0040], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,430][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.7412, 0.0103, 0.0265, 0.0134, 0.0044, 0.0025, 0.0112, 0.0028, 0.0210,
        0.0022, 0.0024, 0.0244, 0.0682, 0.0696], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,431][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([6.2643e-01, 2.2427e-01, 1.0004e-02, 5.3300e-02, 1.0874e-03, 6.2364e-04,
        1.2229e-03, 1.0568e-03, 2.6025e-03, 6.3742e-04, 1.5626e-03, 1.6476e-03,
        1.5307e-02, 6.0244e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,432][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.3436, 0.1246, 0.1014, 0.0552, 0.0235, 0.0255, 0.0199, 0.0473, 0.0277,
        0.0393, 0.0354, 0.0202, 0.0632, 0.0732], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,434][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.9855e-01, 5.5522e-03, 9.9531e-03, 5.4373e-03, 6.1036e-03, 3.0885e-04,
        2.0562e-03, 9.7599e-03, 1.7295e-02, 1.9673e-02, 1.7901e-02, 8.8130e-03,
        1.3853e-01, 5.6007e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,435][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7708, 0.0138, 0.0216, 0.0406, 0.0029, 0.0037, 0.0159, 0.0037, 0.0058,
        0.0290, 0.0062, 0.0058, 0.0524, 0.0277], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,436][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2282, 0.3252, 0.0914, 0.3007, 0.0010, 0.0034, 0.0010, 0.0009, 0.0011,
        0.0018, 0.0022, 0.0004, 0.0216, 0.0210], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,437][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.8771, 0.0103, 0.0053, 0.0161, 0.0035, 0.0010, 0.0030, 0.0029, 0.0031,
        0.0027, 0.0034, 0.0027, 0.0129, 0.0560], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,438][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([3.0020e-07, 2.7604e-18, 1.4388e-19, 3.8569e-16, 1.4336e-18, 7.3679e-22,
        1.1866e-16, 1.2735e-15, 2.9429e-13, 2.0851e-08, 3.7222e-10, 8.6448e-10,
        9.9992e-01, 7.8373e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,439][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ same] are: tensor([4.7294e-01, 1.8958e-03, 3.6881e-03, 2.9647e-02, 5.5623e-05, 1.2615e-04,
        1.2209e-04, 4.6218e-04, 1.7587e-03, 6.0930e-03, 6.2268e-04, 1.6749e-03,
        2.4081e-01, 2.1263e-01, 2.7470e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,440][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ same] are: tensor([6.8970e-01, 4.0927e-04, 2.2226e-03, 2.2109e-03, 4.2551e-04, 2.1420e-04,
        5.6719e-04, 9.4612e-03, 1.4625e-03, 7.3856e-03, 2.5374e-03, 2.0686e-03,
        1.7395e-01, 8.1555e-02, 2.5823e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,442][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.6808, 0.0160, 0.0158, 0.0145, 0.0197, 0.0036, 0.0277, 0.0049, 0.0207,
        0.0499, 0.0203, 0.0107, 0.0253, 0.0614, 0.0289], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,443][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ same] are: tensor([1.4001e-02, 1.3578e-01, 6.1664e-01, 1.8591e-01, 4.9379e-04, 1.7589e-03,
        9.3713e-04, 2.3514e-03, 8.7817e-03, 6.5329e-03, 4.7215e-03, 5.8639e-03,
        1.0080e-02, 4.7549e-03, 1.3912e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,444][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.6052, 0.0257, 0.0162, 0.0075, 0.0038, 0.0010, 0.0079, 0.0013, 0.0091,
        0.0016, 0.0032, 0.0167, 0.0475, 0.0607, 0.1928], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,445][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ same] are: tensor([6.0517e-01, 1.0215e-01, 6.2960e-03, 4.3362e-02, 1.2363e-04, 2.8665e-04,
        1.5692e-03, 6.0182e-04, 2.3866e-03, 1.1979e-03, 1.1512e-03, 4.6725e-03,
        2.6228e-02, 7.1200e-02, 1.3360e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,446][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.3350, 0.2260, 0.0600, 0.0474, 0.0168, 0.0229, 0.0183, 0.0335, 0.0284,
        0.0251, 0.0239, 0.0168, 0.0385, 0.0843, 0.0229], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,448][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ same] are: tensor([3.2029e-01, 1.0064e-02, 1.2455e-02, 9.3581e-04, 2.8744e-03, 4.1355e-04,
        1.9262e-03, 3.7773e-03, 1.0668e-02, 1.0199e-02, 1.1779e-02, 6.3662e-03,
        3.9737e-02, 1.1153e-01, 4.5698e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,449][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.7778, 0.0170, 0.0120, 0.0287, 0.0019, 0.0020, 0.0137, 0.0028, 0.0035,
        0.0318, 0.0066, 0.0041, 0.0320, 0.0266, 0.0395], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,450][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.3879, 0.1440, 0.0902, 0.1186, 0.0012, 0.0035, 0.0015, 0.0023, 0.0018,
        0.0052, 0.0040, 0.0009, 0.0974, 0.0935, 0.0480], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,451][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ same] are: tensor([9.0378e-01, 1.6170e-03, 3.0666e-03, 2.6169e-02, 1.9813e-03, 8.2541e-04,
        1.4679e-03, 3.3478e-03, 1.5766e-03, 3.7414e-03, 4.3864e-04, 1.2494e-03,
        1.0654e-02, 3.6055e-02, 4.0284e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,452][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ same] are: tensor([2.0978e-08, 4.0095e-14, 1.4993e-21, 1.9354e-19, 8.0345e-18, 5.1313e-22,
        4.3260e-22, 3.3751e-19, 1.2117e-20, 3.0279e-12, 9.1514e-13, 1.6457e-16,
        5.0444e-07, 9.9999e-01, 1.3874e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,453][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ as] are: tensor([3.7055e-01, 4.6784e-03, 1.4147e-03, 7.0879e-03, 1.0514e-04, 6.4401e-05,
        5.1046e-04, 7.7625e-04, 1.9877e-03, 1.8938e-02, 6.9628e-04, 2.2491e-03,
        2.0748e-01, 2.4095e-01, 9.5170e-02, 4.7343e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,455][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ as] are: tensor([2.8204e-01, 1.6336e-03, 3.7135e-03, 1.6450e-03, 6.1038e-04, 2.0648e-04,
        1.8833e-03, 9.4190e-03, 4.0508e-03, 1.0683e-02, 4.0516e-03, 3.4060e-03,
        2.1561e-01, 9.8653e-02, 2.7742e-01, 8.4970e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,456][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.6701, 0.0158, 0.0240, 0.0121, 0.0054, 0.0076, 0.0327, 0.0053, 0.0179,
        0.0240, 0.0116, 0.0093, 0.0385, 0.0508, 0.0500, 0.0249],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,457][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0879, 0.0450, 0.6388, 0.1336, 0.0017, 0.0036, 0.0031, 0.0037, 0.0120,
        0.0073, 0.0043, 0.0086, 0.0284, 0.0114, 0.0049, 0.0058],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,458][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.5888, 0.0233, 0.0239, 0.0091, 0.0015, 0.0006, 0.0038, 0.0018, 0.0034,
        0.0008, 0.0024, 0.0037, 0.0166, 0.0199, 0.2057, 0.0947],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,459][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ as] are: tensor([6.7356e-01, 6.8149e-03, 4.4849e-03, 2.9822e-03, 3.0694e-04, 3.7596e-04,
        4.1366e-04, 3.8213e-04, 2.5930e-03, 1.4783e-03, 1.4361e-03, 2.7661e-03,
        2.8369e-02, 1.3632e-01, 1.0889e-01, 2.8829e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,461][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.4427, 0.1209, 0.0658, 0.0289, 0.0154, 0.0110, 0.0128, 0.0155, 0.0263,
        0.0165, 0.0162, 0.0172, 0.0370, 0.0630, 0.0544, 0.0567],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,462][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.3720, 0.0205, 0.0334, 0.0148, 0.0052, 0.0007, 0.0017, 0.0049, 0.0101,
        0.0056, 0.0059, 0.0031, 0.0292, 0.0437, 0.0494, 0.3997],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,463][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.6549, 0.0375, 0.0115, 0.0253, 0.0016, 0.0014, 0.0050, 0.0018, 0.0016,
        0.0052, 0.0067, 0.0015, 0.0738, 0.0391, 0.0866, 0.0465],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,464][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.2589, 0.0224, 0.0612, 0.0821, 0.0030, 0.0089, 0.0116, 0.0201, 0.0262,
        0.0118, 0.0200, 0.0107, 0.1078, 0.0935, 0.2295, 0.0323],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,465][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.8683, 0.0075, 0.0026, 0.0337, 0.0011, 0.0011, 0.0033, 0.0020, 0.0043,
        0.0020, 0.0009, 0.0050, 0.0102, 0.0324, 0.0077, 0.0178],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,467][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ as] are: tensor([3.8195e-08, 1.4882e-13, 5.5370e-14, 8.5323e-16, 9.1418e-17, 7.4978e-16,
        2.7982e-15, 9.9144e-13, 2.5342e-13, 1.5766e-11, 2.4506e-09, 3.2031e-11,
        4.7834e-09, 2.4955e-07, 9.9995e-01, 5.0586e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,468][circuit_into_ebeddingspace.py][line:2310][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([3.8560e-01, 2.2562e-03, 7.8706e-04, 5.4386e-03, 2.1095e-04, 3.3386e-04,
        4.8518e-04, 1.3138e-03, 1.5330e-03, 3.5446e-03, 1.4496e-03, 1.3821e-03,
        5.6757e-02, 1.0014e-02, 2.2337e-02, 4.5403e-01, 5.2530e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,469][circuit_into_ebeddingspace.py][line:2313][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.2234, 0.0017, 0.0022, 0.0030, 0.0012, 0.0003, 0.0023, 0.0144, 0.0040,
        0.0089, 0.0045, 0.0032, 0.0641, 0.0110, 0.2417, 0.3391, 0.0752],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,470][circuit_into_ebeddingspace.py][line:2316][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.6431, 0.0133, 0.0171, 0.0090, 0.0086, 0.0118, 0.0243, 0.0102, 0.0220,
        0.0239, 0.0169, 0.0110, 0.0290, 0.0481, 0.0529, 0.0338, 0.0250],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,471][circuit_into_ebeddingspace.py][line:2319][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3453, 0.0395, 0.3454, 0.0246, 0.0096, 0.0102, 0.0116, 0.0146, 0.0392,
        0.0123, 0.0186, 0.0220, 0.0286, 0.0089, 0.0149, 0.0424, 0.0121],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,473][circuit_into_ebeddingspace.py][line:2322][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.6151, 0.0241, 0.0313, 0.0138, 0.0025, 0.0008, 0.0043, 0.0033, 0.0069,
        0.0014, 0.0042, 0.0085, 0.0244, 0.0403, 0.0653, 0.1140, 0.0398],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,474][circuit_into_ebeddingspace.py][line:2325][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([8.7164e-01, 8.7903e-04, 1.0295e-03, 1.3769e-03, 3.0241e-04, 1.7720e-04,
        1.8878e-04, 1.5940e-04, 7.7772e-04, 2.7557e-04, 5.0884e-04, 5.9155e-04,
        1.7774e-02, 4.4855e-03, 2.6984e-02, 3.6028e-02, 3.6821e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,475][circuit_into_ebeddingspace.py][line:2328][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5397, 0.0266, 0.0510, 0.0140, 0.0090, 0.0081, 0.0086, 0.0154, 0.0115,
        0.0117, 0.0118, 0.0099, 0.0442, 0.0200, 0.0647, 0.1038, 0.0500],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,476][circuit_into_ebeddingspace.py][line:2331][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.0477e-01, 8.5872e-04, 2.2575e-03, 2.4834e-03, 2.8511e-02, 3.3635e-04,
        1.6339e-03, 3.0794e-03, 4.4452e-03, 1.6737e-03, 1.9210e-02, 2.4352e-03,
        1.0178e-02, 1.4139e-02, 7.2814e-03, 7.6968e-02, 7.1974e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,477][circuit_into_ebeddingspace.py][line:2334][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7674, 0.0199, 0.0207, 0.0279, 0.0044, 0.0035, 0.0167, 0.0030, 0.0038,
        0.0137, 0.0041, 0.0028, 0.0289, 0.0192, 0.0189, 0.0285, 0.0167],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,479][circuit_into_ebeddingspace.py][line:2337][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.3935, 0.0143, 0.0637, 0.0297, 0.0023, 0.0110, 0.0070, 0.0100, 0.0078,
        0.0047, 0.0138, 0.0050, 0.0992, 0.0489, 0.1630, 0.0852, 0.0408],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,480][circuit_into_ebeddingspace.py][line:2340][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.8372, 0.0097, 0.0066, 0.0189, 0.0026, 0.0010, 0.0056, 0.0027, 0.0048,
        0.0045, 0.0033, 0.0049, 0.0151, 0.0329, 0.0098, 0.0156, 0.0248],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,481][circuit_into_ebeddingspace.py][line:2343][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.7703e-09, 6.3201e-20, 5.9665e-22, 5.1337e-14, 5.8845e-17, 1.1969e-21,
        1.2254e-18, 2.4873e-18, 1.4967e-18, 3.0631e-13, 1.3096e-12, 3.7990e-16,
        1.1254e-10, 1.0937e-11, 4.3274e-09, 9.9539e-01, 4.6061e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,489][circuit_into_ebeddingspace.py][line:2286][INFO] ################5-th layer#################
[2024-06-27 21:00:53,492][circuit_into_ebeddingspace.py][line:2288][INFO] ##5-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,493][circuit_into_ebeddingspace.py][line:2289][INFO] ##5-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,495][circuit_into_ebeddingspace.py][line:2290][INFO] ##5-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,496][circuit_into_ebeddingspace.py][line:2291][INFO] ##5-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,498][circuit_into_ebeddingspace.py][line:2292][INFO] ##5-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,499][circuit_into_ebeddingspace.py][line:2293][INFO] ##5-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,500][circuit_into_ebeddingspace.py][line:2294][INFO] ##5-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,502][circuit_into_ebeddingspace.py][line:2295][INFO] ##5-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,503][circuit_into_ebeddingspace.py][line:2296][INFO] ##5-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,504][circuit_into_ebeddingspace.py][line:2297][INFO] ##5-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,506][circuit_into_ebeddingspace.py][line:2298][INFO] ##5-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,507][circuit_into_ebeddingspace.py][line:2299][INFO] ##5-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,508][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,509][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,510][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,511][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,512][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,513][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,514][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,515][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,515][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,516][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,517][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,518][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,519][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9938, 0.0062], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,520][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ original] are: tensor([9.9992e-01, 7.8708e-05], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,521][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9778, 0.0222], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,522][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9105, 0.0895], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,523][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9664, 0.0336], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,524][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9982, 0.0018], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,525][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9755, 0.0245], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,526][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9725, 0.0275], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,527][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9864, 0.0136], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,528][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9980, 0.0020], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,529][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9385, 0.0615], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,530][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.8994, 0.1006], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,531][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9788, 0.0063, 0.0149], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,531][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ language] are: tensor([9.9973e-01, 3.0650e-06, 2.7092e-04], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,532][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.6431, 0.3437, 0.0132], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,533][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8074, 0.1130, 0.0796], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,534][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8178, 0.0931, 0.0891], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,535][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.9938, 0.0011, 0.0051], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,536][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.8323, 0.1424, 0.0254], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,537][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.7394, 0.1180, 0.1426], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,538][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9709, 0.0054, 0.0237], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,539][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9784, 0.0096, 0.0120], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,540][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.5355, 0.0208, 0.4436], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,541][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9363, 0.0180, 0.0458], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,542][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.9894, 0.0039, 0.0021, 0.0046], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,543][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ of] are: tensor([9.9985e-01, 2.6924e-06, 5.9986e-06, 1.4572e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,544][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.4196, 0.1729, 0.3685, 0.0390], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,545][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.7913, 0.0860, 0.0704, 0.0523], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,546][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.4302, 0.0886, 0.3341, 0.1471], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,547][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9881, 0.0013, 0.0037, 0.0069], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,548][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8491, 0.0009, 0.1376, 0.0125], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,549][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.2391, 0.1121, 0.6057, 0.0431], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,550][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.9626, 0.0047, 0.0117, 0.0210], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,551][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.9580, 0.0089, 0.0182, 0.0150], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,552][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.8541, 0.0160, 0.0991, 0.0308], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,553][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.6832, 0.0199, 0.0206, 0.2763], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,554][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ De] are: tensor([9.8444e-01, 4.4063e-03, 5.9303e-03, 2.4041e-04, 4.9809e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,555][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ De] are: tensor([9.9976e-01, 8.7286e-06, 1.5824e-05, 2.5499e-08, 2.1658e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,556][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.7798, 0.0384, 0.0803, 0.0812, 0.0203], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,557][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.7726, 0.0840, 0.0637, 0.0523, 0.0275], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,558][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.4406, 0.0547, 0.1679, 0.3200, 0.0168], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,559][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ De] are: tensor([9.9439e-01, 1.8627e-03, 2.9903e-03, 9.0114e-06, 7.4428e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,560][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9294, 0.0012, 0.0124, 0.0305, 0.0265], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,561][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.7181, 0.0091, 0.1867, 0.0430, 0.0431], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,562][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.9447, 0.0042, 0.0348, 0.0010, 0.0154], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,563][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.9716, 0.0068, 0.0091, 0.0036, 0.0089], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,564][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.6627, 0.0171, 0.2467, 0.0231, 0.0504], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,565][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.9027, 0.0094, 0.0161, 0.0446, 0.0272], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,566][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.9854, 0.0015, 0.0039, 0.0012, 0.0046, 0.0034], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,567][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.9997e-01, 8.2559e-07, 1.1892e-06, 5.9429e-10, 8.7160e-07, 3.1023e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,568][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.8687, 0.0103, 0.0039, 0.0046, 0.0589, 0.0535], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,569][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.7202, 0.0354, 0.0328, 0.0375, 0.1315, 0.0426], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,570][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.4689, 0.0178, 0.0202, 0.0704, 0.3556, 0.0672], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,571][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([9.9564e-01, 2.7733e-03, 5.3728e-04, 6.6951e-07, 2.6272e-04, 7.8753e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,572][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([2.4610e-01, 3.4271e-04, 2.9507e-05, 6.2741e-05, 7.3334e-01, 2.0123e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,573][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.9426, 0.0031, 0.0075, 0.0045, 0.0355, 0.0068], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,574][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([8.9428e-01, 2.4658e-03, 3.7433e-03, 3.9441e-04, 2.5638e-03, 9.6549e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,575][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([9.5051e-01, 2.6798e-03, 1.5471e-02, 8.2240e-03, 2.2350e-02, 7.6122e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,576][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.9110, 0.0018, 0.0019, 0.0038, 0.0167, 0.0648], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,577][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.9490, 0.0040, 0.0024, 0.0156, 0.0076, 0.0214], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,578][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.9325, 0.0015, 0.0025, 0.0029, 0.0125, 0.0081, 0.0401],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,579][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([9.9967e-01, 3.8562e-07, 8.3913e-08, 1.2874e-07, 9.6583e-05, 6.9435e-09,
        2.3463e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,580][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.1885, 0.0065, 0.0095, 0.0110, 0.0306, 0.7514, 0.0025],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,581][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.6587, 0.0537, 0.0429, 0.1336, 0.0562, 0.0377, 0.0173],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,582][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4383, 0.0403, 0.1070, 0.1694, 0.1001, 0.0725, 0.0725],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,583][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([9.9647e-01, 2.9080e-04, 1.6230e-04, 1.3380e-04, 2.6439e-03, 2.9939e-06,
        2.9645e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,584][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([7.2678e-01, 2.4380e-04, 9.6020e-06, 1.2074e-04, 5.0614e-03, 2.5254e-01,
        1.5241e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,585][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.7921, 0.0283, 0.0701, 0.0410, 0.0339, 0.0132, 0.0215],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,586][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.9307, 0.0011, 0.0027, 0.0086, 0.0172, 0.0241, 0.0156],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,587][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.9545, 0.0053, 0.0106, 0.0117, 0.0081, 0.0081, 0.0017],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,588][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.8278, 0.0038, 0.0058, 0.0055, 0.0135, 0.1063, 0.0373],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,589][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.8638, 0.0130, 0.0084, 0.0365, 0.0225, 0.0170, 0.0387],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,590][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([8.0629e-01, 1.1154e-03, 1.8541e-04, 3.9676e-04, 4.0306e-03, 6.4146e-03,
        6.0460e-02, 1.2110e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,591][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([9.9968e-01, 1.1492e-06, 5.7599e-09, 2.3714e-10, 1.5791e-06, 4.4508e-10,
        1.3150e-04, 1.8436e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,592][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.4149, 0.0049, 0.0139, 0.0056, 0.0338, 0.3552, 0.0814, 0.0905],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,593][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.6751, 0.0326, 0.0291, 0.0562, 0.1349, 0.0272, 0.0299, 0.0151],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,594][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.6529, 0.0115, 0.0321, 0.0522, 0.0320, 0.0173, 0.1651, 0.0371],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,595][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([9.9464e-01, 3.4727e-03, 4.3472e-05, 4.4389e-07, 1.0403e-03, 7.7180e-09,
        2.6983e-04, 5.3413e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,596][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([1.3749e-01, 5.4336e-05, 6.3718e-04, 5.1459e-04, 1.4894e-02, 3.2963e-03,
        8.2184e-01, 2.1272e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,598][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.8916, 0.0059, 0.0299, 0.0168, 0.0080, 0.0053, 0.0288, 0.0137],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,599][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([9.0298e-01, 7.4412e-04, 7.2853e-04, 2.4006e-04, 4.9034e-03, 1.4468e-03,
        4.4889e-02, 4.4067e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,600][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([9.5463e-01, 4.4238e-03, 8.3546e-03, 6.9767e-03, 1.0341e-02, 7.5800e-03,
        7.1516e-03, 5.3791e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,601][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.9007, 0.0050, 0.0014, 0.0062, 0.0062, 0.0494, 0.0213, 0.0098],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,602][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.9065, 0.0049, 0.0027, 0.0096, 0.0121, 0.0377, 0.0119, 0.0146],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,603][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.5231, 0.0007, 0.0012, 0.0029, 0.0132, 0.0007, 0.0077, 0.3921, 0.0583],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,604][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.9731e-01, 3.2536e-06, 1.0025e-07, 1.7195e-07, 4.5159e-05, 5.7223e-11,
        1.9291e-07, 2.2900e-03, 3.4837e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,605][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0595, 0.0050, 0.0037, 0.0069, 0.0308, 0.3480, 0.0072, 0.5364, 0.0026],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,606][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.4031, 0.0903, 0.0484, 0.2467, 0.1189, 0.0262, 0.0224, 0.0233, 0.0206],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,607][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4537, 0.0215, 0.0516, 0.2128, 0.0204, 0.0123, 0.0721, 0.0508, 0.1048],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,608][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [orum] are: tensor([9.7509e-01, 1.5788e-03, 1.5293e-04, 6.4325e-05, 2.4647e-03, 3.6802e-08,
        1.1942e-05, 1.5683e-02, 4.9491e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,609][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [orum] are: tensor([8.1082e-01, 1.0354e-04, 2.7542e-05, 1.5089e-04, 2.9024e-04, 1.6130e-03,
        4.6516e-03, 1.6687e-01, 1.5477e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,610][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.7122, 0.0104, 0.0905, 0.0549, 0.0130, 0.0018, 0.0501, 0.0328, 0.0343],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,611][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.5086, 0.0006, 0.0012, 0.0040, 0.0099, 0.0022, 0.0021, 0.4536, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,612][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.8725, 0.0066, 0.0174, 0.0178, 0.0313, 0.0154, 0.0178, 0.0167, 0.0044],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,613][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.8049, 0.0033, 0.0032, 0.0103, 0.0091, 0.0470, 0.0167, 0.0715, 0.0340],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,615][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.8209, 0.0056, 0.0068, 0.0279, 0.0234, 0.0217, 0.0210, 0.0148, 0.0579],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,616][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ et] are: tensor([7.4810e-01, 6.2380e-04, 6.0908e-04, 2.3055e-04, 3.9731e-03, 2.7925e-03,
        4.2697e-03, 1.0806e-01, 6.8795e-02, 6.2543e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,617][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ et] are: tensor([9.9756e-01, 3.6501e-06, 2.6870e-06, 1.4354e-07, 1.5668e-04, 2.1264e-06,
        1.0648e-06, 1.5131e-03, 7.5121e-05, 6.8337e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,618][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.2663, 0.0083, 0.0166, 0.0080, 0.0429, 0.4284, 0.0453, 0.1211, 0.0469,
        0.0159], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,619][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.5173, 0.0901, 0.0455, 0.1622, 0.1034, 0.0221, 0.0146, 0.0128, 0.0214,
        0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,620][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.6415, 0.0356, 0.0463, 0.1102, 0.0099, 0.0038, 0.0443, 0.0146, 0.0693,
        0.0246], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,621][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ et] are: tensor([9.7989e-01, 6.2106e-03, 1.9884e-03, 1.3358e-04, 4.6380e-03, 1.4875e-03,
        3.0866e-05, 5.3286e-03, 1.2872e-04, 1.6788e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,622][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ et] are: tensor([7.4964e-01, 4.2907e-05, 5.8313e-05, 1.4290e-04, 5.8285e-05, 2.1417e-05,
        7.2929e-03, 1.5434e-02, 2.1825e-01, 9.0506e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,623][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ et] are: tensor([9.4253e-01, 9.5446e-03, 2.2498e-02, 1.5793e-02, 2.1901e-03, 3.2009e-04,
        1.9499e-03, 1.8960e-03, 1.7556e-03, 1.5213e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,624][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.6887, 0.0014, 0.0064, 0.0016, 0.0075, 0.0473, 0.0030, 0.2285, 0.0097,
        0.0060], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,625][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.8778, 0.0082, 0.0134, 0.0109, 0.0088, 0.0102, 0.0193, 0.0175, 0.0288,
        0.0050], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,627][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.8351, 0.0116, 0.0056, 0.0172, 0.0123, 0.0264, 0.0222, 0.0186, 0.0372,
        0.0138], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,628][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.8781, 0.0032, 0.0024, 0.0229, 0.0204, 0.0236, 0.0093, 0.0079, 0.0119,
        0.0204], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,629][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.5425, 0.0006, 0.0018, 0.0008, 0.0024, 0.0040, 0.0642, 0.0981, 0.1291,
        0.1352, 0.0213], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,630][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([9.9967e-01, 2.8448e-05, 2.7920e-06, 4.8389e-07, 3.2086e-05, 5.0064e-07,
        1.4653e-04, 1.0265e-05, 5.4524e-05, 7.4126e-06, 4.9566e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,631][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.4817, 0.0179, 0.0130, 0.0198, 0.0467, 0.0660, 0.0780, 0.0744, 0.0614,
        0.1338, 0.0074], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,632][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.4956, 0.0620, 0.0537, 0.1080, 0.1236, 0.0197, 0.0265, 0.0176, 0.0255,
        0.0511, 0.0167], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,633][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.6701, 0.0146, 0.0170, 0.0405, 0.0060, 0.0032, 0.0244, 0.0210, 0.0680,
        0.0981, 0.0371], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,634][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([9.7842e-01, 1.6804e-02, 4.5489e-04, 2.1379e-04, 2.7706e-03, 1.2273e-05,
        6.8836e-04, 4.8515e-05, 4.3260e-04, 1.1159e-04, 3.9012e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,635][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([6.5603e-01, 2.7277e-04, 7.0177e-04, 4.3194e-04, 2.1429e-03, 3.0605e-04,
        4.1468e-03, 5.7008e-03, 1.7055e-02, 3.0451e-01, 8.7047e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,636][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([9.6405e-01, 1.5170e-03, 9.1302e-03, 7.8082e-03, 1.2386e-03, 3.8546e-04,
        3.3984e-03, 3.2542e-03, 2.8852e-03, 2.7755e-03, 3.5565e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,638][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.9070, 0.0037, 0.0049, 0.0032, 0.0027, 0.0024, 0.0339, 0.0068, 0.0292,
        0.0036, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,639][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([8.1939e-01, 1.8396e-03, 3.8006e-03, 8.4627e-03, 3.0078e-02, 1.5889e-02,
        3.4873e-02, 2.8407e-02, 2.1678e-02, 3.5294e-02, 2.8943e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,640][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.7523, 0.0123, 0.0173, 0.0179, 0.0160, 0.0352, 0.0299, 0.0272, 0.0677,
        0.0062, 0.0180], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,641][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.8423, 0.0014, 0.0034, 0.0151, 0.0100, 0.0095, 0.0194, 0.0071, 0.0573,
        0.0164, 0.0181], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,642][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [orum] are: tensor([3.5374e-01, 4.1454e-04, 5.9567e-04, 2.2342e-03, 5.0206e-03, 1.7967e-04,
        1.8715e-03, 1.0798e-01, 7.3132e-03, 4.8628e-01, 2.6102e-02, 8.2586e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,643][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.8484e-01, 1.1283e-05, 5.3793e-07, 4.0656e-06, 4.7674e-05, 9.7625e-11,
        3.1716e-08, 1.7630e-04, 7.9698e-07, 1.4819e-02, 2.0594e-05, 7.4937e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,644][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.1383, 0.0131, 0.0059, 0.0127, 0.0842, 0.1774, 0.0070, 0.2640, 0.0034,
        0.1227, 0.1671, 0.0042], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,645][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2926, 0.1210, 0.0631, 0.2610, 0.1314, 0.0153, 0.0200, 0.0132, 0.0151,
        0.0351, 0.0206, 0.0116], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,646][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.3394, 0.0225, 0.0346, 0.1288, 0.0090, 0.0041, 0.0254, 0.0199, 0.0632,
        0.1835, 0.0676, 0.1020], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,648][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [orum] are: tensor([8.7710e-01, 2.6556e-03, 2.9806e-04, 3.5239e-04, 3.9913e-03, 1.2827e-07,
        3.2809e-06, 3.0880e-03, 7.0855e-05, 1.0965e-01, 1.0336e-03, 1.7506e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,649][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [orum] are: tensor([9.3234e-01, 3.3925e-04, 5.8461e-05, 1.7768e-04, 3.3609e-04, 2.6537e-04,
        1.0025e-04, 3.9481e-04, 8.6510e-05, 7.6009e-04, 5.9175e-02, 5.9619e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,650][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [orum] are: tensor([8.1374e-01, 1.4680e-02, 7.6502e-02, 4.6037e-02, 3.4818e-03, 2.8202e-04,
        1.1234e-02, 2.8946e-03, 8.1621e-03, 3.0122e-03, 5.9335e-03, 1.4040e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,651][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [orum] are: tensor([3.8695e-01, 3.4947e-04, 6.1342e-04, 3.3949e-03, 4.6061e-03, 2.4223e-04,
        2.7642e-04, 7.6022e-02, 1.4839e-03, 4.8153e-01, 4.0396e-02, 4.1349e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,652][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.7310, 0.0090, 0.0147, 0.0247, 0.0350, 0.0224, 0.0235, 0.0380, 0.0095,
        0.0598, 0.0192, 0.0133], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,653][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.6489, 0.0048, 0.0099, 0.0172, 0.0236, 0.0355, 0.0196, 0.0672, 0.0484,
        0.0052, 0.0572, 0.0625], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,654][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.8301, 0.0041, 0.0045, 0.0214, 0.0108, 0.0062, 0.0067, 0.0053, 0.0186,
        0.0160, 0.0092, 0.0670], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,655][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ is] are: tensor([9.2835e-01, 1.2325e-03, 3.8245e-04, 5.4149e-04, 2.3090e-03, 1.4294e-04,
        9.8774e-04, 1.1698e-02, 5.3302e-03, 3.1679e-02, 8.0425e-03, 4.6501e-03,
        4.6514e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,656][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ is] are: tensor([9.9830e-01, 8.6402e-06, 2.1292e-07, 4.7806e-07, 8.8050e-05, 4.1263e-09,
        2.0260e-08, 4.1762e-05, 1.4438e-06, 1.9694e-04, 9.3664e-04, 2.1090e-06,
        4.2832e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,658][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0205, 0.1404, 0.0425, 0.7369, 0.0209, 0.0106, 0.0035, 0.0019, 0.0025,
        0.0106, 0.0059, 0.0021, 0.0017], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,659][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.4306, 0.1899, 0.1025, 0.0695, 0.0245, 0.0328, 0.0038, 0.0110, 0.0089,
        0.0125, 0.0157, 0.0051, 0.0933], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,660][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.2809, 0.0622, 0.0773, 0.2319, 0.0081, 0.0105, 0.0153, 0.0225, 0.0230,
        0.0603, 0.0169, 0.0096, 0.1814], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,661][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ is] are: tensor([8.9556e-01, 1.1669e-02, 3.9939e-03, 4.3137e-02, 2.4523e-02, 2.1686e-04,
        4.1408e-05, 1.1986e-03, 1.0000e-04, 1.9287e-03, 5.7180e-03, 9.2822e-04,
        1.0986e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,662][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ is] are: tensor([8.7686e-01, 1.5086e-04, 1.7775e-04, 3.5547e-04, 2.1649e-04, 9.8258e-06,
        3.3771e-04, 2.1284e-03, 8.2857e-03, 8.9929e-03, 3.8238e-03, 4.2064e-02,
        5.6596e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,663][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ is] are: tensor([1.7289e-01, 4.0305e-02, 6.4939e-01, 1.2505e-01, 2.5938e-04, 2.1145e-04,
        3.0539e-04, 1.8868e-04, 1.5922e-03, 6.1690e-04, 2.9788e-04, 6.5098e-04,
        8.2430e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,664][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ is] are: tensor([8.4940e-01, 1.9535e-02, 7.1555e-03, 1.5061e-02, 2.5444e-02, 1.2076e-03,
        4.4319e-04, 1.1044e-02, 4.2116e-03, 1.2756e-02, 3.6094e-02, 2.7232e-03,
        1.4924e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,666][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.7547, 0.0157, 0.0384, 0.0359, 0.0103, 0.0023, 0.0118, 0.0070, 0.0204,
        0.0216, 0.0054, 0.0116, 0.0648], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,667][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.3423, 0.0256, 0.1533, 0.2727, 0.0357, 0.0052, 0.0263, 0.0150, 0.0251,
        0.0062, 0.0074, 0.0244, 0.0607], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,668][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.5992, 0.0072, 0.0648, 0.0337, 0.0100, 0.0059, 0.0129, 0.0050, 0.0156,
        0.0154, 0.0032, 0.0149, 0.2122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,669][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([9.3891e-01, 7.2107e-03, 2.4365e-03, 4.8841e-03, 1.6505e-03, 6.9803e-04,
        2.5958e-04, 3.1865e-03, 1.5258e-03, 6.0521e-03, 1.3241e-03, 2.2765e-03,
        5.1588e-03, 2.4429e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,670][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.9511e-01, 3.6142e-03, 1.9101e-05, 1.5111e-06, 1.8908e-05, 1.7648e-08,
        4.2876e-09, 8.6916e-07, 2.8646e-07, 1.3600e-06, 3.4266e-05, 5.9615e-07,
        1.6768e-06, 1.2002e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,671][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1217, 0.1814, 0.0544, 0.5070, 0.0139, 0.0215, 0.0125, 0.0049, 0.0114,
        0.0231, 0.0043, 0.0081, 0.0200, 0.0156], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,672][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3814, 0.1318, 0.0988, 0.0372, 0.0120, 0.0346, 0.0060, 0.0086, 0.0085,
        0.0108, 0.0131, 0.0048, 0.0935, 0.1589], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,674][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2925, 0.0451, 0.1066, 0.1366, 0.0029, 0.0070, 0.0162, 0.0371, 0.0236,
        0.0320, 0.0092, 0.0114, 0.2299, 0.0498], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,675][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([8.8219e-01, 7.4611e-02, 1.0024e-02, 8.9228e-03, 7.2863e-03, 8.5338e-05,
        4.5610e-05, 1.5713e-04, 3.5077e-04, 3.2650e-04, 2.1882e-03, 4.3555e-04,
        2.2928e-03, 1.1082e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,676][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([6.7197e-01, 5.2523e-04, 3.5183e-03, 2.0428e-03, 8.9988e-04, 1.3966e-05,
        4.0509e-04, 8.6325e-04, 1.5368e-02, 1.6707e-02, 3.2753e-03, 7.5226e-03,
        2.5865e-01, 1.8238e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,677][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.4204e-01, 3.7974e-02, 6.8167e-01, 1.1032e-01, 4.2406e-04, 8.6071e-04,
        1.1025e-03, 9.9049e-04, 1.8431e-03, 1.6269e-03, 1.3959e-03, 1.0043e-03,
        1.3829e-02, 4.9139e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,678][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([7.0411e-01, 1.0601e-01, 5.4709e-02, 5.2827e-02, 1.3133e-02, 1.8895e-03,
        5.6553e-04, 3.3188e-03, 1.6372e-03, 2.9433e-03, 9.2703e-03, 7.7903e-04,
        1.5625e-02, 3.3181e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,679][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.7830, 0.0114, 0.0250, 0.0229, 0.0155, 0.0035, 0.0131, 0.0123, 0.0269,
        0.0097, 0.0076, 0.0125, 0.0283, 0.0282], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,680][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3838, 0.0156, 0.0962, 0.1494, 0.0502, 0.0093, 0.0232, 0.0280, 0.0417,
        0.0116, 0.0069, 0.0319, 0.1262, 0.0261], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,682][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.6343, 0.0140, 0.0503, 0.0072, 0.0122, 0.0063, 0.0087, 0.0114, 0.0123,
        0.0136, 0.0058, 0.0110, 0.0458, 0.1670], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,683][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ same] are: tensor([9.5077e-01, 3.3146e-04, 1.6131e-03, 1.7059e-03, 7.7835e-04, 2.3777e-05,
        5.6325e-05, 3.8951e-04, 1.0048e-04, 1.0006e-03, 1.6868e-04, 2.1964e-04,
        9.3274e-04, 2.3259e-02, 1.8653e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,684][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ same] are: tensor([9.9859e-01, 7.9841e-06, 1.1433e-04, 5.1842e-05, 3.3577e-05, 8.3889e-11,
        1.3440e-09, 2.3397e-08, 4.9751e-08, 1.7175e-07, 6.3246e-07, 8.2814e-07,
        5.5557e-06, 1.0606e-04, 1.0900e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,685][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.2207, 0.0873, 0.0480, 0.3911, 0.0349, 0.0128, 0.0049, 0.0027, 0.0032,
        0.0077, 0.0030, 0.0019, 0.0323, 0.1418, 0.0078], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,686][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.4465, 0.0602, 0.0415, 0.0157, 0.0091, 0.0272, 0.0047, 0.0088, 0.0095,
        0.0224, 0.0103, 0.0078, 0.1285, 0.1738, 0.0342], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,687][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.5054, 0.0260, 0.0454, 0.0489, 0.0011, 0.0040, 0.0064, 0.0153, 0.0112,
        0.0164, 0.0099, 0.0083, 0.1302, 0.0877, 0.0837], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,689][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ same] are: tensor([8.3769e-01, 9.9036e-03, 8.1666e-03, 5.7342e-02, 6.9481e-02, 1.3760e-05,
        5.5155e-06, 1.1627e-05, 4.2812e-05, 4.3322e-05, 8.9119e-04, 7.2729e-05,
        1.5801e-03, 1.0569e-02, 4.1896e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,690][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ same] are: tensor([6.0676e-01, 2.2274e-03, 5.2607e-04, 3.6663e-03, 2.2163e-03, 1.6711e-04,
        3.8310e-04, 9.2507e-04, 5.0348e-03, 6.1681e-02, 6.8440e-03, 4.2362e-03,
        1.8432e-01, 8.1094e-02, 3.9914e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,691][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ same] are: tensor([4.4184e-01, 1.8869e-02, 3.4838e-01, 1.3681e-01, 8.0960e-04, 5.1264e-04,
        1.4812e-04, 1.7440e-04, 8.0563e-04, 1.2340e-03, 7.0083e-04, 3.5896e-04,
        1.2351e-02, 3.6562e-03, 3.3347e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,692][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ same] are: tensor([6.8376e-01, 5.8743e-03, 4.6997e-02, 1.3517e-01, 2.4721e-02, 2.4703e-04,
        1.7668e-04, 1.3397e-03, 7.3668e-04, 2.7845e-03, 6.9456e-03, 5.0130e-04,
        1.5401e-02, 6.6523e-02, 8.8147e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,693][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.8049, 0.0105, 0.0163, 0.0362, 0.0118, 0.0035, 0.0059, 0.0086, 0.0207,
        0.0070, 0.0052, 0.0082, 0.0265, 0.0316, 0.0032], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,695][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.3971, 0.0202, 0.0703, 0.2373, 0.0270, 0.0055, 0.0225, 0.0222, 0.0220,
        0.0091, 0.0065, 0.0216, 0.0881, 0.0209, 0.0299], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,696][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.7905, 0.0121, 0.0157, 0.0045, 0.0073, 0.0073, 0.0029, 0.0128, 0.0119,
        0.0040, 0.0034, 0.0195, 0.0165, 0.0175, 0.0741], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,697][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ as] are: tensor([9.0046e-01, 8.8869e-04, 1.6472e-03, 1.1108e-03, 1.3502e-02, 8.9051e-05,
        1.1073e-04, 2.6068e-04, 1.7604e-04, 1.1162e-04, 2.5757e-04, 5.1236e-05,
        6.8208e-04, 1.4781e-02, 1.3654e-02, 5.2220e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,698][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ as] are: tensor([9.8911e-01, 3.4706e-06, 7.6508e-06, 3.8136e-06, 3.2864e-03, 3.4737e-10,
        3.0807e-09, 1.1255e-07, 2.7074e-07, 5.1946e-08, 2.8560e-06, 2.5361e-08,
        7.8358e-07, 1.8253e-04, 2.7735e-04, 7.1288e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,699][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.3364, 0.0596, 0.0376, 0.1104, 0.0129, 0.0198, 0.0098, 0.0099, 0.0174,
        0.0152, 0.0085, 0.0111, 0.1192, 0.0730, 0.1096, 0.0496],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,700][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.3842, 0.0905, 0.0643, 0.0190, 0.0064, 0.0168, 0.0089, 0.0038, 0.0134,
        0.0149, 0.0092, 0.0112, 0.1731, 0.1259, 0.0324, 0.0261],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,702][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.3495, 0.0300, 0.0150, 0.0096, 0.0028, 0.0026, 0.0033, 0.0087, 0.0056,
        0.0043, 0.0091, 0.0044, 0.0564, 0.0953, 0.2770, 0.1264],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,703][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ as] are: tensor([7.9871e-01, 9.3261e-03, 1.1406e-02, 2.3328e-03, 1.1256e-01, 2.3270e-05,
        4.6454e-06, 5.3393e-05, 1.6268e-04, 6.3565e-05, 9.3393e-04, 9.8593e-05,
        2.7460e-03, 1.8534e-02, 3.9688e-03, 3.9074e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,704][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ as] are: tensor([7.8633e-01, 4.0251e-03, 2.6590e-03, 1.1998e-03, 1.0417e-03, 7.6706e-05,
        8.4096e-05, 8.4024e-04, 7.7123e-04, 1.5367e-03, 1.8770e-03, 1.5772e-03,
        1.5331e-02, 2.1150e-02, 1.1537e-01, 4.6124e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,705][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ as] are: tensor([3.6064e-01, 1.2826e-02, 2.0347e-01, 3.1248e-02, 1.0366e-03, 2.4532e-04,
        1.2207e-03, 1.8701e-03, 8.2249e-03, 1.4492e-03, 2.5730e-03, 4.3491e-03,
        1.7586e-02, 3.2661e-02, 2.9518e-01, 2.5421e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,706][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ as] are: tensor([6.7747e-01, 1.2732e-02, 6.7112e-03, 4.3473e-03, 1.2601e-01, 3.3308e-03,
        5.7661e-04, 4.0402e-03, 4.7307e-03, 5.7126e-03, 1.3342e-02, 6.5155e-04,
        3.4238e-02, 7.4036e-02, 1.3752e-02, 1.8321e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,708][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.7789, 0.0221, 0.0146, 0.0264, 0.0061, 0.0025, 0.0057, 0.0053, 0.0128,
        0.0075, 0.0066, 0.0075, 0.0269, 0.0429, 0.0146, 0.0197],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,709][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.4796, 0.0254, 0.0776, 0.1589, 0.0106, 0.0036, 0.0081, 0.0106, 0.0145,
        0.0047, 0.0053, 0.0122, 0.0667, 0.0150, 0.0687, 0.0387],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,710][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.5879, 0.0163, 0.0244, 0.0241, 0.0086, 0.0074, 0.0054, 0.0220, 0.0134,
        0.0285, 0.0102, 0.0175, 0.0242, 0.0250, 0.0366, 0.1486],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,711][circuit_into_ebeddingspace.py][line:2310][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.0957e-01, 7.0756e-03, 1.7994e-02, 1.7232e-02, 6.9871e-03, 2.6373e-03,
        1.9436e-03, 4.5373e-03, 1.0808e-03, 1.4450e-03, 1.9927e-04, 8.5967e-04,
        5.6007e-03, 2.0064e-03, 7.2147e-02, 2.2757e-02, 2.5924e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,712][circuit_into_ebeddingspace.py][line:2313][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([9.8794e-01, 9.6015e-04, 9.5634e-05, 2.3420e-06, 9.0478e-05, 1.1600e-08,
        1.6755e-08, 2.5522e-07, 2.8531e-07, 5.2711e-08, 1.0090e-06, 3.8537e-07,
        7.3084e-07, 6.1253e-06, 5.7641e-03, 6.9281e-04, 4.4423e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,714][circuit_into_ebeddingspace.py][line:2316][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.4882, 0.0252, 0.0151, 0.0552, 0.0022, 0.0054, 0.0035, 0.0062, 0.0063,
        0.0145, 0.0028, 0.0049, 0.0587, 0.0134, 0.0988, 0.1915, 0.0080],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,715][circuit_into_ebeddingspace.py][line:2319][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.4516, 0.0596, 0.0546, 0.0139, 0.0051, 0.0178, 0.0086, 0.0043, 0.0140,
        0.0076, 0.0058, 0.0092, 0.1190, 0.0669, 0.0640, 0.0645, 0.0334],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,716][circuit_into_ebeddingspace.py][line:2322][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.3854, 0.0117, 0.0116, 0.0109, 0.0040, 0.0045, 0.0075, 0.0171, 0.0092,
        0.0108, 0.0112, 0.0068, 0.0710, 0.0120, 0.1150, 0.2508, 0.0606],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,717][circuit_into_ebeddingspace.py][line:2325][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([7.0888e-01, 1.2873e-01, 3.0373e-02, 3.6645e-03, 1.1588e-02, 1.0777e-04,
        7.7046e-05, 1.9622e-04, 2.0574e-03, 4.4343e-04, 2.4951e-03, 8.1350e-04,
        4.7598e-03, 8.0122e-03, 5.9227e-02, 2.9392e-02, 9.1855e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,718][circuit_into_ebeddingspace.py][line:2328][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([8.8351e-01, 6.0309e-04, 2.2996e-03, 1.5133e-03, 9.3916e-04, 2.6599e-05,
        7.0099e-05, 3.1398e-04, 6.2956e-04, 3.6075e-04, 2.9749e-04, 3.7572e-04,
        1.6434e-03, 3.4732e-04, 1.7016e-02, 7.8834e-02, 1.1223e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,720][circuit_into_ebeddingspace.py][line:2331][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.5113, 0.0253, 0.2322, 0.0336, 0.0006, 0.0014, 0.0057, 0.0058, 0.0082,
        0.0016, 0.0042, 0.0051, 0.0127, 0.0141, 0.0840, 0.0403, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,721][circuit_into_ebeddingspace.py][line:2334][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.6284, 0.0769, 0.0644, 0.0322, 0.0202, 0.0024, 0.0007, 0.0036, 0.0030,
        0.0022, 0.0062, 0.0012, 0.0383, 0.0053, 0.0877, 0.0223, 0.0051],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,722][circuit_into_ebeddingspace.py][line:2337][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.6607, 0.0264, 0.0377, 0.0247, 0.0400, 0.0099, 0.0138, 0.0150, 0.0252,
        0.0076, 0.0108, 0.0119, 0.0341, 0.0356, 0.0108, 0.0199, 0.0161],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,723][circuit_into_ebeddingspace.py][line:2340][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3992, 0.0377, 0.1010, 0.1141, 0.0200, 0.0077, 0.0148, 0.0144, 0.0223,
        0.0074, 0.0051, 0.0152, 0.0959, 0.0307, 0.0567, 0.0431, 0.0145],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,725][circuit_into_ebeddingspace.py][line:2343][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5101, 0.0073, 0.0168, 0.0120, 0.0136, 0.0033, 0.0053, 0.0097, 0.0050,
        0.0199, 0.0058, 0.0081, 0.0156, 0.0619, 0.0113, 0.0478, 0.2465],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,732][circuit_into_ebeddingspace.py][line:2286][INFO] ################6-th layer#################
[2024-06-27 21:00:53,735][circuit_into_ebeddingspace.py][line:2288][INFO] ##6-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,737][circuit_into_ebeddingspace.py][line:2289][INFO] ##6-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,738][circuit_into_ebeddingspace.py][line:2290][INFO] ##6-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,740][circuit_into_ebeddingspace.py][line:2291][INFO] ##6-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,741][circuit_into_ebeddingspace.py][line:2292][INFO] ##6-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,742][circuit_into_ebeddingspace.py][line:2293][INFO] ##6-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,744][circuit_into_ebeddingspace.py][line:2294][INFO] ##6-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,745][circuit_into_ebeddingspace.py][line:2295][INFO] ##6-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [1, 0],
         [0, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,746][circuit_into_ebeddingspace.py][line:2296][INFO] ##6-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,748][circuit_into_ebeddingspace.py][line:2297][INFO] ##6-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,749][circuit_into_ebeddingspace.py][line:2298][INFO] ##6-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,751][circuit_into_ebeddingspace.py][line:2299][INFO] ##6-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,752][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,753][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,753][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,754][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,755][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,756][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,757][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,758][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,759][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,760][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,761][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,762][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,763][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9525, 0.0475], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,764][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9891, 0.0109], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,764][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9716, 0.0284], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,765][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9831, 0.0169], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,766][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9334, 0.0666], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,767][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9723, 0.0277], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,768][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9571, 0.0429], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,769][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9653, 0.0347], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,770][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9845, 0.0155], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,771][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ original] are: tensor([9.9979e-01, 2.1102e-04], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,772][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9741, 0.0259], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,773][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9589, 0.0411], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:53,774][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.7149, 0.2321, 0.0530], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,775][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9148, 0.0351, 0.0501], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,776][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9404, 0.0235, 0.0361], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,777][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.9576, 0.0313, 0.0111], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,778][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.9000, 0.0439, 0.0561], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,779][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8772, 0.0823, 0.0404], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,780][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9295, 0.0098, 0.0607], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,781][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8098, 0.0422, 0.1480], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,782][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.7322, 0.2172, 0.0506], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,783][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ language] are: tensor([9.9644e-01, 2.5547e-04, 3.3085e-03], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,784][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.9507, 0.0155, 0.0338], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,785][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8001, 0.1750, 0.0249], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:53,786][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.8438, 0.0665, 0.0651, 0.0246], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,787][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8112, 0.0318, 0.1391, 0.0178], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,788][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.9407, 0.0097, 0.0206, 0.0290], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,789][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.9506, 0.0187, 0.0217, 0.0090], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,790][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.8154, 0.0271, 0.0921, 0.0654], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,791][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.8715, 0.0455, 0.0317, 0.0513], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,792][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.9321, 0.0097, 0.0180, 0.0402], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,793][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.7464, 0.0367, 0.1893, 0.0276], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,794][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.2378, 0.1303, 0.6226, 0.0093], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,794][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ of] are: tensor([9.9800e-01, 3.3237e-04, 1.2164e-03, 4.4819e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,795][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.9198, 0.0145, 0.0517, 0.0140], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,796][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.7233, 0.0947, 0.1431, 0.0389], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:53,797][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.3746, 0.0230, 0.0393, 0.4919, 0.0713], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,798][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.9830, 0.0036, 0.0012, 0.0012, 0.0110], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,799][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.9569, 0.0080, 0.0141, 0.0162, 0.0048], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,800][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.9758, 0.0068, 0.0103, 0.0041, 0.0030], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,801][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.7560, 0.0325, 0.1122, 0.0577, 0.0415], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,802][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8106, 0.0287, 0.0354, 0.0693, 0.0560], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,803][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9386, 0.0033, 0.0050, 0.0052, 0.0478], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,804][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.8368, 0.0139, 0.0997, 0.0342, 0.0154], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,805][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.8407, 0.0252, 0.0371, 0.0825, 0.0145], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,806][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ De] are: tensor([9.9728e-01, 1.7298e-04, 1.6001e-03, 9.3819e-07, 9.4829e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,807][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.9737, 0.0054, 0.0135, 0.0034, 0.0040], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,808][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.8526, 0.0262, 0.0236, 0.0733, 0.0243], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:53,809][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.5242, 0.0138, 0.0251, 0.0463, 0.3451, 0.0455], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,810][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.1352e-01, 7.4712e-04, 3.1971e-03, 1.4455e-03, 6.2045e-02, 1.9043e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,811][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.9432, 0.0100, 0.0152, 0.0181, 0.0105, 0.0030], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,812][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.9873, 0.0012, 0.0030, 0.0035, 0.0036, 0.0014], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,813][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.6029, 0.0046, 0.0298, 0.0158, 0.2516, 0.0952], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,814][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.7905, 0.0085, 0.0167, 0.0246, 0.1018, 0.0579], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,816][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.9035, 0.0013, 0.0012, 0.0010, 0.0197, 0.0732], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,817][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.8819, 0.0086, 0.0224, 0.0064, 0.0567, 0.0240], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,818][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([8.7026e-01, 2.9203e-03, 8.5529e-04, 3.8197e-03, 1.0342e-01, 1.8732e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,819][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([9.9973e-01, 5.1472e-05, 5.8740e-05, 9.1399e-08, 1.1020e-04, 4.7880e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,820][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([9.8018e-01, 3.4110e-03, 4.0006e-03, 5.6783e-04, 1.1023e-03, 1.0741e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,821][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.7114, 0.0207, 0.0190, 0.0081, 0.1994, 0.0415], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:53,822][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.1318, 0.0135, 0.0289, 0.3672, 0.4134, 0.0364, 0.0089],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,823][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.9276, 0.0040, 0.0040, 0.0042, 0.0217, 0.0240, 0.0144],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,824][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.9130, 0.0076, 0.0192, 0.0185, 0.0108, 0.0054, 0.0255],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,825][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.8573, 0.0054, 0.0160, 0.0574, 0.0471, 0.0120, 0.0048],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,826][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4580, 0.0066, 0.0462, 0.0465, 0.0642, 0.0304, 0.3481],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,827][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.5400, 0.0185, 0.0433, 0.0867, 0.0652, 0.0898, 0.1565],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,828][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.8725, 0.0055, 0.0143, 0.0077, 0.0185, 0.0264, 0.0550],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,829][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.6458, 0.0204, 0.0696, 0.0594, 0.0939, 0.0234, 0.0876],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,830][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.0811, 0.0057, 0.0031, 0.0300, 0.3758, 0.4937, 0.0106],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,831][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([9.9779e-01, 4.0486e-05, 1.0562e-04, 2.6862e-05, 1.9458e-03, 6.3413e-08,
        9.2050e-05], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,832][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.9506, 0.0077, 0.0145, 0.0117, 0.0018, 0.0056, 0.0080],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,833][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.5132, 0.0490, 0.0397, 0.2567, 0.0598, 0.0657, 0.0159],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:53,834][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.4846, 0.0184, 0.0377, 0.0876, 0.1402, 0.0817, 0.0871, 0.0627],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,835][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.7657, 0.0019, 0.0046, 0.0020, 0.0776, 0.0310, 0.0409, 0.0763],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,836][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.9121, 0.0120, 0.0221, 0.0203, 0.0101, 0.0041, 0.0131, 0.0061],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,837][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.9645, 0.0030, 0.0043, 0.0070, 0.0080, 0.0064, 0.0036, 0.0032],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,838][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.1437, 0.0011, 0.0133, 0.0059, 0.0204, 0.0424, 0.5178, 0.2554],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,839][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.6886, 0.0175, 0.0203, 0.0342, 0.0322, 0.0499, 0.0913, 0.0660],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,840][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.8120, 0.0023, 0.0020, 0.0010, 0.0257, 0.0357, 0.0296, 0.0916],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,841][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.7644, 0.0067, 0.0288, 0.0069, 0.0320, 0.0385, 0.0923, 0.0306],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,843][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.5317, 0.0017, 0.0061, 0.0029, 0.0213, 0.0363, 0.3631, 0.0368],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,844][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([9.9958e-01, 2.3969e-04, 7.7674e-06, 2.4872e-09, 6.3300e-05, 3.3644e-10,
        1.0565e-04, 4.8776e-06], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,845][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([9.7855e-01, 1.6957e-03, 2.5266e-03, 7.9339e-04, 9.6000e-04, 3.8996e-03,
        6.4285e-03, 5.1428e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,846][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.5807, 0.0182, 0.0278, 0.0172, 0.0936, 0.0601, 0.1309, 0.0716],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:53,847][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0839, 0.0326, 0.0572, 0.3012, 0.3154, 0.0409, 0.0657, 0.0767, 0.0264],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,848][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.8641, 0.0055, 0.0053, 0.0118, 0.0278, 0.0047, 0.0155, 0.0425, 0.0228],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,849][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8933, 0.0077, 0.0200, 0.0194, 0.0099, 0.0038, 0.0202, 0.0046, 0.0212],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,850][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.6643, 0.0164, 0.0223, 0.1335, 0.0799, 0.0307, 0.0078, 0.0322, 0.0128],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,851][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.1238, 0.0017, 0.0168, 0.0167, 0.0057, 0.0100, 0.0937, 0.1268, 0.6048],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,852][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.5430, 0.0103, 0.0440, 0.0990, 0.0181, 0.0353, 0.0933, 0.0663, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,853][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8184, 0.0064, 0.0181, 0.0132, 0.0130, 0.0108, 0.0372, 0.0175, 0.0653],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,854][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.4901, 0.0176, 0.0477, 0.0262, 0.0537, 0.0280, 0.1437, 0.0951, 0.0979],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,855][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0682, 0.0099, 0.0145, 0.0152, 0.1295, 0.1479, 0.0283, 0.5749, 0.0116],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,856][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [orum] are: tensor([9.8995e-01, 1.0193e-04, 6.9026e-05, 7.0357e-06, 9.1064e-04, 1.3439e-09,
        2.0299e-07, 8.7681e-03, 1.9381e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,857][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.9319, 0.0096, 0.0135, 0.0101, 0.0011, 0.0026, 0.0033, 0.0089, 0.0191],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,859][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2621, 0.0543, 0.0922, 0.5103, 0.0243, 0.0157, 0.0176, 0.0152, 0.0084],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:53,860][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.3647, 0.0116, 0.0388, 0.2182, 0.1067, 0.0481, 0.0438, 0.0742, 0.0662,
        0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,861][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.5177, 0.0038, 0.0034, 0.0059, 0.0385, 0.0205, 0.0258, 0.1799, 0.1765,
        0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,862][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.8481, 0.0098, 0.0262, 0.0194, 0.0186, 0.0044, 0.0184, 0.0086, 0.0182,
        0.0284], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,863][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.8612, 0.0102, 0.0094, 0.0385, 0.0164, 0.0275, 0.0051, 0.0158, 0.0111,
        0.0050], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,864][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ et] are: tensor([1.0960e-01, 5.4994e-04, 3.4592e-03, 1.6301e-03, 4.0360e-03, 1.0127e-02,
        9.8716e-02, 9.3040e-02, 6.6634e-01, 1.2500e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,865][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.6427, 0.0201, 0.0139, 0.0378, 0.0211, 0.0277, 0.0409, 0.0383, 0.0998,
        0.0575], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,866][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.7811, 0.0038, 0.0041, 0.0035, 0.0207, 0.0161, 0.0352, 0.0305, 0.0504,
        0.0545], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,867][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.6242, 0.0141, 0.0257, 0.0190, 0.0250, 0.0372, 0.0621, 0.0647, 0.0981,
        0.0301], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,868][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.1541, 0.0015, 0.0094, 0.0052, 0.0232, 0.0585, 0.1134, 0.2532, 0.3430,
        0.0385], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,869][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ et] are: tensor([9.9869e-01, 1.1717e-04, 4.3921e-04, 1.7093e-06, 4.7484e-04, 9.9838e-06,
        1.6086e-05, 7.4747e-05, 1.2537e-04, 4.8620e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,870][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ et] are: tensor([9.7651e-01, 2.2936e-03, 2.1725e-03, 1.1025e-03, 4.2145e-04, 1.9754e-03,
        1.2917e-03, 2.7514e-03, 1.0170e-02, 1.3132e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,872][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.8064, 0.0174, 0.0133, 0.0616, 0.0304, 0.0073, 0.0146, 0.0181, 0.0097,
        0.0212], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:53,873][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.3152, 0.0111, 0.0420, 0.0421, 0.0826, 0.0324, 0.0740, 0.0562, 0.0533,
        0.2641, 0.0269], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,874][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.7678, 0.0056, 0.0247, 0.0088, 0.0127, 0.0086, 0.0358, 0.0284, 0.0651,
        0.0246, 0.0179], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,875][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.8588, 0.0111, 0.0106, 0.0148, 0.0119, 0.0044, 0.0158, 0.0105, 0.0182,
        0.0372, 0.0067], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,876][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.9162, 0.0039, 0.0074, 0.0183, 0.0076, 0.0078, 0.0047, 0.0080, 0.0055,
        0.0164, 0.0041], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,877][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.1445, 0.0011, 0.0078, 0.0019, 0.0086, 0.0230, 0.1424, 0.1801, 0.4426,
        0.0226, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,878][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.6651, 0.0160, 0.0213, 0.0409, 0.0154, 0.0113, 0.0280, 0.0333, 0.0400,
        0.0896, 0.0391], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,879][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.7956, 0.0016, 0.0028, 0.0021, 0.0267, 0.0349, 0.0169, 0.0285, 0.0243,
        0.0236, 0.0430], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,880][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.6165, 0.0162, 0.0434, 0.0177, 0.0234, 0.0179, 0.0751, 0.0487, 0.0758,
        0.0391, 0.0261], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,882][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.5757, 0.0080, 0.0074, 0.0024, 0.0082, 0.0055, 0.0211, 0.0243, 0.0706,
        0.2739, 0.0030], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,883][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([9.9785e-01, 2.0496e-04, 9.3284e-05, 9.2596e-06, 1.2332e-04, 9.1345e-07,
        1.2873e-03, 9.1766e-09, 4.2588e-04, 9.3932e-07, 7.1230e-07],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,884][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.4479e-01, 1.5461e-03, 5.1281e-03, 5.1446e-04, 2.3549e-04, 3.5278e-03,
        4.5098e-03, 4.5767e-03, 1.8866e-02, 1.5948e-03, 1.4710e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,885][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.6792, 0.0233, 0.0346, 0.0439, 0.0355, 0.0075, 0.0107, 0.0298, 0.0109,
        0.1059, 0.0187], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:53,886][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0668, 0.0158, 0.0701, 0.1566, 0.2912, 0.0252, 0.0525, 0.0634, 0.0250,
        0.1628, 0.0577, 0.0129], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,887][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.6016, 0.0311, 0.0167, 0.0256, 0.0204, 0.0045, 0.0292, 0.0333, 0.0945,
        0.0908, 0.0170, 0.0352], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,888][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8614, 0.0048, 0.0142, 0.0137, 0.0073, 0.0021, 0.0182, 0.0042, 0.0231,
        0.0187, 0.0064, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,889][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5771, 0.0134, 0.0174, 0.1081, 0.0989, 0.0198, 0.0089, 0.0391, 0.0110,
        0.0831, 0.0182, 0.0051], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,891][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.1289, 0.0007, 0.0087, 0.0059, 0.0025, 0.0054, 0.0582, 0.0633, 0.3814,
        0.0206, 0.0184, 0.3061], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,892][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.4211, 0.0113, 0.0158, 0.0533, 0.0087, 0.0266, 0.0390, 0.0461, 0.0481,
        0.1086, 0.0935, 0.1279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,893][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.7778, 0.0046, 0.0103, 0.0130, 0.0072, 0.0053, 0.0127, 0.0099, 0.0358,
        0.0151, 0.0057, 0.1026], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,894][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2886, 0.0119, 0.0437, 0.0241, 0.0416, 0.0296, 0.1127, 0.0722, 0.0954,
        0.1220, 0.0593, 0.0989], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,895][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0711, 0.0098, 0.0281, 0.0122, 0.1380, 0.0265, 0.0071, 0.0697, 0.0032,
        0.2229, 0.4070, 0.0044], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,896][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [orum] are: tensor([9.2017e-01, 1.3862e-04, 4.0734e-04, 2.2733e-04, 1.5721e-03, 2.9449e-08,
        9.0274e-07, 2.3022e-03, 3.2562e-05, 7.4609e-02, 1.0392e-04, 4.3510e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,897][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [orum] are: tensor([9.5107e-01, 5.4234e-03, 8.5998e-03, 4.3891e-03, 4.3679e-04, 1.6279e-03,
        9.6741e-04, 2.7710e-03, 3.7643e-03, 1.1505e-03, 4.7489e-03, 1.5047e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,898][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.3223, 0.0568, 0.0943, 0.4092, 0.0087, 0.0055, 0.0125, 0.0087, 0.0062,
        0.0572, 0.0121, 0.0065], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:53,900][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.2049, 0.3783, 0.1658, 0.1318, 0.0153, 0.0225, 0.0051, 0.0106, 0.0168,
        0.0119, 0.0097, 0.0046, 0.0227], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,901][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ is] are: tensor([1.7564e-01, 3.3478e-02, 6.5058e-01, 1.2691e-01, 6.0985e-04, 9.0515e-04,
        4.2318e-04, 5.8178e-04, 3.6143e-03, 6.4563e-04, 1.1332e-03, 9.5007e-04,
        4.5285e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,902][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7210, 0.0077, 0.0288, 0.0106, 0.0109, 0.0071, 0.0219, 0.0078, 0.0398,
        0.0262, 0.0114, 0.0493, 0.0575], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,903][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.4432, 0.1376, 0.0253, 0.0973, 0.0605, 0.0284, 0.0379, 0.0201, 0.0621,
        0.0275, 0.0114, 0.0232, 0.0254], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,904][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.4786, 0.0107, 0.0281, 0.0296, 0.0140, 0.0057, 0.0388, 0.0142, 0.1493,
        0.0106, 0.0114, 0.0983, 0.1108], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,905][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.4799, 0.0512, 0.0389, 0.0356, 0.0144, 0.0104, 0.0200, 0.0286, 0.0574,
        0.0688, 0.0508, 0.0531, 0.0908], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,906][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.7882, 0.0101, 0.0410, 0.0102, 0.0111, 0.0025, 0.0079, 0.0043, 0.0127,
        0.0024, 0.0027, 0.0075, 0.0993], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,908][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.1773, 0.0246, 0.0725, 0.1136, 0.0334, 0.0111, 0.1036, 0.0322, 0.1869,
        0.0772, 0.0230, 0.1239, 0.0207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,909][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.5279, 0.0832, 0.1252, 0.0431, 0.0218, 0.0158, 0.0201, 0.0102, 0.0419,
        0.0360, 0.0389, 0.0184, 0.0174], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,910][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ is] are: tensor([9.8961e-01, 2.3817e-04, 6.6179e-04, 2.2313e-04, 4.0646e-03, 2.0940e-06,
        1.0646e-06, 3.5206e-04, 2.5646e-05, 2.6344e-04, 1.5423e-03, 4.0632e-05,
        2.9795e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,911][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ is] are: tensor([9.5441e-01, 4.4377e-03, 8.6439e-03, 5.1328e-03, 1.7243e-03, 6.3448e-04,
        1.5688e-03, 2.4000e-03, 2.7320e-03, 1.4118e-03, 2.2419e-03, 3.6514e-03,
        1.1006e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,912][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.5395, 0.1068, 0.1042, 0.1641, 0.0041, 0.0092, 0.0063, 0.0030, 0.0093,
        0.0074, 0.0046, 0.0020, 0.0395], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:53,913][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2068, 0.1303, 0.1575, 0.0416, 0.0107, 0.0090, 0.0134, 0.0102, 0.0258,
        0.0103, 0.0055, 0.0093, 0.2812, 0.0883], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,914][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([4.6118e-01, 2.5558e-02, 4.0787e-01, 8.1486e-02, 9.7695e-04, 7.5743e-04,
        1.1862e-03, 6.2856e-04, 6.8154e-03, 4.4553e-04, 5.4420e-04, 1.7082e-03,
        4.5129e-03, 6.3282e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,916][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7686, 0.0082, 0.0225, 0.0122, 0.0101, 0.0051, 0.0153, 0.0054, 0.0267,
        0.0159, 0.0091, 0.0322, 0.0428, 0.0260], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,917][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.6933, 0.0323, 0.0161, 0.0412, 0.0406, 0.0146, 0.0231, 0.0139, 0.0448,
        0.0216, 0.0059, 0.0147, 0.0165, 0.0214], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,918][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.5780, 0.0074, 0.0409, 0.0257, 0.0144, 0.0073, 0.0332, 0.0167, 0.0620,
        0.0070, 0.0102, 0.0365, 0.0683, 0.0924], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,919][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.5239, 0.0463, 0.0514, 0.0312, 0.0144, 0.0089, 0.0128, 0.0203, 0.0268,
        0.0183, 0.0318, 0.0160, 0.0625, 0.1353], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,920][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.7801, 0.0150, 0.0340, 0.0096, 0.0118, 0.0015, 0.0048, 0.0030, 0.0073,
        0.0017, 0.0016, 0.0034, 0.0559, 0.0703], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,922][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.3213, 0.0170, 0.1268, 0.0521, 0.0253, 0.0200, 0.0616, 0.0559, 0.0969,
        0.0699, 0.0353, 0.0695, 0.0342, 0.0141], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,923][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7632, 0.0121, 0.0207, 0.0064, 0.0015, 0.0066, 0.0057, 0.0039, 0.0080,
        0.0121, 0.0093, 0.0070, 0.1022, 0.0411], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,924][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([9.8205e-01, 1.0221e-02, 1.7301e-03, 1.0881e-04, 1.0152e-03, 1.1601e-06,
        6.8252e-07, 2.0482e-05, 2.7709e-05, 6.4716e-05, 2.1088e-03, 1.1980e-04,
        4.3257e-04, 2.0974e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,925][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([9.6467e-01, 4.4929e-03, 7.7581e-03, 2.9291e-03, 2.1591e-03, 5.2003e-04,
        1.0088e-03, 9.1989e-04, 1.3849e-03, 3.7511e-04, 7.9818e-04, 1.0924e-03,
        6.4457e-03, 5.4454e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,926][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.4832, 0.0623, 0.1455, 0.0776, 0.0089, 0.0157, 0.0129, 0.0076, 0.0109,
        0.0103, 0.0083, 0.0035, 0.1162, 0.0370], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:53,927][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1787, 0.1034, 0.1428, 0.0273, 0.0078, 0.0094, 0.0109, 0.0058, 0.0191,
        0.0157, 0.0052, 0.0110, 0.1831, 0.2157, 0.0642], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,928][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ same] are: tensor([6.2255e-01, 3.9976e-02, 2.3888e-01, 6.2901e-02, 5.3827e-04, 4.0687e-04,
        1.3936e-03, 2.9070e-04, 3.5216e-03, 9.9282e-04, 3.1341e-04, 1.6312e-03,
        2.4898e-03, 1.1578e-02, 1.2535e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,930][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.7895, 0.0078, 0.0197, 0.0077, 0.0141, 0.0044, 0.0241, 0.0046, 0.0316,
        0.0151, 0.0115, 0.0188, 0.0298, 0.0141, 0.0072], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,931][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.6533, 0.0315, 0.0194, 0.0305, 0.0884, 0.0057, 0.0170, 0.0112, 0.0193,
        0.0249, 0.0045, 0.0071, 0.0312, 0.0415, 0.0146], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,932][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.6493, 0.0166, 0.0235, 0.0382, 0.0056, 0.0022, 0.0250, 0.0023, 0.0165,
        0.0011, 0.0027, 0.0055, 0.0596, 0.1117, 0.0402], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,933][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.6186, 0.0356, 0.0153, 0.0158, 0.0115, 0.0056, 0.0157, 0.0106, 0.0481,
        0.0330, 0.0209, 0.0355, 0.0354, 0.0822, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,934][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ same] are: tensor([8.5889e-01, 9.5132e-03, 2.3382e-02, 7.8831e-03, 3.1975e-03, 2.5289e-04,
        2.9328e-03, 1.4470e-03, 5.1492e-03, 8.4994e-04, 8.7574e-04, 2.9503e-03,
        3.0538e-02, 2.4098e-02, 2.8039e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,936][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.1711, 0.0114, 0.0996, 0.1299, 0.0288, 0.0065, 0.0419, 0.0320, 0.1010,
        0.1084, 0.0324, 0.0807, 0.0594, 0.0772, 0.0199], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,937][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.4292, 0.0055, 0.0095, 0.0045, 0.0013, 0.0007, 0.0016, 0.0010, 0.0028,
        0.0029, 0.0046, 0.0050, 0.1477, 0.3635, 0.0201], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,938][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ same] are: tensor([9.9381e-01, 3.0878e-04, 3.5271e-03, 2.5650e-04, 7.4778e-04, 2.6733e-09,
        1.6029e-07, 2.1309e-08, 1.6300e-05, 2.4856e-06, 1.5886e-05, 8.0983e-05,
        4.4188e-05, 1.0733e-04, 1.0807e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,939][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ same] are: tensor([9.6128e-01, 2.5844e-03, 6.5666e-03, 3.8161e-03, 2.0334e-03, 7.8148e-04,
        7.6494e-04, 3.7587e-04, 6.7335e-04, 3.4216e-04, 6.9106e-04, 7.7284e-04,
        4.7098e-03, 3.2558e-03, 1.1354e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,940][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.5137, 0.0288, 0.0257, 0.0255, 0.0010, 0.0014, 0.0018, 0.0014, 0.0044,
        0.0063, 0.0025, 0.0017, 0.1940, 0.1252, 0.0665], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:53,941][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.4259, 0.0653, 0.1184, 0.0143, 0.0116, 0.0089, 0.0062, 0.0055, 0.0103,
        0.0029, 0.0032, 0.0056, 0.1560, 0.0370, 0.1227, 0.0062],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,943][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.6603, 0.0788, 0.0521, 0.0658, 0.0262, 0.0134, 0.0106, 0.0049, 0.0269,
        0.0062, 0.0053, 0.0081, 0.0072, 0.0297, 0.0026, 0.0018],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,944][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.7753, 0.0078, 0.0183, 0.0101, 0.0080, 0.0045, 0.0171, 0.0048, 0.0273,
        0.0180, 0.0078, 0.0202, 0.0319, 0.0172, 0.0086, 0.0232],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,945][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.6025, 0.0528, 0.0339, 0.0330, 0.0419, 0.0110, 0.0203, 0.0105, 0.0250,
        0.0185, 0.0060, 0.0088, 0.0547, 0.0427, 0.0216, 0.0169],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,946][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.6112, 0.0191, 0.0168, 0.0147, 0.0063, 0.0075, 0.0302, 0.0081, 0.0452,
        0.0042, 0.0072, 0.0281, 0.0698, 0.0524, 0.0484, 0.0308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,947][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.6672, 0.0344, 0.0110, 0.0120, 0.0188, 0.0052, 0.0135, 0.0076, 0.0292,
        0.0179, 0.0154, 0.0339, 0.0195, 0.0391, 0.0170, 0.0584],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,949][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.7497, 0.0116, 0.0173, 0.0285, 0.0108, 0.0010, 0.0035, 0.0041, 0.0027,
        0.0017, 0.0016, 0.0017, 0.0358, 0.0257, 0.0092, 0.0950],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,950][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.1994, 0.0159, 0.1565, 0.1382, 0.0152, 0.0105, 0.0473, 0.0324, 0.1218,
        0.0291, 0.0263, 0.0622, 0.0574, 0.0520, 0.0256, 0.0103],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,951][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3947, 0.0152, 0.0160, 0.0023, 0.0027, 0.0044, 0.0061, 0.0027, 0.0028,
        0.0055, 0.0093, 0.0058, 0.1062, 0.1145, 0.1959, 0.1159],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,952][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ as] are: tensor([9.6865e-01, 5.1994e-04, 1.4152e-03, 2.5616e-05, 1.2674e-02, 7.4968e-07,
        5.3176e-07, 3.1744e-06, 5.1948e-05, 1.2356e-05, 3.0961e-04, 7.9357e-06,
        1.7827e-04, 8.4604e-04, 2.6292e-04, 1.5040e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,953][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ as] are: tensor([9.5703e-01, 2.0606e-03, 5.5773e-03, 2.6916e-03, 1.2205e-03, 3.0237e-04,
        7.5138e-04, 3.8403e-04, 8.3501e-04, 1.0953e-04, 2.9208e-04, 5.0410e-04,
        4.5741e-03, 4.4756e-03, 9.9404e-03, 9.2473e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,955][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.6089, 0.0194, 0.0208, 0.0088, 0.0014, 0.0069, 0.0076, 0.0028, 0.0162,
        0.0016, 0.0056, 0.0044, 0.0466, 0.0496, 0.1229, 0.0765],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:53,956][circuit_into_ebeddingspace.py][line:2310][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.3508, 0.0223, 0.0564, 0.0185, 0.0068, 0.0046, 0.0074, 0.0088, 0.0068,
        0.0046, 0.0033, 0.0050, 0.2238, 0.0408, 0.0799, 0.1378, 0.0226],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,957][circuit_into_ebeddingspace.py][line:2313][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.3574, 0.2473, 0.1473, 0.1281, 0.0131, 0.0030, 0.0121, 0.0024, 0.0286,
        0.0026, 0.0031, 0.0067, 0.0077, 0.0092, 0.0244, 0.0035, 0.0034],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,958][circuit_into_ebeddingspace.py][line:2316][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7991, 0.0053, 0.0159, 0.0065, 0.0077, 0.0040, 0.0153, 0.0037, 0.0223,
        0.0110, 0.0081, 0.0200, 0.0227, 0.0121, 0.0078, 0.0232, 0.0153],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,960][circuit_into_ebeddingspace.py][line:2319][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.6780, 0.0223, 0.0315, 0.0200, 0.0426, 0.0097, 0.0166, 0.0101, 0.0251,
        0.0162, 0.0051, 0.0083, 0.0352, 0.0129, 0.0259, 0.0297, 0.0108],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,961][circuit_into_ebeddingspace.py][line:2322][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.4488, 0.0161, 0.0318, 0.0166, 0.0100, 0.0065, 0.0442, 0.0209, 0.0748,
        0.0087, 0.0088, 0.0368, 0.0968, 0.0554, 0.0388, 0.0326, 0.0525],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,962][circuit_into_ebeddingspace.py][line:2325][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.5847, 0.0306, 0.0149, 0.0105, 0.0154, 0.0066, 0.0135, 0.0100, 0.0196,
        0.0150, 0.0167, 0.0243, 0.0283, 0.0319, 0.0261, 0.0779, 0.0740],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,963][circuit_into_ebeddingspace.py][line:2328][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6946, 0.0177, 0.0148, 0.0156, 0.0178, 0.0010, 0.0022, 0.0031, 0.0021,
        0.0020, 0.0018, 0.0025, 0.0250, 0.0600, 0.0065, 0.0193, 0.1139],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,964][circuit_into_ebeddingspace.py][line:2331][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2303, 0.0209, 0.1816, 0.0852, 0.0139, 0.0150, 0.0410, 0.0286, 0.0676,
        0.0208, 0.0310, 0.0499, 0.0800, 0.0360, 0.0448, 0.0241, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,966][circuit_into_ebeddingspace.py][line:2334][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.4181, 0.0050, 0.0081, 0.0060, 0.0013, 0.0016, 0.0020, 0.0023, 0.0019,
        0.0036, 0.0041, 0.0039, 0.0721, 0.0197, 0.1057, 0.3204, 0.0242],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,967][circuit_into_ebeddingspace.py][line:2337][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([9.2016e-01, 2.9672e-02, 4.2480e-03, 1.1151e-04, 6.1164e-03, 7.6457e-06,
        9.8951e-06, 4.0058e-05, 5.0885e-04, 1.4479e-04, 1.5723e-03, 6.3389e-04,
        5.7247e-04, 3.4957e-04, 1.0378e-02, 1.0032e-02, 1.5444e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,968][circuit_into_ebeddingspace.py][line:2340][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([9.1756e-01, 3.9616e-03, 1.5484e-02, 4.9187e-03, 2.5407e-03, 5.5363e-04,
        1.2392e-03, 1.0298e-03, 8.6349e-04, 1.6884e-04, 6.6345e-04, 8.7367e-04,
        9.7643e-03, 5.8322e-03, 1.2591e-02, 7.1286e-03, 1.4825e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,969][circuit_into_ebeddingspace.py][line:2343][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.4329, 0.0094, 0.0173, 0.0117, 0.0015, 0.0040, 0.0050, 0.0060, 0.0079,
        0.0045, 0.0064, 0.0043, 0.0703, 0.0296, 0.0409, 0.3058, 0.0426],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:53,977][circuit_into_ebeddingspace.py][line:2286][INFO] ################7-th layer#################
[2024-06-27 21:00:53,980][circuit_into_ebeddingspace.py][line:2288][INFO] ##7-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,982][circuit_into_ebeddingspace.py][line:2289][INFO] ##7-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,983][circuit_into_ebeddingspace.py][line:2290][INFO] ##7-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,984][circuit_into_ebeddingspace.py][line:2291][INFO] ##7-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,986][circuit_into_ebeddingspace.py][line:2292][INFO] ##7-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,987][circuit_into_ebeddingspace.py][line:2293][INFO] ##7-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,989][circuit_into_ebeddingspace.py][line:2294][INFO] ##7-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,990][circuit_into_ebeddingspace.py][line:2295][INFO] ##7-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,991][circuit_into_ebeddingspace.py][line:2296][INFO] ##7-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,993][circuit_into_ebeddingspace.py][line:2297][INFO] ##7-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:53,994][circuit_into_ebeddingspace.py][line:2298][INFO] ##7-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1]]], device='cuda:0')
[2024-06-27 21:00:53,996][circuit_into_ebeddingspace.py][line:2299][INFO] ##7-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:53,997][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,997][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,998][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:53,999][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,000][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,001][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,002][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,003][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,004][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,005][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,006][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,007][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,007][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9351, 0.0649], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,008][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9502, 0.0498], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,009][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9964, 0.0036], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,010][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9489, 0.0511], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,011][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9702, 0.0298], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,012][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9563, 0.0437], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,013][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9639, 0.0361], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,014][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9852, 0.0148], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,015][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9852, 0.0148], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,016][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9629, 0.0371], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,017][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9943, 0.0057], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,018][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9612, 0.0388], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,019][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.7281, 0.2517, 0.0202], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,020][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9723, 0.0030, 0.0248], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,021][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ language] are: tensor([9.9495e-01, 2.2073e-04, 4.8278e-03], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,022][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8784, 0.0744, 0.0472], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,023][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8984, 0.0654, 0.0362], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,024][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8980, 0.0859, 0.0161], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,025][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9602, 0.0122, 0.0276], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,026][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.9349, 0.0319, 0.0332], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,027][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9265, 0.0391, 0.0344], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,028][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9208, 0.0568, 0.0225], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,029][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ language] are: tensor([9.9752e-01, 1.3258e-04, 2.3461e-03], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,029][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9772, 0.0058, 0.0170], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,030][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.8486, 0.0353, 0.0971, 0.0189], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,031][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.9163, 0.0073, 0.0676, 0.0088], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,032][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ of] are: tensor([9.9792e-01, 2.2412e-04, 1.2467e-03, 6.0641e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,033][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.5491, 0.0887, 0.2969, 0.0653], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,034][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9655, 0.0164, 0.0142, 0.0039], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,035][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.7771, 0.1183, 0.0966, 0.0080], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,036][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.9215, 0.0122, 0.0175, 0.0488], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,037][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.9311, 0.0287, 0.0328, 0.0074], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,038][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.7577, 0.0354, 0.1869, 0.0199], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,039][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.8306, 0.1344, 0.0244, 0.0106], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,040][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ of] are: tensor([9.9124e-01, 6.7996e-04, 5.5341e-03, 2.5446e-03], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,041][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.9880, 0.0073, 0.0026, 0.0020], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,042][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.6040, 0.0056, 0.0293, 0.3316, 0.0295], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,043][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.8962, 0.0040, 0.0205, 0.0028, 0.0765], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,044][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ De] are: tensor([9.9324e-01, 2.6063e-04, 1.7351e-03, 4.9009e-05, 4.7144e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,045][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.8944, 0.0212, 0.0301, 0.0153, 0.0390], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,046][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9692, 0.0080, 0.0044, 0.0145, 0.0039], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,047][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8034, 0.0195, 0.0903, 0.0327, 0.0542], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,048][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9236, 0.0162, 0.0153, 0.0181, 0.0268], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,049][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.9735, 0.0079, 0.0108, 0.0016, 0.0062], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,050][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.9778, 0.0019, 0.0055, 0.0117, 0.0031], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,051][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.9660, 0.0124, 0.0042, 0.0066, 0.0109], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,052][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ De] are: tensor([9.9625e-01, 2.3084e-04, 1.7184e-03, 2.3754e-05, 1.7814e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,053][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ De] are: tensor([9.9350e-01, 1.4483e-03, 3.0900e-03, 4.8575e-04, 1.4730e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,054][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.1044, 0.0012, 0.0015, 0.0086, 0.8758, 0.0086], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,055][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.8005e-01, 2.1000e-03, 1.4416e-03, 2.2316e-04, 2.8833e-03, 1.3297e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,056][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([9.9974e-01, 8.3944e-05, 7.0071e-05, 9.8580e-06, 1.7484e-05, 7.8187e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,057][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.9552, 0.0029, 0.0068, 0.0022, 0.0095, 0.0233], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,058][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.8785, 0.0140, 0.0072, 0.0293, 0.0629, 0.0081], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,059][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.7472, 0.0131, 0.0485, 0.0159, 0.1268, 0.0484], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,060][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.9772, 0.0043, 0.0042, 0.0027, 0.0041, 0.0074], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,061][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.9869, 0.0020, 0.0045, 0.0026, 0.0017, 0.0022], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,062][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.8822, 0.0032, 0.0019, 0.0026, 0.0913, 0.0188], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,063][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.9851, 0.0015, 0.0010, 0.0016, 0.0038, 0.0070], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,064][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([9.9950e-01, 2.6795e-04, 6.2935e-05, 1.6449e-06, 1.0435e-04, 6.1850e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,065][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([9.9072e-01, 1.4011e-03, 1.1735e-03, 8.8161e-04, 5.6711e-04, 5.2608e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,066][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0419, 0.0054, 0.0055, 0.1282, 0.7565, 0.0249, 0.0377],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,067][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.9127, 0.0026, 0.0036, 0.0011, 0.0080, 0.0072, 0.0647],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,068][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([9.9263e-01, 1.5876e-04, 8.4416e-04, 1.0118e-03, 1.2732e-03, 9.7688e-06,
        4.0722e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,069][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.6829, 0.0364, 0.0397, 0.0463, 0.0663, 0.0462, 0.0824],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,070][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.1688, 0.1220, 0.0510, 0.2132, 0.3893, 0.0442, 0.0114],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,071][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.5638, 0.0408, 0.1502, 0.1266, 0.0550, 0.0290, 0.0346],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,072][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.8682, 0.0211, 0.0304, 0.0432, 0.0161, 0.0043, 0.0166],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,073][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.9565, 0.0090, 0.0035, 0.0047, 0.0105, 0.0041, 0.0118],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,074][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.5414, 0.0117, 0.0543, 0.1393, 0.1201, 0.1023, 0.0308],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,075][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.7576, 0.0397, 0.0291, 0.0638, 0.0546, 0.0474, 0.0078],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,076][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([9.9051e-01, 2.0620e-04, 2.6095e-03, 1.0145e-03, 5.2244e-03, 4.2509e-06,
        4.3351e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,078][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.9570, 0.0019, 0.0025, 0.0249, 0.0084, 0.0017, 0.0035],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,079][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.2989, 0.0013, 0.0186, 0.0267, 0.0983, 0.0067, 0.5177, 0.0317],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,080][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([9.5809e-01, 1.2648e-03, 7.9307e-04, 1.7377e-04, 2.2124e-03, 5.8841e-03,
        1.7359e-02, 1.4226e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,081][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([9.9810e-01, 1.2060e-04, 3.5742e-05, 5.1089e-06, 1.1121e-04, 9.5934e-07,
        1.4714e-03, 1.4999e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,082][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.9181, 0.0030, 0.0073, 0.0023, 0.0066, 0.0261, 0.0172, 0.0193],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,083][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.9545, 0.0076, 0.0087, 0.0166, 0.0075, 0.0022, 0.0011, 0.0017],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,084][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.7516, 0.0083, 0.0362, 0.0051, 0.0260, 0.0447, 0.0521, 0.0760],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,085][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.9413, 0.0071, 0.0062, 0.0027, 0.0046, 0.0062, 0.0253, 0.0065],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,086][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.9380, 0.0065, 0.0075, 0.0039, 0.0050, 0.0074, 0.0266, 0.0051],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,087][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.8784, 0.0022, 0.0038, 0.0044, 0.0141, 0.0137, 0.0525, 0.0310],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,088][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.9793, 0.0041, 0.0019, 0.0029, 0.0040, 0.0027, 0.0030, 0.0022],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,089][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([9.9913e-01, 3.2565e-04, 2.1432e-05, 4.9870e-07, 1.1462e-04, 8.4037e-08,
        4.0538e-04, 4.4082e-06], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,090][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([9.6611e-01, 7.5925e-04, 2.6136e-04, 1.5326e-04, 1.3169e-03, 3.1710e-03,
        2.7753e-02, 4.7756e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,091][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.1564, 0.0070, 0.0311, 0.3827, 0.1546, 0.0099, 0.1508, 0.0723, 0.0352],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,092][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.5728e-01, 1.7886e-03, 1.5867e-03, 5.0609e-04, 2.4925e-03, 4.8113e-03,
        4.6999e-03, 1.4282e-02, 1.2548e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,093][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [orum] are: tensor([9.9306e-01, 6.8641e-05, 1.1959e-04, 6.6075e-05, 3.0239e-04, 8.9997e-07,
        2.9160e-05, 5.6001e-03, 7.5403e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,094][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.6960, 0.0169, 0.0267, 0.0383, 0.0324, 0.0232, 0.0389, 0.0775, 0.0500],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,095][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.1354, 0.0956, 0.0780, 0.1837, 0.3444, 0.0671, 0.0212, 0.0657, 0.0090],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,096][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.5391, 0.0415, 0.0852, 0.1320, 0.0222, 0.0208, 0.0525, 0.0606, 0.0461],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,097][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8937, 0.0098, 0.0155, 0.0234, 0.0093, 0.0034, 0.0140, 0.0156, 0.0153],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,099][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.9676, 0.0033, 0.0020, 0.0014, 0.0039, 0.0018, 0.0040, 0.0022, 0.0137],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,100][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1937, 0.0026, 0.0359, 0.0764, 0.0302, 0.0291, 0.0381, 0.5729, 0.0209],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,101][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6048, 0.0318, 0.0433, 0.0753, 0.0707, 0.0459, 0.0084, 0.1113, 0.0086],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,102][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [orum] are: tensor([9.7010e-01, 3.1908e-04, 8.7055e-04, 2.7373e-04, 2.7977e-03, 1.4169e-07,
        2.3722e-05, 2.3990e-02, 1.6247e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,103][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [orum] are: tensor([9.6818e-01, 1.9173e-03, 1.6929e-03, 7.2140e-03, 4.5982e-03, 2.8573e-04,
        5.4491e-04, 1.3593e-02, 1.9773e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,104][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.4862, 0.0025, 0.0308, 0.1351, 0.0381, 0.0067, 0.1010, 0.0252, 0.1451,
        0.0293], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,105][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ et] are: tensor([9.6405e-01, 1.1553e-03, 2.4343e-03, 6.8264e-04, 3.2626e-03, 1.7314e-02,
        2.8185e-03, 5.2178e-03, 2.3315e-03, 7.3341e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,106][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ et] are: tensor([9.9472e-01, 1.7619e-04, 5.5638e-04, 1.3293e-05, 2.3999e-04, 3.8979e-04,
        3.3657e-04, 8.9595e-04, 1.5069e-03, 1.1633e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,107][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.8894, 0.0071, 0.0071, 0.0038, 0.0058, 0.0289, 0.0113, 0.0277, 0.0112,
        0.0075], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,108][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.7546, 0.0354, 0.0308, 0.0543, 0.0678, 0.0194, 0.0189, 0.0081, 0.0068,
        0.0039], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,109][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.7812, 0.0108, 0.0335, 0.0252, 0.0053, 0.0221, 0.0426, 0.0387, 0.0327,
        0.0079], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,110][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.8712, 0.0073, 0.0061, 0.0044, 0.0080, 0.0070, 0.0272, 0.0191, 0.0350,
        0.0146], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,111][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.7844, 0.0124, 0.0105, 0.0054, 0.0069, 0.0458, 0.0432, 0.0138, 0.0553,
        0.0222], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,113][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.6412, 0.0042, 0.0441, 0.0237, 0.0527, 0.0126, 0.0654, 0.0832, 0.0628,
        0.0099], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,114][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.9097, 0.0123, 0.0191, 0.0178, 0.0201, 0.0060, 0.0023, 0.0063, 0.0022,
        0.0042], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,115][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ et] are: tensor([9.9335e-01, 1.5687e-03, 1.3812e-03, 4.3559e-05, 1.2323e-03, 4.2986e-04,
        5.1257e-05, 8.0979e-04, 9.7860e-04, 1.5867e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,116][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ et] are: tensor([9.7909e-01, 3.8178e-03, 6.9146e-04, 5.2472e-04, 4.6759e-03, 3.6963e-03,
        2.0700e-03, 1.7465e-03, 2.8483e-03, 8.4190e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,117][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([1.0627e-01, 6.4993e-04, 1.8725e-03, 6.4408e-03, 7.8052e-03, 1.1924e-03,
        2.7389e-02, 2.0959e-02, 5.4120e-02, 7.5891e-01, 1.4387e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,118][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.8769, 0.0040, 0.0056, 0.0009, 0.0054, 0.0156, 0.0344, 0.0151, 0.0199,
        0.0072, 0.0150], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,119][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([9.9540e-01, 1.6231e-04, 3.7983e-04, 2.5436e-05, 7.7368e-05, 5.5236e-05,
        3.3346e-03, 4.5229e-06, 4.8818e-04, 1.5257e-05, 5.3331e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,120][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.8929, 0.0017, 0.0092, 0.0037, 0.0050, 0.0101, 0.0110, 0.0130, 0.0178,
        0.0124, 0.0233], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,121][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.9055, 0.0118, 0.0114, 0.0312, 0.0105, 0.0066, 0.0043, 0.0040, 0.0028,
        0.0087, 0.0033], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,122][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.7407, 0.0028, 0.0468, 0.0191, 0.0083, 0.0166, 0.0299, 0.0273, 0.0637,
        0.0229, 0.0217], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,124][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.8524, 0.0042, 0.0055, 0.0026, 0.0055, 0.0139, 0.0350, 0.0244, 0.0353,
        0.0156, 0.0058], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,125][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.8712, 0.0038, 0.0059, 0.0025, 0.0055, 0.0123, 0.0291, 0.0157, 0.0243,
        0.0233, 0.0062], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,126][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.8611, 0.0021, 0.0081, 0.0068, 0.0049, 0.0085, 0.0068, 0.0791, 0.0041,
        0.0115, 0.0071], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,127][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.9413, 0.0049, 0.0074, 0.0125, 0.0056, 0.0034, 0.0034, 0.0038, 0.0028,
        0.0113, 0.0038], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,128][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.8921e-01, 3.8942e-04, 1.6334e-04, 2.0988e-05, 1.2333e-03, 5.0712e-05,
        7.7253e-03, 4.2030e-07, 1.1634e-03, 1.2972e-05, 3.2610e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,129][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([8.4756e-01, 1.1644e-03, 2.2977e-03, 8.9653e-04, 3.0599e-03, 8.4303e-03,
        9.3396e-02, 8.0349e-04, 3.9761e-02, 6.1647e-04, 2.0111e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,130][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0759, 0.0102, 0.0190, 0.3351, 0.0842, 0.0046, 0.0384, 0.0227, 0.0154,
        0.3091, 0.0684, 0.0170], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,131][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.8729, 0.0025, 0.0044, 0.0014, 0.0049, 0.0040, 0.0031, 0.0142, 0.0102,
        0.0066, 0.0534, 0.0225], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,132][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [orum] are: tensor([9.8697e-01, 1.2292e-04, 4.1999e-04, 2.5272e-04, 3.0322e-04, 2.0724e-06,
        1.9044e-05, 1.7791e-03, 1.1573e-04, 8.6058e-03, 4.4232e-04, 9.6621e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,133][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5398, 0.0219, 0.0330, 0.0627, 0.0256, 0.0086, 0.0261, 0.0473, 0.0505,
        0.0843, 0.0427, 0.0574], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,135][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0823, 0.0795, 0.0671, 0.2028, 0.2889, 0.0557, 0.0443, 0.0411, 0.0128,
        0.0819, 0.0375, 0.0062], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,136][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.5266, 0.0274, 0.1218, 0.1249, 0.0102, 0.0117, 0.0447, 0.0253, 0.0583,
        0.0181, 0.0094, 0.0215], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,137][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8288, 0.0072, 0.0122, 0.0231, 0.0066, 0.0038, 0.0197, 0.0215, 0.0259,
        0.0249, 0.0054, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,138][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.9726, 0.0044, 0.0023, 0.0018, 0.0025, 0.0012, 0.0013, 0.0010, 0.0038,
        0.0042, 0.0027, 0.0022], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,139][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1088, 0.0037, 0.0714, 0.1493, 0.0256, 0.0173, 0.0253, 0.3452, 0.0258,
        0.1773, 0.0396, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,140][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.5365, 0.0422, 0.0788, 0.0878, 0.0397, 0.0139, 0.0097, 0.0483, 0.0108,
        0.0847, 0.0399, 0.0078], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,141][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [orum] are: tensor([9.2308e-01, 4.9542e-04, 2.8913e-03, 2.2619e-03, 4.6019e-03, 3.1678e-06,
        4.9267e-05, 1.9704e-02, 5.2218e-04, 4.0030e-02, 2.5200e-03, 3.8421e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,143][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [orum] are: tensor([8.8975e-01, 4.1027e-03, 2.2938e-03, 7.3047e-03, 5.0138e-03, 3.7823e-04,
        5.7588e-04, 1.3437e-02, 2.4546e-03, 6.9627e-02, 3.4993e-03, 1.5661e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,144][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.9317, 0.0014, 0.0067, 0.0066, 0.0046, 0.0018, 0.0071, 0.0025, 0.0046,
        0.0014, 0.0052, 0.0073, 0.0190], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,145][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.6525, 0.0046, 0.0228, 0.0320, 0.0281, 0.0028, 0.0281, 0.0136, 0.0469,
        0.0299, 0.0182, 0.0802, 0.0403], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,146][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ is] are: tensor([9.9319e-01, 8.7546e-05, 1.6317e-04, 1.6034e-04, 1.1507e-03, 3.5192e-06,
        5.4301e-05, 4.4753e-04, 2.3254e-04, 1.1725e-03, 1.2122e-03, 4.6208e-04,
        1.6600e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,147][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.4900, 0.1557, 0.0192, 0.0545, 0.0329, 0.0085, 0.0647, 0.0127, 0.0399,
        0.0159, 0.0117, 0.0247, 0.0696], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,148][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.5370, 0.1201, 0.0933, 0.1509, 0.0110, 0.0119, 0.0165, 0.0053, 0.0125,
        0.0139, 0.0035, 0.0037, 0.0203], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,149][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.5424, 0.0341, 0.1675, 0.1152, 0.0133, 0.0069, 0.0129, 0.0101, 0.0266,
        0.0070, 0.0121, 0.0138, 0.0382], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,150][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.6711, 0.0088, 0.0209, 0.0177, 0.0123, 0.0036, 0.0397, 0.0128, 0.0690,
        0.0274, 0.0075, 0.0509, 0.0584], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,152][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.9377, 0.0107, 0.0062, 0.0031, 0.0064, 0.0063, 0.0017, 0.0027, 0.0027,
        0.0041, 0.0073, 0.0048, 0.0066], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,153][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0732, 0.0984, 0.3335, 0.4122, 0.0023, 0.0048, 0.0060, 0.0085, 0.0110,
        0.0087, 0.0158, 0.0106, 0.0151], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,154][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.6894, 0.0464, 0.0183, 0.0050, 0.0140, 0.0065, 0.0395, 0.0036, 0.0493,
        0.0137, 0.0190, 0.0248, 0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,155][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ is] are: tensor([9.3159e-01, 1.7083e-04, 2.7193e-03, 6.0452e-03, 1.0249e-02, 2.0802e-05,
        3.7569e-04, 6.6814e-03, 1.1189e-03, 1.0731e-02, 1.0886e-02, 3.5259e-03,
        1.5888e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,156][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ is] are: tensor([9.6506e-01, 3.7795e-03, 1.6865e-03, 2.5853e-03, 7.1068e-03, 2.5906e-04,
        5.5693e-04, 8.8074e-04, 6.4907e-04, 4.8638e-03, 3.2896e-03, 6.4332e-04,
        8.6440e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,157][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([3.7824e-01, 2.9730e-04, 2.0329e-03, 4.7101e-03, 7.0313e-03, 6.8140e-04,
        2.9820e-03, 2.8160e-03, 2.8043e-03, 5.4603e-03, 3.6748e-03, 3.5686e-03,
        5.4373e-01, 4.1969e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,158][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.3763, 0.0047, 0.0790, 0.0426, 0.0352, 0.0067, 0.0683, 0.0331, 0.1054,
        0.0309, 0.0403, 0.1167, 0.0342, 0.0268], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,160][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([9.8925e-01, 2.7113e-03, 1.7980e-03, 8.5327e-05, 7.8490e-04, 8.9993e-06,
        6.4216e-06, 9.4175e-05, 2.2793e-05, 3.0442e-04, 1.1010e-03, 1.4394e-04,
        1.7201e-04, 3.5219e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,161][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5837, 0.0536, 0.0178, 0.0341, 0.0255, 0.0065, 0.0510, 0.0162, 0.0497,
        0.0111, 0.0119, 0.0344, 0.0663, 0.0382], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,162][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.9197, 0.0130, 0.0097, 0.0112, 0.0011, 0.0023, 0.0069, 0.0019, 0.0029,
        0.0021, 0.0015, 0.0017, 0.0144, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,163][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2570, 0.0243, 0.1639, 0.1804, 0.0084, 0.0109, 0.0616, 0.0293, 0.0613,
        0.0050, 0.0250, 0.0356, 0.1212, 0.0160], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,164][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5710, 0.0087, 0.0308, 0.0186, 0.0259, 0.0047, 0.0536, 0.0281, 0.0765,
        0.0285, 0.0161, 0.0631, 0.0475, 0.0269], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,165][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.9170, 0.0063, 0.0192, 0.0095, 0.0068, 0.0115, 0.0014, 0.0021, 0.0019,
        0.0023, 0.0038, 0.0032, 0.0064, 0.0086], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,167][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2161, 0.0131, 0.2122, 0.1288, 0.0131, 0.0200, 0.0656, 0.0454, 0.0813,
        0.0370, 0.0417, 0.0809, 0.0296, 0.0152], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,168][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.6307, 0.0406, 0.0171, 0.0030, 0.0093, 0.0036, 0.0366, 0.0039, 0.0308,
        0.0102, 0.0084, 0.0184, 0.1040, 0.0834], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,169][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([8.9842e-01, 1.0762e-02, 2.5760e-02, 2.0074e-03, 4.3830e-03, 3.6072e-05,
        1.4169e-04, 2.3208e-03, 9.5964e-04, 2.8092e-03, 2.0849e-02, 3.1306e-03,
        7.7922e-03, 2.0627e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,170][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.3375e-01, 4.2743e-02, 6.9462e-03, 5.9986e-04, 2.2801e-03, 2.0664e-04,
        9.5441e-05, 1.2751e-04, 3.1096e-04, 3.2400e-04, 6.4185e-04, 5.6337e-04,
        2.0545e-03, 9.3606e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,171][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ same] are: tensor([7.1654e-01, 6.2240e-04, 4.4002e-04, 1.1667e-03, 3.4827e-03, 5.5926e-04,
        4.9356e-04, 6.1809e-04, 5.3379e-04, 9.1131e-03, 2.7186e-03, 1.1677e-03,
        1.5806e-01, 5.3208e-02, 5.1275e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,172][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.6196, 0.0086, 0.0369, 0.0656, 0.0325, 0.0045, 0.0258, 0.0064, 0.0231,
        0.0130, 0.0049, 0.0265, 0.0770, 0.0392, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,174][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ same] are: tensor([9.9753e-01, 1.7993e-04, 4.4412e-04, 7.8305e-05, 2.0185e-04, 4.3836e-07,
        2.2136e-06, 2.0451e-06, 8.7921e-06, 4.7754e-05, 9.1255e-05, 1.6951e-04,
        3.3049e-04, 5.0816e-04, 4.0860e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,175][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.3721, 0.2104, 0.0470, 0.0533, 0.0727, 0.0100, 0.0445, 0.0079, 0.0286,
        0.0071, 0.0078, 0.0196, 0.0701, 0.0328, 0.0160], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,176][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.8100, 0.0390, 0.0217, 0.0213, 0.0020, 0.0063, 0.0032, 0.0027, 0.0016,
        0.0028, 0.0023, 0.0010, 0.0477, 0.0265, 0.0120], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,177][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.5525, 0.0837, 0.0703, 0.0185, 0.0060, 0.0038, 0.0705, 0.0082, 0.0539,
        0.0046, 0.0066, 0.0200, 0.0522, 0.0177, 0.0314], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,178][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.5949, 0.0097, 0.0322, 0.0214, 0.0110, 0.0037, 0.0297, 0.0127, 0.0563,
        0.0175, 0.0105, 0.0362, 0.1131, 0.0431, 0.0081], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,179][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ same] are: tensor([9.4590e-01, 4.7603e-03, 3.2757e-03, 2.9076e-03, 7.3518e-03, 2.7217e-03,
        1.7067e-03, 1.3145e-03, 9.4205e-04, 1.0507e-03, 1.3531e-03, 9.1189e-04,
        6.9560e-03, 1.5050e-02, 3.7946e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,181][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.2159, 0.0546, 0.2647, 0.1900, 0.0088, 0.0069, 0.0068, 0.0047, 0.0136,
        0.0095, 0.0122, 0.0108, 0.0936, 0.0980, 0.0100], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,182][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.5033, 0.0313, 0.0202, 0.0016, 0.0090, 0.0035, 0.0277, 0.0032, 0.0299,
        0.0174, 0.0166, 0.0090, 0.1546, 0.1526, 0.0201], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,183][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ same] are: tensor([9.2938e-01, 4.1353e-03, 1.4298e-02, 1.2536e-02, 5.2221e-03, 4.2297e-06,
        4.9760e-05, 5.3028e-05, 5.4263e-04, 3.0270e-04, 2.6900e-03, 1.9044e-03,
        3.6540e-03, 1.2030e-02, 1.3201e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,184][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ same] are: tensor([9.6984e-01, 8.9466e-03, 1.0388e-02, 2.9912e-03, 1.1098e-03, 9.7963e-05,
        1.0168e-04, 7.5777e-05, 2.9563e-05, 7.4691e-04, 3.2102e-04, 2.5977e-04,
        1.0143e-03, 9.5982e-04, 3.1188e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,185][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ as] are: tensor([8.1031e-01, 1.0364e-03, 2.2436e-03, 6.0000e-04, 1.8827e-03, 5.9512e-04,
        1.9549e-03, 4.6588e-04, 1.7943e-03, 9.3149e-04, 2.4761e-03, 6.0799e-03,
        4.5335e-02, 1.7160e-02, 8.4163e-02, 2.2971e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,187][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.4035, 0.0146, 0.0188, 0.0082, 0.0495, 0.0092, 0.0352, 0.0118, 0.1291,
        0.0150, 0.0504, 0.1554, 0.0226, 0.0330, 0.0352, 0.0086],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,188][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ as] are: tensor([9.7441e-01, 1.2683e-04, 4.9039e-04, 7.2297e-05, 1.1931e-02, 9.9716e-05,
        1.6122e-04, 9.9256e-05, 4.5285e-04, 6.0439e-05, 1.7470e-03, 2.6269e-04,
        2.5523e-04, 4.0762e-03, 9.5013e-04, 4.8066e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,189][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.4169, 0.2930, 0.0291, 0.0118, 0.0322, 0.0044, 0.0117, 0.0025, 0.0067,
        0.0016, 0.0033, 0.0045, 0.0565, 0.0528, 0.0375, 0.0355],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,190][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.7640, 0.0349, 0.0422, 0.0154, 0.0027, 0.0057, 0.0142, 0.0023, 0.0072,
        0.0025, 0.0016, 0.0025, 0.0333, 0.0117, 0.0410, 0.0188],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,191][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.6555, 0.0300, 0.0633, 0.0025, 0.0039, 0.0052, 0.0186, 0.0111, 0.0317,
        0.0063, 0.0131, 0.0120, 0.0617, 0.0291, 0.0460, 0.0101],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,193][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.5997, 0.0199, 0.0187, 0.0241, 0.0220, 0.0066, 0.0405, 0.0151, 0.0445,
        0.0287, 0.0145, 0.0464, 0.0563, 0.0361, 0.0097, 0.0173],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,194][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.5962, 0.0512, 0.0346, 0.0245, 0.0646, 0.1000, 0.0234, 0.0219, 0.0202,
        0.0055, 0.0073, 0.0073, 0.0277, 0.0111, 0.0013, 0.0031],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,195][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0481, 0.0241, 0.2839, 0.0939, 0.0146, 0.0043, 0.0153, 0.0036, 0.0226,
        0.0049, 0.0146, 0.0127, 0.3194, 0.0781, 0.0484, 0.0116],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,196][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.6583, 0.0473, 0.0077, 0.0021, 0.0198, 0.0034, 0.0156, 0.0012, 0.0159,
        0.0037, 0.0040, 0.0070, 0.0443, 0.1050, 0.0434, 0.0212],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,197][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ as] are: tensor([8.5863e-01, 2.3589e-03, 1.8599e-03, 9.3512e-04, 6.5338e-02, 1.2779e-05,
        1.8037e-04, 8.0088e-04, 2.3913e-03, 8.6542e-04, 1.1150e-02, 1.1087e-03,
        3.3014e-03, 3.1996e-02, 1.4838e-03, 1.7592e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,199][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ as] are: tensor([9.3536e-01, 4.3008e-03, 1.4655e-03, 1.5150e-03, 2.0893e-02, 9.9991e-05,
        6.6876e-04, 1.6929e-04, 3.4064e-04, 1.4568e-04, 1.3174e-03, 1.7999e-04,
        2.2108e-03, 2.2680e-02, 4.5733e-03, 4.0811e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,200][circuit_into_ebeddingspace.py][line:2310][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([3.3054e-01, 4.8979e-04, 8.4539e-04, 1.6695e-03, 1.8369e-03, 2.2055e-04,
        6.1964e-04, 7.5587e-04, 3.8299e-04, 8.6728e-04, 8.1369e-04, 1.0928e-03,
        2.9877e-02, 3.9159e-03, 3.3520e-02, 5.6914e-01, 2.3410e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,201][circuit_into_ebeddingspace.py][line:2313][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4043, 0.0150, 0.0974, 0.0559, 0.0478, 0.0069, 0.0408, 0.0103, 0.0641,
        0.0098, 0.0365, 0.0828, 0.0343, 0.0141, 0.0327, 0.0210, 0.0265],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,202][circuit_into_ebeddingspace.py][line:2316][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([9.7836e-01, 5.3158e-03, 3.3139e-03, 8.3241e-05, 3.2791e-03, 3.9267e-05,
        3.6102e-05, 8.9481e-05, 7.8605e-05, 4.4043e-05, 8.7050e-04, 2.1833e-04,
        8.1698e-05, 1.0486e-04, 2.8190e-03, 1.3295e-03, 3.9414e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,203][circuit_into_ebeddingspace.py][line:2319][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5330, 0.1577, 0.0530, 0.0140, 0.0234, 0.0041, 0.0126, 0.0054, 0.0092,
        0.0020, 0.0066, 0.0078, 0.0504, 0.0214, 0.0351, 0.0338, 0.0307],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,205][circuit_into_ebeddingspace.py][line:2322][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([9.3264e-01, 1.1864e-02, 5.6088e-03, 4.4951e-03, 6.1723e-04, 9.3072e-04,
        4.1733e-03, 1.1231e-03, 1.2164e-03, 9.9432e-04, 7.7536e-04, 8.7523e-04,
        9.9792e-03, 2.6923e-03, 6.7433e-03, 1.2478e-02, 2.7909e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,206][circuit_into_ebeddingspace.py][line:2325][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.7313, 0.0145, 0.0534, 0.0047, 0.0014, 0.0037, 0.0129, 0.0077, 0.0216,
        0.0045, 0.0120, 0.0146, 0.0428, 0.0073, 0.0381, 0.0195, 0.0100],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,207][circuit_into_ebeddingspace.py][line:2328][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5692, 0.0285, 0.0388, 0.0249, 0.0242, 0.0058, 0.0377, 0.0236, 0.0404,
        0.0183, 0.0156, 0.0332, 0.0589, 0.0331, 0.0124, 0.0143, 0.0211],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,208][circuit_into_ebeddingspace.py][line:2331][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.4954, 0.0454, 0.1265, 0.1922, 0.0458, 0.0203, 0.0117, 0.0062, 0.0072,
        0.0022, 0.0023, 0.0035, 0.0270, 0.0045, 0.0022, 0.0038, 0.0040],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,209][circuit_into_ebeddingspace.py][line:2334][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1236, 0.0147, 0.3063, 0.1283, 0.0146, 0.0100, 0.0215, 0.0120, 0.0243,
        0.0148, 0.0194, 0.0164, 0.2123, 0.0211, 0.0316, 0.0229, 0.0063],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,211][circuit_into_ebeddingspace.py][line:2337][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.5528, 0.0519, 0.0205, 0.0029, 0.0159, 0.0040, 0.0101, 0.0039, 0.0090,
        0.0047, 0.0091, 0.0056, 0.0605, 0.0364, 0.0573, 0.1069, 0.0485],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,212][circuit_into_ebeddingspace.py][line:2340][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([8.4646e-01, 5.2104e-02, 3.0190e-02, 1.1506e-03, 1.6619e-02, 2.3627e-04,
        3.0643e-04, 2.5753e-03, 3.0217e-03, 1.4794e-03, 2.0212e-02, 3.1979e-03,
        5.8321e-03, 2.9488e-03, 7.2761e-03, 3.1195e-03, 3.2676e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,213][circuit_into_ebeddingspace.py][line:2343][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([8.4539e-01, 1.0965e-01, 1.2004e-02, 8.2138e-04, 4.1979e-03, 2.2045e-04,
        1.9347e-04, 1.0322e-04, 1.6960e-04, 7.3972e-05, 2.7005e-04, 1.8027e-04,
        6.2413e-04, 1.3202e-03, 1.6054e-02, 3.3330e-03, 5.3937e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,221][circuit_into_ebeddingspace.py][line:2286][INFO] ################8-th layer#################
[2024-06-27 21:00:54,224][circuit_into_ebeddingspace.py][line:2288][INFO] ##8-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,225][circuit_into_ebeddingspace.py][line:2289][INFO] ##8-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,227][circuit_into_ebeddingspace.py][line:2290][INFO] ##8-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,228][circuit_into_ebeddingspace.py][line:2291][INFO] ##8-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,229][circuit_into_ebeddingspace.py][line:2292][INFO] ##8-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,231][circuit_into_ebeddingspace.py][line:2293][INFO] ##8-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,232][circuit_into_ebeddingspace.py][line:2294][INFO] ##8-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:54,234][circuit_into_ebeddingspace.py][line:2295][INFO] ##8-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,235][circuit_into_ebeddingspace.py][line:2296][INFO] ##8-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,236][circuit_into_ebeddingspace.py][line:2297][INFO] ##8-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,238][circuit_into_ebeddingspace.py][line:2298][INFO] ##8-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,239][circuit_into_ebeddingspace.py][line:2299][INFO] ##8-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,240][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,241][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,242][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,243][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,244][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,245][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,246][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,247][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,247][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,248][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,249][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,250][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,251][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9658, 0.0342], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,252][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9870, 0.0130], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,253][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9598, 0.0402], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,254][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9170, 0.0830], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,255][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9622, 0.0378], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,256][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9452, 0.0548], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,257][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9689, 0.0311], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,258][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9003, 0.0997], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,259][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9441, 0.0559], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,260][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9794, 0.0206], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,261][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9054, 0.0946], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,262][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9742, 0.0258], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,263][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9378, 0.0271, 0.0351], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,264][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9742, 0.0052, 0.0205], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,265][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9341, 0.0202, 0.0458], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,265][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.9195, 0.0154, 0.0651], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,266][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8485, 0.1245, 0.0270], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,267][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8218, 0.1030, 0.0753], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,268][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9490, 0.0121, 0.0389], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,269][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.7020, 0.2649, 0.0332], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,270][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.8835, 0.0282, 0.0882], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,271][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.8196, 0.0138, 0.1666], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,272][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.8448, 0.0389, 0.1163], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,273][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9609, 0.0123, 0.0268], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,274][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.8211, 0.0213, 0.0322, 0.1254], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,275][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.9785, 0.0053, 0.0076, 0.0086], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,276][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.7336, 0.0361, 0.1695, 0.0608], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,277][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.9259, 0.0175, 0.0498, 0.0068], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,278][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.6571, 0.1525, 0.1419, 0.0485], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,279][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.7944, 0.1056, 0.0698, 0.0301], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,280][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.9788, 0.0092, 0.0074, 0.0046], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,281][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3852, 0.0483, 0.3859, 0.1806], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,282][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.5730, 0.0577, 0.3211, 0.0482], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,283][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.8208, 0.0106, 0.1424, 0.0262], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,284][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.7697, 0.0457, 0.1164, 0.0682], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,285][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.8731, 0.0312, 0.0764, 0.0194], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,286][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.8576, 0.0338, 0.0236, 0.0588, 0.0263], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,287][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ De] are: tensor([9.7878e-01, 1.2688e-02, 3.0850e-03, 6.0149e-04, 4.8491e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,288][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.9470, 0.0117, 0.0168, 0.0161, 0.0085], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,289][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.9509, 0.0124, 0.0159, 0.0074, 0.0134], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,290][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9699, 0.0060, 0.0046, 0.0091, 0.0105], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,291][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8055, 0.0401, 0.0294, 0.1047, 0.0203], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,292][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9554, 0.0047, 0.0025, 0.0034, 0.0341], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,293][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.3158, 0.0160, 0.0880, 0.5485, 0.0317], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,294][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.8537, 0.0030, 0.0954, 0.0173, 0.0305], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,295][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.8212, 0.0041, 0.0657, 0.0376, 0.0714], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,296][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.8813, 0.0187, 0.0242, 0.0236, 0.0522], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,297][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.7882, 0.0170, 0.0957, 0.0272, 0.0719], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,298][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.8665, 0.0301, 0.0534, 0.0206, 0.0121, 0.0173], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,299][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.9593e-01, 7.0601e-04, 1.1957e-03, 6.3108e-04, 4.2223e-04, 1.1162e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,300][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.9867, 0.0022, 0.0017, 0.0019, 0.0036, 0.0039], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,301][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.9821, 0.0065, 0.0019, 0.0023, 0.0050, 0.0023], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,302][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.7885e-01, 5.5429e-03, 9.6649e-04, 3.0426e-03, 7.8591e-03, 3.7428e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,303][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.7830, 0.0236, 0.0142, 0.0794, 0.0814, 0.0185], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,304][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.8139, 0.0021, 0.0009, 0.0011, 0.0069, 0.1750], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,305][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.6175, 0.0152, 0.0195, 0.0330, 0.2377, 0.0771], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,306][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([9.8556e-01, 6.3059e-04, 4.2842e-03, 4.5097e-03, 3.5337e-03, 1.4834e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,307][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([9.6913e-01, 9.6013e-04, 2.9573e-03, 2.1287e-03, 1.3844e-02, 1.0977e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,308][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.8028, 0.0053, 0.0016, 0.0057, 0.0182, 0.1664], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,309][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.7906, 0.0262, 0.0138, 0.0314, 0.1036, 0.0343], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,310][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.8492, 0.0286, 0.0160, 0.0326, 0.0054, 0.0038, 0.0644],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,311][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([9.7755e-01, 5.2957e-04, 3.4570e-03, 8.7113e-03, 2.2058e-03, 4.1283e-04,
        7.1313e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,312][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.8283, 0.0302, 0.0199, 0.0338, 0.0346, 0.0306, 0.0226],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,313][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.8567, 0.0159, 0.0088, 0.0084, 0.0225, 0.0137, 0.0740],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,314][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.6026, 0.0955, 0.0436, 0.2108, 0.0115, 0.0055, 0.0306],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,315][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.2320, 0.1073, 0.2288, 0.3190, 0.0508, 0.0348, 0.0274],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,316][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.5171, 0.0016, 0.0012, 0.0036, 0.0265, 0.0580, 0.3920],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,317][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2123, 0.0301, 0.0403, 0.3491, 0.1527, 0.0702, 0.1452],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,319][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.7055, 0.0163, 0.0700, 0.0411, 0.0414, 0.0111, 0.1147],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,320][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.7064, 0.0108, 0.0585, 0.0495, 0.0649, 0.0209, 0.0891],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,321][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.7169, 0.0156, 0.0152, 0.0250, 0.0305, 0.0655, 0.1313],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,322][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.4109, 0.0318, 0.0524, 0.0165, 0.1493, 0.0335, 0.3056],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,323][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.6942, 0.0176, 0.0342, 0.0305, 0.0126, 0.0130, 0.1764, 0.0216],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,324][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([9.9083e-01, 4.4978e-04, 2.3595e-04, 1.4645e-04, 3.5809e-04, 2.8373e-04,
        7.1187e-03, 5.7800e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,325][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([9.9287e-01, 1.2931e-03, 9.8008e-04, 9.3351e-04, 1.7605e-03, 9.1782e-04,
        8.4613e-04, 4.0181e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,326][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.9571, 0.0090, 0.0014, 0.0025, 0.0040, 0.0012, 0.0222, 0.0025],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,327][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([9.8277e-01, 2.6135e-03, 2.5754e-03, 2.2654e-03, 2.7729e-03, 7.5872e-04,
        2.7650e-03, 3.4814e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,328][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.8732, 0.0108, 0.0168, 0.0301, 0.0181, 0.0175, 0.0246, 0.0089],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,329][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.6118, 0.0010, 0.0017, 0.0010, 0.0038, 0.0392, 0.3146, 0.0268],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,330][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.6331, 0.0103, 0.0452, 0.0220, 0.0507, 0.0294, 0.1824, 0.0269],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,331][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.8360, 0.0022, 0.0731, 0.0218, 0.0049, 0.0022, 0.0540, 0.0059],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,332][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([9.7219e-01, 2.8129e-04, 3.1930e-03, 1.6800e-03, 6.7303e-03, 2.3384e-03,
        7.5377e-03, 6.0465e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,333][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.7279, 0.0045, 0.0024, 0.0044, 0.0134, 0.0532, 0.0828, 0.1114],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,334][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.6037, 0.0401, 0.0269, 0.0178, 0.0301, 0.0132, 0.2546, 0.0135],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,335][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.8684, 0.0166, 0.0102, 0.0249, 0.0041, 0.0023, 0.0290, 0.0019, 0.0426],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,336][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.3496e-01, 2.1570e-04, 1.3000e-03, 4.3690e-03, 8.3963e-04, 7.4509e-05,
        1.7341e-03, 1.8460e-02, 3.8043e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,337][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8903, 0.0185, 0.0167, 0.0302, 0.0209, 0.0054, 0.0087, 0.0012, 0.0080],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,339][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.8321, 0.0136, 0.0083, 0.0076, 0.0263, 0.0067, 0.0531, 0.0105, 0.0417],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,340][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4780, 0.0955, 0.0750, 0.2299, 0.0118, 0.0051, 0.0468, 0.0115, 0.0463],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,341][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.2298, 0.1115, 0.2826, 0.2170, 0.0403, 0.0315, 0.0326, 0.0268, 0.0280],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,342][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [orum] are: tensor([7.5889e-01, 6.8753e-04, 9.4985e-04, 3.9178e-03, 1.1732e-02, 7.7364e-03,
        6.0756e-02, 6.1787e-02, 9.3546e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,343][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1935, 0.0220, 0.0715, 0.2600, 0.0394, 0.0272, 0.1789, 0.1361, 0.0714],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,344][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.5800, 0.0258, 0.0799, 0.0456, 0.0377, 0.0053, 0.1035, 0.0331, 0.0891],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,345][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.5965, 0.0046, 0.0276, 0.0465, 0.0415, 0.0127, 0.0889, 0.0606, 0.1212],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,346][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.6697, 0.0121, 0.0230, 0.0218, 0.0280, 0.0332, 0.0734, 0.0621, 0.0768],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,347][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.4982, 0.0262, 0.0272, 0.0121, 0.0651, 0.0204, 0.2307, 0.0190, 0.1011],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,348][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.6933, 0.0292, 0.0119, 0.0192, 0.0060, 0.0020, 0.0805, 0.0098, 0.1051,
        0.0432], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,349][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.7297, 0.0224, 0.0199, 0.0368, 0.0263, 0.0090, 0.0228, 0.0208, 0.1016,
        0.0108], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,350][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ et] are: tensor([9.8750e-01, 1.7049e-03, 1.6010e-03, 1.8482e-03, 4.6689e-03, 7.7475e-04,
        5.8326e-04, 2.0112e-04, 4.9166e-04, 6.2304e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,351][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.7240, 0.0223, 0.0069, 0.0127, 0.0494, 0.0222, 0.0513, 0.0532, 0.0477,
        0.0103], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,353][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.9111, 0.0064, 0.0074, 0.0201, 0.0049, 0.0017, 0.0115, 0.0061, 0.0198,
        0.0109], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,354][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.7066, 0.0278, 0.0608, 0.0640, 0.0256, 0.0210, 0.0358, 0.0135, 0.0274,
        0.0175], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,355][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.2766, 0.0016, 0.0055, 0.0010, 0.0095, 0.0358, 0.4281, 0.0546, 0.1500,
        0.0373], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,356][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.7833, 0.0057, 0.0227, 0.0305, 0.0292, 0.0121, 0.0523, 0.0169, 0.0404,
        0.0069], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,357][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.7485, 0.0231, 0.0499, 0.0230, 0.0264, 0.0042, 0.0512, 0.0232, 0.0477,
        0.0027], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,358][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.9388, 0.0029, 0.0072, 0.0048, 0.0122, 0.0035, 0.0095, 0.0069, 0.0105,
        0.0038], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,359][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.6777, 0.0071, 0.0046, 0.0096, 0.0220, 0.0284, 0.0775, 0.0507, 0.0901,
        0.0324], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,360][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.3180, 0.0619, 0.0631, 0.0175, 0.0756, 0.0307, 0.2449, 0.0588, 0.0911,
        0.0386], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,361][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.5898, 0.0290, 0.0541, 0.0226, 0.0111, 0.0087, 0.1258, 0.0190, 0.0931,
        0.0348, 0.0119], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,362][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.8731, 0.0012, 0.0045, 0.0023, 0.0015, 0.0029, 0.0426, 0.0016, 0.0642,
        0.0044, 0.0017], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,364][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([9.7981e-01, 8.8023e-04, 2.3638e-03, 2.2423e-03, 2.8254e-03, 2.2191e-03,
        4.6350e-03, 4.4886e-04, 1.6351e-03, 1.1865e-03, 1.7517e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,365][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([9.0185e-01, 5.4290e-03, 7.1498e-03, 8.6481e-03, 5.0906e-03, 8.0159e-04,
        2.0910e-02, 3.6794e-03, 1.3748e-02, 2.3786e-02, 8.9044e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,366][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([9.6741e-01, 2.8410e-03, 1.4675e-03, 3.8323e-03, 1.6002e-03, 2.1132e-04,
        7.8285e-04, 3.7063e-03, 1.1933e-03, 1.4036e-02, 2.9147e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,367][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.6738, 0.0183, 0.0380, 0.1109, 0.0175, 0.0158, 0.0188, 0.0135, 0.0172,
        0.0644, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,368][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.3882, 0.0010, 0.0027, 0.0009, 0.0055, 0.0504, 0.2685, 0.0307, 0.1455,
        0.0386, 0.0680], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,369][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.6439, 0.0030, 0.0125, 0.0151, 0.0125, 0.0047, 0.0136, 0.0139, 0.0203,
        0.2366, 0.0240], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,370][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.7681, 0.0022, 0.0910, 0.0204, 0.0111, 0.0041, 0.0398, 0.0197, 0.0325,
        0.0047, 0.0064], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,371][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([9.6056e-01, 5.5349e-04, 7.2447e-03, 2.5237e-03, 2.7133e-03, 1.8845e-03,
        7.3550e-03, 5.0230e-03, 5.6199e-03, 2.5502e-03, 3.9696e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,372][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.7343, 0.0028, 0.0036, 0.0073, 0.0075, 0.0187, 0.0613, 0.0329, 0.0548,
        0.0325, 0.0442], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,373][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.4888, 0.0089, 0.0186, 0.0150, 0.0127, 0.0412, 0.2045, 0.0366, 0.1129,
        0.0452, 0.0155], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,375][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.8313, 0.0154, 0.0143, 0.0164, 0.0029, 0.0023, 0.0184, 0.0022, 0.0398,
        0.0078, 0.0021, 0.0471], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,376][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.4113e-01, 2.5668e-04, 2.3007e-03, 6.9483e-03, 1.0074e-03, 5.6545e-05,
        1.4058e-03, 6.7164e-03, 1.9832e-02, 1.1331e-02, 3.7232e-04, 8.6435e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,377][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8851, 0.0185, 0.0149, 0.0339, 0.0110, 0.0030, 0.0075, 0.0012, 0.0076,
        0.0083, 0.0017, 0.0073], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,378][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.8067, 0.0088, 0.0053, 0.0060, 0.0194, 0.0030, 0.0533, 0.0066, 0.0425,
        0.0087, 0.0190, 0.0207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,379][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.5170, 0.1068, 0.0575, 0.1824, 0.0067, 0.0017, 0.0243, 0.0055, 0.0249,
        0.0483, 0.0085, 0.0164], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,380][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.1504, 0.1250, 0.3040, 0.2533, 0.0209, 0.0075, 0.0160, 0.0084, 0.0151,
        0.0801, 0.0068, 0.0126], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,381][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [orum] are: tensor([7.1086e-01, 6.8095e-04, 1.1911e-03, 2.9295e-03, 7.1039e-03, 7.5607e-03,
        4.9122e-02, 3.0500e-02, 5.6174e-02, 7.1634e-02, 1.2464e-02, 4.9781e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,382][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1329, 0.0274, 0.0517, 0.2237, 0.0352, 0.0165, 0.1031, 0.0678, 0.0489,
        0.2292, 0.0403, 0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,384][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.6386, 0.0256, 0.0553, 0.0350, 0.0182, 0.0027, 0.0625, 0.0217, 0.0731,
        0.0100, 0.0048, 0.0526], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,385][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.6866, 0.0062, 0.0313, 0.0500, 0.0226, 0.0057, 0.0484, 0.0202, 0.0491,
        0.0281, 0.0104, 0.0414], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,386][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.7065, 0.0093, 0.0146, 0.0209, 0.0144, 0.0176, 0.0447, 0.0357, 0.0518,
        0.0373, 0.0169, 0.0304], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,387][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.5945, 0.0206, 0.0199, 0.0084, 0.0353, 0.0098, 0.1534, 0.0107, 0.0725,
        0.0164, 0.0066, 0.0519], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,388][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.7606, 0.0267, 0.0137, 0.0207, 0.0055, 0.0013, 0.0195, 0.0016, 0.0401,
        0.0130, 0.0047, 0.0354, 0.0573], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,389][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ is] are: tensor([8.8836e-01, 4.0721e-03, 4.1940e-03, 1.4626e-02, 5.6068e-03, 6.2808e-04,
        8.7349e-03, 1.4623e-03, 2.9767e-02, 2.6060e-03, 1.2287e-03, 9.8491e-03,
        2.8868e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,390][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7123, 0.0280, 0.0286, 0.0220, 0.0122, 0.0088, 0.0241, 0.0118, 0.0631,
        0.0156, 0.0093, 0.0411, 0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,392][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.3329, 0.0079, 0.0771, 0.0542, 0.0317, 0.0019, 0.1110, 0.0063, 0.1069,
        0.0414, 0.0203, 0.1100, 0.0984], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,393][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.3222, 0.0173, 0.2317, 0.1009, 0.0023, 0.0046, 0.0393, 0.0127, 0.1060,
        0.0237, 0.0137, 0.0846, 0.0408], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,394][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.5690, 0.0369, 0.0858, 0.0058, 0.0066, 0.0048, 0.0572, 0.0048, 0.0888,
        0.0150, 0.0036, 0.0603, 0.0613], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,395][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.6526, 0.0047, 0.0071, 0.0053, 0.0296, 0.0036, 0.0733, 0.0108, 0.1072,
        0.0116, 0.0105, 0.0568, 0.0270], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,396][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.4247, 0.0115, 0.1588, 0.0098, 0.0021, 0.0010, 0.0563, 0.0015, 0.0610,
        0.0209, 0.0024, 0.0399, 0.2101], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,397][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.2723, 0.0382, 0.2256, 0.0995, 0.0086, 0.0050, 0.0593, 0.0405, 0.0829,
        0.0135, 0.0127, 0.0847, 0.0573], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,398][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.4054, 0.0322, 0.1492, 0.0471, 0.0177, 0.0060, 0.0653, 0.0096, 0.0625,
        0.0189, 0.0126, 0.0647, 0.1088], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,400][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.6400, 0.0345, 0.0368, 0.0257, 0.0651, 0.0058, 0.0385, 0.0093, 0.0337,
        0.0101, 0.0089, 0.0213, 0.0702], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,401][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.7182, 0.0174, 0.0475, 0.0081, 0.0292, 0.0098, 0.0471, 0.0118, 0.0397,
        0.0119, 0.0232, 0.0222, 0.0139], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,402][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.7669, 0.0295, 0.0148, 0.0160, 0.0064, 0.0022, 0.0122, 0.0015, 0.0185,
        0.0060, 0.0053, 0.0124, 0.0444, 0.0639], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,403][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([8.7431e-01, 3.8300e-02, 3.8581e-02, 1.0486e-02, 2.1441e-03, 7.8419e-04,
        1.5651e-03, 1.9798e-04, 1.0326e-03, 6.4403e-04, 1.2740e-03, 1.7686e-03,
        8.0148e-03, 2.0899e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,404][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7843, 0.0223, 0.0271, 0.0245, 0.0127, 0.0129, 0.0163, 0.0036, 0.0308,
        0.0028, 0.0036, 0.0214, 0.0190, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,405][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2505, 0.0031, 0.0648, 0.1108, 0.0252, 0.0013, 0.1279, 0.0041, 0.1120,
        0.0507, 0.0223, 0.1156, 0.0565, 0.0553], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,407][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.3757, 0.0157, 0.1229, 0.0896, 0.0033, 0.0040, 0.0420, 0.0070, 0.1022,
        0.0088, 0.0170, 0.0811, 0.0986, 0.0323], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,408][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.5464, 0.0216, 0.0280, 0.0061, 0.0132, 0.0047, 0.0577, 0.0054, 0.0528,
        0.0103, 0.0048, 0.0424, 0.1660, 0.0405], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,409][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([8.2720e-01, 1.1356e-02, 3.7365e-02, 1.6419e-02, 4.7926e-03, 4.6037e-04,
        3.0656e-03, 8.8350e-04, 3.9240e-03, 1.1245e-03, 2.1444e-03, 3.0423e-03,
        1.2224e-02, 7.5993e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,410][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.4164e-02, 3.1685e-03, 4.4420e-02, 9.1136e-03, 1.7270e-03, 4.2019e-04,
        9.9235e-03, 6.1713e-04, 4.3469e-03, 7.7788e-03, 1.8753e-03, 6.1431e-03,
        7.3288e-01, 9.3426e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,411][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1197, 0.0072, 0.1841, 0.0428, 0.0089, 0.0071, 0.1174, 0.0500, 0.1835,
        0.0194, 0.0245, 0.1792, 0.0411, 0.0151], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,412][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.6088, 0.0055, 0.0795, 0.0154, 0.0121, 0.0051, 0.0473, 0.0119, 0.0518,
        0.0139, 0.0200, 0.0503, 0.0309, 0.0476], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,413][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4289, 0.0430, 0.0968, 0.0391, 0.0392, 0.0043, 0.0588, 0.0075, 0.0533,
        0.0095, 0.0105, 0.0350, 0.0814, 0.0926], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,415][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.4466, 0.0189, 0.1169, 0.0118, 0.0296, 0.0140, 0.1726, 0.0187, 0.0679,
        0.0201, 0.0200, 0.0390, 0.0174, 0.0066], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,416][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ same] are: tensor([9.0498e-01, 8.1870e-03, 1.2214e-02, 8.3107e-03, 1.4656e-03, 1.1129e-03,
        4.2231e-03, 6.0941e-04, 5.1446e-03, 3.3420e-03, 9.3899e-04, 3.9186e-03,
        2.9174e-02, 1.1282e-02, 5.0941e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,417][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ same] are: tensor([8.2485e-01, 4.2814e-03, 3.2815e-02, 2.7019e-02, 3.6728e-03, 1.6326e-04,
        1.1476e-03, 3.7224e-04, 1.1902e-03, 2.6767e-03, 6.8473e-04, 2.6133e-03,
        4.5847e-02, 4.2966e-02, 9.6967e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,418][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.7216, 0.0300, 0.0353, 0.0389, 0.0087, 0.0085, 0.0129, 0.0041, 0.0270,
        0.0068, 0.0031, 0.0211, 0.0384, 0.0292, 0.0143], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,419][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.3408, 0.0061, 0.0688, 0.2258, 0.0205, 0.0025, 0.0424, 0.0026, 0.0387,
        0.0187, 0.0082, 0.0342, 0.0922, 0.0633, 0.0351], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,421][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.6538, 0.0035, 0.0133, 0.0018, 0.0021, 0.0025, 0.0105, 0.0060, 0.0154,
        0.0162, 0.0041, 0.0128, 0.0502, 0.0258, 0.1821], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,422][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.5718, 0.0208, 0.0459, 0.0029, 0.0045, 0.0019, 0.0240, 0.0025, 0.0300,
        0.0096, 0.0019, 0.0196, 0.1560, 0.0495, 0.0592], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,423][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ same] are: tensor([7.8934e-01, 2.3516e-03, 1.2977e-02, 4.7926e-03, 2.1063e-03, 1.4107e-04,
        1.0722e-03, 5.8288e-04, 1.1598e-03, 1.0292e-03, 4.2101e-04, 7.7144e-04,
        9.8778e-02, 6.7483e-02, 1.6994e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,424][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.3003, 0.0037, 0.0163, 0.0028, 0.0033, 0.0007, 0.0055, 0.0010, 0.0038,
        0.0128, 0.0040, 0.0032, 0.3382, 0.2425, 0.0619], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,425][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.4201, 0.0180, 0.1241, 0.0524, 0.0158, 0.0064, 0.0538, 0.0250, 0.0621,
        0.0099, 0.0163, 0.0432, 0.1061, 0.0350, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,426][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.5065, 0.0066, 0.0747, 0.0177, 0.0072, 0.0043, 0.0305, 0.0080, 0.0565,
        0.0280, 0.0145, 0.0562, 0.0807, 0.0716, 0.0369], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,428][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.4903, 0.0230, 0.1270, 0.0577, 0.0136, 0.0019, 0.0069, 0.0020, 0.0086,
        0.0062, 0.0019, 0.0084, 0.1251, 0.0458, 0.0818], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,429][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.6645, 0.0170, 0.0421, 0.0096, 0.0115, 0.0018, 0.0421, 0.0149, 0.0851,
        0.0321, 0.0094, 0.0435, 0.0150, 0.0077, 0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,430][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ as] are: tensor([8.4816e-01, 1.1627e-02, 7.0387e-03, 7.2489e-03, 1.9675e-03, 7.1325e-04,
        6.8960e-03, 7.6466e-04, 9.1399e-03, 5.0163e-03, 1.3740e-03, 7.0871e-03,
        2.5812e-02, 4.2166e-02, 1.3703e-02, 1.1283e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,431][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.3204, 0.0317, 0.1667, 0.1096, 0.0361, 0.0013, 0.0219, 0.0015, 0.0307,
        0.0006, 0.0014, 0.0098, 0.1946, 0.0508, 0.0041, 0.0189],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,432][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.7790, 0.0164, 0.0314, 0.0271, 0.0133, 0.0075, 0.0067, 0.0018, 0.0144,
        0.0036, 0.0026, 0.0083, 0.0347, 0.0205, 0.0211, 0.0117],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,434][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.4247, 0.0114, 0.0336, 0.0781, 0.0361, 0.0065, 0.0138, 0.0092, 0.0195,
        0.0084, 0.0064, 0.0133, 0.1391, 0.0760, 0.0621, 0.0618],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,435][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.7210, 0.0022, 0.0196, 0.0033, 0.0018, 0.0021, 0.0068, 0.0049, 0.0097,
        0.0033, 0.0018, 0.0081, 0.0152, 0.0156, 0.1390, 0.0457],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,436][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.5234, 0.0176, 0.0389, 0.0019, 0.0040, 0.0031, 0.0166, 0.0023, 0.0203,
        0.0084, 0.0021, 0.0143, 0.1163, 0.0298, 0.1787, 0.0223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,437][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.5148, 0.0245, 0.0331, 0.0061, 0.0641, 0.0015, 0.1062, 0.0054, 0.0418,
        0.0032, 0.0041, 0.0161, 0.0398, 0.1151, 0.0148, 0.0093],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,438][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.5435, 0.0035, 0.0269, 0.0016, 0.0021, 0.0008, 0.0083, 0.0011, 0.0036,
        0.0021, 0.0023, 0.0029, 0.0770, 0.0337, 0.2370, 0.0534],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,440][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3592, 0.1054, 0.1390, 0.0170, 0.0663, 0.0048, 0.0557, 0.0118, 0.0750,
        0.0082, 0.0176, 0.0349, 0.0772, 0.0211, 0.0049, 0.0020],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,441][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.5913, 0.0084, 0.0321, 0.0157, 0.0311, 0.0034, 0.0195, 0.0046, 0.0282,
        0.0145, 0.0098, 0.0263, 0.0626, 0.1084, 0.0312, 0.0129],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,442][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.6005, 0.0430, 0.0323, 0.0203, 0.0370, 0.0025, 0.0135, 0.0021, 0.0090,
        0.0021, 0.0020, 0.0088, 0.0445, 0.0521, 0.0689, 0.0613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,443][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.7739, 0.0166, 0.0353, 0.0095, 0.0138, 0.0040, 0.0136, 0.0189, 0.0144,
        0.0185, 0.0094, 0.0102, 0.0236, 0.0123, 0.0210, 0.0049],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,444][circuit_into_ebeddingspace.py][line:2310][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.7792, 0.0192, 0.0075, 0.0128, 0.0022, 0.0014, 0.0122, 0.0011, 0.0127,
        0.0044, 0.0026, 0.0097, 0.0204, 0.0435, 0.0256, 0.0101, 0.0353],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,446][circuit_into_ebeddingspace.py][line:2313][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.3114, 0.3005, 0.2337, 0.0650, 0.0139, 0.0031, 0.0058, 0.0006, 0.0020,
        0.0004, 0.0010, 0.0011, 0.0105, 0.0033, 0.0365, 0.0086, 0.0027],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,447][circuit_into_ebeddingspace.py][line:2316][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7852, 0.0214, 0.0332, 0.0297, 0.0139, 0.0047, 0.0056, 0.0024, 0.0165,
        0.0026, 0.0035, 0.0111, 0.0180, 0.0152, 0.0154, 0.0136, 0.0079],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,448][circuit_into_ebeddingspace.py][line:2319][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3050, 0.0071, 0.0871, 0.1592, 0.0224, 0.0018, 0.0255, 0.0035, 0.0267,
        0.0093, 0.0045, 0.0212, 0.0886, 0.0528, 0.0620, 0.0812, 0.0421],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,449][circuit_into_ebeddingspace.py][line:2322][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.7677, 0.0040, 0.0121, 0.0100, 0.0013, 0.0016, 0.0114, 0.0032, 0.0186,
        0.0042, 0.0046, 0.0191, 0.0202, 0.0065, 0.0246, 0.0683, 0.0225],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,451][circuit_into_ebeddingspace.py][line:2325][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.6392, 0.0123, 0.0139, 0.0024, 0.0055, 0.0021, 0.0094, 0.0032, 0.0077,
        0.0073, 0.0035, 0.0094, 0.0633, 0.0242, 0.0838, 0.0903, 0.0225],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,452][circuit_into_ebeddingspace.py][line:2328][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([5.4086e-01, 1.3724e-01, 1.4575e-01, 2.7980e-02, 2.0450e-02, 4.7103e-04,
        4.1870e-03, 4.8853e-04, 1.4964e-03, 2.1987e-04, 5.9150e-04, 6.6206e-04,
        1.6521e-02, 2.0433e-02, 5.3869e-02, 1.6066e-02, 1.2720e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,453][circuit_into_ebeddingspace.py][line:2331][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.1353e-01, 1.4483e-03, 4.9266e-03, 4.3611e-03, 1.0767e-03, 3.2976e-04,
        2.3710e-03, 5.7928e-04, 9.9317e-04, 1.7982e-03, 1.3534e-03, 1.3332e-03,
        5.3179e-02, 1.3069e-02, 1.1288e-01, 6.4114e-01, 4.5633e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,454][circuit_into_ebeddingspace.py][line:2334][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1624, 0.1225, 0.2884, 0.0405, 0.0237, 0.0055, 0.0611, 0.0117, 0.0620,
        0.0116, 0.0163, 0.0449, 0.0875, 0.0223, 0.0222, 0.0095, 0.0080],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,455][circuit_into_ebeddingspace.py][line:2337][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4566, 0.0051, 0.0833, 0.0173, 0.0167, 0.0027, 0.0435, 0.0070, 0.0491,
        0.0146, 0.0118, 0.0456, 0.0501, 0.1042, 0.0270, 0.0146, 0.0507],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,457][circuit_into_ebeddingspace.py][line:2340][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4078, 0.0464, 0.1026, 0.0301, 0.0254, 0.0024, 0.0157, 0.0022, 0.0104,
        0.0019, 0.0026, 0.0100, 0.0526, 0.0439, 0.1313, 0.0588, 0.0558],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,458][circuit_into_ebeddingspace.py][line:2343][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.6634, 0.0164, 0.0679, 0.0089, 0.0113, 0.0078, 0.0298, 0.0297, 0.0188,
        0.0233, 0.0153, 0.0107, 0.0161, 0.0052, 0.0574, 0.0119, 0.0062],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,466][circuit_into_ebeddingspace.py][line:2286][INFO] ################9-th layer#################
[2024-06-27 21:00:54,469][circuit_into_ebeddingspace.py][line:2288][INFO] ##9-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,470][circuit_into_ebeddingspace.py][line:2289][INFO] ##9-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 0],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:54,472][circuit_into_ebeddingspace.py][line:2290][INFO] ##9-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,473][circuit_into_ebeddingspace.py][line:2291][INFO] ##9-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,474][circuit_into_ebeddingspace.py][line:2292][INFO] ##9-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:54,476][circuit_into_ebeddingspace.py][line:2293][INFO] ##9-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,477][circuit_into_ebeddingspace.py][line:2294][INFO] ##9-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,478][circuit_into_ebeddingspace.py][line:2295][INFO] ##9-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [1, 1],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,480][circuit_into_ebeddingspace.py][line:2296][INFO] ##9-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,481][circuit_into_ebeddingspace.py][line:2297][INFO] ##9-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,483][circuit_into_ebeddingspace.py][line:2298][INFO] ##9-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,484][circuit_into_ebeddingspace.py][line:2299][INFO] ##9-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,485][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,486][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,487][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,488][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,489][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,490][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,490][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,491][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,492][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,493][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,494][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,495][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,496][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9493, 0.0507], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,497][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9730, 0.0270], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,498][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9678, 0.0322], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,499][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.8605, 0.1395], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,500][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9815, 0.0185], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,501][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9501, 0.0499], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,502][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9793, 0.0207], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,503][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9225, 0.0775], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,504][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9838, 0.0162], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,504][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9850, 0.0150], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,505][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9670, 0.0330], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,506][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9922, 0.0078], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,507][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9310, 0.0132, 0.0559], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,508][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9818, 0.0151, 0.0030], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,509][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9500, 0.0165, 0.0334], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,510][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8157, 0.1376, 0.0467], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,511][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.9771, 0.0123, 0.0106], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,512][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.9331, 0.0266, 0.0402], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,513][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9809, 0.0021, 0.0170], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,514][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8924, 0.0848, 0.0228], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,515][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9478, 0.0062, 0.0460], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,516][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9870, 0.0016, 0.0114], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,517][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.7529, 0.2220, 0.0251], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,518][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9941, 0.0015, 0.0043], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,519][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.9294, 0.0035, 0.0566, 0.0105], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,520][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.9758, 0.0097, 0.0067, 0.0078], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,521][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.9626, 0.0043, 0.0300, 0.0032], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,522][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.5146, 0.1029, 0.2706, 0.1119], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,523][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9762, 0.0073, 0.0150, 0.0016], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,524][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.8011, 0.0141, 0.1399, 0.0448], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,525][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ of] are: tensor([9.8912e-01, 1.0591e-03, 8.8473e-03, 9.7809e-04], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,526][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.9226, 0.0385, 0.0198, 0.0191], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,527][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.8506, 0.0114, 0.1240, 0.0140], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,528][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.9897, 0.0024, 0.0054, 0.0025], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,529][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.9209, 0.0411, 0.0244, 0.0136], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,530][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.9833, 0.0072, 0.0062, 0.0034], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,531][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.9172, 0.0023, 0.0347, 0.0112, 0.0346], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,532][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ De] are: tensor([9.9610e-01, 4.6840e-04, 1.5206e-03, 9.9392e-04, 9.1477e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,533][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.9604, 0.0028, 0.0180, 0.0015, 0.0172], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,534][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.4305, 0.0213, 0.0360, 0.4550, 0.0572], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,535][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ De] are: tensor([9.9180e-01, 6.3789e-04, 4.8827e-03, 3.4331e-04, 2.3401e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,536][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8626, 0.0113, 0.0431, 0.0297, 0.0533], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,537][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ De] are: tensor([9.9284e-01, 1.0517e-03, 3.6119e-03, 6.5915e-04, 1.8323e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,538][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.9338, 0.0160, 0.0055, 0.0189, 0.0258], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,539][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.8182, 0.0082, 0.0583, 0.0279, 0.0874], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,540][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ De] are: tensor([9.9182e-01, 1.8734e-03, 1.0269e-03, 2.6978e-04, 5.0114e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,541][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.9837, 0.0035, 0.0029, 0.0068, 0.0030], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,542][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ De] are: tensor([9.9552e-01, 1.2506e-03, 1.4935e-03, 1.2866e-03, 4.4425e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,543][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([9.7370e-01, 9.0119e-04, 7.9761e-03, 5.9836e-03, 1.1039e-02, 4.0133e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,544][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.9812e-01, 5.6935e-04, 3.8620e-04, 5.9404e-04, 1.4594e-04, 1.8151e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,545][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([9.9043e-01, 6.5380e-04, 3.1556e-03, 4.2415e-04, 3.4856e-03, 1.8510e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,546][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.5838, 0.0352, 0.0448, 0.0923, 0.2123, 0.0316], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,547][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.9622e-01, 1.3166e-04, 6.7660e-04, 2.0997e-04, 2.3003e-03, 4.5934e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,548][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.9343, 0.0052, 0.0077, 0.0055, 0.0186, 0.0287], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,549][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([9.9592e-01, 6.9032e-04, 6.9508e-04, 1.0928e-03, 5.2839e-04, 1.0746e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,550][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([9.5727e-01, 3.1961e-03, 3.7673e-03, 1.5606e-02, 1.9474e-02, 6.8580e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,551][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.4010, 0.0074, 0.0169, 0.0064, 0.5436, 0.0247], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,552][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([9.9444e-01, 3.5418e-03, 6.6443e-04, 1.9582e-04, 1.7109e-04, 9.9003e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,553][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.9536, 0.0050, 0.0119, 0.0134, 0.0122, 0.0040], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,554][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([9.9552e-01, 3.4380e-04, 5.0862e-04, 1.9850e-03, 4.9042e-04, 1.1551e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,555][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.8112, 0.0080, 0.0900, 0.0227, 0.0311, 0.0036, 0.0334],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,556][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([9.8884e-01, 3.7691e-03, 1.5189e-03, 2.6023e-03, 1.9421e-03, 5.0698e-04,
        8.2458e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,557][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.8771, 0.0034, 0.0183, 0.0026, 0.0399, 0.0144, 0.0442],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,558][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.4319, 0.0410, 0.1097, 0.1479, 0.0293, 0.1027, 0.1375],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,559][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.9348, 0.0023, 0.0029, 0.0019, 0.0349, 0.0057, 0.0176],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,560][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.8769, 0.0050, 0.0298, 0.0127, 0.0070, 0.0037, 0.0649],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,561][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([9.6541e-01, 5.9365e-04, 7.4400e-03, 7.1758e-04, 7.1117e-03, 3.0397e-03,
        1.5683e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,562][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.8881, 0.0385, 0.0122, 0.0137, 0.0179, 0.0021, 0.0275],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,563][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3045, 0.0016, 0.0121, 0.0030, 0.1770, 0.0280, 0.4738],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,564][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([9.8484e-01, 7.4604e-04, 3.2134e-03, 1.2925e-03, 5.1897e-03, 3.4142e-04,
        4.3748e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,565][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.2593, 0.1728, 0.2497, 0.2571, 0.0149, 0.0116, 0.0347],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,566][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([9.8906e-01, 3.2000e-04, 9.7441e-04, 3.4846e-03, 2.1023e-03, 7.3684e-04,
        3.3258e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,567][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([9.3824e-01, 1.9168e-03, 2.0762e-02, 8.0498e-03, 1.4313e-02, 7.8293e-04,
        1.0352e-02, 5.5859e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,568][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([9.9566e-01, 1.9442e-03, 7.1712e-04, 1.8568e-04, 1.6245e-04, 2.6650e-04,
        5.6153e-04, 5.0000e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,569][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([9.6786e-01, 8.2685e-04, 7.0633e-03, 1.1447e-03, 5.0537e-03, 2.9808e-03,
        1.2100e-02, 2.9748e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,570][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.7400, 0.0050, 0.0489, 0.0256, 0.0154, 0.0132, 0.1323, 0.0195],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,571][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([9.9572e-01, 7.8220e-05, 4.4484e-04, 1.4468e-04, 1.9041e-03, 2.0249e-04,
        9.1531e-04, 5.9446e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,572][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.9131, 0.0039, 0.0097, 0.0037, 0.0046, 0.0068, 0.0133, 0.0449],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,573][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([9.9586e-01, 1.8634e-04, 4.1381e-04, 2.2162e-04, 2.5679e-04, 2.1685e-04,
        2.2869e-03, 5.5338e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,574][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([9.6569e-01, 1.2353e-03, 2.3549e-03, 7.2356e-03, 1.0373e-02, 8.3838e-04,
        6.3572e-03, 5.9161e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,575][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.8918, 0.0043, 0.0052, 0.0068, 0.0529, 0.0016, 0.0298, 0.0076],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,576][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([9.9593e-01, 1.0212e-03, 1.1778e-03, 9.8152e-05, 1.8286e-04, 6.8838e-06,
        1.5017e-03, 8.5278e-05], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,577][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([9.6869e-01, 3.6041e-03, 9.2509e-03, 5.0101e-03, 4.8530e-03, 8.9702e-04,
        6.7183e-03, 9.7283e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,578][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([9.9258e-01, 1.4726e-04, 5.3152e-04, 8.0374e-04, 2.5092e-04, 8.4194e-04,
        4.6145e-03, 2.3453e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,579][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.7335, 0.0073, 0.0765, 0.0351, 0.0310, 0.0052, 0.0383, 0.0398, 0.0333],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,580][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.8329e-01, 4.8485e-03, 2.1252e-03, 2.1672e-03, 2.0938e-03, 4.5783e-04,
        8.5507e-04, 1.5528e-03, 2.6082e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,582][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8386, 0.0040, 0.0211, 0.0045, 0.0246, 0.0087, 0.0410, 0.0118, 0.0457],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,583][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.3247, 0.0363, 0.0634, 0.1432, 0.0135, 0.0815, 0.1746, 0.0653, 0.0974],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,584][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.9273, 0.0014, 0.0054, 0.0022, 0.0376, 0.0037, 0.0075, 0.0066, 0.0083],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,585][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.8775, 0.0023, 0.0159, 0.0084, 0.0036, 0.0018, 0.0296, 0.0102, 0.0507],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,586][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [orum] are: tensor([9.6164e-01, 5.6976e-04, 7.0320e-03, 8.5021e-04, 6.4521e-03, 6.4707e-04,
        3.2003e-03, 8.3250e-03, 1.1288e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,587][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.8398, 0.0375, 0.0300, 0.0207, 0.0101, 0.0028, 0.0224, 0.0059, 0.0307],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,588][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.6773, 0.0018, 0.0082, 0.0037, 0.0530, 0.0054, 0.0637, 0.0968, 0.0901],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,589][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [orum] are: tensor([9.8708e-01, 8.9369e-04, 2.4572e-03, 4.4994e-04, 3.7759e-03, 4.5680e-05,
        5.3246e-04, 2.1386e-03, 2.6299e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,590][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1151, 0.1635, 0.2368, 0.2897, 0.0204, 0.0113, 0.1062, 0.0141, 0.0429],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,591][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [orum] are: tensor([9.8376e-01, 4.7885e-04, 1.0509e-03, 3.9119e-03, 1.7014e-03, 2.7396e-04,
        1.7034e-03, 3.1200e-03, 3.9961e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,592][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.8693, 0.0021, 0.0235, 0.0096, 0.0179, 0.0016, 0.0337, 0.0117, 0.0146,
        0.0160], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,593][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ et] are: tensor([9.7312e-01, 8.8419e-03, 2.9882e-03, 1.5321e-03, 9.4402e-04, 2.6596e-03,
        1.6997e-03, 2.9126e-03, 1.9678e-03, 3.3375e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,594][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.7804, 0.0092, 0.0452, 0.0062, 0.0266, 0.0202, 0.0485, 0.0229, 0.0321,
        0.0087], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,595][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.6202, 0.0089, 0.0470, 0.0501, 0.0168, 0.0176, 0.0993, 0.0157, 0.1071,
        0.0173], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,597][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.9251, 0.0012, 0.0050, 0.0012, 0.0099, 0.0042, 0.0167, 0.0147, 0.0098,
        0.0122], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,598][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.8338, 0.0048, 0.0189, 0.0097, 0.0092, 0.0025, 0.0311, 0.0220, 0.0263,
        0.0417], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,599][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ et] are: tensor([9.0379e-01, 1.0219e-03, 5.4674e-03, 2.5701e-04, 6.6190e-03, 1.5598e-02,
        8.4970e-03, 4.4018e-02, 1.3019e-02, 1.7170e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,600][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.9291, 0.0080, 0.0063, 0.0038, 0.0155, 0.0015, 0.0089, 0.0124, 0.0109,
        0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,601][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ et] are: tensor([8.7711e-01, 8.1199e-04, 3.1635e-03, 1.2903e-03, 3.2001e-02, 8.8726e-04,
        4.0386e-02, 1.2445e-02, 2.4055e-02, 7.8468e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,602][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ et] are: tensor([9.7525e-01, 1.2355e-03, 3.0421e-03, 8.7418e-04, 9.2862e-03, 1.1177e-03,
        1.9817e-03, 4.6153e-03, 1.3456e-03, 1.2537e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,603][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.7882, 0.0241, 0.0905, 0.0178, 0.0051, 0.0125, 0.0213, 0.0191, 0.0126,
        0.0090], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,604][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ et] are: tensor([9.8458e-01, 8.7688e-04, 1.0665e-03, 2.0047e-03, 8.6640e-04, 4.6913e-04,
        1.0863e-03, 1.3216e-04, 6.2822e-03, 2.6313e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,605][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.9457, 0.0015, 0.0095, 0.0045, 0.0040, 0.0013, 0.0108, 0.0053, 0.0055,
        0.0063, 0.0055], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,606][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([9.9428e-01, 5.8215e-04, 3.7731e-04, 5.5073e-04, 1.0181e-04, 2.4550e-04,
        4.5224e-04, 8.9855e-04, 1.2081e-03, 5.9698e-04, 7.0713e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,607][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.8622, 0.0057, 0.0195, 0.0060, 0.0049, 0.0081, 0.0241, 0.0075, 0.0369,
        0.0047, 0.0206], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,608][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.4143, 0.0082, 0.0346, 0.0529, 0.0478, 0.0038, 0.0344, 0.0160, 0.0559,
        0.3134, 0.0186], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,610][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([9.7394e-01, 4.9546e-05, 4.7804e-04, 1.5150e-04, 2.2357e-03, 1.0815e-03,
        3.6444e-03, 7.2521e-03, 4.1324e-03, 3.0261e-03, 4.0134e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,611][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.7542, 0.0038, 0.0149, 0.0067, 0.0111, 0.0031, 0.0490, 0.0315, 0.0455,
        0.0461, 0.0340], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,612][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([9.2597e-01, 2.2936e-04, 1.2486e-03, 1.6194e-03, 2.1873e-04, 1.3297e-03,
        4.1440e-02, 2.9465e-03, 2.1210e-02, 1.9099e-03, 1.8786e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,613][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.9152, 0.0036, 0.0055, 0.0143, 0.0111, 0.0012, 0.0112, 0.0086, 0.0110,
        0.0103, 0.0079], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,614][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.3135, 0.0010, 0.0080, 0.0026, 0.0368, 0.0192, 0.2272, 0.0672, 0.1853,
        0.1349, 0.0043], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,615][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([9.5179e-01, 2.0903e-03, 2.8911e-03, 8.3104e-04, 7.2222e-04, 1.9423e-04,
        2.2360e-02, 9.3303e-05, 1.5629e-02, 6.3872e-04, 2.7619e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,616][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.4697e-01, 5.2358e-03, 1.1940e-02, 9.4454e-03, 3.1442e-03, 4.1177e-04,
        6.6410e-03, 1.1819e-03, 2.8784e-03, 8.9181e-03, 3.2360e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,617][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([9.1802e-01, 1.0615e-04, 1.1147e-03, 1.4685e-03, 6.8015e-04, 1.9542e-03,
        1.3364e-02, 1.3931e-03, 2.4905e-02, 3.5366e-02, 1.6310e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,618][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.7170, 0.0095, 0.0877, 0.0243, 0.0163, 0.0047, 0.0247, 0.0264, 0.0190,
        0.0519, 0.0088, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,619][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [orum] are: tensor([9.7754e-01, 3.3428e-03, 1.6388e-03, 1.8353e-03, 1.3088e-03, 3.3518e-04,
        5.9735e-04, 8.6410e-04, 2.3932e-03, 5.7388e-03, 1.7132e-03, 2.6881e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,621][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8081, 0.0062, 0.0345, 0.0050, 0.0268, 0.0044, 0.0313, 0.0084, 0.0319,
        0.0110, 0.0119, 0.0204], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,622][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.3716, 0.0446, 0.0962, 0.1015, 0.0097, 0.0315, 0.1249, 0.0248, 0.0916,
        0.0510, 0.0195, 0.0332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,623][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.8958, 0.0011, 0.0031, 0.0012, 0.0409, 0.0034, 0.0100, 0.0081, 0.0111,
        0.0116, 0.0077, 0.0060], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,624][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.8234, 0.0019, 0.0152, 0.0072, 0.0033, 0.0019, 0.0220, 0.0095, 0.0405,
        0.0367, 0.0064, 0.0320], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,625][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [orum] are: tensor([9.1669e-01, 4.5340e-04, 7.1522e-03, 1.4251e-03, 8.6483e-03, 1.0901e-03,
        5.9417e-03, 1.1464e-02, 1.7209e-02, 1.1838e-02, 5.9177e-03, 1.2169e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,626][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.8288, 0.0256, 0.0216, 0.0146, 0.0067, 0.0020, 0.0207, 0.0045, 0.0268,
        0.0238, 0.0035, 0.0214], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,627][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4683, 0.0011, 0.0054, 0.0026, 0.0364, 0.0060, 0.0856, 0.0911, 0.1290,
        0.1069, 0.0055, 0.0621], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,628][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [orum] are: tensor([9.5842e-01, 8.6695e-04, 3.6338e-03, 1.2048e-03, 8.3787e-03, 1.0629e-04,
        1.1569e-03, 4.1063e-03, 4.4899e-03, 8.0506e-03, 3.1825e-03, 6.4050e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,630][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0817, 0.1111, 0.3383, 0.1749, 0.0082, 0.0057, 0.1247, 0.0062, 0.0683,
        0.0365, 0.0054, 0.0390], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,631][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [orum] are: tensor([9.5948e-01, 4.4074e-04, 1.0704e-03, 3.1176e-03, 1.2945e-03, 1.1384e-04,
        8.0422e-04, 6.8893e-04, 1.6386e-03, 2.7893e-02, 1.6075e-03, 1.8552e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,632][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.4280, 0.0186, 0.2324, 0.0249, 0.0474, 0.0071, 0.0668, 0.0121, 0.0501,
        0.0126, 0.0082, 0.0158, 0.0760], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,633][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.9265, 0.0158, 0.0030, 0.0102, 0.0020, 0.0028, 0.0024, 0.0030, 0.0051,
        0.0012, 0.0021, 0.0031, 0.0227], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,634][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.5762, 0.0065, 0.0392, 0.0103, 0.0302, 0.0357, 0.0572, 0.0161, 0.0875,
        0.0219, 0.0295, 0.0698, 0.0199], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,635][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.5439, 0.0041, 0.0691, 0.0027, 0.0013, 0.0033, 0.0446, 0.0025, 0.1226,
        0.0060, 0.0038, 0.1022, 0.0939], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,636][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.4686, 0.0046, 0.0073, 0.0071, 0.0818, 0.0160, 0.1267, 0.0399, 0.0887,
        0.0601, 0.0256, 0.0553, 0.0183], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,638][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.6314, 0.0152, 0.0787, 0.0353, 0.0210, 0.0019, 0.0567, 0.0133, 0.0474,
        0.0151, 0.0054, 0.0293, 0.0494], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,639][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.7285, 0.0017, 0.0249, 0.0021, 0.0720, 0.0179, 0.0196, 0.0358, 0.0148,
        0.0017, 0.0567, 0.0106, 0.0136], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,640][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.6705, 0.0234, 0.0130, 0.0089, 0.0302, 0.0031, 0.0533, 0.0069, 0.0609,
        0.0379, 0.0053, 0.0614, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,641][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0631, 0.0007, 0.0130, 0.0025, 0.0181, 0.0051, 0.1893, 0.0535, 0.3339,
        0.1485, 0.0093, 0.1549, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,642][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.8348, 0.0059, 0.0083, 0.0056, 0.0462, 0.0042, 0.0060, 0.0147, 0.0052,
        0.0057, 0.0456, 0.0027, 0.0153], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,643][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.5974, 0.0368, 0.0485, 0.0092, 0.0033, 0.0021, 0.0221, 0.0021, 0.0402,
        0.0079, 0.0089, 0.0551, 0.1665], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,644][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ is] are: tensor([9.5934e-01, 5.3904e-03, 4.8179e-03, 3.1424e-03, 8.7762e-03, 1.4577e-03,
        3.1767e-03, 6.0452e-04, 1.2007e-03, 1.1922e-03, 2.8197e-03, 1.2325e-03,
        6.8443e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,646][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.5915, 0.0038, 0.1408, 0.0221, 0.0163, 0.0016, 0.0541, 0.0053, 0.0574,
        0.0091, 0.0051, 0.0325, 0.0296, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,647][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([8.3693e-01, 1.0474e-01, 1.0514e-02, 1.0451e-02, 8.1377e-04, 9.1071e-04,
        3.4051e-03, 7.4751e-04, 6.8703e-03, 2.3001e-03, 1.3351e-03, 3.9562e-03,
        4.7673e-03, 1.2265e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,648][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.4233, 0.0021, 0.0262, 0.0045, 0.0165, 0.0235, 0.0833, 0.0174, 0.1483,
        0.0242, 0.0443, 0.1716, 0.0087, 0.0061], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,649][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2394, 0.0056, 0.0185, 0.0064, 0.0033, 0.0010, 0.0384, 0.0027, 0.0487,
        0.0104, 0.0063, 0.0547, 0.5009, 0.0635], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,650][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.8737, 0.0012, 0.0434, 0.0078, 0.0172, 0.0010, 0.0144, 0.0022, 0.0117,
        0.0016, 0.0035, 0.0090, 0.0088, 0.0046], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,651][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.6650, 0.0157, 0.0460, 0.0257, 0.0104, 0.0008, 0.0404, 0.0076, 0.0428,
        0.0087, 0.0030, 0.0290, 0.0580, 0.0468], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,652][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([8.7656e-01, 5.6910e-03, 5.8845e-02, 1.6276e-03, 1.4674e-02, 3.3705e-03,
        5.6084e-03, 7.4650e-03, 3.8943e-03, 5.2755e-04, 1.2816e-02, 3.4520e-03,
        3.8362e-03, 1.6353e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,654][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.7445, 0.0386, 0.0169, 0.0055, 0.0163, 0.0024, 0.0332, 0.0060, 0.0385,
        0.0321, 0.0023, 0.0353, 0.0159, 0.0124], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,655][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2724, 0.0014, 0.0237, 0.0038, 0.0483, 0.0053, 0.1162, 0.0322, 0.2553,
        0.0674, 0.0064, 0.1174, 0.0285, 0.0218], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,656][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([8.7241e-01, 2.8829e-02, 4.1083e-02, 2.1427e-03, 2.1748e-02, 1.8855e-03,
        3.5243e-03, 1.2639e-03, 3.2120e-03, 8.4143e-04, 1.2954e-02, 2.1533e-03,
        3.4675e-03, 4.4864e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,657][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.6013, 0.0130, 0.0232, 0.0169, 0.0024, 0.0006, 0.0092, 0.0015, 0.0094,
        0.0033, 0.0032, 0.0092, 0.2470, 0.0596], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,658][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.3821e-01, 2.5199e-02, 1.5223e-02, 2.3903e-03, 5.7127e-03, 9.7373e-04,
        7.6317e-04, 8.8932e-04, 8.0341e-04, 6.0561e-04, 3.4688e-03, 7.7789e-04,
        1.7834e-03, 3.2006e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,659][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.5802, 0.0103, 0.2295, 0.0228, 0.0146, 0.0015, 0.0167, 0.0019, 0.0153,
        0.0040, 0.0033, 0.0074, 0.0682, 0.0184, 0.0058], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,661][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ same] are: tensor([9.3925e-01, 3.0685e-02, 4.0535e-03, 1.0153e-02, 2.0610e-04, 7.1809e-05,
        5.0545e-04, 1.0026e-04, 4.7735e-04, 6.9754e-04, 8.6756e-05, 3.7099e-04,
        2.8640e-03, 3.6004e-03, 6.8816e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,662][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.7549, 0.0041, 0.0134, 0.0071, 0.0220, 0.0226, 0.0306, 0.0070, 0.0508,
        0.0086, 0.0237, 0.0345, 0.0114, 0.0071, 0.0022], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,663][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.6076, 0.0026, 0.0116, 0.0016, 0.0008, 0.0012, 0.0080, 0.0009, 0.0074,
        0.0023, 0.0015, 0.0055, 0.1671, 0.1407, 0.0412], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,664][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.8536, 0.0056, 0.0374, 0.0205, 0.0206, 0.0011, 0.0099, 0.0016, 0.0067,
        0.0060, 0.0022, 0.0049, 0.0079, 0.0135, 0.0084], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,665][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ same] are: tensor([8.9184e-01, 1.2874e-02, 1.2631e-02, 1.0876e-02, 5.0439e-03, 3.1555e-04,
        4.8967e-03, 1.4739e-03, 7.0196e-03, 3.1983e-03, 8.5344e-04, 5.6818e-03,
        1.8636e-02, 1.3556e-02, 1.1107e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,666][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ same] are: tensor([9.0860e-01, 3.4953e-03, 4.7541e-02, 1.9484e-03, 4.9025e-03, 7.8647e-04,
        2.4532e-03, 3.4528e-03, 2.8657e-03, 1.0694e-03, 5.3808e-03, 3.6295e-03,
        7.1798e-03, 2.2012e-03, 4.4994e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,668][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.6146, 0.0138, 0.0080, 0.0104, 0.0078, 0.0031, 0.0398, 0.0178, 0.0701,
        0.0729, 0.0018, 0.0495, 0.0449, 0.0174, 0.0282], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,669][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.7230, 0.0037, 0.0317, 0.0106, 0.0252, 0.0027, 0.0259, 0.0113, 0.0524,
        0.0289, 0.0057, 0.0313, 0.0189, 0.0254, 0.0034], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,670][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ same] are: tensor([8.9680e-01, 1.0967e-02, 3.8236e-02, 6.4827e-03, 1.5344e-02, 4.6838e-05,
        2.1782e-03, 1.2159e-03, 4.1516e-03, 5.0862e-03, 2.2862e-03, 6.2162e-03,
        4.7693e-03, 2.2788e-03, 3.9364e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,671][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.4383, 0.0131, 0.0084, 0.0022, 0.0011, 0.0006, 0.0039, 0.0010, 0.0038,
        0.0029, 0.0042, 0.0043, 0.1127, 0.1148, 0.2890], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,672][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ same] are: tensor([9.7901e-01, 2.7260e-03, 3.1144e-03, 2.7092e-03, 2.1699e-03, 6.4630e-04,
        4.1657e-04, 4.4001e-04, 1.5655e-04, 2.5723e-03, 7.8897e-04, 2.1380e-04,
        3.2246e-03, 1.6359e-03, 1.7653e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,674][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.6986, 0.0067, 0.0979, 0.0060, 0.0293, 0.0045, 0.0332, 0.0103, 0.0285,
        0.0112, 0.0102, 0.0180, 0.0239, 0.0120, 0.0065, 0.0031],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,675][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ as] are: tensor([8.7730e-01, 3.8159e-02, 4.5123e-03, 5.6617e-03, 3.5516e-04, 6.1196e-04,
        7.7278e-04, 5.0578e-04, 1.0925e-03, 1.4573e-04, 7.8626e-04, 6.9400e-04,
        3.7562e-03, 1.0115e-02, 1.3881e-02, 4.1646e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,676][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.6884, 0.0063, 0.0177, 0.0046, 0.0553, 0.0359, 0.0319, 0.0234, 0.0513,
        0.0123, 0.0259, 0.0338, 0.0040, 0.0052, 0.0025, 0.0015],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,677][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.6409, 0.0041, 0.0322, 0.0018, 0.0042, 0.0085, 0.0472, 0.0120, 0.0519,
        0.0064, 0.0064, 0.0322, 0.0361, 0.0059, 0.0752, 0.0350],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,678][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.5558, 0.0360, 0.0317, 0.0098, 0.0790, 0.0082, 0.0322, 0.0149, 0.0343,
        0.0219, 0.0031, 0.0197, 0.0758, 0.0319, 0.0119, 0.0337],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,679][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.7960, 0.0149, 0.0255, 0.0233, 0.0148, 0.0016, 0.0120, 0.0053, 0.0113,
        0.0039, 0.0029, 0.0083, 0.0216, 0.0210, 0.0146, 0.0231],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,681][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ as] are: tensor([7.4769e-01, 6.4789e-03, 6.1290e-02, 2.6579e-03, 6.4548e-02, 1.6168e-02,
        2.1277e-02, 1.7328e-02, 8.4294e-03, 4.5938e-04, 2.2871e-02, 3.8228e-03,
        1.0494e-02, 1.2407e-02, 1.4499e-03, 2.6303e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,682][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.6750, 0.0247, 0.0228, 0.0046, 0.0652, 0.0096, 0.0314, 0.0109, 0.0450,
        0.0098, 0.0082, 0.0318, 0.0339, 0.0213, 0.0032, 0.0027],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,683][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3607, 0.0019, 0.0228, 0.0077, 0.0318, 0.0034, 0.0747, 0.0329, 0.1931,
        0.0776, 0.0139, 0.1376, 0.0209, 0.0097, 0.0062, 0.0051],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,684][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.7978, 0.0112, 0.0441, 0.0207, 0.0679, 0.0013, 0.0081, 0.0062, 0.0055,
        0.0029, 0.0111, 0.0034, 0.0048, 0.0097, 0.0011, 0.0043],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,685][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.8542, 0.0057, 0.0034, 0.0011, 0.0013, 0.0011, 0.0028, 0.0011, 0.0029,
        0.0022, 0.0030, 0.0030, 0.0157, 0.0085, 0.0522, 0.0417],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,687][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ as] are: tensor([9.4889e-01, 5.0719e-03, 3.7904e-03, 1.9613e-03, 1.2101e-02, 2.8644e-03,
        1.8593e-03, 8.5410e-05, 4.9318e-04, 2.9376e-04, 9.2898e-04, 3.3092e-04,
        2.9969e-03, 1.6483e-02, 1.0717e-03, 7.7543e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,688][circuit_into_ebeddingspace.py][line:2310][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.5823, 0.0078, 0.2019, 0.0201, 0.0116, 0.0022, 0.0380, 0.0050, 0.0386,
        0.0031, 0.0064, 0.0234, 0.0322, 0.0095, 0.0050, 0.0072, 0.0057],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,689][circuit_into_ebeddingspace.py][line:2313][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([5.6603e-01, 3.4689e-01, 1.8090e-02, 5.5546e-03, 2.4754e-04, 3.7724e-04,
        1.7454e-03, 2.9582e-04, 2.1007e-03, 4.0376e-04, 5.1243e-04, 1.1837e-03,
        2.0855e-03, 3.3747e-03, 2.9878e-02, 1.3809e-02, 7.4234e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,690][circuit_into_ebeddingspace.py][line:2316][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.5349, 0.0055, 0.0491, 0.0098, 0.0176, 0.0372, 0.0549, 0.0238, 0.0922,
        0.0130, 0.0455, 0.0858, 0.0144, 0.0047, 0.0052, 0.0046, 0.0019],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,692][circuit_into_ebeddingspace.py][line:2319][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2001, 0.0052, 0.0133, 0.0113, 0.0031, 0.0007, 0.0091, 0.0020, 0.0109,
        0.0070, 0.0033, 0.0128, 0.0856, 0.0259, 0.0759, 0.5015, 0.0323],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,693][circuit_into_ebeddingspace.py][line:2322][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([6.3443e-01, 8.6736e-03, 2.3095e-01, 2.2126e-02, 2.3022e-02, 5.3270e-04,
        4.5471e-03, 1.2241e-03, 3.1333e-03, 8.5133e-04, 1.0751e-03, 2.3055e-03,
        3.5008e-02, 4.5774e-03, 5.9554e-03, 1.6085e-02, 5.4978e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,694][circuit_into_ebeddingspace.py][line:2325][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([8.0498e-01, 1.7462e-02, 2.9849e-02, 1.9673e-02, 9.6187e-03, 5.9656e-04,
        7.8680e-03, 2.5417e-03, 9.0490e-03, 2.2730e-03, 1.8809e-03, 6.1088e-03,
        1.5782e-02, 2.5878e-02, 1.1420e-02, 8.1520e-03, 2.6864e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,695][circuit_into_ebeddingspace.py][line:2328][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([6.8102e-01, 2.2198e-02, 2.3238e-01, 1.9682e-03, 1.8657e-02, 3.0757e-03,
        5.5602e-03, 8.2361e-03, 2.3422e-03, 1.5745e-04, 1.0903e-02, 1.4766e-03,
        4.1454e-03, 1.3170e-03, 3.3284e-03, 2.2338e-03, 1.0014e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,696][circuit_into_ebeddingspace.py][line:2331][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.6108, 0.1921, 0.0373, 0.0041, 0.0248, 0.0059, 0.0153, 0.0065, 0.0189,
        0.0100, 0.0065, 0.0141, 0.0132, 0.0100, 0.0145, 0.0090, 0.0069],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,698][circuit_into_ebeddingspace.py][line:2334][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2444, 0.0015, 0.0283, 0.0033, 0.0391, 0.0119, 0.1021, 0.0524, 0.2432,
        0.0805, 0.0175, 0.1445, 0.0124, 0.0072, 0.0037, 0.0026, 0.0056],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,699][circuit_into_ebeddingspace.py][line:2337][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.6753, 0.0788, 0.1767, 0.0044, 0.0230, 0.0008, 0.0043, 0.0023, 0.0035,
        0.0013, 0.0092, 0.0024, 0.0036, 0.0032, 0.0057, 0.0023, 0.0030],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,700][circuit_into_ebeddingspace.py][line:2340][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([6.6575e-01, 5.6507e-03, 3.7421e-03, 8.1974e-03, 1.3325e-03, 3.7695e-04,
        1.3020e-03, 1.0029e-03, 1.0883e-03, 2.7148e-03, 1.7335e-03, 1.2478e-03,
        2.4880e-02, 1.4846e-02, 1.4698e-02, 2.1035e-01, 4.1097e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,701][circuit_into_ebeddingspace.py][line:2343][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([8.9978e-01, 5.3284e-02, 1.9065e-02, 1.8044e-03, 1.0584e-02, 1.9978e-03,
        1.0063e-03, 3.6133e-04, 1.0675e-03, 1.7885e-04, 2.9413e-03, 6.0276e-04,
        1.1241e-03, 1.3057e-03, 2.4128e-03, 3.5595e-04, 2.1307e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,709][circuit_into_ebeddingspace.py][line:2286][INFO] ################10-th layer#################
[2024-06-27 21:00:54,712][circuit_into_ebeddingspace.py][line:2288][INFO] ##10-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,713][circuit_into_ebeddingspace.py][line:2289][INFO] ##10-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,715][circuit_into_ebeddingspace.py][line:2290][INFO] ##10-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,716][circuit_into_ebeddingspace.py][line:2291][INFO] ##10-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,718][circuit_into_ebeddingspace.py][line:2292][INFO] ##10-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,719][circuit_into_ebeddingspace.py][line:2293][INFO] ##10-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,720][circuit_into_ebeddingspace.py][line:2294][INFO] ##10-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 0],
         [0, 0],
         [1, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,722][circuit_into_ebeddingspace.py][line:2295][INFO] ##10-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,723][circuit_into_ebeddingspace.py][line:2296][INFO] ##10-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,724][circuit_into_ebeddingspace.py][line:2297][INFO] ##10-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,726][circuit_into_ebeddingspace.py][line:2298][INFO] ##10-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,727][circuit_into_ebeddingspace.py][line:2299][INFO] ##10-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 0],
         [1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,728][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,729][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,730][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,731][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,732][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,733][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,734][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,735][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,736][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,736][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,737][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,738][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,739][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9467, 0.0533], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,740][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9602, 0.0398], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,741][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9710, 0.0290], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,742][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9528, 0.0472], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,743][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9366, 0.0634], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,744][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9705, 0.0295], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,745][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9593, 0.0407], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,746][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9151, 0.0849], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,747][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9524, 0.0476], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,748][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8962, 0.1038], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,749][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9801, 0.0199], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,750][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.7454, 0.2546], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,751][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9704, 0.0082, 0.0214], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,752][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9714, 0.0061, 0.0225], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,753][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9701, 0.0053, 0.0247], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,753][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8824, 0.0340, 0.0836], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,754][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.9102, 0.0594, 0.0304], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,755][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.6064, 0.2932, 0.1005], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,756][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9791, 0.0069, 0.0140], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,757][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8244, 0.0093, 0.1663], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,758][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9753, 0.0074, 0.0174], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,759][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.5240, 0.3400, 0.1360], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,760][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.9899, 0.0028, 0.0073], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,761][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.7763, 0.1635, 0.0602], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,762][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.8612, 0.0087, 0.0889, 0.0413], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,763][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.9793, 0.0069, 0.0112, 0.0026], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,764][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.9740, 0.0043, 0.0164, 0.0052], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,765][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.8018, 0.0338, 0.1233, 0.0411], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,766][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.8850, 0.0328, 0.0499, 0.0323], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,767][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9506, 0.0182, 0.0172, 0.0141], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,768][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.9753, 0.0039, 0.0150, 0.0058], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,769][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.8239, 0.0135, 0.1118, 0.0508], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,770][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.9628, 0.0179, 0.0084, 0.0109], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,771][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.7640, 0.1354, 0.0362, 0.0643], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,772][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.9791, 0.0048, 0.0071, 0.0090], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,773][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.8006, 0.0068, 0.0598, 0.1327], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:54,774][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.9434, 0.0071, 0.0231, 0.0130, 0.0134], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,775][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ De] are: tensor([9.8625e-01, 2.1708e-03, 3.5052e-03, 4.8845e-04, 7.5898e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,776][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ De] are: tensor([9.9441e-01, 9.5034e-04, 2.1091e-03, 1.1734e-03, 1.3545e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,777][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.9857, 0.0019, 0.0058, 0.0035, 0.0031], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,778][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9226, 0.0075, 0.0118, 0.0032, 0.0549], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,779][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.9890, 0.0027, 0.0033, 0.0031, 0.0019], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,780][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9826, 0.0012, 0.0039, 0.0025, 0.0098], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,781][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ De] are: tensor([9.8016e-01, 4.4761e-04, 7.6555e-03, 2.2291e-03, 9.5076e-03],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,782][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.9848, 0.0041, 0.0016, 0.0018, 0.0076], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,783][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.9520, 0.0071, 0.0034, 0.0224, 0.0151], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,784][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.9495, 0.0034, 0.0043, 0.0246, 0.0182], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,785][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.9166, 0.0022, 0.0054, 0.0554, 0.0204], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:54,786][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([9.7958e-01, 8.3860e-04, 8.8104e-03, 4.8524e-03, 8.7563e-04, 5.0390e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,787][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.9327e-01, 5.9182e-04, 1.0731e-03, 3.7131e-04, 2.2880e-03, 2.4100e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,788][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([9.8998e-01, 8.7722e-04, 2.6186e-03, 2.3877e-03, 1.6985e-03, 2.4360e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,789][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.9812, 0.0020, 0.0083, 0.0036, 0.0031, 0.0018], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,790][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.9386, 0.0071, 0.0020, 0.0028, 0.0460, 0.0034], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,791][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.9250, 0.0311, 0.0152, 0.0105, 0.0151, 0.0030], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,792][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([9.8891e-01, 3.1637e-04, 1.9063e-03, 3.4725e-04, 6.9279e-03, 1.5935e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,793][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([9.9502e-01, 4.2493e-05, 5.2775e-04, 2.3622e-03, 1.4300e-03, 6.1611e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,794][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([9.9081e-01, 1.4596e-03, 1.9484e-03, 2.6311e-03, 2.3144e-03, 8.3871e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,795][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.8413, 0.0144, 0.0381, 0.0101, 0.0714, 0.0245], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,796][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([9.9165e-01, 5.8827e-04, 8.6575e-04, 3.7369e-03, 2.1296e-03, 1.0245e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,797][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.9710, 0.0012, 0.0031, 0.0075, 0.0140, 0.0033], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:54,798][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.9267, 0.0013, 0.0189, 0.0034, 0.0186, 0.0051, 0.0261],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,799][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.9364, 0.0018, 0.0078, 0.0038, 0.0287, 0.0040, 0.0176],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,800][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.9043, 0.0012, 0.0049, 0.0066, 0.0159, 0.0288, 0.0384],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,801][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.9091, 0.0109, 0.0409, 0.0140, 0.0150, 0.0039, 0.0062],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,802][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.7305, 0.0736, 0.0373, 0.0141, 0.0291, 0.0124, 0.1031],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,803][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.2683, 0.3499, 0.1755, 0.1075, 0.0113, 0.0178, 0.0696],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,804][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.8886, 0.0011, 0.0085, 0.0026, 0.0350, 0.0092, 0.0549],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,805][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([8.0834e-01, 9.6054e-04, 1.2278e-02, 2.0410e-02, 1.8186e-02, 5.0063e-04,
        1.3932e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,806][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([9.7019e-01, 3.3762e-03, 4.1934e-03, 7.5548e-03, 8.5677e-03, 6.2818e-04,
        5.4932e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,807][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1968, 0.0667, 0.4292, 0.0481, 0.0284, 0.0418, 0.1891],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,808][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.9140, 0.0009, 0.0028, 0.0046, 0.0514, 0.0071, 0.0191],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,809][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.8376, 0.0085, 0.0180, 0.0506, 0.0094, 0.0057, 0.0703],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:54,810][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([9.7159e-01, 1.3749e-03, 6.2804e-03, 3.1288e-03, 2.0263e-03, 8.9860e-04,
        1.2492e-02, 2.2080e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,811][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([9.9292e-01, 3.9455e-04, 1.1220e-03, 7.6060e-05, 1.3469e-03, 2.2256e-04,
        3.6527e-03, 2.6049e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,812][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([9.9367e-01, 2.7298e-04, 9.6500e-04, 5.8294e-04, 6.5003e-04, 6.0839e-04,
        3.0223e-03, 2.2583e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,813][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([9.8388e-01, 2.6802e-03, 6.8560e-03, 1.5325e-03, 8.6136e-04, 1.8588e-03,
        1.9113e-03, 4.1860e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,814][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([9.4753e-01, 5.0278e-03, 3.3593e-03, 4.8652e-04, 7.8270e-03, 6.8712e-03,
        2.1845e-02, 7.0527e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,816][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.9819, 0.0036, 0.0017, 0.0014, 0.0057, 0.0016, 0.0021, 0.0020],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,817][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([9.7711e-01, 3.8671e-04, 1.7079e-03, 3.4691e-04, 1.3297e-03, 7.0749e-04,
        1.7280e-02, 1.1364e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,818][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([9.6775e-01, 1.3890e-04, 8.0581e-04, 1.7595e-03, 6.3522e-04, 2.9832e-04,
        2.7333e-02, 1.2822e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,819][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([9.9148e-01, 1.5776e-03, 1.2768e-03, 9.3562e-04, 1.4949e-03, 1.8810e-04,
        2.7267e-03, 3.2268e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,820][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.8368, 0.0015, 0.0070, 0.0027, 0.0051, 0.0086, 0.1081, 0.0302],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,821][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([9.8381e-01, 1.6908e-03, 1.2723e-03, 5.4248e-03, 2.3155e-03, 2.4669e-04,
        4.9423e-03, 3.0225e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,822][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([9.7520e-01, 3.5793e-04, 9.9691e-04, 2.9218e-03, 4.4492e-03, 2.3771e-03,
        1.0228e-02, 3.4708e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:54,823][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.9144, 0.0011, 0.0130, 0.0017, 0.0088, 0.0019, 0.0116, 0.0183, 0.0292],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,824][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.9182, 0.0016, 0.0099, 0.0030, 0.0244, 0.0009, 0.0070, 0.0206, 0.0144],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,825][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8739, 0.0022, 0.0050, 0.0083, 0.0174, 0.0116, 0.0170, 0.0086, 0.0560],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,826][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.8745, 0.0104, 0.0489, 0.0266, 0.0105, 0.0019, 0.0072, 0.0035, 0.0165],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,827][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.5963, 0.1284, 0.0859, 0.0187, 0.0098, 0.0053, 0.0638, 0.0034, 0.0882],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,828][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0545, 0.3651, 0.1918, 0.0831, 0.0069, 0.0158, 0.1794, 0.0080, 0.0953],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,829][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8724, 0.0018, 0.0072, 0.0042, 0.0162, 0.0021, 0.0189, 0.0158, 0.0613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,830][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [orum] are: tensor([8.0593e-01, 1.7459e-03, 2.1227e-02, 2.9545e-02, 8.0040e-03, 2.9589e-04,
        6.7781e-02, 2.5625e-03, 6.2908e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,831][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [orum] are: tensor([9.7714e-01, 1.6744e-03, 2.5874e-03, 7.7840e-03, 4.7328e-03, 1.1083e-04,
        2.4332e-03, 1.7089e-03, 1.8330e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,833][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0353, 0.0291, 0.4254, 0.0185, 0.0107, 0.0211, 0.3364, 0.0267, 0.0969],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,834][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [orum] are: tensor([9.1882e-01, 9.1793e-04, 2.5303e-03, 3.9520e-03, 2.9469e-02, 1.9644e-03,
        6.7538e-03, 1.7775e-02, 1.7821e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,835][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.7899, 0.0055, 0.0079, 0.0637, 0.0046, 0.0030, 0.0409, 0.0371, 0.0474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:54,836][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.8204, 0.0027, 0.0188, 0.0036, 0.0446, 0.0075, 0.0239, 0.0488, 0.0187,
        0.0110], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,837][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.7025, 0.0298, 0.0198, 0.0045, 0.0944, 0.0327, 0.0375, 0.0388, 0.0288,
        0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,838][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.9418, 0.0027, 0.0048, 0.0013, 0.0078, 0.0054, 0.0118, 0.0023, 0.0178,
        0.0045], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,839][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.8114, 0.0471, 0.0607, 0.0100, 0.0075, 0.0317, 0.0067, 0.0099, 0.0055,
        0.0096], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,840][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.4603, 0.0521, 0.0116, 0.0060, 0.0072, 0.0406, 0.1683, 0.0606, 0.1649,
        0.0284], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,841][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.9229, 0.0093, 0.0095, 0.0095, 0.0119, 0.0038, 0.0089, 0.0025, 0.0060,
        0.0157], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,842][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.7724, 0.0052, 0.0162, 0.0053, 0.0335, 0.0392, 0.0382, 0.0396, 0.0438,
        0.0065], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,843][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.6731, 0.0023, 0.0054, 0.0209, 0.0274, 0.0019, 0.1214, 0.0043, 0.0892,
        0.0542], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,844][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.8936, 0.0297, 0.0120, 0.0087, 0.0208, 0.0019, 0.0052, 0.0039, 0.0044,
        0.0196], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,846][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.7447, 0.0198, 0.0223, 0.0227, 0.0147, 0.0020, 0.0604, 0.0049, 0.0230,
        0.0854], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,847][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.6895, 0.0051, 0.0060, 0.0056, 0.1106, 0.0267, 0.0260, 0.0770, 0.0273,
        0.0261], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,848][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.4678, 0.0260, 0.1243, 0.0272, 0.0107, 0.0355, 0.1349, 0.1001, 0.0535,
        0.0200], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:54,849][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([9.1103e-01, 7.2707e-04, 4.0450e-03, 5.6679e-03, 4.0464e-03, 4.1604e-03,
        4.1721e-02, 3.3537e-03, 1.7629e-02, 4.4708e-03, 3.1454e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,850][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([9.2532e-01, 3.2660e-04, 2.4538e-03, 4.1395e-04, 2.5752e-03, 3.9796e-03,
        4.5047e-02, 6.3971e-04, 1.6154e-02, 1.6365e-03, 1.4495e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,851][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([9.5081e-01, 3.2189e-04, 1.1637e-03, 1.5183e-03, 1.7593e-03, 2.0346e-03,
        6.3675e-03, 6.8494e-04, 1.7087e-02, 1.7795e-02, 4.5966e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,852][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([9.6465e-01, 8.2463e-04, 4.3101e-03, 3.6342e-03, 1.0155e-03, 3.7252e-03,
        4.9967e-03, 1.4253e-03, 8.8843e-03, 6.0626e-03, 4.7522e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,853][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.8103, 0.0019, 0.0046, 0.0012, 0.0094, 0.0183, 0.0492, 0.0169, 0.0446,
        0.0149, 0.0287], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,854][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.9132, 0.0137, 0.0069, 0.0040, 0.0036, 0.0019, 0.0156, 0.0021, 0.0110,
        0.0204, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,855][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([9.3754e-01, 2.9559e-04, 1.1824e-03, 4.3846e-04, 4.1525e-03, 6.8653e-03,
        2.4596e-02, 1.4567e-03, 2.1449e-02, 1.5859e-03, 4.4259e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,856][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([8.4355e-01, 5.2800e-05, 4.0307e-04, 1.8600e-03, 8.8048e-04, 1.1805e-03,
        7.4995e-02, 2.5827e-03, 4.3508e-02, 3.0166e-02, 8.1932e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,858][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([9.4376e-01, 1.0146e-03, 1.2828e-03, 7.0637e-03, 3.1149e-03, 4.0865e-04,
        6.1377e-03, 1.1892e-03, 5.0221e-03, 2.6350e-02, 4.6609e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,859][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.5738, 0.0029, 0.0167, 0.0044, 0.0054, 0.0033, 0.0530, 0.0166, 0.0301,
        0.2700, 0.0237], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,860][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.3928e-01, 6.1049e-04, 6.2799e-04, 1.3373e-02, 4.6569e-03, 4.0884e-03,
        2.2329e-02, 1.5179e-03, 7.6754e-03, 4.8972e-03, 9.4286e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,861][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([8.6127e-01, 2.4445e-04, 4.8680e-03, 3.3145e-03, 1.3063e-03, 4.4830e-03,
        4.9517e-02, 1.6989e-02, 3.5528e-02, 1.6182e-02, 6.2988e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:54,862][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [orum] are: tensor([8.7190e-01, 8.1618e-04, 8.1798e-03, 1.6264e-03, 9.5405e-03, 2.4439e-03,
        1.1401e-02, 2.0733e-02, 2.4383e-02, 2.5498e-02, 6.8721e-03, 1.6605e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,863][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.8971, 0.0010, 0.0065, 0.0050, 0.0242, 0.0015, 0.0089, 0.0171, 0.0121,
        0.0111, 0.0036, 0.0118], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,864][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.7886, 0.0016, 0.0033, 0.0085, 0.0133, 0.0109, 0.0130, 0.0052, 0.0395,
        0.0936, 0.0032, 0.0194], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,865][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.8916, 0.0078, 0.0299, 0.0217, 0.0084, 0.0012, 0.0055, 0.0018, 0.0117,
        0.0106, 0.0021, 0.0076], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,866][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.6656, 0.0703, 0.0559, 0.0109, 0.0080, 0.0065, 0.0564, 0.0032, 0.0771,
        0.0130, 0.0025, 0.0305], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,868][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0514, 0.2743, 0.1591, 0.0539, 0.0054, 0.0089, 0.1695, 0.0037, 0.1329,
        0.0557, 0.0198, 0.0655], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,869][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8004, 0.0016, 0.0060, 0.0040, 0.0181, 0.0036, 0.0250, 0.0157, 0.0732,
        0.0224, 0.0022, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,870][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [orum] are: tensor([6.6813e-01, 7.0122e-04, 8.7161e-03, 1.7256e-02, 3.1546e-03, 1.3247e-04,
        3.6900e-02, 1.4483e-03, 4.2828e-02, 1.4502e-01, 7.1904e-04, 7.4995e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,871][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [orum] are: tensor([9.5691e-01, 1.4627e-03, 2.0498e-03, 8.6632e-03, 3.8509e-03, 5.0725e-05,
        1.1944e-03, 1.2029e-03, 1.0670e-03, 2.1200e-02, 1.0639e-03, 1.2816e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,872][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0267, 0.0303, 0.4025, 0.0129, 0.0060, 0.0100, 0.2304, 0.0086, 0.0820,
        0.0772, 0.0417, 0.0717], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,873][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [orum] are: tensor([8.8386e-01, 7.0110e-04, 1.5516e-03, 5.3010e-03, 2.5223e-02, 2.8965e-03,
        7.3583e-03, 1.6187e-02, 1.5785e-02, 2.5059e-02, 7.2540e-03, 8.8218e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,874][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.8018, 0.0039, 0.0070, 0.0375, 0.0064, 0.0028, 0.0439, 0.0282, 0.0374,
        0.0183, 0.0014, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:54,875][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.4511, 0.0047, 0.0684, 0.0037, 0.0638, 0.0361, 0.0308, 0.1313, 0.0364,
        0.0143, 0.1390, 0.0178, 0.0025], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,877][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.5574, 0.0171, 0.0186, 0.0048, 0.0728, 0.0526, 0.0296, 0.0635, 0.0195,
        0.0066, 0.1346, 0.0166, 0.0062], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,878][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7775, 0.0061, 0.0214, 0.0125, 0.0348, 0.0274, 0.0335, 0.0072, 0.0324,
        0.0174, 0.0042, 0.0161, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,879][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.6130, 0.0688, 0.1681, 0.0157, 0.0177, 0.0090, 0.0062, 0.0074, 0.0080,
        0.0053, 0.0349, 0.0086, 0.0372], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,880][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.2997, 0.0164, 0.0246, 0.0072, 0.0059, 0.0192, 0.0946, 0.0296, 0.2197,
        0.0189, 0.0937, 0.1190, 0.0513], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,881][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ is] are: tensor([9.1113e-01, 2.1651e-02, 2.1516e-02, 4.9798e-03, 1.6985e-03, 6.5782e-04,
        3.9961e-03, 2.7133e-04, 4.8589e-03, 1.4522e-02, 5.2987e-04, 2.5945e-03,
        1.1596e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,882][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.5837, 0.0029, 0.0241, 0.0029, 0.0986, 0.0579, 0.0514, 0.0396, 0.0714,
        0.0100, 0.0267, 0.0220, 0.0087], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,883][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.3722, 0.0148, 0.1145, 0.0570, 0.1561, 0.0008, 0.0636, 0.0054, 0.0464,
        0.0093, 0.0212, 0.0416, 0.0972], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,885][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.8393, 0.0075, 0.0047, 0.0135, 0.0069, 0.0009, 0.0033, 0.0058, 0.0030,
        0.0274, 0.0211, 0.0043, 0.0623], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,886][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.7974, 0.0135, 0.0308, 0.0035, 0.0093, 0.0045, 0.0248, 0.0044, 0.0101,
        0.0266, 0.0054, 0.0092, 0.0602], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,887][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.4736, 0.0061, 0.0091, 0.0089, 0.2058, 0.0530, 0.0257, 0.0678, 0.0200,
        0.0138, 0.1022, 0.0092, 0.0049], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,888][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.3393, 0.0034, 0.2528, 0.0088, 0.0083, 0.0103, 0.1215, 0.0404, 0.0931,
        0.0077, 0.0182, 0.0383, 0.0580], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:54,889][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.7034, 0.0060, 0.0537, 0.0049, 0.0248, 0.0123, 0.0345, 0.0329, 0.0455,
        0.0095, 0.0355, 0.0287, 0.0045, 0.0039], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,890][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.7529, 0.0401, 0.0570, 0.0035, 0.0320, 0.0292, 0.0218, 0.0093, 0.0127,
        0.0016, 0.0198, 0.0095, 0.0048, 0.0057], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,892][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.8465, 0.0109, 0.0465, 0.0060, 0.0156, 0.0090, 0.0130, 0.0026, 0.0125,
        0.0146, 0.0022, 0.0072, 0.0067, 0.0068], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,893][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5900, 0.0859, 0.2136, 0.0245, 0.0082, 0.0032, 0.0084, 0.0036, 0.0089,
        0.0050, 0.0088, 0.0071, 0.0158, 0.0172], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,894][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.5678, 0.0361, 0.0550, 0.0126, 0.0093, 0.0083, 0.0509, 0.0111, 0.0865,
        0.0087, 0.0180, 0.0447, 0.0555, 0.0356], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,895][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.7978e-01, 1.9962e-03, 3.1230e-03, 1.4215e-03, 7.1820e-04, 6.2306e-04,
        1.2534e-03, 2.6716e-04, 7.7335e-04, 2.3625e-03, 2.9477e-04, 4.6193e-04,
        4.1540e-03, 2.7698e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,896][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.7328, 0.0112, 0.0289, 0.0061, 0.0272, 0.0229, 0.0342, 0.0113, 0.0690,
        0.0073, 0.0073, 0.0240, 0.0097, 0.0082], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,897][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.3520, 0.0266, 0.2427, 0.0370, 0.0536, 0.0007, 0.0636, 0.0031, 0.0604,
        0.0047, 0.0060, 0.0493, 0.0460, 0.0544], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,899][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([8.7907e-01, 2.2515e-02, 2.1067e-02, 2.9559e-03, 1.3255e-02, 4.2403e-04,
        4.1293e-03, 1.8837e-03, 4.5516e-03, 1.7745e-02, 7.8886e-03, 4.8721e-03,
        8.8487e-03, 1.0800e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,900][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.8357, 0.0067, 0.0060, 0.0069, 0.0057, 0.0022, 0.0157, 0.0013, 0.0063,
        0.0124, 0.0035, 0.0077, 0.0610, 0.0290], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,901][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.7328, 0.0102, 0.0284, 0.0058, 0.1072, 0.0200, 0.0131, 0.0195, 0.0117,
        0.0060, 0.0312, 0.0057, 0.0025, 0.0057], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,902][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1795, 0.0027, 0.2125, 0.0231, 0.0035, 0.0008, 0.0821, 0.0042, 0.0978,
        0.0039, 0.0015, 0.0377, 0.2983, 0.0523], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:54,903][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.7239, 0.0053, 0.1387, 0.0038, 0.0034, 0.0018, 0.0140, 0.0295, 0.0223,
        0.0104, 0.0153, 0.0200, 0.0073, 0.0020, 0.0024], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,904][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.7654, 0.0163, 0.1335, 0.0129, 0.0104, 0.0012, 0.0036, 0.0053, 0.0046,
        0.0010, 0.0052, 0.0049, 0.0079, 0.0023, 0.0255], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,906][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.7092, 0.0167, 0.0679, 0.0152, 0.0183, 0.0122, 0.0158, 0.0052, 0.0145,
        0.0610, 0.0037, 0.0103, 0.0312, 0.0120, 0.0068], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,907][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.3450, 0.0702, 0.3706, 0.0332, 0.0097, 0.0010, 0.0139, 0.0037, 0.0229,
        0.0035, 0.0060, 0.0172, 0.0589, 0.0318, 0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,908][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.5370, 0.0354, 0.0446, 0.0182, 0.0061, 0.0030, 0.0177, 0.0014, 0.0245,
        0.0023, 0.0030, 0.0142, 0.1698, 0.1119, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,909][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.6782, 0.0337, 0.0351, 0.0101, 0.0030, 0.0013, 0.0053, 0.0017, 0.0028,
        0.0124, 0.0020, 0.0027, 0.0716, 0.0867, 0.0535], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,910][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.8388, 0.0074, 0.0767, 0.0024, 0.0093, 0.0011, 0.0079, 0.0025, 0.0191,
        0.0050, 0.0026, 0.0129, 0.0082, 0.0014, 0.0047], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,911][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ same] are: tensor([4.8070e-01, 1.0193e-02, 1.9618e-01, 6.4987e-02, 2.9501e-03, 6.7547e-05,
        1.1976e-02, 8.9822e-04, 1.7039e-02, 8.9219e-03, 5.5273e-04, 2.3908e-02,
        1.2951e-01, 3.7540e-02, 1.4580e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,913][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ same] are: tensor([8.9490e-01, 8.3978e-03, 1.2592e-02, 3.0755e-02, 7.3881e-03, 1.6930e-04,
        1.6651e-03, 1.9361e-03, 1.0771e-03, 7.8359e-03, 2.2423e-03, 1.0140e-03,
        1.3019e-02, 8.9740e-03, 8.0321e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,914][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.5895, 0.0104, 0.0168, 0.0059, 0.0087, 0.0061, 0.0182, 0.0037, 0.0079,
        0.0131, 0.0117, 0.0130, 0.0371, 0.1567, 0.1013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,915][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.8370, 0.0066, 0.0435, 0.0069, 0.0279, 0.0020, 0.0056, 0.0116, 0.0095,
        0.0125, 0.0126, 0.0072, 0.0062, 0.0065, 0.0046], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,916][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ same] are: tensor([1.8491e-01, 1.1568e-03, 2.7512e-01, 8.6945e-03, 1.8026e-03, 1.5939e-04,
        1.0957e-02, 1.7096e-03, 1.3583e-02, 2.5887e-03, 4.3196e-04, 1.0869e-02,
        4.0074e-01, 6.4187e-02, 2.3098e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:54,917][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.4497, 0.0126, 0.1314, 0.0113, 0.0236, 0.0107, 0.0693, 0.0722, 0.0719,
        0.0190, 0.0597, 0.0417, 0.0092, 0.0074, 0.0038, 0.0065],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,919][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.5344, 0.0326, 0.1138, 0.0347, 0.0672, 0.0093, 0.0375, 0.0347, 0.0209,
        0.0053, 0.0430, 0.0163, 0.0102, 0.0093, 0.0097, 0.0212],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,920][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.5302, 0.0059, 0.0530, 0.0117, 0.0355, 0.0193, 0.0647, 0.0136, 0.0861,
        0.0503, 0.0161, 0.0361, 0.0250, 0.0342, 0.0035, 0.0149],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,921][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.3458, 0.0553, 0.3729, 0.0256, 0.0271, 0.0032, 0.0133, 0.0062, 0.0141,
        0.0039, 0.0058, 0.0076, 0.0314, 0.0431, 0.0080, 0.0365],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,922][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.4712, 0.0185, 0.0238, 0.0077, 0.0296, 0.0185, 0.0892, 0.0127, 0.0883,
        0.0209, 0.0184, 0.0461, 0.0528, 0.0600, 0.0232, 0.0192],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,923][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ as] are: tensor([9.2761e-01, 8.3298e-03, 5.5439e-03, 1.7060e-03, 1.4056e-03, 6.5632e-04,
        1.1240e-03, 2.8457e-04, 9.0232e-04, 4.4035e-03, 4.3372e-04, 4.4996e-04,
        3.9695e-03, 4.8280e-03, 1.0548e-02, 2.7805e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,925][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.5799, 0.0112, 0.0461, 0.0025, 0.0553, 0.0227, 0.0702, 0.0259, 0.1002,
        0.0148, 0.0123, 0.0371, 0.0088, 0.0054, 0.0030, 0.0045],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,926][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.3174, 0.0285, 0.2213, 0.0487, 0.1246, 0.0005, 0.0396, 0.0033, 0.0295,
        0.0068, 0.0056, 0.0203, 0.0674, 0.0605, 0.0093, 0.0167],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,927][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.6331, 0.0094, 0.0068, 0.0628, 0.0136, 0.0020, 0.0040, 0.0049, 0.0020,
        0.0071, 0.0068, 0.0026, 0.0333, 0.0864, 0.0099, 0.1152],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,928][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.6418, 0.0119, 0.0375, 0.0028, 0.0073, 0.0042, 0.0237, 0.0039, 0.0131,
        0.0167, 0.0039, 0.0137, 0.0469, 0.0109, 0.0555, 0.1062],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,929][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.5731, 0.0116, 0.0468, 0.0065, 0.1707, 0.0216, 0.0281, 0.0292, 0.0219,
        0.0063, 0.0541, 0.0115, 0.0037, 0.0092, 0.0021, 0.0034],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,931][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.1399, 0.0077, 0.3933, 0.0057, 0.0132, 0.0077, 0.1349, 0.0147, 0.1183,
        0.0079, 0.0042, 0.0590, 0.0697, 0.0067, 0.0127, 0.0043],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:54,932][circuit_into_ebeddingspace.py][line:2310][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.6733, 0.0197, 0.1070, 0.0101, 0.0136, 0.0093, 0.0297, 0.0259, 0.0361,
        0.0076, 0.0181, 0.0192, 0.0094, 0.0055, 0.0068, 0.0043, 0.0046],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,933][circuit_into_ebeddingspace.py][line:2313][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([3.9393e-01, 1.5966e-01, 3.0813e-01, 9.7046e-03, 1.8429e-02, 8.9540e-03,
        8.2696e-03, 4.0395e-03, 3.6814e-03, 3.8183e-04, 5.2242e-03, 2.2400e-03,
        7.9499e-03, 1.3138e-02, 3.9588e-02, 6.0601e-03, 1.0622e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,934][circuit_into_ebeddingspace.py][line:2316][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.6886, 0.0261, 0.1470, 0.0121, 0.0235, 0.0056, 0.0123, 0.0021, 0.0126,
        0.0077, 0.0019, 0.0053, 0.0113, 0.0146, 0.0062, 0.0118, 0.0113],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,936][circuit_into_ebeddingspace.py][line:2319][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1461, 0.2107, 0.4854, 0.0419, 0.0041, 0.0019, 0.0034, 0.0018, 0.0028,
        0.0011, 0.0016, 0.0017, 0.0127, 0.0174, 0.0084, 0.0238, 0.0353],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,937][circuit_into_ebeddingspace.py][line:2322][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2764, 0.0911, 0.0703, 0.0154, 0.0049, 0.0067, 0.0569, 0.0054, 0.0595,
        0.0100, 0.0063, 0.0322, 0.0967, 0.0745, 0.0624, 0.0559, 0.0753],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,938][circuit_into_ebeddingspace.py][line:2325][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.7139e-01, 2.0022e-03, 2.2848e-03, 1.0935e-03, 1.0031e-03, 5.5496e-04,
        4.9111e-04, 1.7793e-04, 3.6071e-04, 1.0268e-03, 2.3105e-04, 1.7438e-04,
        1.5270e-03, 1.4303e-03, 2.3572e-03, 9.5582e-03, 4.3397e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,939][circuit_into_ebeddingspace.py][line:2328][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6249, 0.0534, 0.0954, 0.0131, 0.0195, 0.0095, 0.0226, 0.0034, 0.0505,
        0.0044, 0.0023, 0.0190, 0.0259, 0.0148, 0.0134, 0.0095, 0.0184],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,940][circuit_into_ebeddingspace.py][line:2331][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([6.8776e-02, 6.3141e-02, 6.8485e-01, 2.8029e-02, 2.5376e-02, 8.0641e-05,
        1.2948e-02, 5.4750e-04, 8.4818e-03, 1.0376e-03, 1.1956e-03, 5.8106e-03,
        2.7102e-02, 2.8130e-02, 1.3839e-02, 6.5056e-03, 2.4148e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,942][circuit_into_ebeddingspace.py][line:2334][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.6951, 0.1144, 0.0454, 0.0109, 0.0234, 0.0008, 0.0040, 0.0024, 0.0032,
        0.0076, 0.0055, 0.0031, 0.0099, 0.0135, 0.0282, 0.0222, 0.0103],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,943][circuit_into_ebeddingspace.py][line:2337][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([7.8694e-01, 3.9338e-03, 3.3439e-03, 8.0891e-03, 2.9627e-03, 1.0479e-03,
        3.6412e-03, 6.8890e-04, 1.2884e-03, 4.5976e-03, 1.3890e-03, 1.6826e-03,
        1.7637e-02, 1.4201e-02, 1.6446e-02, 1.1247e-01, 1.9649e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,944][circuit_into_ebeddingspace.py][line:2340][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.6880, 0.0243, 0.0944, 0.0075, 0.0986, 0.0101, 0.0080, 0.0118, 0.0062,
        0.0029, 0.0207, 0.0028, 0.0029, 0.0056, 0.0071, 0.0031, 0.0060],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,945][circuit_into_ebeddingspace.py][line:2343][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([3.9058e-02, 4.8932e-03, 6.9377e-01, 1.5002e-02, 5.5641e-04, 1.5534e-04,
        1.1680e-02, 5.1504e-04, 1.1239e-02, 6.6349e-04, 1.7101e-04, 5.6952e-03,
        1.0919e-01, 1.3439e-02, 2.6483e-02, 5.5173e-02, 1.2314e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:54,953][circuit_into_ebeddingspace.py][line:2286][INFO] ################11-th layer#################
[2024-06-27 21:00:54,956][circuit_into_ebeddingspace.py][line:2288][INFO] ##11-th layer ##Inspire##: The head1 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,958][circuit_into_ebeddingspace.py][line:2289][INFO] ##11-th layer ##Inspire##: The head2 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,959][circuit_into_ebeddingspace.py][line:2290][INFO] ##11-th layer ##Inspire##: The head3 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,960][circuit_into_ebeddingspace.py][line:2291][INFO] ##11-th layer ##Inspire##: The head4 Inspire status of source tokens is 
 tensor([[[1, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,962][circuit_into_ebeddingspace.py][line:2292][INFO] ##11-th layer ##Inspire##: The head5 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,963][circuit_into_ebeddingspace.py][line:2293][INFO] ##11-th layer ##Inspire##: The head6 Inspire status of source tokens is 
 tensor([[[0, 0],
         [1, 1],
         [0, 1],
         [0, 1],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 0],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1]]], device='cuda:0')
[2024-06-27 21:00:54,964][circuit_into_ebeddingspace.py][line:2294][INFO] ##11-th layer ##Inspire##: The head7 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,966][circuit_into_ebeddingspace.py][line:2295][INFO] ##11-th layer ##Inspire##: The head8 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,967][circuit_into_ebeddingspace.py][line:2296][INFO] ##11-th layer ##Inspire##: The head9 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 1],
         [0, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,969][circuit_into_ebeddingspace.py][line:2297][INFO] ##11-th layer ##Inspire##: The head10 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [1, 0]]], device='cuda:0')
[2024-06-27 21:00:54,970][circuit_into_ebeddingspace.py][line:2298][INFO] ##11-th layer ##Inspire##: The head11 Inspire status of source tokens is 
 tensor([[[0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,971][circuit_into_ebeddingspace.py][line:2299][INFO] ##11-th layer ##Inspire##: The head12 Inspire status of source tokens is 
 tensor([[[1, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 1],
         [0, 1],
         [0, 0],
         [0, 1],
         [0, 1],
         [0, 0],
         [0, 1],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0],
         [0, 0]]], device='cuda:0')
[2024-06-27 21:00:54,972][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,973][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,974][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,975][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,976][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,977][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,978][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,979][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,980][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,981][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,981][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,982][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-06-27 21:00:54,983][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.4761, 0.5239], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,984][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9307, 0.0693], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,985][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9661, 0.0339], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,986][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.8493, 0.1507], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,987][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9241, 0.0759], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,988][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9257, 0.0743], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,989][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9728, 0.0272], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,990][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9492, 0.0508], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,991][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ original] are: tensor([1.4127e-04, 9.9986e-01], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,992][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.9683, 0.0317], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,993][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.7910, 0.2090], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,994][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9497, 0.0503], device='cuda:0') for source tokens [The original]
[2024-06-27 21:00:54,995][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.4935, 0.2963, 0.2102], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,996][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9213, 0.0452, 0.0335], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,997][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9523, 0.0192, 0.0285], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,998][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8198, 0.1154, 0.0648], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,998][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.9071, 0.0350, 0.0579], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:54,999][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.9395, 0.0392, 0.0213], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,000][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9578, 0.0146, 0.0276], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,001][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.9250, 0.0264, 0.0486], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,002][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ language] are: tensor([2.1289e-04, 7.5641e-01, 2.4338e-01], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,003][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9797, 0.0108, 0.0095], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,004][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6138, 0.2491, 0.1372], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,005][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.7420, 0.1754, 0.0827], device='cuda:0') for source tokens [The original language]
[2024-06-27 21:00:55,006][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.2783, 0.2582, 0.2221, 0.2414], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,007][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8962, 0.0389, 0.0138, 0.0512], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,008][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.9108, 0.0134, 0.0228, 0.0530], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,009][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.5840, 0.1693, 0.1342, 0.1124], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,010][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.8451, 0.0374, 0.0534, 0.0641], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,011][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9145, 0.0513, 0.0179, 0.0163], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,012][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.9620, 0.0131, 0.0121, 0.0128], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,013][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.8760, 0.0317, 0.0289, 0.0634], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,014][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ of] are: tensor([2.5986e-04, 4.2684e-01, 1.8815e-01, 3.8475e-01], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,015][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.9716, 0.0111, 0.0103, 0.0070], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,016][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.6347, 0.1003, 0.1012, 0.1637], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,017][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.7872, 0.0561, 0.0804, 0.0763], device='cuda:0') for source tokens [The original language of]
[2024-06-27 21:00:55,018][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.8507, 0.0290, 0.0297, 0.0233, 0.0673], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,019][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.9748, 0.0057, 0.0025, 0.0090, 0.0081], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,020][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.9813, 0.0033, 0.0059, 0.0077, 0.0017], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,021][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.8368, 0.0319, 0.0289, 0.0712, 0.0313], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,022][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.8382, 0.0201, 0.0316, 0.0969, 0.0132], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,023][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.9792, 0.0068, 0.0055, 0.0026, 0.0060], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,024][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.9749, 0.0046, 0.0029, 0.0017, 0.0160], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,025][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.9777, 0.0046, 0.0074, 0.0051, 0.0053], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,026][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ De] are: tensor([2.8488e-04, 3.2856e-01, 7.9596e-02, 2.3350e-01, 3.5806e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,027][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.9853, 0.0043, 0.0020, 0.0027, 0.0056], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,028][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.9620, 0.0048, 0.0031, 0.0182, 0.0119], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,029][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.9467, 0.0069, 0.0190, 0.0226, 0.0048], device='cuda:0') for source tokens [The original language of De]
[2024-06-27 21:00:55,030][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.8006, 0.0385, 0.0426, 0.0216, 0.0549, 0.0418], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,031][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([9.8379e-01, 9.2566e-04, 2.4373e-03, 7.4816e-03, 3.7504e-03, 1.6125e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,032][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([9.8422e-01, 1.1494e-03, 3.6632e-03, 4.3998e-03, 8.4282e-04, 5.7282e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,033][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.8757, 0.0161, 0.0119, 0.0225, 0.0667, 0.0071], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,034][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.9024, 0.0103, 0.0246, 0.0398, 0.0100, 0.0128], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,035][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.9426, 0.0062, 0.0048, 0.0042, 0.0286, 0.0136], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,036][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.9788, 0.0016, 0.0051, 0.0029, 0.0038, 0.0078], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,037][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.9869, 0.0012, 0.0043, 0.0033, 0.0026, 0.0018], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,038][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([4.6015e-04, 1.4606e-01, 9.1046e-02, 9.4024e-02, 5.3605e-01, 1.3235e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,039][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.9885, 0.0017, 0.0017, 0.0021, 0.0018, 0.0042], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,040][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.9602, 0.0053, 0.0018, 0.0181, 0.0112, 0.0034], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,041][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.9576, 0.0060, 0.0050, 0.0200, 0.0090, 0.0024], device='cuda:0') for source tokens [The original language of De fin]
[2024-06-27 21:00:55,042][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.1147, 0.1738, 0.1505, 0.1655, 0.1025, 0.1115, 0.1816],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,043][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.8194, 0.0162, 0.0231, 0.0241, 0.0416, 0.0233, 0.0523],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,044][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.8806, 0.0054, 0.0197, 0.0232, 0.0218, 0.0275, 0.0218],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,045][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.4818, 0.0549, 0.1529, 0.1025, 0.1034, 0.0206, 0.0840],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,046][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.7509, 0.0150, 0.0282, 0.0287, 0.0245, 0.0101, 0.1426],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,047][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.7316, 0.0197, 0.0220, 0.0244, 0.0762, 0.0219, 0.1042],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,048][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.8688, 0.0047, 0.0143, 0.0085, 0.0280, 0.0255, 0.0502],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,049][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.8582, 0.0096, 0.0416, 0.0310, 0.0245, 0.0073, 0.0279],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,050][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([6.7898e-05, 1.0227e-01, 1.3763e-01, 7.9927e-02, 5.0257e-02, 8.2111e-02,
        5.4773e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,051][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.9044, 0.0059, 0.0070, 0.0075, 0.0093, 0.0283, 0.0375],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,053][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.5585, 0.0533, 0.0772, 0.1571, 0.0878, 0.0030, 0.0632],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,054][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.7619, 0.0231, 0.0550, 0.0507, 0.0142, 0.0137, 0.0814],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-06-27 21:00:55,055][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.8563, 0.0148, 0.0113, 0.0087, 0.0294, 0.0174, 0.0133, 0.0489],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,056][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.9818, 0.0024, 0.0031, 0.0044, 0.0015, 0.0012, 0.0028, 0.0028],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,057][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([9.8179e-01, 1.5180e-03, 3.0016e-03, 3.6641e-03, 4.8754e-04, 2.3513e-03,
        6.3318e-03, 8.5142e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,058][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.8489, 0.0103, 0.0088, 0.0307, 0.0134, 0.0188, 0.0605, 0.0086],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,059][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.8372, 0.0149, 0.0175, 0.0493, 0.0049, 0.0098, 0.0616, 0.0048],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,060][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.7559, 0.0064, 0.0046, 0.0020, 0.0179, 0.0084, 0.1969, 0.0079],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,061][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([9.8476e-01, 9.6988e-04, 1.7336e-03, 1.2888e-03, 2.3193e-03, 8.5104e-04,
        6.6630e-03, 1.4126e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,062][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.9737, 0.0013, 0.0091, 0.0053, 0.0020, 0.0014, 0.0038, 0.0033],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,063][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.0014, 0.1029, 0.0436, 0.0531, 0.2256, 0.1672, 0.1989, 0.2075],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,064][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([9.7493e-01, 3.2884e-03, 2.1272e-03, 7.5848e-04, 5.6926e-04, 8.2569e-04,
        1.6653e-02, 8.4834e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,065][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.9782, 0.0016, 0.0010, 0.0057, 0.0011, 0.0018, 0.0082, 0.0023],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,066][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.9614, 0.0027, 0.0045, 0.0076, 0.0021, 0.0019, 0.0179, 0.0019],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-06-27 21:00:55,067][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0512, 0.1696, 0.1489, 0.1429, 0.0428, 0.0566, 0.1516, 0.0633, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,068][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.6209, 0.0169, 0.0324, 0.0264, 0.0373, 0.0131, 0.0549, 0.1086, 0.0895],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,069][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8396, 0.0075, 0.0206, 0.0217, 0.0249, 0.0135, 0.0214, 0.0126, 0.0382],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,070][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.3323, 0.0769, 0.1454, 0.0894, 0.0506, 0.0165, 0.1200, 0.0343, 0.1347],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,071][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.5911, 0.0133, 0.0290, 0.0323, 0.0295, 0.0042, 0.1070, 0.0098, 0.1838],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,073][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.6368, 0.0232, 0.0214, 0.0500, 0.0419, 0.0158, 0.0897, 0.0404, 0.0807],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,074][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.8098, 0.0043, 0.0148, 0.0091, 0.0241, 0.0079, 0.0272, 0.0666, 0.0362],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,075][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.7826, 0.0120, 0.0531, 0.0322, 0.0258, 0.0045, 0.0239, 0.0189, 0.0470],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,076][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [orum] are: tensor([2.3894e-05, 2.8107e-02, 8.3465e-02, 2.0421e-02, 1.0206e-02, 2.6863e-02,
        5.2374e-01, 2.2176e-02, 2.8500e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,077][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.8598, 0.0065, 0.0093, 0.0117, 0.0052, 0.0110, 0.0299, 0.0363, 0.0304],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,078][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2480, 0.0900, 0.3881, 0.1030, 0.0206, 0.0013, 0.0657, 0.0024, 0.0809],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,079][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.5426, 0.0399, 0.0812, 0.0555, 0.0096, 0.0080, 0.1241, 0.0320, 0.1071],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-06-27 21:00:55,080][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.4108, 0.0647, 0.0858, 0.1110, 0.1145, 0.0574, 0.0547, 0.0269, 0.0251,
        0.0490], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,081][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.4488, 0.0466, 0.0586, 0.0217, 0.0558, 0.0453, 0.0767, 0.1262, 0.0635,
        0.0568], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,082][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.8173, 0.0085, 0.0381, 0.0167, 0.0108, 0.0184, 0.0289, 0.0079, 0.0289,
        0.0246], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,083][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.4170, 0.0292, 0.0780, 0.1665, 0.0586, 0.0152, 0.0957, 0.0195, 0.0874,
        0.0330], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,084][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.6382, 0.0477, 0.0245, 0.0599, 0.0421, 0.0145, 0.0651, 0.0074, 0.0519,
        0.0485], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,085][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.5511, 0.0295, 0.0309, 0.0047, 0.0612, 0.0475, 0.0968, 0.0561, 0.0345,
        0.0877], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,087][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.7954, 0.0114, 0.0149, 0.0087, 0.0848, 0.0179, 0.0149, 0.0222, 0.0107,
        0.0192], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,088][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.4974, 0.0240, 0.1247, 0.0319, 0.0240, 0.0225, 0.0559, 0.0657, 0.0509,
        0.1030], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,089][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ et] are: tensor([1.8843e-04, 3.4285e-02, 1.8297e-02, 5.6720e-02, 1.4634e-01, 5.0695e-02,
        2.1148e-01, 1.8737e-01, 5.6136e-02, 2.3850e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,090][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.7597, 0.0170, 0.0263, 0.0064, 0.0200, 0.0431, 0.0592, 0.0297, 0.0207,
        0.0177], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,091][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.5142, 0.0182, 0.0112, 0.1044, 0.0301, 0.0035, 0.0421, 0.0207, 0.0313,
        0.2243], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,092][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.6752, 0.0119, 0.0270, 0.0592, 0.0103, 0.0105, 0.0713, 0.0461, 0.0607,
        0.0278], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-06-27 21:00:55,093][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.6930, 0.0315, 0.0220, 0.0250, 0.0343, 0.0136, 0.0335, 0.0313, 0.0322,
        0.0347, 0.0489], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,094][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.9556, 0.0022, 0.0037, 0.0036, 0.0047, 0.0026, 0.0063, 0.0054, 0.0108,
        0.0035, 0.0016], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,095][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([9.5257e-01, 6.9607e-04, 2.4239e-03, 2.4311e-03, 6.7086e-04, 6.1684e-03,
        1.3420e-02, 1.0585e-03, 1.3274e-02, 5.3492e-03, 1.9356e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,096][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.8374, 0.0052, 0.0169, 0.0307, 0.0310, 0.0069, 0.0198, 0.0119, 0.0086,
        0.0223, 0.0092], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,098][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.8050, 0.0061, 0.0077, 0.0461, 0.0067, 0.0087, 0.0380, 0.0064, 0.0363,
        0.0353, 0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,099][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.8014, 0.0029, 0.0033, 0.0035, 0.0118, 0.0096, 0.0724, 0.0064, 0.0285,
        0.0578, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,100][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.9181, 0.0013, 0.0031, 0.0023, 0.0032, 0.0056, 0.0268, 0.0078, 0.0179,
        0.0091, 0.0047], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,101][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([9.7305e-01, 7.8396e-04, 4.6389e-03, 2.2249e-03, 2.2884e-03, 8.5462e-04,
        4.6050e-03, 2.2682e-03, 3.7134e-03, 3.1383e-03, 2.4373e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,102][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.0017, 0.0377, 0.0311, 0.0221, 0.1477, 0.0875, 0.2115, 0.0700, 0.0854,
        0.1115, 0.1937], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,103][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.9123, 0.0020, 0.0020, 0.0011, 0.0011, 0.0039, 0.0510, 0.0019, 0.0199,
        0.0022, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,104][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([9.5339e-01, 3.3123e-03, 8.2101e-04, 8.6311e-03, 1.3612e-03, 8.2330e-04,
        8.0989e-03, 5.4773e-04, 7.7829e-03, 1.4468e-02, 7.6213e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,105][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([9.3775e-01, 8.6604e-04, 2.7565e-03, 4.6345e-03, 2.3321e-03, 2.0436e-03,
        1.4925e-02, 6.0734e-03, 1.1853e-02, 1.3737e-02, 3.0305e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-06-27 21:00:55,106][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0417, 0.1035, 0.0841, 0.1049, 0.0280, 0.0512, 0.1080, 0.0423, 0.1253,
        0.0792, 0.0927, 0.1391], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,107][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.5780, 0.0098, 0.0240, 0.0214, 0.0224, 0.0145, 0.0576, 0.0810, 0.0841,
        0.0510, 0.0124, 0.0440], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,109][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.8306, 0.0045, 0.0120, 0.0149, 0.0160, 0.0123, 0.0159, 0.0069, 0.0214,
        0.0473, 0.0038, 0.0146], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,110][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.4166, 0.0468, 0.1130, 0.0657, 0.0561, 0.0156, 0.0727, 0.0296, 0.0694,
        0.0267, 0.0228, 0.0651], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,111][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4656, 0.0084, 0.0152, 0.0280, 0.0180, 0.0035, 0.0961, 0.0085, 0.1465,
        0.1354, 0.0045, 0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,112][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.5114, 0.0124, 0.0140, 0.0349, 0.0372, 0.0188, 0.0656, 0.0290, 0.0595,
        0.1679, 0.0044, 0.0450], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,113][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.6869, 0.0033, 0.0107, 0.0080, 0.0157, 0.0071, 0.0243, 0.0651, 0.0327,
        0.0971, 0.0251, 0.0241], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,114][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.7114, 0.0069, 0.0317, 0.0308, 0.0274, 0.0048, 0.0232, 0.0187, 0.0376,
        0.0647, 0.0127, 0.0300], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,115][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [orum] are: tensor([1.5278e-05, 1.1098e-02, 4.1942e-02, 8.1801e-03, 5.3654e-03, 1.8473e-02,
        4.0929e-01, 1.8766e-02, 2.2073e-01, 4.7684e-02, 4.9883e-02, 1.6858e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,116][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.8372, 0.0049, 0.0048, 0.0086, 0.0062, 0.0106, 0.0251, 0.0286, 0.0220,
        0.0308, 0.0070, 0.0140], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,118][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.3160, 0.0670, 0.2474, 0.0840, 0.0172, 0.0009, 0.0554, 0.0023, 0.0618,
        0.0722, 0.0023, 0.0735], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,119][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.5536, 0.0338, 0.0482, 0.0352, 0.0094, 0.0071, 0.1041, 0.0339, 0.0738,
        0.0476, 0.0058, 0.0476], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-06-27 21:00:55,120][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1703, 0.0705, 0.0727, 0.0837, 0.0489, 0.0941, 0.0719, 0.0443, 0.0649,
        0.0427, 0.0531, 0.0544, 0.1285], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,121][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.4031, 0.0403, 0.0513, 0.0343, 0.0940, 0.0244, 0.0532, 0.0795, 0.0657,
        0.0226, 0.0248, 0.0435, 0.0631], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,122][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7428, 0.0156, 0.0327, 0.0345, 0.0312, 0.0292, 0.0172, 0.0092, 0.0141,
        0.0379, 0.0167, 0.0082, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,123][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.5555, 0.0267, 0.0665, 0.0689, 0.0298, 0.0189, 0.0454, 0.0192, 0.0457,
        0.0235, 0.0331, 0.0360, 0.0308], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,125][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.5804, 0.0252, 0.0213, 0.0292, 0.0571, 0.0122, 0.0564, 0.0103, 0.0578,
        0.0630, 0.0165, 0.0293, 0.0413], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,126][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.5510, 0.0834, 0.0206, 0.0218, 0.0394, 0.0151, 0.0181, 0.0308, 0.0120,
        0.1079, 0.0120, 0.0093, 0.0785], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,127][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.4507, 0.0157, 0.0158, 0.0092, 0.1483, 0.0446, 0.0443, 0.0864, 0.0264,
        0.0460, 0.0864, 0.0163, 0.0098], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,128][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.3857, 0.0280, 0.1076, 0.0357, 0.0546, 0.0336, 0.0367, 0.0243, 0.0411,
        0.0787, 0.0781, 0.0363, 0.0597], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,129][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0006, 0.0536, 0.0801, 0.0922, 0.0378, 0.0108, 0.0966, 0.0134, 0.0706,
        0.0624, 0.0177, 0.0545, 0.4099], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,130][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.6841, 0.0146, 0.0101, 0.0073, 0.0251, 0.0723, 0.0250, 0.0378, 0.0116,
        0.0118, 0.0811, 0.0075, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,131][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.3144, 0.0477, 0.0660, 0.0994, 0.2139, 0.0079, 0.0390, 0.0082, 0.0174,
        0.0550, 0.0332, 0.0190, 0.0789], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,133][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.4681, 0.0340, 0.1330, 0.0651, 0.0131, 0.0112, 0.0475, 0.0330, 0.0519,
        0.0092, 0.0091, 0.0265, 0.0984], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-06-27 21:00:55,134][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2185, 0.0498, 0.0788, 0.0792, 0.0530, 0.0580, 0.0628, 0.0207, 0.0599,
        0.0306, 0.0255, 0.0430, 0.1026, 0.1176], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,135][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5782, 0.0559, 0.0346, 0.0348, 0.0296, 0.0184, 0.0256, 0.0286, 0.0259,
        0.0121, 0.0084, 0.0209, 0.0366, 0.0903], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,136][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7648, 0.0194, 0.0293, 0.0340, 0.0149, 0.0132, 0.0130, 0.0073, 0.0154,
        0.0242, 0.0073, 0.0085, 0.0127, 0.0361], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,137][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.6505, 0.0158, 0.0301, 0.0251, 0.0151, 0.0106, 0.0361, 0.0130, 0.0430,
        0.0207, 0.0168, 0.0340, 0.0375, 0.0517], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,138][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.6727, 0.0570, 0.0260, 0.0486, 0.0312, 0.0114, 0.0165, 0.0076, 0.0171,
        0.0359, 0.0059, 0.0089, 0.0267, 0.0345], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,140][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.6772, 0.1168, 0.0294, 0.0102, 0.0190, 0.0089, 0.0183, 0.0095, 0.0103,
        0.0436, 0.0040, 0.0083, 0.0283, 0.0161], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,141][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6871, 0.0247, 0.0279, 0.0081, 0.0740, 0.0274, 0.0226, 0.0382, 0.0154,
        0.0206, 0.0302, 0.0098, 0.0061, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,142][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.6365, 0.0523, 0.1362, 0.0205, 0.0152, 0.0089, 0.0117, 0.0053, 0.0115,
        0.0143, 0.0273, 0.0113, 0.0186, 0.0303], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,143][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([3.0290e-04, 4.9355e-02, 6.0652e-02, 7.0429e-02, 2.0084e-02, 5.2407e-03,
        3.4528e-02, 3.7899e-03, 3.2875e-02, 1.3053e-02, 4.8530e-03, 2.1484e-02,
        4.5358e-01, 2.2977e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,144][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.8195, 0.0231, 0.0206, 0.0066, 0.0109, 0.0243, 0.0179, 0.0137, 0.0082,
        0.0058, 0.0310, 0.0060, 0.0072, 0.0052], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,145][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4007, 0.0724, 0.1032, 0.1023, 0.0769, 0.0060, 0.0334, 0.0049, 0.0223,
        0.0190, 0.0054, 0.0172, 0.0407, 0.0957], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,147][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.6928, 0.0137, 0.0205, 0.0160, 0.0043, 0.0076, 0.0375, 0.0362, 0.0353,
        0.0126, 0.0092, 0.0190, 0.0469, 0.0483], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-06-27 21:00:55,148][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1166, 0.0681, 0.0943, 0.0835, 0.0548, 0.0507, 0.0779, 0.0215, 0.0748,
        0.0246, 0.0222, 0.0606, 0.1313, 0.0852, 0.0338], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,149][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.4717, 0.0303, 0.0532, 0.0242, 0.0300, 0.0248, 0.0417, 0.0283, 0.0647,
        0.0081, 0.0062, 0.0448, 0.0638, 0.0500, 0.0581], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,150][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.6927, 0.0120, 0.0475, 0.0137, 0.0077, 0.0124, 0.0258, 0.0151, 0.0505,
        0.0413, 0.0069, 0.0298, 0.0242, 0.0144, 0.0060], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,151][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.5814, 0.0071, 0.0144, 0.0088, 0.0055, 0.0090, 0.0421, 0.0083, 0.0515,
        0.0171, 0.0084, 0.0452, 0.0649, 0.1242, 0.0122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,152][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.6470, 0.0241, 0.0308, 0.0306, 0.0415, 0.0060, 0.0149, 0.0038, 0.0317,
        0.0273, 0.0033, 0.0209, 0.0626, 0.0443, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,154][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.6418, 0.0692, 0.0311, 0.0197, 0.0107, 0.0022, 0.0206, 0.0082, 0.0147,
        0.0618, 0.0015, 0.0118, 0.0498, 0.0169, 0.0399], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,155][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.6661, 0.0118, 0.0330, 0.0082, 0.0366, 0.0086, 0.0267, 0.0400, 0.0322,
        0.0711, 0.0125, 0.0220, 0.0135, 0.0092, 0.0085], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,156][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.5450, 0.0341, 0.2255, 0.0199, 0.0113, 0.0067, 0.0126, 0.0032, 0.0228,
        0.0121, 0.0083, 0.0222, 0.0307, 0.0262, 0.0194], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,157][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ same] are: tensor([6.6271e-05, 5.5957e-02, 2.1954e-01, 3.0844e-02, 9.7503e-03, 7.2689e-03,
        5.3121e-02, 3.4581e-03, 7.8918e-02, 5.5656e-03, 4.2502e-03, 6.3118e-02,
        2.1469e-01, 1.2378e-01, 1.2968e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,158][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.7529, 0.0208, 0.0352, 0.0062, 0.0102, 0.0157, 0.0262, 0.0329, 0.0184,
        0.0103, 0.0163, 0.0140, 0.0160, 0.0040, 0.0209], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,159][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.3391, 0.0265, 0.1732, 0.0384, 0.0027, 0.0009, 0.0179, 0.0006, 0.0150,
        0.0101, 0.0012, 0.0172, 0.1492, 0.1547, 0.0532], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,161][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.3077, 0.0191, 0.0994, 0.0257, 0.0071, 0.0050, 0.0303, 0.0113, 0.0331,
        0.0108, 0.0032, 0.0234, 0.1681, 0.2122, 0.0435], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-06-27 21:00:55,162][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.1380, 0.0584, 0.0612, 0.0785, 0.0617, 0.1205, 0.0648, 0.0283, 0.0569,
        0.0260, 0.0316, 0.0456, 0.0993, 0.0626, 0.0336, 0.0329],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,163][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.2683, 0.0575, 0.0356, 0.0200, 0.1124, 0.0242, 0.0650, 0.0547, 0.0671,
        0.0164, 0.0173, 0.0426, 0.0444, 0.1210, 0.0279, 0.0255],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,164][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.6817, 0.0279, 0.0367, 0.0421, 0.0257, 0.0140, 0.0183, 0.0045, 0.0227,
        0.0270, 0.0088, 0.0123, 0.0160, 0.0316, 0.0078, 0.0228],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,165][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.4765, 0.0261, 0.0462, 0.0345, 0.0384, 0.0110, 0.0368, 0.0236, 0.0552,
        0.0236, 0.0166, 0.0421, 0.0593, 0.0596, 0.0194, 0.0313],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,167][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.5803, 0.0331, 0.0258, 0.0315, 0.0704, 0.0077, 0.0348, 0.0052, 0.0452,
        0.0299, 0.0046, 0.0259, 0.0347, 0.0489, 0.0071, 0.0149],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,168][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.3447, 0.1017, 0.0291, 0.0178, 0.0461, 0.0139, 0.0606, 0.0150, 0.0302,
        0.0702, 0.0033, 0.0190, 0.1318, 0.0365, 0.0311, 0.0489],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,169][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.4203, 0.0171, 0.0237, 0.0096, 0.1232, 0.0440, 0.0610, 0.0993, 0.0419,
        0.0535, 0.0454, 0.0230, 0.0074, 0.0201, 0.0041, 0.0064],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,170][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.2925, 0.0471, 0.1919, 0.0429, 0.0392, 0.0202, 0.0184, 0.0063, 0.0274,
        0.0124, 0.0120, 0.0177, 0.0803, 0.1040, 0.0335, 0.0543],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,171][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ as] are: tensor([1.8688e-04, 4.6987e-02, 8.0693e-02, 5.4658e-02, 1.5144e-02, 8.5156e-03,
        3.4029e-02, 4.1781e-03, 2.8469e-02, 1.8597e-02, 6.1822e-03, 2.0476e-02,
        2.1798e-01, 1.1857e-01, 6.5467e-02, 2.7987e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,173][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.6015, 0.0427, 0.0298, 0.0072, 0.0415, 0.0914, 0.0309, 0.0413, 0.0201,
        0.0125, 0.0392, 0.0108, 0.0086, 0.0039, 0.0124, 0.0063],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,174][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.2352, 0.0817, 0.0767, 0.0744, 0.1403, 0.0061, 0.0314, 0.0042, 0.0194,
        0.0338, 0.0066, 0.0192, 0.0875, 0.1020, 0.0293, 0.0524],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,175][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.4480, 0.0383, 0.0606, 0.0461, 0.0134, 0.0201, 0.0393, 0.0500, 0.0383,
        0.0146, 0.0091, 0.0275, 0.0985, 0.0366, 0.0225, 0.0371],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-06-27 21:00:55,176][circuit_into_ebeddingspace.py][line:2310][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1535, 0.0542, 0.0619, 0.0913, 0.0371, 0.0676, 0.0471, 0.0126, 0.0429,
        0.0233, 0.0152, 0.0329, 0.1179, 0.0985, 0.0323, 0.0386, 0.0732],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,177][circuit_into_ebeddingspace.py][line:2313][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4070, 0.0865, 0.0381, 0.0293, 0.0279, 0.0181, 0.0236, 0.0208, 0.0260,
        0.0060, 0.0049, 0.0190, 0.0351, 0.0894, 0.0530, 0.0285, 0.0869],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,179][circuit_into_ebeddingspace.py][line:2316][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.7225, 0.0336, 0.0334, 0.0416, 0.0139, 0.0080, 0.0114, 0.0028, 0.0144,
        0.0143, 0.0038, 0.0066, 0.0102, 0.0300, 0.0096, 0.0180, 0.0260],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,180][circuit_into_ebeddingspace.py][line:2319][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5608, 0.0322, 0.0172, 0.0206, 0.0159, 0.0073, 0.0356, 0.0106, 0.0535,
        0.0215, 0.0159, 0.0406, 0.0316, 0.0718, 0.0205, 0.0301, 0.0141],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,181][circuit_into_ebeddingspace.py][line:2322][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.6117, 0.0748, 0.0310, 0.0514, 0.0428, 0.0078, 0.0087, 0.0034, 0.0104,
        0.0114, 0.0024, 0.0050, 0.0254, 0.0356, 0.0118, 0.0235, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,182][circuit_into_ebeddingspace.py][line:2325][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.4670, 0.2639, 0.0476, 0.0103, 0.0149, 0.0063, 0.0192, 0.0052, 0.0087,
        0.0242, 0.0017, 0.0060, 0.0392, 0.0196, 0.0369, 0.0151, 0.0142],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,184][circuit_into_ebeddingspace.py][line:2328][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6148, 0.0343, 0.0393, 0.0086, 0.1240, 0.0235, 0.0220, 0.0441, 0.0164,
        0.0155, 0.0154, 0.0085, 0.0047, 0.0078, 0.0071, 0.0060, 0.0080],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,185][circuit_into_ebeddingspace.py][line:2331][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.3875, 0.0859, 0.2601, 0.0286, 0.0131, 0.0085, 0.0092, 0.0041, 0.0117,
        0.0049, 0.0158, 0.0083, 0.0193, 0.0392, 0.0347, 0.0190, 0.0502],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,186][circuit_into_ebeddingspace.py][line:2334][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0007, 0.0355, 0.0247, 0.0477, 0.0105, 0.0026, 0.0127, 0.0021, 0.0140,
        0.0108, 0.0027, 0.0108, 0.2541, 0.2165, 0.0205, 0.1525, 0.1815],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,187][circuit_into_ebeddingspace.py][line:2337][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.7422, 0.0766, 0.0454, 0.0077, 0.0144, 0.0299, 0.0149, 0.0094, 0.0064,
        0.0035, 0.0106, 0.0041, 0.0063, 0.0036, 0.0172, 0.0045, 0.0031],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,188][circuit_into_ebeddingspace.py][line:2340][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1844, 0.1544, 0.1882, 0.1063, 0.0339, 0.0023, 0.0223, 0.0010, 0.0165,
        0.0109, 0.0020, 0.0119, 0.0396, 0.0947, 0.0406, 0.0398, 0.0511],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-06-27 21:00:55,190][circuit_into_ebeddingspace.py][line:2343][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5237, 0.0364, 0.0266, 0.0216, 0.0042, 0.0083, 0.0257, 0.0258, 0.0298,
        0.0098, 0.0079, 0.0183, 0.0670, 0.0837, 0.0264, 0.0583, 0.0267],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
