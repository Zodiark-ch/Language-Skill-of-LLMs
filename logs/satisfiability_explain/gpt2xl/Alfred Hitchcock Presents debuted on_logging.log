[2024-07-23 21:06:53,366][explain_satisfiability.py][line:85][INFO] ############ CASE TEXT isAlfred Hitchcock Presents debuted on
[2024-07-23 21:06:53,366][explain_satisfiability.py][line:86][INFO] ############ CASE Prediction is  the
[2024-07-23 21:06:53,366][explain_satisfiability.py][line:87][INFO] ############ Refined Forward Graph
[2024-07-23 21:06:53,366][explain_satisfiability.py][line:88][INFO] ****** Layer 1
[2024-07-23 21:06:53,366][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 0
[2024-07-23 21:06:53,366][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 1
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 2
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 3
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 4
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 5
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 6
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 7
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 8
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 9
[2024-07-23 21:06:53,367][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 10
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 11
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 12
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 13
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 14
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 15
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 16
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 17
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 18
[2024-07-23 21:06:53,368][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 19
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 20
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 21
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 22
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 23
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 24
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 25
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 26
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 27
[2024-07-23 21:06:53,369][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 28
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 0
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 1
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 2
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 3
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 4
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,370][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 5
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 6
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 7
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 8
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 9
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 10
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,371][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 11
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 12
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 13
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 14
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 15
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 16
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,372][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 17
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 18
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 19
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 20
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 21
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 22
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 23
[2024-07-23 21:06:53,373][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 24
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 25
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 26
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 27
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 28
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,374][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 0
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit25']
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 1
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 2
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 3
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 4
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,375][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 5
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 6
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 7
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 8
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,376][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 9
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 10
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 11
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 12
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 13
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,377][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 14
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 15
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 16
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 17
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 18
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 19
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,378][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 20
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 21
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 22
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 23
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 24
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 25
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 26
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,379][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 27
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 28
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 0
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit24']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 1
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 2
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 3
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,380][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 4
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 5
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 6
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 7
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 8
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 9
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,381][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 10
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 11
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 12
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 13
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 14
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 15
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,382][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 16
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 17
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 18
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 19
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 20
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 21
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,383][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 22
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 23
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 24
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 25
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 26
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,384][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 27
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 28
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 0
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit25']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 1
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 2
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,385][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 3
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 4
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 5
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 6
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 7
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,386][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 8
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 9
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 10
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 11
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 12
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,387][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 13
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 14
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 15
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 16
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 17
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,388][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 18
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 19
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 20
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 21
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 22
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,389][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 23
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 24
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 25
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 26
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 27
[2024-07-23 21:06:53,390][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 28
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 0
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit18', 'circuit20', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit19', 'circuit21', 'circuit22']
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 1
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 2
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,391][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 3
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 4
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 5
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 6
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,392][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 7
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 8
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 9
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 10
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,393][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 11
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 12
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 13
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 14
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,394][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 15
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 16
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 17
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 18
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 19
[2024-07-23 21:06:53,395][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 20
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 21
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 22
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 23
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,396][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 24
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 25
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 26
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 27
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,397][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 28
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 0
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit22']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit18', 'circuit22']
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 1
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 2
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,398][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 3
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 4
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 5
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,399][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 6
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 7
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 8
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 9
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,400][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 10
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 11
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 12
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 13
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,401][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 14
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 15
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 16
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,402][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 17
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 18
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 19
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 20
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,403][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 21
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 22
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 23
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 24
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,404][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 25
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 26
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 27
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,405][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 28
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 0
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit22']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit16', 'circuit22']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 1
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 2
[2024-07-23 21:06:53,406][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 3
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 4
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 5
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,407][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 6
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 7
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 8
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,408][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 9
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 10
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 11
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,409][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 12
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 13
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 14
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 15
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,410][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 16
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 17
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 18
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,411][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 19
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 20
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 21
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,412][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 22
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 23
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 24
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 25
[2024-07-23 21:06:53,413][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 26
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 27
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 28
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,414][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 0
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit24']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit24', 'circuit27']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit16']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit15', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 1
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 2
[2024-07-23 21:06:53,415][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 3
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 4
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 5
[2024-07-23 21:06:53,416][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 6
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 7
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,417][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 8
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 9
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 10
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,418][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 11
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 12
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 13
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,419][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 14
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 15
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 16
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,420][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 17
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 18
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 19
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,421][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 20
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 21
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 22
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,422][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 23
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 24
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 25
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,423][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 26
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 27
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 28
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,424][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 0
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit25']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit22']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit18', 'circuit20']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit24']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 1
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 2
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,425][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 3
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 4
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,426][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 5
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 6
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 7
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,427][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 8
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 9
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 10
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,428][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 11
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 12
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 13
[2024-07-23 21:06:53,429][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 14
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 15
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,430][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 16
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 17
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 18
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,431][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 19
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 20
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 21
[2024-07-23 21:06:53,432][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 22
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 23
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,433][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 24
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 25
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 26
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,434][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 27
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 28
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,435][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 0
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit26']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit27']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit13', 'circuit16', 'circuit25']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit17', 'circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 1
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 2
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,436][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 3
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 4
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,437][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 5
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 6
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 7
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,438][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 8
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 9
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit27']
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,439][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 10
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 11
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit19', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,440][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 12
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 13
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit22', 'circuit23']
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 14
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,441][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 15
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 16
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,442][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 17
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 18
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 19
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,443][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 20
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 21
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,444][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 22
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 23
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 24
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,445][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 25
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 26
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,446][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 27
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 28
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:53,447][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:55,059][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:55,060][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,061][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,062][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,063][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,063][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,067][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,069][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,069][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,070][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,071][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,072][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,075][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,077][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.9113, 0.0887], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,077][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [lf] are: tensor([3.1313e-04, 9.9969e-01], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,082][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.8804, 0.1196], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,088][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.0212, 0.9788], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,095][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.1551, 0.8449], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,101][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.0084, 0.9916], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,102][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.9023, 0.0977], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,102][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.9833, 0.0167], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,103][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.9318, 0.0682], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,104][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.9121, 0.0879], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,106][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.7146, 0.2854], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,111][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.7226, 0.2774], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,117][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.7172, 0.2023, 0.0805], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,121][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [red] are: tensor([4.2069e-04, 1.0173e-03, 9.9856e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,129][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.7908, 0.1342, 0.0751], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,132][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.0133, 0.0150, 0.9717], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,132][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.0552, 0.4190, 0.5258], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,133][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [red] are: tensor([8.8361e-03, 2.2032e-04, 9.9094e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,134][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.4558, 0.4181, 0.1261], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,134][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.3164, 0.6788, 0.0049], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,137][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.7161, 0.1265, 0.1574], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,142][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.7045, 0.1765, 0.1190], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,148][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.5288, 0.0667, 0.4046], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,155][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.5140, 0.3542, 0.1318], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,162][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.4813, 0.2455, 0.1291, 0.1441], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,163][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([7.7793e-06, 6.4020e-05, 1.2170e-06, 9.9993e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,164][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.4969, 0.1655, 0.1630, 0.1746], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,164][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([2.0693e-04, 9.3391e-04, 4.1933e-05, 9.9882e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,165][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.0121, 0.0444, 0.0129, 0.9306], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,167][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([1.1858e-05, 2.7570e-06, 4.9137e-08, 9.9999e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,171][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.4994, 0.1550, 0.1332, 0.2124], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,177][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.3496, 0.4884, 0.1526, 0.0094], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,184][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.7071, 0.0677, 0.1706, 0.0546], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,191][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.7546, 0.0633, 0.1698, 0.0122], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,193][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.5326, 0.0668, 0.0519, 0.3487], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,194][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.3224, 0.2369, 0.1718, 0.2688], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,195][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.3302, 0.2394, 0.1140, 0.2564, 0.0600], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,195][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([1.9418e-04, 1.7327e-04, 8.4083e-06, 3.5312e-03, 9.9609e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,196][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.5154, 0.2439, 0.1088, 0.0437, 0.0882], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,198][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([4.7020e-04, 1.8942e-04, 3.2692e-05, 3.5819e-03, 9.9573e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,203][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.0350, 0.0095, 0.0128, 0.0310, 0.9118], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,206][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([2.7542e-02, 1.0125e-04, 5.0507e-05, 1.7976e-04, 9.7213e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,213][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.2949, 0.1114, 0.0629, 0.1922, 0.3387], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,221][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0612, 0.0487, 0.0473, 0.7188, 0.1239], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,223][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.5888, 0.0828, 0.2679, 0.0279, 0.0325], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,224][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.4540, 0.1250, 0.1018, 0.2213, 0.0979], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,225][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.4216, 0.0720, 0.0618, 0.1060, 0.3386], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,226][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1727, 0.1477, 0.0266, 0.5182, 0.1347], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,226][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.2295, 0.1982, 0.1036, 0.1827, 0.1655, 0.1206], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,228][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([1.0832e-04, 5.5278e-05, 1.0554e-04, 1.2576e-04, 2.0494e-03, 9.9756e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,232][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.3667, 0.1195, 0.0537, 0.0855, 0.2352, 0.1394], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,235][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([6.0725e-03, 1.6033e-04, 3.6198e-04, 7.1477e-04, 1.1748e-01, 8.7521e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,243][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.0549, 0.0024, 0.0056, 0.0096, 0.0563, 0.8711], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,247][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([4.4711e-04, 1.4850e-06, 2.8666e-05, 5.9997e-07, 3.3710e-05, 9.9949e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,254][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.2052, 0.0674, 0.0616, 0.2188, 0.2332, 0.2138], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,255][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.2325, 0.0201, 0.0176, 0.3413, 0.1763, 0.2122], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,256][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.4153, 0.1130, 0.0833, 0.1071, 0.1973, 0.0840], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,256][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.3994, 0.1047, 0.1047, 0.1429, 0.1812, 0.0672], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,257][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.3449, 0.0386, 0.0608, 0.0648, 0.0987, 0.3922], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,260][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.2629, 0.1073, 0.0339, 0.1371, 0.2060, 0.2527], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,264][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.1751, 0.1349, 0.0643, 0.2776, 0.1172, 0.2103, 0.0206],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,268][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ on] are: tensor([7.0031e-04, 6.2928e-05, 4.9117e-04, 2.1263e-04, 2.0602e-04, 2.5202e-04,
        9.9807e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,275][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.3620, 0.0475, 0.0749, 0.0343, 0.1030, 0.1731, 0.2052],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,282][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0103, 0.0012, 0.0030, 0.0093, 0.0151, 0.0275, 0.9336],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,285][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.0745, 0.0328, 0.0255, 0.0140, 0.2339, 0.4651, 0.1542],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,286][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ on] are: tensor([1.2158e-01, 1.9039e-03, 5.8988e-03, 7.0346e-03, 3.4530e-03, 3.9704e-04,
        8.5973e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,287][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0781, 0.0734, 0.0613, 0.1962, 0.2647, 0.3087, 0.0176],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,287][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0963, 0.0303, 0.0244, 0.0414, 0.1580, 0.3695, 0.2800],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,288][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.2259, 0.0236, 0.1257, 0.0473, 0.0418, 0.0829, 0.4528],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,292][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.3442, 0.0723, 0.1008, 0.0862, 0.1044, 0.1020, 0.1901],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,296][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.3423, 0.0445, 0.0732, 0.0593, 0.0549, 0.0852, 0.3405],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,303][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.2167, 0.0906, 0.0383, 0.1720, 0.1039, 0.1823, 0.1961],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,315][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:55,316][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,316][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,317][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,318][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,318][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,319][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,320][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,320][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,321][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,322][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,322][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,323][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,324][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.9113, 0.0887], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,324][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([3.1313e-04, 9.9969e-01], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,325][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.8804, 0.1196], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,326][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.0212, 0.9788], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,332][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.1551, 0.8449], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,337][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.0084, 0.9916], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,338][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.9023, 0.0977], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,339][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.9833, 0.0167], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,339][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.9318, 0.0682], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,340][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.9121, 0.0879], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,343][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.7146, 0.2854], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,348][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.7226, 0.2774], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,354][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.7172, 0.2023, 0.0805], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,358][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([4.2069e-04, 1.0173e-03, 9.9856e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,366][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.7908, 0.1342, 0.0751], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,368][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.0133, 0.0150, 0.9717], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,369][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.0552, 0.4190, 0.5258], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,370][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([8.8361e-03, 2.2032e-04, 9.9094e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,370][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.4558, 0.4181, 0.1261], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,371][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.3164, 0.6788, 0.0049], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,374][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.7161, 0.1265, 0.1574], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,378][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.7045, 0.1765, 0.1190], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,385][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.5288, 0.0667, 0.4046], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,392][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.5140, 0.3542, 0.1318], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,399][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.4813, 0.2455, 0.1291, 0.1441], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,400][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([7.7793e-06, 6.4020e-05, 1.2170e-06, 9.9993e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,400][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.4969, 0.1655, 0.1630, 0.1746], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,401][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([2.0693e-04, 9.3391e-04, 4.1933e-05, 9.9882e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,402][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.0121, 0.0444, 0.0129, 0.9306], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,404][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([1.1858e-05, 2.7570e-06, 4.9137e-08, 9.9999e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,409][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.4994, 0.1550, 0.1332, 0.2124], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,415][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.3496, 0.4884, 0.1526, 0.0094], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,422][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.7071, 0.0677, 0.1706, 0.0546], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,429][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.7546, 0.0633, 0.1698, 0.0122], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,430][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.5326, 0.0668, 0.0519, 0.3487], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,431][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.3224, 0.2369, 0.1718, 0.2688], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,431][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.3302, 0.2394, 0.1140, 0.2564, 0.0600], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,432][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([1.9418e-04, 1.7327e-04, 8.4083e-06, 3.5312e-03, 9.9609e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,433][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.5154, 0.2439, 0.1088, 0.0437, 0.0882], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,435][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([4.7020e-04, 1.8942e-04, 3.2692e-05, 3.5819e-03, 9.9573e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,439][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.0350, 0.0095, 0.0128, 0.0310, 0.9118], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,442][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([2.7542e-02, 1.0125e-04, 5.0507e-05, 1.7976e-04, 9.7213e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,449][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.2949, 0.1114, 0.0629, 0.1922, 0.3387], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,456][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0612, 0.0487, 0.0473, 0.7188, 0.1239], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,461][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.5888, 0.0828, 0.2679, 0.0279, 0.0325], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,461][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.4540, 0.1250, 0.1018, 0.2213, 0.0979], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,462][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.4216, 0.0720, 0.0618, 0.1060, 0.3386], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,463][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1727, 0.1477, 0.0266, 0.5182, 0.1347], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,464][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.2295, 0.1982, 0.1036, 0.1827, 0.1655, 0.1206], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,465][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([1.0832e-04, 5.5278e-05, 1.0554e-04, 1.2576e-04, 2.0494e-03, 9.9756e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,469][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.3667, 0.1195, 0.0537, 0.0855, 0.2352, 0.1394], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,472][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([6.0725e-03, 1.6033e-04, 3.6198e-04, 7.1477e-04, 1.1748e-01, 8.7521e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,480][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.0549, 0.0024, 0.0056, 0.0096, 0.0563, 0.8711], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,484][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([4.4711e-04, 1.4850e-06, 2.8666e-05, 5.9997e-07, 3.3710e-05, 9.9949e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,491][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.2052, 0.0674, 0.0616, 0.2188, 0.2332, 0.2138], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,492][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.2325, 0.0201, 0.0176, 0.3413, 0.1763, 0.2122], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,493][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.4153, 0.1130, 0.0833, 0.1071, 0.1973, 0.0840], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,494][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.3994, 0.1047, 0.1047, 0.1429, 0.1812, 0.0672], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,494][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.3449, 0.0386, 0.0608, 0.0648, 0.0987, 0.3922], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,497][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.2629, 0.1073, 0.0339, 0.1371, 0.2060, 0.2527], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,502][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.1751, 0.1349, 0.0643, 0.2776, 0.1172, 0.2103, 0.0206],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,505][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([7.0031e-04, 6.2928e-05, 4.9117e-04, 2.1263e-04, 2.0602e-04, 2.5202e-04,
        9.9807e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,512][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.3620, 0.0475, 0.0749, 0.0343, 0.1030, 0.1731, 0.2052],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,520][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0103, 0.0012, 0.0030, 0.0093, 0.0151, 0.0275, 0.9336],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,522][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.0745, 0.0328, 0.0255, 0.0140, 0.2339, 0.4651, 0.1542],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,523][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([1.2158e-01, 1.9039e-03, 5.8988e-03, 7.0346e-03, 3.4530e-03, 3.9704e-04,
        8.5973e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,524][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0781, 0.0734, 0.0613, 0.1962, 0.2647, 0.3087, 0.0176],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,525][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0963, 0.0303, 0.0244, 0.0414, 0.1580, 0.3695, 0.2800],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,527][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.2259, 0.0236, 0.1257, 0.0473, 0.0418, 0.0829, 0.4528],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,532][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.3442, 0.0723, 0.1008, 0.0862, 0.1044, 0.1020, 0.1901],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,538][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.3423, 0.0445, 0.0732, 0.0593, 0.0549, 0.0852, 0.3405],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,544][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.2167, 0.0906, 0.0383, 0.1720, 0.1039, 0.1823, 0.1961],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,548][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:55,549][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  17],
        [2046],
        [1114],
        [ 489],
        [ 274],
        [  67],
        [   1]], device='cuda:0')
[2024-07-23 21:06:55,552][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 5300],
        [22864],
        [17621],
        [31434],
        [39116],
        [39571],
        [ 5344]], device='cuda:0')
[2024-07-23 21:06:55,555][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 268],
        [ 295],
        [ 533],
        [ 853],
        [1147],
        [ 927],
        [ 673]], device='cuda:0')
[2024-07-23 21:06:55,556][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[23530],
        [36500],
        [ 9417],
        [  161],
        [11646],
        [ 5935],
        [  108]], device='cuda:0')
[2024-07-23 21:06:55,557][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[4613],
        [4637],
        [4579],
        [6466],
        [5097],
        [7771],
        [7470]], device='cuda:0')
[2024-07-23 21:06:55,558][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 7993],
        [12801],
        [23093],
        [ 3736],
        [  934],
        [ 2877],
        [ 1305]], device='cuda:0')
[2024-07-23 21:06:55,559][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[   7],
        [1760],
        [ 268],
        [5585],
        [ 144],
        [1612],
        [ 184]], device='cuda:0')
[2024-07-23 21:06:55,561][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[1664],
        [ 717],
        [1551],
        [1584],
        [4593],
        [2167],
        [  89]], device='cuda:0')
[2024-07-23 21:06:55,563][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[5181],
        [5559],
        [8345],
        [5009],
        [2366],
        [2617],
        [3132]], device='cuda:0')
[2024-07-23 21:06:55,566][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[1196],
        [1187],
        [2565],
        [1746],
        [ 387],
        [ 443],
        [1691]], device='cuda:0')
[2024-07-23 21:06:55,570][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 532],
        [ 642],
        [ 749],
        [ 845],
        [1031],
        [8980],
        [ 155]], device='cuda:0')
[2024-07-23 21:06:55,573][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[2063],
        [1601],
        [ 987],
        [1272],
        [  78],
        [ 212],
        [3217]], device='cuda:0')
[2024-07-23 21:06:55,576][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 281],
        [ 264],
        [ 115],
        [1144],
        [ 441],
        [  63],
        [   6]], device='cuda:0')
[2024-07-23 21:06:55,579][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[  68],
        [ 107],
        [ 181],
        [1325],
        [6483],
        [ 785],
        [ 322]], device='cuda:0')
[2024-07-23 21:06:55,582][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 1476],
        [21138],
        [22212],
        [ 5034],
        [ 3274],
        [11118],
        [   16]], device='cuda:0')
[2024-07-23 21:06:55,585][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[7492],
        [7971],
        [9045],
        [7933],
        [6124],
        [7807],
        [7042]], device='cuda:0')
[2024-07-23 21:06:55,588][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[  1],
        [ 76],
        [116],
        [371],
        [ 31],
        [185],
        [259]], device='cuda:0')
[2024-07-23 21:06:55,589][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1449],
        [1404],
        [1373],
        [1414],
        [1423],
        [ 995],
        [ 719]], device='cuda:0')
[2024-07-23 21:06:55,590][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[1823],
        [  15],
        [   6],
        [ 177],
        [  10],
        [   3],
        [ 421]], device='cuda:0')
[2024-07-23 21:06:55,591][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 785],
        [ 112],
        [ 150],
        [ 481],
        [ 500],
        [ 834],
        [2119]], device='cuda:0')
[2024-07-23 21:06:55,593][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[1190],
        [ 507],
        [2909],
        [1924],
        [4167],
        [ 615],
        [3921]], device='cuda:0')
[2024-07-23 21:06:55,596][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[564],
        [419],
        [ 81],
        [315],
        [449],
        [581],
        [533]], device='cuda:0')
[2024-07-23 21:06:55,599][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[14124],
        [14177],
        [10960],
        [11034],
        [ 4250],
        [ 8564],
        [ 8056]], device='cuda:0')
[2024-07-23 21:06:55,602][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 370],
        [ 455],
        [ 755],
        [ 750],
        [1128],
        [3580],
        [ 337]], device='cuda:0')
[2024-07-23 21:06:55,605][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 9042],
        [ 8857],
        [ 7808],
        [ 7935],
        [10013],
        [13090],
        [13345]], device='cuda:0')
[2024-07-23 21:06:55,609][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[15321],
        [14707],
        [14712],
        [16587],
        [14858],
        [12407],
        [20342]], device='cuda:0')
[2024-07-23 21:06:55,612][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[  26],
        [  86],
        [ 682],
        [ 782],
        [1731],
        [4847],
        [6321]], device='cuda:0')
[2024-07-23 21:06:55,615][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 782],
        [1257],
        [1028],
        [1024],
        [1190],
        [ 915],
        [ 509]], device='cuda:0')
[2024-07-23 21:06:55,618][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 1915],
        [  331],
        [  175],
        [ 1738],
        [ 2363],
        [  388],
        [32672]], device='cuda:0')
[2024-07-23 21:06:55,621][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:55,636][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:55,637][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,638][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,638][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,639][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,640][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,640][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,641][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,642][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,642][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,643][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,644][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,645][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,646][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.4999, 0.5001], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,646][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.4999, 0.5001], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,647][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.5002, 0.4998], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,652][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,658][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.4998, 0.5002], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,663][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,667][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,667][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,668][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.5005, 0.4995], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,669][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.4995, 0.5005], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,669][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.4996, 0.5004], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,672][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,676][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.3330, 0.3332, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,681][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.3330, 0.3332, 0.3338], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,686][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.3334, 0.3332, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,691][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.3332, 0.3333, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,696][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.3331, 0.3333, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,698][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.3333, 0.3331, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,698][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.3334, 0.3332, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,699][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.3333, 0.3333, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,700][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.3332, 0.3326, 0.3342], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,701][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.3329, 0.3335, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,704][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.3329, 0.3334, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,708][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.3333, 0.3332, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,713][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.2498, 0.2499, 0.2503, 0.2501], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,718][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.2498, 0.2499, 0.2504, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,723][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,728][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.2500, 0.2500, 0.2502, 0.2498], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,729][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.2498, 0.2500, 0.2503, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,730][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.2500, 0.2498, 0.2502, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,731][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,731][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.2500, 0.2500, 0.2501, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,734][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.2498, 0.2493, 0.2506, 0.2502], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,738][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.2496, 0.2500, 0.2502, 0.2502], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,744][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.2497, 0.2501, 0.2503, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,751][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,757][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.1999, 0.2000, 0.2003, 0.2001, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,759][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.1999, 0.2000, 0.2004, 0.2000, 0.1997], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,760][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.2001, 0.1999, 0.2001, 0.2001, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,761][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.2000, 0.2000, 0.2002, 0.1999, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,762][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.1999, 0.2000, 0.2003, 0.2000, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,762][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,766][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.2000], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,770][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.2000, 0.2000, 0.2001, 0.1999, 0.2000], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,777][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1998, 0.1994, 0.2004, 0.2001, 0.2004], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,785][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.1997, 0.2001, 0.2002, 0.2002, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,790][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1997, 0.2001, 0.2002, 0.1999, 0.2001], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,791][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:55,791][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.1665, 0.1666, 0.1668, 0.1667, 0.1665, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,792][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.1665, 0.1666, 0.1669, 0.1666, 0.1664, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,793][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1667, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,796][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.1666, 0.1666, 0.1668, 0.1665, 0.1665, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,800][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.1665, 0.1666, 0.1668, 0.1666, 0.1665, 0.1670], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,807][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.1667, 0.1666, 0.1668, 0.1667, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,813][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1666, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,821][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1666, 0.1666, 0.1667, 0.1665, 0.1667, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,822][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.1665, 0.1661, 0.1670, 0.1667, 0.1669, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,822][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1665, 0.1668, 0.1669, 0.1669, 0.1666, 0.1664], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,823][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1664, 0.1667, 0.1668, 0.1666, 0.1667, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,824][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1666, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:55,828][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.1427, 0.1428, 0.1430, 0.1429, 0.1427, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,833][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.1428, 0.1428, 0.1431, 0.1428, 0.1426, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,840][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,847][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.1428, 0.1429, 0.1430, 0.1428, 0.1428, 0.1431, 0.1427],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,854][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.1427, 0.1429, 0.1430, 0.1428, 0.1427, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,855][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.1429, 0.1428, 0.1430, 0.1429, 0.1428, 0.1429, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,856][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1428, 0.1428, 0.1429, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,856][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1430, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,857][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.1427, 0.1424, 0.1431, 0.1429, 0.1431, 0.1430, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,861][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.1427, 0.1430, 0.1431, 0.1430, 0.1428, 0.1426, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,865][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.1426, 0.1429, 0.1430, 0.1428, 0.1429, 0.1430, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,872][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:55,895][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:55,896][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,897][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,898][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,898][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,899][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,901][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,902][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,903][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,903][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,904][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,905][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,905][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:55,906][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.4999, 0.5001], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,907][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.4999, 0.5001], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,907][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.5002, 0.4998], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,908][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,909][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.4998, 0.5002], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,909][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,910][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,911][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.5000, 0.5000], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,914][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.5005, 0.4995], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,920][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.4995, 0.5005], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,928][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.4996, 0.5004], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,935][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.5001, 0.4999], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:55,937][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.3330, 0.3332, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,937][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.3330, 0.3332, 0.3338], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,938][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.3334, 0.3332, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,939][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.3332, 0.3333, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,939][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.3331, 0.3333, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,943][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.3333, 0.3331, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,947][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.3334, 0.3332, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,954][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.3333, 0.3333, 0.3334], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,962][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.3332, 0.3326, 0.3342], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,967][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.3329, 0.3335, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,968][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.3329, 0.3334, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,969][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.3333, 0.3332, 0.3335], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:55,969][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.2498, 0.2499, 0.2503, 0.2501], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,970][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.2498, 0.2499, 0.2504, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,973][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,977][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2500, 0.2502, 0.2498], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,984][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.2498, 0.2500, 0.2503, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,990][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2498, 0.2502, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,998][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,999][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2500, 0.2501, 0.2499], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:55,999][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.2498, 0.2493, 0.2506, 0.2502], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,000][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.2496, 0.2500, 0.2502, 0.2502], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,001][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.2497, 0.2501, 0.2503, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,003][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.2500, 0.2499, 0.2501, 0.2500], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,008][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.1999, 0.2000, 0.2003, 0.2001, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,015][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.1999, 0.2000, 0.2004, 0.2000, 0.1997], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,021][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.2001, 0.1999, 0.2001, 0.2001, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,029][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.2000, 0.2000, 0.2002, 0.1999, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,029][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.1999, 0.2000, 0.2003, 0.2000, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,030][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,031][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.2000], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,032][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.2000, 0.2000, 0.2001, 0.1999, 0.2000], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,034][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1998, 0.1994, 0.2004, 0.2001, 0.2004], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,039][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.1997, 0.2001, 0.2002, 0.2002, 0.1998], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,045][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1997, 0.2001, 0.2002, 0.1999, 0.2001], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,051][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.2000, 0.1999, 0.2001, 0.2000, 0.1999], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,058][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.1665, 0.1666, 0.1668, 0.1667, 0.1665, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,060][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.1665, 0.1666, 0.1669, 0.1666, 0.1664, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,061][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1667, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,061][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.1666, 0.1666, 0.1668, 0.1665, 0.1665, 0.1669], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,062][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.1665, 0.1666, 0.1668, 0.1666, 0.1665, 0.1670], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,063][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.1667, 0.1666, 0.1668, 0.1667, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,067][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1666, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,071][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1666, 0.1666, 0.1667, 0.1665, 0.1667, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,077][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.1665, 0.1661, 0.1670, 0.1667, 0.1669, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,085][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1665, 0.1668, 0.1669, 0.1669, 0.1666, 0.1664], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,091][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1664, 0.1667, 0.1668, 0.1666, 0.1667, 0.1668], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,091][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.1667, 0.1666, 0.1667, 0.1666, 0.1666, 0.1667], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,092][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.1427, 0.1428, 0.1430, 0.1429, 0.1427, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,093][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.1428, 0.1428, 0.1431, 0.1428, 0.1426, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,094][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,097][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.1428, 0.1429, 0.1430, 0.1428, 0.1428, 0.1431, 0.1427],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,102][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.1427, 0.1429, 0.1430, 0.1428, 0.1427, 0.1431, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,108][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.1429, 0.1428, 0.1430, 0.1429, 0.1428, 0.1429, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,116][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1428, 0.1428, 0.1429, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,121][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1430, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,122][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.1427, 0.1424, 0.1431, 0.1429, 0.1431, 0.1430, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,123][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.1427, 0.1430, 0.1431, 0.1430, 0.1428, 0.1426, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,124][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.1426, 0.1429, 0.1430, 0.1428, 0.1429, 0.1430, 0.1429],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,124][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429, 0.1428],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,128][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:56,131][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  357],
        [14132],
        [13491],
        [ 2767],
        [ 1783],
        [ 7166],
        [    6]], device='cuda:0')
[2024-07-23 21:06:56,132][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 1230],
        [20497],
        [20221],
        [ 4813],
        [ 3108],
        [10600],
        [   10]], device='cuda:0')
[2024-07-23 21:06:56,135][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[733],
        [733],
        [734],
        [733],
        [732],
        [732],
        [732]], device='cuda:0')
[2024-07-23 21:06:56,138][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[734],
        [732],
        [735],
        [734],
        [735],
        [732],
        [732]], device='cuda:0')
[2024-07-23 21:06:56,141][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[304],
        [303],
        [303],
        [302],
        [302],
        [303],
        [303]], device='cuda:0')
[2024-07-23 21:06:56,144][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[23],
        [23],
        [23],
        [23],
        [23],
        [23],
        [23]], device='cuda:0')
[2024-07-23 21:06:56,147][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[11035],
        [11028],
        [11041],
        [11053],
        [11039],
        [11046],
        [11048]], device='cuda:0')
[2024-07-23 21:06:56,151][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[79],
        [81],
        [80],
        [80],
        [80],
        [80],
        [80]], device='cuda:0')
[2024-07-23 21:06:56,154][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[241],
        [237],
        [239],
        [238],
        [239],
        [237],
        [237]], device='cuda:0')
[2024-07-23 21:06:56,157][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[2642],
        [2652],
        [2649],
        [2647],
        [2640],
        [2642],
        [2649]], device='cuda:0')
[2024-07-23 21:06:56,158][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[22],
        [22],
        [22],
        [22],
        [22],
        [22],
        [22]], device='cuda:0')
[2024-07-23 21:06:56,159][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[3],
        [3],
        [3],
        [3],
        [3],
        [3],
        [3]], device='cuda:0')
[2024-07-23 21:06:56,160][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[3],
        [3],
        [3],
        [3],
        [3],
        [3],
        [3]], device='cuda:0')
[2024-07-23 21:06:56,161][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[597],
        [596],
        [596],
        [596],
        [596],
        [596],
        [596]], device='cuda:0')
[2024-07-23 21:06:56,164][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,168][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[3580],
        [3574],
        [3572],
        [3575],
        [3571],
        [3575],
        [3577]], device='cuda:0')
[2024-07-23 21:06:56,171][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[848],
        [848],
        [848],
        [849],
        [849],
        [852],
        [851]], device='cuda:0')
[2024-07-23 21:06:56,174][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,177][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[48],
        [48],
        [48],
        [48],
        [48],
        [48],
        [48]], device='cuda:0')
[2024-07-23 21:06:56,180][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[102],
        [102],
        [102],
        [101],
        [101],
        [101],
        [102]], device='cuda:0')
[2024-07-23 21:06:56,184][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[1336],
        [1339],
        [1345],
        [1346],
        [1346],
        [1345],
        [1347]], device='cuda:0')
[2024-07-23 21:06:56,187][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[8],
        [8],
        [8],
        [8],
        [8],
        [8],
        [8]], device='cuda:0')
[2024-07-23 21:06:56,190][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[5],
        [5],
        [5],
        [5],
        [5],
        [5],
        [5]], device='cuda:0')
[2024-07-23 21:06:56,191][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[51],
        [51],
        [51],
        [51],
        [51],
        [51],
        [51]], device='cuda:0')
[2024-07-23 21:06:56,191][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,192][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,194][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[63],
        [63],
        [63],
        [63],
        [63],
        [63],
        [62]], device='cuda:0')
[2024-07-23 21:06:56,197][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[1330],
        [1333],
        [1333],
        [1334],
        [1334],
        [1334],
        [1333]], device='cuda:0')
[2024-07-23 21:06:56,201][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[285],
        [288],
        [292],
        [282],
        [291],
        [283],
        [291]], device='cuda:0')
[2024-07-23 21:06:56,204][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2],
        [2],
        [2],
        [2],
        [2],
        [2],
        [2]], device='cuda:0')
[2024-07-23 21:06:56,226][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:56,226][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,227][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,228][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,228][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,229][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,230][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,230][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,233][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,236][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,240][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,244][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,248][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,253][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.5610, 0.4390], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,257][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.8993, 0.1007], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,258][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.0033, 0.9967], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,259][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.1072, 0.8928], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,260][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.0224, 0.9776], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,260][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.0461, 0.9539], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,263][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.4322, 0.5678], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,267][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.2041, 0.7959], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,272][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.0402, 0.9598], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,277][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.0154, 0.9846], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,282][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.3518, 0.6482], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,288][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.0705, 0.9295], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,289][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.1912, 0.6461, 0.1627], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,290][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.8616, 0.0553, 0.0831], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,290][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [red] are: tensor([3.3385e-04, 7.7066e-01, 2.2901e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,292][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.0217, 0.5043, 0.4740], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,295][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [red] are: tensor([4.0965e-04, 9.7674e-01, 2.2847e-02], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,299][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.0056, 0.5750, 0.4194], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,304][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.3394, 0.2688, 0.3918], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,309][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.0343, 0.1019, 0.8638], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,314][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.0128, 0.4321, 0.5551], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,319][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.0014, 0.7623, 0.2363], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,319][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.1793, 0.3376, 0.4831], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,320][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.0398, 0.4259, 0.5343], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,321][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.1455, 0.2375, 0.2587, 0.3584], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,322][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.2518, 0.0773, 0.5153, 0.1556], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,322][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([9.3683e-07, 1.9301e-03, 2.9251e-03, 9.9514e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,325][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.0020, 0.1207, 0.1377, 0.7396], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,328][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([2.7194e-04, 9.4815e-02, 5.8246e-02, 8.4667e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,331][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([1.9907e-04, 1.0498e-02, 4.0679e-02, 9.4862e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,336][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.0969, 0.0952, 0.2228, 0.5850], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,341][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.0090, 0.0320, 0.2325, 0.7266], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,347][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.0014, 0.0358, 0.1608, 0.8020], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,350][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([1.1353e-04, 4.2820e-02, 8.0894e-02, 8.7617e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,351][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.0759, 0.1886, 0.2212, 0.5143], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,351][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.0326, 0.2993, 0.3358, 0.3323], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,352][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.0683, 0.0984, 0.1559, 0.4855, 0.1919], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,353][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.2021, 0.0329, 0.1925, 0.1003, 0.4722], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,355][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([7.1856e-08, 3.0505e-05, 9.3935e-05, 3.4677e-01, 6.5311e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,357][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([2.8840e-04, 6.2114e-03, 7.8998e-03, 3.4367e-01, 6.4193e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,360][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([6.3697e-05, 1.1506e-03, 2.9481e-03, 3.8231e-01, 6.1353e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,363][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([8.3280e-05, 1.3853e-03, 3.8085e-03, 3.8631e-01, 6.0841e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,367][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.0408, 0.0379, 0.0564, 0.2774, 0.5875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,372][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0012, 0.0029, 0.0234, 0.0849, 0.8875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,377][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.0010, 0.0067, 0.0132, 0.2502, 0.7288], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,381][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([8.5461e-06, 2.2814e-03, 3.3675e-03, 4.5783e-01, 5.3651e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,381][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.0429, 0.0730, 0.1196, 0.2632, 0.5013], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,382][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.0382, 0.2038, 0.2327, 0.2630, 0.2623], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,383][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.0228, 0.0297, 0.0628, 0.1557, 0.3768, 0.3522], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,384][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.2113, 0.0533, 0.3697, 0.0805, 0.1636, 0.1217], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,385][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([1.0988e-08, 9.8079e-06, 1.3354e-05, 5.4740e-02, 3.9422e-01, 5.5102e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,387][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([7.8573e-05, 1.6633e-03, 1.4157e-03, 7.3622e-02, 3.1179e-01, 6.1143e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,390][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([4.0547e-05, 5.7999e-04, 2.3350e-04, 9.7338e-02, 2.9849e-01, 6.0332e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,393][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([1.5389e-05, 4.8046e-04, 7.2574e-04, 1.1590e-01, 2.7308e-01, 6.0980e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,398][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.0295, 0.0234, 0.0391, 0.1527, 0.3916, 0.3637], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,403][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.0006, 0.0012, 0.0102, 0.0406, 0.4869, 0.4605], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,406][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([1.9414e-04, 1.3412e-03, 3.7341e-03, 4.5812e-02, 1.3919e-01, 8.0973e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,409][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([1.7973e-06, 3.6328e-04, 1.2038e-03, 7.3953e-02, 4.9412e-01, 4.3035e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,411][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.0211, 0.0361, 0.0589, 0.1587, 0.3297, 0.3954], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,412][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0255, 0.1225, 0.1505, 0.1760, 0.2010, 0.3245], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,413][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.0099, 0.0085, 0.0112, 0.0602, 0.1099, 0.3506, 0.4497],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,414][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.4176, 0.0636, 0.1743, 0.0580, 0.0541, 0.0470, 0.1855],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,415][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ on] are: tensor([7.3312e-09, 3.3724e-07, 9.4750e-07, 1.5724e-03, 3.2635e-02, 4.6439e-01,
        5.0140e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,416][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ on] are: tensor([5.6324e-05, 2.6398e-04, 6.4801e-04, 2.4146e-02, 1.7291e-01, 5.0019e-01,
        3.0179e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,418][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ on] are: tensor([4.4114e-06, 1.5275e-04, 1.6767e-04, 2.5211e-02, 1.9338e-01, 5.9285e-01,
        1.8824e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,421][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ on] are: tensor([9.3209e-06, 1.0450e-04, 2.8003e-04, 2.0262e-02, 9.3427e-02, 2.6246e-01,
        6.2346e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,426][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0287, 0.0150, 0.0179, 0.0813, 0.1510, 0.1874, 0.5187],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,429][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ on] are: tensor([2.1991e-04, 3.3912e-04, 2.7620e-03, 1.1265e-02, 1.3450e-01, 1.2659e-01,
        7.2432e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,432][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ on] are: tensor([8.4439e-05, 1.9595e-04, 6.6248e-04, 1.9283e-02, 5.0750e-02, 6.3849e-01,
        2.9053e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,435][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ on] are: tensor([1.1393e-06, 1.2624e-04, 2.5440e-04, 1.5516e-02, 8.0432e-02, 4.1398e-01,
        4.8969e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,440][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0171, 0.0283, 0.0387, 0.0871, 0.1714, 0.2371, 0.4202],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,442][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0208, 0.0863, 0.0993, 0.1047, 0.1342, 0.2423, 0.3124],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,483][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:56,498][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,501][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,506][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,506][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,506][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,507][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,507][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,507][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,508][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,508][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,508][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,509][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,509][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.5610, 0.4390], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,511][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.8993, 0.1007], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,516][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.0033, 0.9967], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,520][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.1072, 0.8928], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,525][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.0224, 0.9776], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,530][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.0461, 0.9539], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,535][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.4322, 0.5678], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,537][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.2041, 0.7959], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,537][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.0402, 0.9598], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,537][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.0154, 0.9846], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,538][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.3518, 0.6482], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,538][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.0705, 0.9295], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,538][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.1912, 0.6461, 0.1627], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,539][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.8616, 0.0553, 0.0831], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,539][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([3.3385e-04, 7.7066e-01, 2.2901e-01], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,539][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.0217, 0.5043, 0.4740], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,540][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([4.0965e-04, 9.7674e-01, 2.2847e-02], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,540][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.0056, 0.5750, 0.4194], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,542][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.3394, 0.2688, 0.3918], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,546][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.0343, 0.1019, 0.8638], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,551][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.0128, 0.4321, 0.5551], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,556][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.0014, 0.7623, 0.2363], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,560][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.1793, 0.3376, 0.4831], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,565][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.0398, 0.4259, 0.5343], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,568][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.1455, 0.2375, 0.2587, 0.3584], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,568][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.2518, 0.0773, 0.5153, 0.1556], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,569][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([9.3683e-07, 1.9301e-03, 2.9251e-03, 9.9514e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,569][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.0020, 0.1207, 0.1377, 0.7396], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,569][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([2.7194e-04, 9.4815e-02, 5.8246e-02, 8.4667e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,570][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([1.9907e-04, 1.0498e-02, 4.0679e-02, 9.4862e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,570][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.0969, 0.0952, 0.2228, 0.5850], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,570][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.0090, 0.0320, 0.2325, 0.7266], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,572][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.0014, 0.0358, 0.1608, 0.8020], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,574][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([1.1353e-04, 4.2820e-02, 8.0894e-02, 8.7617e-01], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,579][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.0759, 0.1886, 0.2212, 0.5143], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,586][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.0326, 0.2993, 0.3358, 0.3323], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,593][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.0683, 0.0984, 0.1559, 0.4855, 0.1919], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,597][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.2021, 0.0329, 0.1925, 0.1003, 0.4722], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,597][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([7.1856e-08, 3.0505e-05, 9.3935e-05, 3.4677e-01, 6.5311e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,598][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([2.8840e-04, 6.2114e-03, 7.8998e-03, 3.4367e-01, 6.4193e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,598][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([6.3697e-05, 1.1506e-03, 2.9481e-03, 3.8231e-01, 6.1353e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,598][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([8.3280e-05, 1.3853e-03, 3.8085e-03, 3.8631e-01, 6.0841e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,599][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.0408, 0.0379, 0.0564, 0.2774, 0.5875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,599][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0012, 0.0029, 0.0234, 0.0849, 0.8875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,599][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.0010, 0.0067, 0.0132, 0.2502, 0.7288], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,600][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([8.5461e-06, 2.2814e-03, 3.3675e-03, 4.5783e-01, 5.3651e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,600][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.0429, 0.0730, 0.1196, 0.2632, 0.5013], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,605][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.0382, 0.2038, 0.2327, 0.2630, 0.2623], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,612][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.0228, 0.0297, 0.0628, 0.1557, 0.3768, 0.3522], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,619][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.2113, 0.0533, 0.3697, 0.0805, 0.1636, 0.1217], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,623][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([1.0988e-08, 9.8079e-06, 1.3354e-05, 5.4740e-02, 3.9422e-01, 5.5102e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,626][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([7.8573e-05, 1.6633e-03, 1.4157e-03, 7.3622e-02, 3.1179e-01, 6.1143e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,626][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([4.0547e-05, 5.7999e-04, 2.3350e-04, 9.7338e-02, 2.9849e-01, 6.0332e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,627][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([1.5389e-05, 4.8046e-04, 7.2574e-04, 1.1590e-01, 2.7308e-01, 6.0980e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,627][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.0295, 0.0234, 0.0391, 0.1527, 0.3916, 0.3637], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,627][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.0006, 0.0012, 0.0102, 0.0406, 0.4869, 0.4605], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,628][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([1.9414e-04, 1.3412e-03, 3.7341e-03, 4.5812e-02, 1.3919e-01, 8.0973e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,628][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([1.7973e-06, 3.6328e-04, 1.2038e-03, 7.3953e-02, 4.9412e-01, 4.3035e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,628][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.0211, 0.0361, 0.0589, 0.1587, 0.3297, 0.3954], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,629][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0255, 0.1225, 0.1505, 0.1760, 0.2010, 0.3245], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,633][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.0099, 0.0085, 0.0112, 0.0602, 0.1099, 0.3506, 0.4497],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,639][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.4176, 0.0636, 0.1743, 0.0580, 0.0541, 0.0470, 0.1855],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,644][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([7.3312e-09, 3.3724e-07, 9.4750e-07, 1.5724e-03, 3.2635e-02, 4.6439e-01,
        5.0140e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,648][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([5.6324e-05, 2.6398e-04, 6.4801e-04, 2.4146e-02, 1.7291e-01, 5.0019e-01,
        3.0179e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,652][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([4.4114e-06, 1.5275e-04, 1.6767e-04, 2.5211e-02, 1.9338e-01, 5.9285e-01,
        1.8824e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,655][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([9.3209e-06, 1.0450e-04, 2.8003e-04, 2.0262e-02, 9.3427e-02, 2.6246e-01,
        6.2346e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,655][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0287, 0.0150, 0.0179, 0.0813, 0.1510, 0.1874, 0.5187],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,656][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([2.1991e-04, 3.3912e-04, 2.7620e-03, 1.1265e-02, 1.3450e-01, 1.2659e-01,
        7.2432e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,656][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([8.4439e-05, 1.9595e-04, 6.6248e-04, 1.9283e-02, 5.0750e-02, 6.3849e-01,
        2.9053e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,656][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([1.1393e-06, 1.2624e-04, 2.5440e-04, 1.5516e-02, 8.0432e-02, 4.1398e-01,
        4.8969e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,657][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0171, 0.0283, 0.0387, 0.0871, 0.1714, 0.2371, 0.4202],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,657][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0208, 0.0863, 0.0993, 0.1047, 0.1342, 0.2423, 0.3124],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,658][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:56,661][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  6],
        [185],
        [129],
        [ 45],
        [ 85],
        [112],
        [  1]], device='cuda:0')
[2024-07-23 21:06:56,664][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  79],
        [6542],
        [5889],
        [1148],
        [ 806],
        [3751],
        [   3]], device='cuda:0')
[2024-07-23 21:06:56,667][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[2116],
        [2674],
        [4256],
        [5632],
        [4363],
        [1242],
        [2643]], device='cuda:0')
[2024-07-23 21:06:56,670][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 333],
        [ 439],
        [ 524],
        [3443],
        [5747],
        [4089],
        [2806]], device='cuda:0')
[2024-07-23 21:06:56,673][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 2754],
        [  395],
        [  697],
        [ 4531],
        [12276],
        [10579],
        [ 1164]], device='cuda:0')
[2024-07-23 21:06:56,676][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[17155],
        [26149],
        [26600],
        [22506],
        [18202],
        [17381],
        [15542]], device='cuda:0')
[2024-07-23 21:06:56,680][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 1403],
        [ 1647],
        [ 1737],
        [ 5149],
        [10520],
        [10743],
        [ 9376]], device='cuda:0')
[2024-07-23 21:06:56,683][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 161],
        [1061],
        [1494],
        [ 310],
        [ 341],
        [ 651],
        [2167]], device='cuda:0')
[2024-07-23 21:06:56,686][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 179],
        [ 427],
        [ 373],
        [1705],
        [2128],
        [2895],
        [5224]], device='cuda:0')
[2024-07-23 21:06:56,688][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[1781],
        [2919],
        [4873],
        [1952],
        [7084],
        [6138],
        [4401]], device='cuda:0')
[2024-07-23 21:06:56,689][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[2536],
        [4817],
        [2377],
        [ 617],
        [ 697],
        [ 726],
        [ 950]], device='cuda:0')
[2024-07-23 21:06:56,689][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[196],
        [179],
        [154],
        [360],
        [548],
        [549],
        [386]], device='cuda:0')
[2024-07-23 21:06:56,690][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[275],
        [134],
        [ 60],
        [ 32],
        [ 15],
        [ 33],
        [ 27]], device='cuda:0')
[2024-07-23 21:06:56,691][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[2228],
        [6708],
        [5846],
        [5739],
        [5757],
        [5694],
        [5710]], device='cuda:0')
[2024-07-23 21:06:56,692][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[11],
        [ 3],
        [ 2],
        [ 4],
        [ 3],
        [ 1],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:56,693][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[14],
        [19],
        [26],
        [38],
        [27],
        [ 7],
        [ 6]], device='cuda:0')
[2024-07-23 21:06:56,696][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[  1],
        [  1],
        [  1],
        [  2],
        [304],
        [ 13],
        [  2]], device='cuda:0')
[2024-07-23 21:06:56,699][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 8],
        [ 2],
        [ 2],
        [ 4],
        [ 1],
        [ 1],
        [14]], device='cuda:0')
[2024-07-23 21:06:56,702][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 101],
        [1156],
        [1104],
        [ 974],
        [ 215],
        [  48],
        [  54]], device='cuda:0')
[2024-07-23 21:06:56,705][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[2],
        [1],
        [1],
        [1],
        [1],
        [4],
        [2]], device='cuda:0')
[2024-07-23 21:06:56,709][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[   2],
        [  52],
        [  63],
        [ 237],
        [1205],
        [ 162],
        [   4]], device='cuda:0')
[2024-07-23 21:06:56,712][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 7],
        [12],
        [ 9],
        [ 4],
        [ 6],
        [ 4],
        [68]], device='cuda:0')
[2024-07-23 21:06:56,715][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[2],
        [1],
        [1],
        [1],
        [4],
        [3],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,718][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 19],
        [152],
        [ 51],
        [  8],
        [ 23],
        [ 18],
        [ 31]], device='cuda:0')
[2024-07-23 21:06:56,721][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[3],
        [1],
        [1],
        [2],
        [2],
        [2],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,722][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[1],
        [2],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:56,722][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[1],
        [1],
        [7],
        [3],
        [3],
        [3],
        [5]], device='cuda:0')
[2024-07-23 21:06:56,723][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[4],
        [1],
        [1],
        [1],
        [1],
        [2],
        [2]], device='cuda:0')
[2024-07-23 21:06:56,724][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[  521],
        [ 8857],
        [14877],
        [ 1373],
        [ 5481],
        [ 4455],
        [  399]], device='cuda:0')
[2024-07-23 21:06:56,725][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[1702],
        [1702],
        [1702],
        [1702],
        [1702],
        [1702],
        [1702]], device='cuda:0')
[2024-07-23 21:06:56,758][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:56,759][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,759][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,760][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,760][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,760][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,761][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,761][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,761][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,761][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,762][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,762][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,762][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,763][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [lf] are: tensor([3.9830e-05, 9.9996e-01], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,767][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.9309, 0.0691], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,771][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,771][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.9971, 0.0029], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,772][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.9831, 0.0169], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,772][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.9318, 0.0682], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,772][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.7767, 0.2233], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,773][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.9731, 0.0269], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,773][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.9828, 0.0172], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,773][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.5427, 0.4573], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,774][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.7796, 0.2204], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,774][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.5045, 0.4955], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,774][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [red] are: tensor([1.4521e-06, 2.4219e-06, 1.0000e+00], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,779][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.6722, 0.3074, 0.0205], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,786][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.5512, 0.4303, 0.0185], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,792][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.0760, 0.9229, 0.0011], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,799][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.7170, 0.2647, 0.0183], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,802][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.1557, 0.8163, 0.0281], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,802][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.1290, 0.8648, 0.0062], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,802][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.4305, 0.5683, 0.0012], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,803][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.1047, 0.8940, 0.0013], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,803][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.2641, 0.6264, 0.1095], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,803][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.4717, 0.2519, 0.2764], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,804][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.6062, 0.2960, 0.0978], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,804][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([1.2885e-06, 2.8766e-06, 5.9517e-08, 1.0000e+00], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,804][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.6809, 0.1294, 0.1453, 0.0444], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,805][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.5047, 0.0282, 0.3562, 0.1109], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,809][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.6540, 0.0439, 0.2445, 0.0577], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,814][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([6.7208e-01, 2.1865e-01, 1.0917e-01, 1.0277e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,820][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.2463, 0.3514, 0.1532, 0.2491], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,827][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.2505, 0.1932, 0.2846, 0.2717], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,829][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([4.7565e-01, 3.1622e-01, 2.0774e-01, 3.8580e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,830][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.5131, 0.0753, 0.4060, 0.0056], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,830][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.4485, 0.1743, 0.0714, 0.3058], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,830][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.4991, 0.0949, 0.0546, 0.3515], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,831][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.2646, 0.2144, 0.4764, 0.0446], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:56,831][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([7.6817e-04, 7.1748e-06, 1.1926e-04, 1.5547e-04, 9.9895e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,831][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.4423, 0.0569, 0.0317, 0.1639, 0.3052], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,832][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0791, 0.0083, 0.0225, 0.8623, 0.0278], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,832][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.0473, 0.0546, 0.0012, 0.8952, 0.0017], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,836][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.6189, 0.1030, 0.1199, 0.1455, 0.0128], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,842][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.0486, 0.2662, 0.0230, 0.6277, 0.0344], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,850][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.0189, 0.0365, 0.0222, 0.9009, 0.0215], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,856][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0584, 0.1232, 0.0168, 0.7942, 0.0075], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,857][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.0280, 0.0372, 0.0193, 0.9133, 0.0022], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,858][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.0739, 0.2378, 0.0336, 0.6310, 0.0237], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,858][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.5051, 0.0660, 0.0657, 0.3121, 0.0512], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,859][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1995, 0.1526, 0.0653, 0.2984, 0.2842], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:56,859][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([7.0913e-04, 1.4048e-05, 2.8041e-03, 1.6551e-05, 3.5081e-03, 9.9295e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,859][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.2321, 0.0690, 0.0207, 0.1407, 0.4975, 0.0400], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,860][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0562, 0.0180, 0.0408, 0.8047, 0.0657, 0.0146], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,860][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.0314, 0.0273, 0.0139, 0.8455, 0.0634, 0.0184], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,860][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.3159, 0.0603, 0.0854, 0.2018, 0.0473, 0.2892], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,864][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.0773, 0.0357, 0.0884, 0.3938, 0.1188, 0.2860], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,870][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.0148, 0.1144, 0.0099, 0.7928, 0.0569, 0.0113], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,878][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1414, 0.0739, 0.0813, 0.5808, 0.1210, 0.0016], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,884][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.0944, 0.2292, 0.0894, 0.2553, 0.3143, 0.0172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,885][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1492, 0.0959, 0.0052, 0.3098, 0.1221, 0.3179], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,886][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.2563, 0.0667, 0.0677, 0.2246, 0.2476, 0.1371], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,886][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0766, 0.0665, 0.0426, 0.1415, 0.5391, 0.1337], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:56,886][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ on] are: tensor([1.0083e-05, 4.5775e-07, 7.5575e-06, 1.2100e-03, 1.9827e-07, 1.8270e-07,
        9.9877e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,887][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.2086, 0.1644, 0.0564, 0.0781, 0.3976, 0.0825, 0.0123],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,887][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ on] are: tensor([4.0131e-02, 2.7099e-02, 1.1218e-01, 4.9554e-01, 1.5786e-01, 1.6692e-01,
        2.6179e-04], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,888][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0077, 0.0150, 0.1013, 0.7409, 0.0609, 0.0730, 0.0011],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,888][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.1124, 0.0066, 0.0173, 0.0590, 0.2170, 0.5577, 0.0300],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,888][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.0460, 0.1050, 0.0455, 0.3984, 0.0833, 0.3095, 0.0123],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,893][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0212, 0.0386, 0.0350, 0.5706, 0.1809, 0.1518, 0.0020],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,900][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0374, 0.0301, 0.1138, 0.1025, 0.6484, 0.0640, 0.0037],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,906][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0060, 0.0458, 0.0055, 0.2953, 0.4051, 0.2414, 0.0010],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,911][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ on] are: tensor([1.6022e-04, 1.4426e-02, 1.1633e-03, 3.2202e-02, 1.3095e-01, 8.1952e-01,
        1.5847e-03], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,913][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.1054, 0.0304, 0.0111, 0.1645, 0.0679, 0.0452, 0.5755],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,914][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0369, 0.0497, 0.0483, 0.0346, 0.6502, 0.1775, 0.0028],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:56,958][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:56,958][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,959][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,959][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,959][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,960][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,960][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,960][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,961][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,961][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,961][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,963][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,965][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:56,968][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([3.9830e-05, 9.9996e-01], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,975][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.9309, 0.0691], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,982][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.9959, 0.0041], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,989][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.9971, 0.0029], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,989][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.9831, 0.0169], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,990][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.9318, 0.0682], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,990][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.7767, 0.2233], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,990][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.9731, 0.0269], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,991][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.9828, 0.0172], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,991][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.5427, 0.4573], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,991][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.7796, 0.2204], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,992][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.5045, 0.4955], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:56,992][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([1.4521e-06, 2.4219e-06, 1.0000e+00], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,992][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.6722, 0.3074, 0.0205], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:56,997][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.5512, 0.4303, 0.0185], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,005][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.0760, 0.9229, 0.0011], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,011][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.7170, 0.2647, 0.0183], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,019][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.1557, 0.8163, 0.0281], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,019][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.1290, 0.8648, 0.0062], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,020][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.4305, 0.5683, 0.0012], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,020][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.1047, 0.8940, 0.0013], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,020][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.2641, 0.6264, 0.1095], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,021][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.4717, 0.2519, 0.2764], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,021][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.6062, 0.2960, 0.0978], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,021][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([1.2885e-06, 2.8766e-06, 5.9517e-08, 1.0000e+00], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,022][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.6809, 0.1294, 0.1453, 0.0444], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,022][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.5047, 0.0282, 0.3562, 0.1109], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,026][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.6540, 0.0439, 0.2445, 0.0577], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,030][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([6.7208e-01, 2.1865e-01, 1.0917e-01, 1.0277e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,038][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.2463, 0.3514, 0.1532, 0.2491], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,044][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.2505, 0.1932, 0.2846, 0.2717], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,049][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([4.7565e-01, 3.1622e-01, 2.0774e-01, 3.8580e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,049][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.5131, 0.0753, 0.4060, 0.0056], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,049][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.4485, 0.1743, 0.0714, 0.3058], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,050][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.4991, 0.0949, 0.0546, 0.3515], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,050][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.2646, 0.2144, 0.4764, 0.0446], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,050][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([7.6817e-04, 7.1748e-06, 1.1926e-04, 1.5547e-04, 9.9895e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,051][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.4423, 0.0569, 0.0317, 0.1639, 0.3052], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,051][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0791, 0.0083, 0.0225, 0.8623, 0.0278], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,051][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.0473, 0.0546, 0.0012, 0.8952, 0.0017], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,052][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.6189, 0.1030, 0.1199, 0.1455, 0.0128], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,057][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.0486, 0.2662, 0.0230, 0.6277, 0.0344], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,065][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.0189, 0.0365, 0.0222, 0.9009, 0.0215], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,071][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0584, 0.1232, 0.0168, 0.7942, 0.0075], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,079][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.0280, 0.0372, 0.0193, 0.9133, 0.0022], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,079][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.0739, 0.2378, 0.0336, 0.6310, 0.0237], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,079][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.5051, 0.0660, 0.0657, 0.3121, 0.0512], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,080][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1995, 0.1526, 0.0653, 0.2984, 0.2842], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,080][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([7.0913e-04, 1.4048e-05, 2.8041e-03, 1.6551e-05, 3.5081e-03, 9.9295e-01],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,080][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.2321, 0.0690, 0.0207, 0.1407, 0.4975, 0.0400], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,081][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0562, 0.0180, 0.0408, 0.8047, 0.0657, 0.0146], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,081][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.0314, 0.0273, 0.0139, 0.8455, 0.0634, 0.0184], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,081][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.3159, 0.0603, 0.0854, 0.2018, 0.0473, 0.2892], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,082][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.0773, 0.0357, 0.0884, 0.3938, 0.1188, 0.2860], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,087][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.0148, 0.1144, 0.0099, 0.7928, 0.0569, 0.0113], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,094][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1414, 0.0739, 0.0813, 0.5808, 0.1210, 0.0016], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,101][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.0944, 0.2292, 0.0894, 0.2553, 0.3143, 0.0172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,108][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1492, 0.0959, 0.0052, 0.3098, 0.1221, 0.3179], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,109][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.2563, 0.0667, 0.0677, 0.2246, 0.2476, 0.1371], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,109][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0766, 0.0665, 0.0426, 0.1415, 0.5391, 0.1337], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,110][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([1.0083e-05, 4.5775e-07, 7.5575e-06, 1.2100e-03, 1.9827e-07, 1.8270e-07,
        9.9877e-01], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,110][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.2086, 0.1644, 0.0564, 0.0781, 0.3976, 0.0825, 0.0123],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,110][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([4.0131e-02, 2.7099e-02, 1.1218e-01, 4.9554e-01, 1.5786e-01, 1.6692e-01,
        2.6179e-04], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,111][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0077, 0.0150, 0.1013, 0.7409, 0.0609, 0.0730, 0.0011],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,111][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.1124, 0.0066, 0.0173, 0.0590, 0.2170, 0.5577, 0.0300],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,111][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.0460, 0.1050, 0.0455, 0.3984, 0.0833, 0.3095, 0.0123],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,112][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0212, 0.0386, 0.0350, 0.5706, 0.1809, 0.1518, 0.0020],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,119][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0374, 0.0301, 0.1138, 0.1025, 0.6484, 0.0640, 0.0037],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,125][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0060, 0.0458, 0.0055, 0.2953, 0.4051, 0.2414, 0.0010],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,130][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([1.6022e-04, 1.4426e-02, 1.1633e-03, 3.2202e-02, 1.3095e-01, 8.1952e-01,
        1.5847e-03], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,137][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.1054, 0.0304, 0.0111, 0.1645, 0.0679, 0.0452, 0.5755],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,139][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0369, 0.0497, 0.0483, 0.0346, 0.6502, 0.1775, 0.0028],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,140][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:57,141][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  1],
        [108],
        [ 97],
        [  9],
        [ 25],
        [ 76],
        [  1]], device='cuda:0')
[2024-07-23 21:06:57,141][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  1],
        [197],
        [173],
        [ 19],
        [ 43],
        [152],
        [  1]], device='cuda:0')
[2024-07-23 21:06:57,142][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[6186],
        [5579],
        [6282],
        [7991],
        [3120],
        [9976],
        [5565]], device='cuda:0')
[2024-07-23 21:06:57,144][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[4623],
        [3895],
        [1914],
        [3264],
        [2738],
        [1476],
        [1094]], device='cuda:0')
[2024-07-23 21:06:57,148][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 2095],
        [ 2115],
        [ 5225],
        [ 7387],
        [15342],
        [15716],
        [16830]], device='cuda:0')
[2024-07-23 21:06:57,151][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[10175],
        [10138],
        [ 1397],
        [14282],
        [ 1719],
        [ 1937],
        [ 2252]], device='cuda:0')
[2024-07-23 21:06:57,154][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[1143],
        [1117],
        [ 765],
        [ 932],
        [ 818],
        [ 372],
        [ 441]], device='cuda:0')
[2024-07-23 21:06:57,157][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 915],
        [ 736],
        [ 562],
        [1159],
        [2859],
        [5005],
        [4765]], device='cuda:0')
[2024-07-23 21:06:57,161][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[2024],
        [2163],
        [3570],
        [1186],
        [1039],
        [1071],
        [ 900]], device='cuda:0')
[2024-07-23 21:06:57,164][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[197],
        [190],
        [158],
        [138],
        [132],
        [102],
        [327]], device='cuda:0')
[2024-07-23 21:06:57,167][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[4943],
        [4865],
        [1817],
        [1633],
        [1088],
        [1664],
        [2222]], device='cuda:0')
[2024-07-23 21:06:57,171][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 1894],
        [ 2687],
        [ 3012],
        [ 4352],
        [10723],
        [ 7353],
        [ 4604]], device='cuda:0')
[2024-07-23 21:06:57,173][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 443],
        [1056],
        [1234],
        [ 943],
        [ 740],
        [ 581],
        [1039]], device='cuda:0')
[2024-07-23 21:06:57,173][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[1086],
        [2402],
        [1952],
        [4219],
        [4868],
        [5640],
        [5600]], device='cuda:0')
[2024-07-23 21:06:57,174][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 18],
        [107],
        [540],
        [ 95],
        [103],
        [  5],
        [  1]], device='cuda:0')
[2024-07-23 21:06:57,175][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[109],
        [ 15],
        [130],
        [  2],
        [103],
        [  2],
        [ 57]], device='cuda:0')
[2024-07-23 21:06:57,175][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[31],
        [29],
        [17],
        [69],
        [24],
        [ 6],
        [ 8]], device='cuda:0')
[2024-07-23 21:06:57,177][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[  8],
        [  8],
        [ 51],
        [ 23],
        [179],
        [203],
        [259]], device='cuda:0')
[2024-07-23 21:06:57,180][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[40],
        [40],
        [ 1],
        [ 9],
        [52],
        [37],
        [24]], device='cuda:0')
[2024-07-23 21:06:57,184][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[  6],
        [  6],
        [  5],
        [  5],
        [  3],
        [ 72],
        [357]], device='cuda:0')
[2024-07-23 21:06:57,187][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[1349],
        [1161],
        [   4],
        [  13],
        [   8],
        [   4],
        [   4]], device='cuda:0')
[2024-07-23 21:06:57,190][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[5743],
        [6119],
        [2559],
        [1721],
        [ 131],
        [ 228],
        [ 728]], device='cuda:0')
[2024-07-23 21:06:57,193][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 4],
        [ 4],
        [ 3],
        [ 4],
        [ 1],
        [ 1],
        [17]], device='cuda:0')
[2024-07-23 21:06:57,197][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 9],
        [ 8],
        [ 3],
        [30],
        [19],
        [17],
        [59]], device='cuda:0')
[2024-07-23 21:06:57,200][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[2],
        [4],
        [1],
        [2],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:57,203][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 1],
        [ 1],
        [ 1],
        [ 1],
        [ 1],
        [ 1],
        [59]], device='cuda:0')
[2024-07-23 21:06:57,206][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[2],
        [1],
        [2],
        [1],
        [3],
        [5],
        [5]], device='cuda:0')
[2024-07-23 21:06:57,207][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 517],
        [ 505],
        [1034],
        [ 662],
        [ 570],
        [ 230],
        [  85]], device='cuda:0')
[2024-07-23 21:06:57,207][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 554],
        [2318],
        [ 553],
        [ 717],
        [1954],
        [5576],
        [5912]], device='cuda:0')
[2024-07-23 21:06:57,208][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[5],
        [5],
        [5],
        [5],
        [5],
        [5],
        [5]], device='cuda:0')
[2024-07-23 21:06:57,247][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:57,253][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,254][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,255][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,255][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,255][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,256][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,256][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,256][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,257][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,257][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,257][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,257][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,262][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.4869, 0.5131], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,268][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.5899, 0.4101], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,276][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.4317, 0.5683], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,283][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.5286, 0.4714], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,285][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.2482, 0.7518], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,285][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.7209, 0.2791], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,285][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.8268, 0.1732], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,285][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.3748, 0.6252], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,286][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.4609, 0.5391], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,286][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.7117, 0.2883], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,286][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.1453, 0.8547], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,287][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.9672, 0.0328], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,287][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.4343, 0.3906, 0.1751], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,287][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.2247, 0.6470, 0.1283], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,292][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.2432, 0.4462, 0.3106], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,298][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.1999, 0.6761, 0.1241], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,306][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.3426, 0.2187, 0.4387], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,313][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.2757, 0.4640, 0.2602], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,314][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.4151, 0.4860, 0.0989], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,315][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.2032, 0.5263, 0.2705], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,315][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.1501, 0.5730, 0.2768], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,315][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.3766, 0.2422, 0.3812], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,316][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.2320, 0.5662, 0.2018], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,316][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [red] are: tensor([7.4162e-03, 9.9239e-01, 1.9205e-04], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,316][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.2718, 0.2685, 0.1437, 0.3160], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,317][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.1464, 0.3598, 0.1724, 0.3213], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,317][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.1880, 0.3629, 0.0702, 0.3790], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,317][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.1326, 0.2097, 0.1278, 0.5299], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,322][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.0612, 0.1053, 0.0974, 0.7362], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,328][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.1453, 0.3606, 0.3315, 0.1626], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,336][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.3666, 0.3017, 0.2989, 0.0327], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,343][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.0942, 0.1672, 0.0769, 0.6617], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,344][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.1442, 0.3364, 0.3158, 0.2036], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,345][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.3305, 0.1895, 0.2127, 0.2673], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,345][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.0627, 0.4965, 0.1087, 0.3322], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,345][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.1792, 0.3526, 0.3705, 0.0977], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,346][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.2095, 0.3027, 0.0539, 0.0990, 0.3348], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,346][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.1785, 0.1469, 0.0378, 0.4235, 0.2133], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,346][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0640, 0.1702, 0.0847, 0.6120, 0.0690], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,347][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.0439, 0.1103, 0.0805, 0.6009, 0.1645], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,347][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.1867, 0.2402, 0.1991, 0.2057, 0.1684], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,347][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.3360, 0.1345, 0.1424, 0.2069, 0.1802], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,352][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.2129, 0.2170, 0.1596, 0.2902, 0.1202], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,360][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0251, 0.0797, 0.0239, 0.2787, 0.5925], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,367][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.0899, 0.4126, 0.3494, 0.0922, 0.0559], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,374][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.1341, 0.1420, 0.2362, 0.2356, 0.2521], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,374][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1611, 0.2521, 0.1574, 0.1341, 0.2953], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,375][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([2.5003e-07, 3.7975e-03, 1.7741e-04, 9.9602e-01, 1.6317e-06],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,375][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.0717, 0.2174, 0.0730, 0.1519, 0.2570, 0.2290], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,375][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.0687, 0.2094, 0.0269, 0.2281, 0.4250, 0.0419], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,376][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.1305, 0.1183, 0.0642, 0.4724, 0.0966, 0.1180], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,376][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.0200, 0.1238, 0.0217, 0.2655, 0.4013, 0.1676], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,377][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.0350, 0.0171, 0.0269, 0.0629, 0.0362, 0.8220], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,377][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.1109, 0.1108, 0.1755, 0.2690, 0.2474, 0.0864], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,377][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.1428, 0.2555, 0.1325, 0.1942, 0.2301, 0.0449], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,382][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.0490, 0.1141, 0.0372, 0.2353, 0.3633, 0.2011], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,390][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.0525, 0.1041, 0.1888, 0.1027, 0.1369, 0.4150], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,397][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.0562, 0.0675, 0.0811, 0.3090, 0.1817, 0.3045], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,404][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1033, 0.1383, 0.0604, 0.1468, 0.0624, 0.4890], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,404][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([2.5251e-06, 4.6136e-03, 2.3935e-05, 9.9529e-01, 6.6276e-05, 2.7657e-07],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,405][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.0576, 0.1375, 0.0991, 0.0933, 0.1276, 0.4800, 0.0050],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,405][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.0323, 0.2820, 0.0739, 0.2563, 0.2745, 0.0694, 0.0116],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,405][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.1266, 0.2084, 0.1124, 0.2191, 0.0923, 0.1088, 0.1325],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,406][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0172, 0.1099, 0.0845, 0.2361, 0.1942, 0.3233, 0.0349],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,406][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.0682, 0.1476, 0.1181, 0.2123, 0.0376, 0.2327, 0.1836],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,406][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.0745, 0.0769, 0.2688, 0.1244, 0.2654, 0.1687, 0.0215],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,407][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.1462, 0.0843, 0.1333, 0.1521, 0.3100, 0.1261, 0.0480],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,408][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0490, 0.0691, 0.0200, 0.1702, 0.2776, 0.1008, 0.3134],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,415][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.1022, 0.1296, 0.1601, 0.0537, 0.1602, 0.2389, 0.1553],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,423][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0375, 0.0678, 0.0806, 0.2052, 0.1848, 0.3830, 0.0411],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,429][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0954, 0.1847, 0.0975, 0.1817, 0.0688, 0.1577, 0.2142],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,434][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ on] are: tensor([2.9293e-07, 1.0669e-03, 1.0281e-03, 9.9649e-01, 2.3930e-05, 1.3863e-03,
        7.9151e-08], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,472][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:57,474][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,474][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,475][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,475][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,475][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,476][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,476][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,476][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,477][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,477][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,477][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,479][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,483][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.4869, 0.5131], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,490][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.5899, 0.4101], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,497][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.4317, 0.5683], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,504][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.5286, 0.4714], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,505][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.2482, 0.7518], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,505][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.7209, 0.2791], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,505][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.8268, 0.1732], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,506][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.3748, 0.6252], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,506][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.4609, 0.5391], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,506][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.7117, 0.2883], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,507][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.1453, 0.8547], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,507][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.9672, 0.0328], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,507][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.4343, 0.3906, 0.1751], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,508][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.2247, 0.6470, 0.1283], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,512][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.2432, 0.4462, 0.3106], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,512][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.1999, 0.6761, 0.1241], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,512][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.3426, 0.2187, 0.4387], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,513][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.2757, 0.4640, 0.2602], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,519][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.4151, 0.4860, 0.0989], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,527][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.2032, 0.5263, 0.2705], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,534][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.1501, 0.5730, 0.2768], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,536][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.3766, 0.2422, 0.3812], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,536][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.2320, 0.5662, 0.2018], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,536][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([7.4162e-03, 9.9239e-01, 1.9205e-04], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,537][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.2718, 0.2685, 0.1437, 0.3160], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,537][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.1464, 0.3598, 0.1724, 0.3213], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,537][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.1880, 0.3629, 0.0702, 0.3790], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,538][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.1326, 0.2097, 0.1278, 0.5299], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,538][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.0612, 0.1053, 0.0974, 0.7362], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,538][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.1453, 0.3606, 0.3315, 0.1626], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,539][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.3666, 0.3017, 0.2989, 0.0327], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,543][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.0942, 0.1672, 0.0769, 0.6617], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,551][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.1442, 0.3364, 0.3158, 0.2036], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,558][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.3305, 0.1895, 0.2127, 0.2673], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,565][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.0627, 0.4965, 0.1087, 0.3322], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,566][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.1792, 0.3526, 0.3705, 0.0977], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,566][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.2095, 0.3027, 0.0539, 0.0990, 0.3348], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,566][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.1785, 0.1469, 0.0378, 0.4235, 0.2133], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,567][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0640, 0.1702, 0.0847, 0.6120, 0.0690], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,567][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.0439, 0.1103, 0.0805, 0.6009, 0.1645], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,567][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.1867, 0.2402, 0.1991, 0.2057, 0.1684], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,568][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.3360, 0.1345, 0.1424, 0.2069, 0.1802], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,568][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.2129, 0.2170, 0.1596, 0.2902, 0.1202], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,568][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0251, 0.0797, 0.0239, 0.2787, 0.5925], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,573][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.0899, 0.4126, 0.3494, 0.0922, 0.0559], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,579][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.1341, 0.1420, 0.2362, 0.2356, 0.2521], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,587][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1611, 0.2521, 0.1574, 0.1341, 0.2953], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,591][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([2.5003e-07, 3.7975e-03, 1.7741e-04, 9.9602e-01, 1.6317e-06],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,595][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.0717, 0.2174, 0.0730, 0.1519, 0.2570, 0.2290], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,596][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.0687, 0.2094, 0.0269, 0.2281, 0.4250, 0.0419], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,596][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.1305, 0.1183, 0.0642, 0.4724, 0.0966, 0.1180], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,596][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.0200, 0.1238, 0.0217, 0.2655, 0.4013, 0.1676], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,597][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.0350, 0.0171, 0.0269, 0.0629, 0.0362, 0.8220], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,597][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.1109, 0.1108, 0.1755, 0.2690, 0.2474, 0.0864], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,598][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.1428, 0.2555, 0.1325, 0.1942, 0.2301, 0.0449], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,598][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.0490, 0.1141, 0.0372, 0.2353, 0.3633, 0.2011], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,598][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.0525, 0.1041, 0.1888, 0.1027, 0.1369, 0.4150], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,600][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.0562, 0.0675, 0.0811, 0.3090, 0.1817, 0.3045], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,608][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1033, 0.1383, 0.0604, 0.1468, 0.0624, 0.4890], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,612][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([2.5251e-06, 4.6136e-03, 2.3935e-05, 9.9529e-01, 6.6276e-05, 2.7657e-07],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,620][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.0576, 0.1375, 0.0991, 0.0933, 0.1276, 0.4800, 0.0050],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,625][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.0323, 0.2820, 0.0739, 0.2563, 0.2745, 0.0694, 0.0116],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,626][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.1266, 0.2084, 0.1124, 0.2191, 0.0923, 0.1088, 0.1325],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,626][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0172, 0.1099, 0.0845, 0.2361, 0.1942, 0.3233, 0.0349],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,626][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.0682, 0.1476, 0.1181, 0.2123, 0.0376, 0.2327, 0.1836],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,627][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.0745, 0.0769, 0.2688, 0.1244, 0.2654, 0.1687, 0.0215],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,627][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.1462, 0.0843, 0.1333, 0.1521, 0.3100, 0.1261, 0.0480],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,627][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0490, 0.0691, 0.0200, 0.1702, 0.2776, 0.1008, 0.3134],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,628][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.1022, 0.1296, 0.1601, 0.0537, 0.1602, 0.2389, 0.1553],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,628][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.0375, 0.0678, 0.0806, 0.2052, 0.1848, 0.3830, 0.0411],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,632][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0954, 0.1847, 0.0975, 0.1817, 0.0688, 0.1577, 0.2142],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,636][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([2.9293e-07, 1.0669e-03, 1.0281e-03, 9.9649e-01, 2.3930e-05, 1.3863e-03,
        7.9151e-08], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,637][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:57,640][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 1],
        [ 2],
        [ 4],
        [ 2],
        [ 1],
        [30],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:57,644][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  1],
        [137],
        [146],
        [ 35],
        [  7],
        [177],
        [  2]], device='cuda:0')
[2024-07-23 21:06:57,647][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[2434],
        [2992],
        [3492],
        [1932],
        [4206],
        [3297],
        [2721]], device='cuda:0')
[2024-07-23 21:06:57,650][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[150],
        [227],
        [323],
        [385],
        [904],
        [896],
        [706]], device='cuda:0')
[2024-07-23 21:06:57,654][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[2507],
        [1930],
        [1650],
        [4355],
        [6384],
        [4494],
        [3070]], device='cuda:0')
[2024-07-23 21:06:57,657][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[2595],
        [1661],
        [1381],
        [1427],
        [1389],
        [1190],
        [1258]], device='cuda:0')
[2024-07-23 21:06:57,659][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[  136],
        [ 5405],
        [ 1281],
        [14822],
        [ 2651],
        [ 4040],
        [ 4641]], device='cuda:0')
[2024-07-23 21:06:57,660][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[1903],
        [1650],
        [2684],
        [3097],
        [1833],
        [2671],
        [3995]], device='cuda:0')
[2024-07-23 21:06:57,660][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[3310],
        [3124],
        [2561],
        [2287],
        [2990],
        [2588],
        [2351]], device='cuda:0')
[2024-07-23 21:06:57,661][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[7],
        [5],
        [6],
        [7],
        [4],
        [5],
        [5]], device='cuda:0')
[2024-07-23 21:06:57,662][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 9923],
        [10798],
        [13334],
        [13779],
        [13732],
        [ 3675],
        [ 5169]], device='cuda:0')
[2024-07-23 21:06:57,663][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 5214],
        [ 6454],
        [ 6476],
        [ 6726],
        [ 7609],
        [12366],
        [13303]], device='cuda:0')
[2024-07-23 21:06:57,666][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[11226],
        [ 1540],
        [ 1842],
        [ 4241],
        [ 5838],
        [19038],
        [ 9402]], device='cuda:0')
[2024-07-23 21:06:57,670][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 275],
        [ 287],
        [3536],
        [2601],
        [3576],
        [3569],
        [3582]], device='cuda:0')
[2024-07-23 21:06:57,673][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[1],
        [1],
        [1],
        [2],
        [3],
        [2],
        [1]], device='cuda:0')
[2024-07-23 21:06:57,676][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[11],
        [22],
        [21],
        [32],
        [42],
        [24],
        [14]], device='cuda:0')
[2024-07-23 21:06:57,680][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 2],
        [ 5],
        [ 8],
        [15],
        [ 6],
        [ 4],
        [11]], device='cuda:0')
[2024-07-23 21:06:57,683][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[15],
        [25],
        [21],
        [38],
        [28],
        [17],
        [16]], device='cuda:0')
[2024-07-23 21:06:57,686][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[3],
        [3],
        [2],
        [4],
        [5],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:57,689][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[28],
        [33],
        [30],
        [ 1],
        [ 8],
        [ 3],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:57,692][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[222],
        [173],
        [161],
        [127],
        [118],
        [ 65],
        [138]], device='cuda:0')
[2024-07-23 21:06:57,693][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[2],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:57,694][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[17259],
        [10796],
        [ 9056],
        [11126],
        [14677],
        [11808],
        [10569]], device='cuda:0')
[2024-07-23 21:06:57,694][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [6],
        [1]], device='cuda:0')
[2024-07-23 21:06:57,695][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 4],
        [ 2],
        [ 2],
        [11],
        [ 6],
        [ 2],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:57,696][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[200],
        [191],
        [123],
        [ 44],
        [ 30],
        [  3],
        [ 51]], device='cuda:0')
[2024-07-23 21:06:57,698][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[8138],
        [7708],
        [ 172],
        [1413],
        [ 104],
        [ 104],
        [ 105]], device='cuda:0')
[2024-07-23 21:06:57,701][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[13726],
        [15072],
        [17404],
        [17439],
        [17284],
        [18853],
        [19008]], device='cuda:0')
[2024-07-23 21:06:57,704][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[   3],
        [  11],
        [   5],
        [ 117],
        [1150],
        [ 688],
        [ 211]], device='cuda:0')
[2024-07-23 21:06:57,708][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2],
        [2],
        [2],
        [2],
        [2],
        [2],
        [2]], device='cuda:0')
[2024-07-23 21:06:57,749][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:57,754][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,758][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,760][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,760][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,761][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,761][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,761][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,762][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,762][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,762][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,762][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,763][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,763][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.4941, 0.5059], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,765][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.5732, 0.4268], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,770][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.5105, 0.4895], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,776][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.6765, 0.3235], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,783][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.6246, 0.3754], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,790][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.8223, 0.1777], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,790][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.8446, 0.1554], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,791][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.4630, 0.5370], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,791][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.7012, 0.2988], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,791][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.7566, 0.2434], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,791][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.3624, 0.6376], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,792][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.3417, 0.6583], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,792][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.2903, 0.4491, 0.2606], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,792][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.3143, 0.4832, 0.2024], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,793][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.3320, 0.3989, 0.2691], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,795][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.3441, 0.3649, 0.2910], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,803][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.3892, 0.3500, 0.2608], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,809][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.4309, 0.5140, 0.0551], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,817][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.6124, 0.2695, 0.1181], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,819][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.1936, 0.4789, 0.3274], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,819][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.5356, 0.2992, 0.1652], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,820][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.3511, 0.5595, 0.0894], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,820][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.3863, 0.3000, 0.3137], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,820][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.3488, 0.3182, 0.3330], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:57,821][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.2896, 0.2646, 0.2734, 0.1724], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,821][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.2592, 0.3174, 0.2823, 0.1410], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,821][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.2191, 0.2257, 0.1830, 0.3722], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,822][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.2045, 0.2051, 0.3446, 0.2458], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,822][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.3283, 0.3505, 0.2102, 0.1110], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,826][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.2279, 0.3323, 0.3511, 0.0887], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,833][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.3881, 0.1909, 0.2695, 0.1516], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,840][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.2815, 0.3499, 0.1215, 0.2472], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,847][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.4046, 0.1695, 0.3300, 0.0959], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,848][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.4115, 0.2825, 0.1765, 0.1294], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,849][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.1327, 0.0871, 0.0498, 0.7304], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,849][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.1878, 0.1775, 0.1670, 0.4677], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:57,849][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.1682, 0.2647, 0.1447, 0.2673, 0.1552], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,850][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.0681, 0.2150, 0.2020, 0.4274, 0.0875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,850][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0595, 0.1143, 0.1703, 0.5903, 0.0656], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,850][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.1299, 0.1356, 0.2389, 0.3265, 0.1691], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,851][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.2131, 0.2753, 0.2052, 0.1958, 0.1105], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,851][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.1302, 0.1676, 0.3442, 0.2657, 0.0923], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,853][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.1855, 0.1972, 0.1669, 0.2547, 0.1957], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,861][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.1483, 0.1223, 0.0886, 0.3703, 0.2705], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,867][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1484, 0.0908, 0.4203, 0.2305, 0.1101], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,875][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.2629, 0.1539, 0.1602, 0.3916, 0.0315], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,877][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.2037, 0.1982, 0.0536, 0.4007, 0.1438], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,878][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1624, 0.2753, 0.0731, 0.2950, 0.1942], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:57,878][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.1872, 0.2421, 0.1228, 0.1792, 0.1313, 0.1374], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,878][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.1211, 0.1690, 0.0784, 0.4175, 0.1233, 0.0907], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,879][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0613, 0.1461, 0.1319, 0.4896, 0.0947, 0.0765], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,879][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.1481, 0.0892, 0.2354, 0.1846, 0.2081, 0.1346], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,880][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.2297, 0.2280, 0.1350, 0.2123, 0.1339, 0.0611], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,880][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.2271, 0.1889, 0.0822, 0.2693, 0.1785, 0.0540], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,880][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.2698, 0.1615, 0.1058, 0.1881, 0.1560, 0.1188], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,884][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1082, 0.1433, 0.0740, 0.1893, 0.3407, 0.1445], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,891][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.2742, 0.1339, 0.1085, 0.2816, 0.1348, 0.0670], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,898][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.2697, 0.1962, 0.0981, 0.2743, 0.1053, 0.0565], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,905][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.0821, 0.0443, 0.0430, 0.3047, 0.0738, 0.4521], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,907][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0973, 0.0861, 0.0622, 0.1653, 0.0565, 0.5327], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:57,907][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.2043, 0.1488, 0.1553, 0.0801, 0.0868, 0.2457, 0.0789],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,907][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.1275, 0.1182, 0.1317, 0.1908, 0.0977, 0.3133, 0.0208],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,908][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0461, 0.1189, 0.2285, 0.2818, 0.1248, 0.1338, 0.0661],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,908][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0841, 0.0841, 0.1670, 0.2128, 0.2170, 0.1627, 0.0721],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,908][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.1254, 0.2316, 0.1040, 0.2592, 0.1403, 0.0956, 0.0439],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,909][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.2380, 0.1406, 0.1525, 0.1628, 0.1337, 0.1509, 0.0215],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,909][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0880, 0.1005, 0.1249, 0.1941, 0.2096, 0.2271, 0.0557],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,910][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0791, 0.1860, 0.1156, 0.1456, 0.2147, 0.1448, 0.1142],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,914][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.2767, 0.0750, 0.1198, 0.1433, 0.2712, 0.1056, 0.0085],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,922][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.1269, 0.1449, 0.1253, 0.1618, 0.1462, 0.1937, 0.1012],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,928][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.1073, 0.1289, 0.0925, 0.2586, 0.0988, 0.1810, 0.1328],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,935][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.1058, 0.0979, 0.0530, 0.2016, 0.0897, 0.1416, 0.3104],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:57,975][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:57,980][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,983][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,988][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,991][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,995][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,997][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,997][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,997][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,998][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,998][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,998][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,999][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:57,999][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.4941, 0.5059], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:57,999][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.5732, 0.4268], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,000][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.5105, 0.4895], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,003][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.6765, 0.3235], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,010][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.6246, 0.3754], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,017][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.8223, 0.1777], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,023][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.8446, 0.1554], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,025][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.4630, 0.5370], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,025][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.7012, 0.2988], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,026][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.7566, 0.2434], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,026][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.3624, 0.6376], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,026][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.3417, 0.6583], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,027][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.2903, 0.4491, 0.2606], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,027][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.3143, 0.4832, 0.2024], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,027][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.3320, 0.3989, 0.2691], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,028][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.3441, 0.3649, 0.2910], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,028][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.3892, 0.3500, 0.2608], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,033][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.4309, 0.5140, 0.0551], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,040][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.6124, 0.2695, 0.1181], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,046][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.1936, 0.4789, 0.3274], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,053][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.5356, 0.2992, 0.1652], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,054][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.3511, 0.5595, 0.0894], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,054][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.3863, 0.3000, 0.3137], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,054][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.3488, 0.3182, 0.3330], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,055][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.2896, 0.2646, 0.2734, 0.1724], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,055][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.2592, 0.3174, 0.2823, 0.1410], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,055][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.2191, 0.2257, 0.1830, 0.3722], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,056][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.2045, 0.2051, 0.3446, 0.2458], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,056][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.3283, 0.3505, 0.2102, 0.1110], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,056][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.2279, 0.3323, 0.3511, 0.0887], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,061][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.3881, 0.1909, 0.2695, 0.1516], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,068][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.2815, 0.3499, 0.1215, 0.2472], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,075][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.4046, 0.1695, 0.3300, 0.0959], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,082][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.4115, 0.2825, 0.1765, 0.1294], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,082][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.1327, 0.0871, 0.0498, 0.7304], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,082][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.1878, 0.1775, 0.1670, 0.4677], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,083][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.1682, 0.2647, 0.1447, 0.2673, 0.1552], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,083][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.0681, 0.2150, 0.2020, 0.4274, 0.0875], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,083][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0595, 0.1143, 0.1703, 0.5903, 0.0656], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,084][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.1299, 0.1356, 0.2389, 0.3265, 0.1691], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,084][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.2131, 0.2753, 0.2052, 0.1958, 0.1105], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,085][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.1302, 0.1676, 0.3442, 0.2657, 0.0923], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,087][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.1855, 0.1972, 0.1669, 0.2547, 0.1957], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,094][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.1483, 0.1223, 0.0886, 0.3703, 0.2705], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,100][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1484, 0.0908, 0.4203, 0.2305, 0.1101], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,108][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.2629, 0.1539, 0.1602, 0.3916, 0.0315], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,110][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.2037, 0.1982, 0.0536, 0.4007, 0.1438], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,111][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1624, 0.2753, 0.0731, 0.2950, 0.1942], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,111][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.1872, 0.2421, 0.1228, 0.1792, 0.1313, 0.1374], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,111][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.1211, 0.1690, 0.0784, 0.4175, 0.1233, 0.0907], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,112][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0613, 0.1461, 0.1319, 0.4896, 0.0947, 0.0765], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,112][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.1481, 0.0892, 0.2354, 0.1846, 0.2081, 0.1346], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,112][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.2297, 0.2280, 0.1350, 0.2123, 0.1339, 0.0611], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,113][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.2271, 0.1889, 0.0822, 0.2693, 0.1785, 0.0540], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,113][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.2698, 0.1615, 0.1058, 0.1881, 0.1560, 0.1188], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,117][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1082, 0.1433, 0.0740, 0.1893, 0.3407, 0.1445], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,123][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.2742, 0.1339, 0.1085, 0.2816, 0.1348, 0.0670], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,131][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.2697, 0.1962, 0.0981, 0.2743, 0.1053, 0.0565], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,137][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.0821, 0.0443, 0.0430, 0.3047, 0.0738, 0.4521], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,139][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0973, 0.0861, 0.0622, 0.1653, 0.0565, 0.5327], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,139][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.2043, 0.1488, 0.1553, 0.0801, 0.0868, 0.2457, 0.0789],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,140][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.1275, 0.1182, 0.1317, 0.1908, 0.0977, 0.3133, 0.0208],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,140][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0461, 0.1189, 0.2285, 0.2818, 0.1248, 0.1338, 0.0661],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,140][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0841, 0.0841, 0.1670, 0.2128, 0.2170, 0.1627, 0.0721],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,141][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.1254, 0.2316, 0.1040, 0.2592, 0.1403, 0.0956, 0.0439],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,141][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.2380, 0.1406, 0.1525, 0.1628, 0.1337, 0.1509, 0.0215],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,141][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0880, 0.1005, 0.1249, 0.1941, 0.2096, 0.2271, 0.0557],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,142][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0791, 0.1860, 0.1156, 0.1456, 0.2147, 0.1448, 0.1142],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,148][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.2767, 0.0750, 0.1198, 0.1433, 0.2712, 0.1056, 0.0085],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,154][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.1269, 0.1449, 0.1253, 0.1618, 0.1462, 0.1937, 0.1012],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,162][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.1073, 0.1289, 0.0925, 0.2586, 0.0988, 0.1810, 0.1328],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,167][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.1058, 0.0979, 0.0530, 0.2016, 0.0897, 0.1416, 0.3104],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,168][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:58,169][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[1],
        [4],
        [3],
        [1],
        [1],
        [7],
        [1]], device='cuda:0')
[2024-07-23 21:06:58,170][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 3],
        [21],
        [12],
        [ 2],
        [ 2],
        [22],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:58,170][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[8577],
        [3655],
        [1204],
        [2004],
        [3312],
        [2037],
        [1366]], device='cuda:0')
[2024-07-23 21:06:58,172][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[5932],
        [4242],
        [4248],
        [5105],
        [5902],
        [6057],
        [5885]], device='cuda:0')
[2024-07-23 21:06:58,175][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[4613],
        [3306],
        [1476],
        [2300],
        [2875],
        [2731],
        [1676]], device='cuda:0')
[2024-07-23 21:06:58,178][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[4533],
        [3173],
        [2461],
        [2040],
        [1716],
        [1895],
        [1878]], device='cuda:0')
[2024-07-23 21:06:58,181][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[3004],
        [3838],
        [7321],
        [6800],
        [7660],
        [6938],
        [7043]], device='cuda:0')
[2024-07-23 21:06:58,184][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[14690],
        [14341],
        [12279],
        [10421],
        [ 8642],
        [ 8827],
        [ 8606]], device='cuda:0')
[2024-07-23 21:06:58,187][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[163],
        [199],
        [351],
        [425],
        [198],
        [231],
        [416]], device='cuda:0')
[2024-07-23 21:06:58,190][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 288],
        [ 242],
        [ 147],
        [ 368],
        [2122],
        [2333],
        [1305]], device='cuda:0')
[2024-07-23 21:06:58,193][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[4523],
        [4412],
        [5124],
        [5852],
        [6153],
        [4403],
        [4670]], device='cuda:0')
[2024-07-23 21:06:58,196][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[264],
        [280],
        [437],
        [560],
        [880],
        [682],
        [577]], device='cuda:0')
[2024-07-23 21:06:58,199][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[4467],
        [3888],
        [3996],
        [4889],
        [4089],
        [2483],
        [3663]], device='cuda:0')
[2024-07-23 21:06:58,200][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 387],
        [2435],
        [ 799],
        [1120],
        [1532],
        [1960],
        [ 917]], device='cuda:0')
[2024-07-23 21:06:58,200][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[11],
        [33],
        [21],
        [48],
        [19],
        [17],
        [ 5]], device='cuda:0')
[2024-07-23 21:06:58,201][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[  3],
        [ 35],
        [153],
        [118],
        [229],
        [313],
        [336]], device='cuda:0')
[2024-07-23 21:06:58,202][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[55],
        [54],
        [63],
        [85],
        [58],
        [50],
        [96]], device='cuda:0')
[2024-07-23 21:06:58,202][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[439],
        [301],
        [352],
        [ 62],
        [ 36],
        [ 48],
        [141]], device='cuda:0')
[2024-07-23 21:06:58,204][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 30],
        [ 29],
        [ 49],
        [ 62],
        [ 95],
        [138],
        [136]], device='cuda:0')
[2024-07-23 21:06:58,207][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 55],
        [ 66],
        [128],
        [125],
        [153],
        [188],
        [177]], device='cuda:0')
[2024-07-23 21:06:58,210][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[23],
        [21],
        [15],
        [15],
        [17],
        [29],
        [24]], device='cuda:0')
[2024-07-23 21:06:58,213][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[1772],
        [1226],
        [ 892],
        [1561],
        [2077],
        [1341],
        [1851]], device='cuda:0')
[2024-07-23 21:06:58,216][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[1],
        [2],
        [2],
        [2],
        [1],
        [1],
        [2]], device='cuda:0')
[2024-07-23 21:06:58,219][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[10],
        [ 9],
        [ 8],
        [ 6],
        [ 4],
        [ 5],
        [ 5]], device='cuda:0')
[2024-07-23 21:06:58,222][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[2478],
        [2621],
        [2690],
        [2420],
        [1782],
        [1775],
        [1358]], device='cuda:0')
[2024-07-23 21:06:58,225][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 38],
        [ 32],
        [ 68],
        [ 50],
        [ 51],
        [161],
        [137]], device='cuda:0')
[2024-07-23 21:06:58,228][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[6471],
        [ 770],
        [1345],
        [ 108],
        [  67],
        [ 170],
        [1112]], device='cuda:0')
[2024-07-23 21:06:58,231][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[4511],
        [4482],
        [3931],
        [5088],
        [5477],
        [5001],
        [4775]], device='cuda:0')
[2024-07-23 21:06:58,232][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 1984],
        [ 2653],
        [ 3136],
        [ 3784],
        [ 8090],
        [ 5907],
        [10133]], device='cuda:0')
[2024-07-23 21:06:58,232][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[46],
        [46],
        [46],
        [46],
        [46],
        [46],
        [46]], device='cuda:0')
[2024-07-23 21:06:58,272][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:58,273][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,273][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,273][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,274][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,274][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,274][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,274][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,275][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,275][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,275][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,276][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,276][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,276][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.5482, 0.4518], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,277][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.4726, 0.5274], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,277][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.3715, 0.6285], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,277][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.4957, 0.5043], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,278][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.5586, 0.4414], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,278][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.5464, 0.4536], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,278][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.4647, 0.5353], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,279][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.3966, 0.6034], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,279][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.5171, 0.4829], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,279][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.6189, 0.3811], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,280][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.3162, 0.6838], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,280][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.6349, 0.3651], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,280][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.3731, 0.3313, 0.2956], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,280][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.2721, 0.5624, 0.1655], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,281][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.2202, 0.3875, 0.3924], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,281][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.2861, 0.4634, 0.2505], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,281][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.3792, 0.2737, 0.3471], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,282][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.3519, 0.3530, 0.2951], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,282][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.3919, 0.2548, 0.3533], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,282][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.2462, 0.4641, 0.2897], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,285][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.3030, 0.4388, 0.2582], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,295][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.2464, 0.6562, 0.0974], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,302][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.1878, 0.4640, 0.3483], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,302][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.3227, 0.2748, 0.4025], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,302][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.1693, 0.1555, 0.3451, 0.3300], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,303][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.2676, 0.2114, 0.1695, 0.3515], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,303][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.1212, 0.2653, 0.2927, 0.3208], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,303][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.1979, 0.2490, 0.2588, 0.2943], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,304][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.3377, 0.2729, 0.1674, 0.2220], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,304][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.2233, 0.2426, 0.2394, 0.2947], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,304][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.3033, 0.2390, 0.1527, 0.3050], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,308][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.1842, 0.3085, 0.1931, 0.3142], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,315][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.1622, 0.1959, 0.2216, 0.4203], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,322][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.1806, 0.2236, 0.4581, 0.1377], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,328][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.2128, 0.3677, 0.2132, 0.2063], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,330][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.1376, 0.1306, 0.2898, 0.4420], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,330][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.0928, 0.1077, 0.1431, 0.5716, 0.0848], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,330][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.1276, 0.1722, 0.1057, 0.5075, 0.0870], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,331][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.1051, 0.1565, 0.2086, 0.3418, 0.1880], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,331][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.1476, 0.2834, 0.1515, 0.3378, 0.0797], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,331][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.1968, 0.2772, 0.1305, 0.2217, 0.1738], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,332][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.2178, 0.1778, 0.2016, 0.2513, 0.1514], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,332][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.2321, 0.1802, 0.1694, 0.1508, 0.2675], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,332][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.1264, 0.1882, 0.1969, 0.3824, 0.1061], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,336][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.0824, 0.1392, 0.2105, 0.5111, 0.0568], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,343][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.0623, 0.1137, 0.2390, 0.5617, 0.0233], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,350][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1331, 0.1635, 0.1994, 0.3835, 0.1205], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,356][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.0647, 0.0754, 0.2255, 0.5576, 0.0768], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,358][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.0868, 0.1285, 0.1710, 0.4058, 0.1165, 0.0914], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,358][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.1283, 0.1694, 0.0889, 0.3101, 0.1341, 0.1692], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,359][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.1006, 0.1169, 0.1263, 0.3473, 0.0846, 0.2244], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,359][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.1544, 0.1860, 0.2088, 0.2657, 0.1078, 0.0773], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,359][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.2103, 0.1716, 0.1288, 0.1698, 0.1591, 0.1604], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,360][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.1242, 0.1218, 0.1851, 0.2470, 0.1662, 0.1557], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,360][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.1572, 0.1039, 0.0920, 0.1264, 0.1067, 0.4138], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,360][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.0874, 0.1790, 0.1291, 0.3259, 0.1671, 0.1116], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,361][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.1207, 0.2253, 0.1283, 0.3957, 0.0731, 0.0568], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,365][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1263, 0.1517, 0.1301, 0.4381, 0.0974, 0.0563], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,373][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1091, 0.1774, 0.1564, 0.1977, 0.1150, 0.2443], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,379][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0835, 0.1301, 0.1428, 0.4241, 0.1550, 0.0643], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,386][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.0911, 0.0984, 0.1501, 0.3705, 0.1059, 0.1346, 0.0495],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,386][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.1564, 0.1574, 0.0738, 0.2370, 0.0956, 0.1710, 0.1088],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,387][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0725, 0.0928, 0.1054, 0.2921, 0.1200, 0.2011, 0.1162],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,387][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.1153, 0.1625, 0.1749, 0.2372, 0.1096, 0.0955, 0.1050],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,387][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.1576, 0.2175, 0.1364, 0.1121, 0.1246, 0.0951, 0.1567],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,388][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.1052, 0.1125, 0.1219, 0.2126, 0.1533, 0.1635, 0.1311],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,388][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.1534, 0.0847, 0.0772, 0.0998, 0.1127, 0.1465, 0.3256],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,389][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0985, 0.2093, 0.1471, 0.1863, 0.1289, 0.1484, 0.0815],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,389][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0886, 0.1154, 0.1659, 0.3453, 0.1228, 0.1084, 0.0536],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,393][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0957, 0.1656, 0.2926, 0.1717, 0.0357, 0.1324, 0.1064],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,399][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0955, 0.1694, 0.1627, 0.1308, 0.0990, 0.1485, 0.1942],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,407][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0579, 0.0593, 0.1549, 0.5059, 0.1101, 0.0876, 0.0243],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,447][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:58,447][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,448][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,448][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,449][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,451][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,454][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,458][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,462][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,465][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,469][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,474][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,475][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,476][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.5482, 0.4518], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,476][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.4726, 0.5274], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,476][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.3715, 0.6285], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,477][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.4957, 0.5043], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,477][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.5586, 0.4414], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,477][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.5464, 0.4536], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,478][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.4647, 0.5353], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,478][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.3966, 0.6034], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,478][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.5171, 0.4829], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,479][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.6189, 0.3811], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,483][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.3162, 0.6838], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,489][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.6349, 0.3651], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,496][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.3731, 0.3313, 0.2956], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,503][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.2721, 0.5624, 0.1655], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,504][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.2202, 0.3875, 0.3924], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,505][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.2861, 0.4634, 0.2505], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,505][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.3792, 0.2737, 0.3471], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,505][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.3519, 0.3530, 0.2951], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,506][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.3919, 0.2548, 0.3533], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,506][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.2462, 0.4641, 0.2897], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,506][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.3030, 0.4388, 0.2582], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,507][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.2464, 0.6562, 0.0974], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,507][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.1878, 0.4640, 0.3483], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,509][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.3227, 0.2748, 0.4025], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,517][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.1693, 0.1555, 0.3451, 0.3300], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,523][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.2676, 0.2114, 0.1695, 0.3515], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,530][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.1212, 0.2653, 0.2927, 0.3208], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,533][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.1979, 0.2490, 0.2588, 0.2943], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,533][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.3377, 0.2729, 0.1674, 0.2220], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,533][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.2233, 0.2426, 0.2394, 0.2947], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,534][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.3033, 0.2390, 0.1527, 0.3050], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,534][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.1842, 0.3085, 0.1931, 0.3142], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,534][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.1622, 0.1959, 0.2216, 0.4203], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,535][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.1806, 0.2236, 0.4581, 0.1377], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,535][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.2128, 0.3677, 0.2132, 0.2063], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,535][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.1376, 0.1306, 0.2898, 0.4420], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,538][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.0928, 0.1077, 0.1431, 0.5716, 0.0848], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,545][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.1276, 0.1722, 0.1057, 0.5075, 0.0870], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,551][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.1051, 0.1565, 0.2086, 0.3418, 0.1880], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,558][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.1476, 0.2834, 0.1515, 0.3378, 0.0797], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,560][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.1968, 0.2772, 0.1305, 0.2217, 0.1738], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,561][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.2178, 0.1778, 0.2016, 0.2513, 0.1514], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,561][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.2321, 0.1802, 0.1694, 0.1508, 0.2675], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,561][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.1264, 0.1882, 0.1969, 0.3824, 0.1061], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,562][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.0824, 0.1392, 0.2105, 0.5111, 0.0568], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,562][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.0623, 0.1137, 0.2390, 0.5617, 0.0233], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,563][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1331, 0.1635, 0.1994, 0.3835, 0.1205], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,563][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.0647, 0.0754, 0.2255, 0.5576, 0.0768], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,563][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.0868, 0.1285, 0.1710, 0.4058, 0.1165, 0.0914], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,567][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.1283, 0.1694, 0.0889, 0.3101, 0.1341, 0.1692], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,573][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.1006, 0.1169, 0.1263, 0.3473, 0.0846, 0.2244], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,580][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.1544, 0.1860, 0.2088, 0.2657, 0.1078, 0.0773], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,586][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.2103, 0.1716, 0.1288, 0.1698, 0.1591, 0.1604], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,590][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.1242, 0.1218, 0.1851, 0.2470, 0.1662, 0.1557], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,591][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.1572, 0.1039, 0.0920, 0.1264, 0.1067, 0.4138], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,591][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.0874, 0.1790, 0.1291, 0.3259, 0.1671, 0.1116], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,592][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.1207, 0.2253, 0.1283, 0.3957, 0.0731, 0.0568], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,592][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1263, 0.1517, 0.1301, 0.4381, 0.0974, 0.0563], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,592][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1091, 0.1774, 0.1564, 0.1977, 0.1150, 0.2443], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,593][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0835, 0.1301, 0.1428, 0.4241, 0.1550, 0.0643], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,593][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.0911, 0.0984, 0.1501, 0.3705, 0.1059, 0.1346, 0.0495],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,593][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.1564, 0.1574, 0.0738, 0.2370, 0.0956, 0.1710, 0.1088],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,598][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0725, 0.0928, 0.1054, 0.2921, 0.1200, 0.2011, 0.1162],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,605][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.1153, 0.1625, 0.1749, 0.2372, 0.1096, 0.0955, 0.1050],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,611][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.1576, 0.2175, 0.1364, 0.1121, 0.1246, 0.0951, 0.1567],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,618][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.1052, 0.1125, 0.1219, 0.2126, 0.1533, 0.1635, 0.1311],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,618][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.1534, 0.0847, 0.0772, 0.0998, 0.1127, 0.1465, 0.3256],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,619][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0985, 0.2093, 0.1471, 0.1863, 0.1289, 0.1484, 0.0815],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,619][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0886, 0.1154, 0.1659, 0.3453, 0.1228, 0.1084, 0.0536],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,619][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.0957, 0.1656, 0.2926, 0.1717, 0.0357, 0.1324, 0.1064],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,620][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0955, 0.1694, 0.1627, 0.1308, 0.0990, 0.1485, 0.1942],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,620][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0579, 0.0593, 0.1549, 0.5059, 0.1101, 0.0876, 0.0243],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,622][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:58,624][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[2],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:58,627][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[5],
        [1],
        [3],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:06:58,630][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[1989],
        [1695],
        [1558],
        [1518],
        [1617],
        [1838],
        [1846]], device='cuda:0')
[2024-07-23 21:06:58,633][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[10515],
        [12048],
        [12605],
        [13283],
        [12974],
        [18945],
        [17487]], device='cuda:0')
[2024-07-23 21:06:58,636][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[8781],
        [4817],
        [3520],
        [3435],
        [3613],
        [3492],
        [3826]], device='cuda:0')
[2024-07-23 21:06:58,639][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[1485],
        [2580],
        [1838],
        [ 669],
        [ 769],
        [ 781],
        [ 882]], device='cuda:0')
[2024-07-23 21:06:58,642][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[1787],
        [1906],
        [2661],
        [3675],
        [4288],
        [4131],
        [3494]], device='cuda:0')
[2024-07-23 21:06:58,645][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[1271],
        [3215],
        [3546],
        [4578],
        [3300],
        [2716],
        [2470]], device='cuda:0')
[2024-07-23 21:06:58,648][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 1706],
        [11753],
        [10827],
        [11082],
        [14572],
        [19466],
        [16899]], device='cuda:0')
[2024-07-23 21:06:58,650][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[1096],
        [ 891],
        [1008],
        [1421],
        [1516],
        [1571],
        [1219]], device='cuda:0')
[2024-07-23 21:06:58,650][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[1506],
        [2826],
        [3369],
        [3369],
        [3815],
        [3942],
        [4462]], device='cuda:0')
[2024-07-23 21:06:58,651][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[11972],
        [15261],
        [16837],
        [21839],
        [23347],
        [21719],
        [18642]], device='cuda:0')
[2024-07-23 21:06:58,652][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[10711],
        [ 6519],
        [ 4429],
        [ 6609],
        [ 8085],
        [ 4788],
        [ 4790]], device='cuda:0')
[2024-07-23 21:06:58,653][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 419],
        [ 394],
        [1538],
        [1679],
        [1734],
        [1367],
        [1586]], device='cuda:0')
[2024-07-23 21:06:58,654][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 5],
        [ 4],
        [11],
        [ 5],
        [ 2],
        [ 1],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:58,657][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[3769],
        [4135],
        [3360],
        [3000],
        [2957],
        [2698],
        [2713]], device='cuda:0')
[2024-07-23 21:06:58,660][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[404],
        [ 85],
        [ 62],
        [ 85],
        [ 63],
        [ 12],
        [ 23]], device='cuda:0')
[2024-07-23 21:06:58,663][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[19],
        [32],
        [29],
        [54],
        [24],
        [30],
        [26]], device='cuda:0')
[2024-07-23 21:06:58,666][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 58],
        [117],
        [177],
        [556],
        [785],
        [737],
        [612]], device='cuda:0')
[2024-07-23 21:06:58,669][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[147],
        [284],
        [130],
        [130],
        [306],
        [190],
        [126]], device='cuda:0')
[2024-07-23 21:06:58,672][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[2511],
        [1681],
        [1225],
        [ 936],
        [1090],
        [1068],
        [1206]], device='cuda:0')
[2024-07-23 21:06:58,675][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[470],
        [ 22],
        [ 45],
        [ 34],
        [ 41],
        [  8],
        [ 13]], device='cuda:0')
[2024-07-23 21:06:58,678][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[213],
        [558],
        [640],
        [598],
        [579],
        [500],
        [551]], device='cuda:0')
[2024-07-23 21:06:58,681][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[6660],
        [6271],
        [7358],
        [4802],
        [3183],
        [3144],
        [1691]], device='cuda:0')
[2024-07-23 21:06:58,681][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[136],
        [196],
        [217],
        [440],
        [929],
        [817],
        [450]], device='cuda:0')
[2024-07-23 21:06:58,682][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 370],
        [ 391],
        [ 355],
        [ 449],
        [ 809],
        [ 863],
        [1022]], device='cuda:0')
[2024-07-23 21:06:58,683][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[1300],
        [1832],
        [2248],
        [2649],
        [2959],
        [2730],
        [3027]], device='cuda:0')
[2024-07-23 21:06:58,683][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 8176],
        [10178],
        [10559],
        [ 8514],
        [ 7547],
        [12161],
        [10873]], device='cuda:0')
[2024-07-23 21:06:58,684][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[32863],
        [25318],
        [18681],
        [24525],
        [29004],
        [37431],
        [41731]], device='cuda:0')
[2024-07-23 21:06:58,686][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[36],
        [36],
        [36],
        [36],
        [36],
        [36],
        [36]], device='cuda:0')
[2024-07-23 21:06:58,735][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:58,739][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,743][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,746][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,752][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,753][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,754][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,754][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,754][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,754][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,755][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,755][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,755][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,756][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.6699, 0.3301], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,756][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.2600, 0.7400], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,760][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.3772, 0.6228], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,766][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.5774, 0.4226], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,773][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.4660, 0.5340], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,780][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.4616, 0.5384], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,781][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.3858, 0.6142], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,782][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.1734, 0.8266], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,782][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.4841, 0.5159], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,782][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.5057, 0.4943], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,783][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.7155, 0.2845], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,783][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.2749, 0.7251], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,783][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.3824, 0.4350, 0.1826], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,784][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.1694, 0.5603, 0.2704], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,784][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.1329, 0.4193, 0.4478], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,786][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.3030, 0.2611, 0.4359], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,793][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.2698, 0.2994, 0.4308], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,799][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.4006, 0.3306, 0.2687], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,807][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.2226, 0.4416, 0.3357], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,809][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.1002, 0.6120, 0.2878], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,809][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.2371, 0.4789, 0.2841], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,810][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.3114, 0.3549, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,810][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.2474, 0.3251, 0.4275], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,810][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.1493, 0.5082, 0.3424], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:58,811][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.2764, 0.1765, 0.1976, 0.3496], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,811][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.1176, 0.1851, 0.1292, 0.5681], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,812][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.0825, 0.1787, 0.1561, 0.5828], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,812][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.2228, 0.1652, 0.2287, 0.3833], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,816][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.1510, 0.1321, 0.2032, 0.5136], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,822][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.2956, 0.3514, 0.1797, 0.1733], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,829][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.2355, 0.2782, 0.2856, 0.2008], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,835][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.1153, 0.4099, 0.2363, 0.2385], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,837][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.1894, 0.3040, 0.2575, 0.2491], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,837][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.2060, 0.1421, 0.2470, 0.4049], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,838][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.1427, 0.1383, 0.5086, 0.2104], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,838][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.1416, 0.2549, 0.3654, 0.2381], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:58,838][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.1309, 0.1319, 0.1231, 0.5519, 0.0622], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,839][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.0915, 0.1597, 0.1964, 0.3417, 0.2106], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,839][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0340, 0.0712, 0.1993, 0.6343, 0.0613], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,839][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.1474, 0.1243, 0.2284, 0.3252, 0.1747], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,840][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.1277, 0.0830, 0.2133, 0.5082, 0.0678], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,844][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.1201, 0.2891, 0.2328, 0.1802, 0.1778], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,850][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.1422, 0.2251, 0.1829, 0.2419, 0.2079], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,857][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0918, 0.2214, 0.1420, 0.3683, 0.1765], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,863][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1209, 0.1626, 0.1555, 0.4240, 0.1370], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,865][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.1090, 0.1499, 0.2262, 0.4016, 0.1133], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,865][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.0369, 0.0713, 0.3122, 0.5257, 0.0539], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,866][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1125, 0.1858, 0.2680, 0.3215, 0.1121], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:58,866][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.1571, 0.1416, 0.1145, 0.4041, 0.1304, 0.0522], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,866][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.0726, 0.0886, 0.0860, 0.4222, 0.1047, 0.2260], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,867][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0509, 0.0927, 0.0900, 0.6245, 0.0679, 0.0741], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,867][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.1490, 0.0876, 0.1898, 0.2987, 0.1576, 0.1172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,867][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.1029, 0.1190, 0.1881, 0.4159, 0.0769, 0.0971], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,868][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.1309, 0.2909, 0.1862, 0.1592, 0.1234, 0.1094], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,872][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.1132, 0.1508, 0.1529, 0.1790, 0.1851, 0.2189], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,880][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.0735, 0.2243, 0.1109, 0.2863, 0.1406, 0.1644], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,886][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.0799, 0.1623, 0.0940, 0.3650, 0.1418, 0.1569], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,893][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1345, 0.1675, 0.1243, 0.3525, 0.1479, 0.0734], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,893][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1237, 0.0516, 0.1976, 0.4457, 0.1010, 0.0804], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,893][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0871, 0.2084, 0.1555, 0.2177, 0.0736, 0.2577], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:58,894][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.1519, 0.1344, 0.1091, 0.3819, 0.1037, 0.0745, 0.0446],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,894][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.0855, 0.1394, 0.1223, 0.2844, 0.1000, 0.1466, 0.1217],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,895][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0345, 0.0746, 0.1177, 0.3070, 0.0575, 0.1825, 0.2262],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,895][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.1184, 0.0856, 0.1784, 0.3127, 0.1113, 0.0954, 0.0983],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,895][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.0609, 0.0760, 0.0944, 0.4461, 0.0471, 0.1261, 0.1493],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,896][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.1236, 0.2420, 0.1750, 0.1459, 0.1005, 0.1014, 0.1116],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,900][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0838, 0.1408, 0.1281, 0.1427, 0.1749, 0.2035, 0.1262],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,906][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0414, 0.1537, 0.1119, 0.1621, 0.1707, 0.2481, 0.1121],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,906][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0662, 0.1378, 0.0644, 0.2124, 0.1635, 0.2714, 0.0842],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,906][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0758, 0.0741, 0.0831, 0.5309, 0.0600, 0.0911, 0.0850],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,907][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0646, 0.0335, 0.2660, 0.2336, 0.0577, 0.2703, 0.0744],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,914][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0941, 0.1363, 0.1631, 0.1537, 0.0522, 0.1797, 0.2208],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:58,963][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:58,963][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,964][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,964][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,964][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,964][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,965][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,966][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,969][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,972][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,976][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,980][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,983][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:58,989][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.6699, 0.3301], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,991][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.2600, 0.7400], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,991][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.3772, 0.6228], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,992][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.5774, 0.4226], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,992][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.4660, 0.5340], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,992][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.4616, 0.5384], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,993][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.3858, 0.6142], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,993][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.1734, 0.8266], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,993][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.4841, 0.5159], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,994][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.5057, 0.4943], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:58,995][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.7155, 0.2845], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,001][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.2749, 0.7251], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,007][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.3824, 0.4350, 0.1826], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,014][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.1694, 0.5603, 0.2704], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,019][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.1329, 0.4193, 0.4478], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,019][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.3030, 0.2611, 0.4359], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,019][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.2698, 0.2994, 0.4308], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,020][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.4006, 0.3306, 0.2687], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,020][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.2226, 0.4416, 0.3357], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,020][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.1002, 0.6120, 0.2878], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,021][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.2371, 0.4789, 0.2841], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,021][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.3114, 0.3549, 0.3337], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,021][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.2474, 0.3251, 0.4275], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,023][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.1493, 0.5082, 0.3424], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,029][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.2764, 0.1765, 0.1976, 0.3496], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,036][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.1176, 0.1851, 0.1292, 0.5681], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,042][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.0825, 0.1787, 0.1561, 0.5828], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,046][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.2228, 0.1652, 0.2287, 0.3833], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,047][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.1510, 0.1321, 0.2032, 0.5136], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,047][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.2956, 0.3514, 0.1797, 0.1733], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,047][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.2355, 0.2782, 0.2856, 0.2008], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,048][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.1153, 0.4099, 0.2363, 0.2385], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,048][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.1894, 0.3040, 0.2575, 0.2491], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,048][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.2060, 0.1421, 0.2470, 0.4049], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,049][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.1427, 0.1383, 0.5086, 0.2104], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,051][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.1416, 0.2549, 0.3654, 0.2381], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,058][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.1309, 0.1319, 0.1231, 0.5519, 0.0622], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,064][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.0915, 0.1597, 0.1964, 0.3417, 0.2106], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,071][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0340, 0.0712, 0.1993, 0.6343, 0.0613], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,073][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.1474, 0.1243, 0.2284, 0.3252, 0.1747], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,074][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.1277, 0.0830, 0.2133, 0.5082, 0.0678], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,074][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.1201, 0.2891, 0.2328, 0.1802, 0.1778], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,075][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.1422, 0.2251, 0.1829, 0.2419, 0.2079], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,075][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0918, 0.2214, 0.1420, 0.3683, 0.1765], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,075][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1209, 0.1626, 0.1555, 0.4240, 0.1370], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,076][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.1090, 0.1499, 0.2262, 0.4016, 0.1133], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,076][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.0369, 0.0713, 0.3122, 0.5257, 0.0539], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,076][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1125, 0.1858, 0.2680, 0.3215, 0.1121], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,077][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.1571, 0.1416, 0.1145, 0.4041, 0.1304, 0.0522], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,079][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.0726, 0.0886, 0.0860, 0.4222, 0.1047, 0.2260], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,086][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0509, 0.0927, 0.0900, 0.6245, 0.0679, 0.0741], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,092][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.1490, 0.0876, 0.1898, 0.2987, 0.1576, 0.1172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,099][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.1029, 0.1190, 0.1881, 0.4159, 0.0769, 0.0971], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,102][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.1309, 0.2909, 0.1862, 0.1592, 0.1234, 0.1094], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,102][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.1132, 0.1508, 0.1529, 0.1790, 0.1851, 0.2189], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,102][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.0735, 0.2243, 0.1109, 0.2863, 0.1406, 0.1644], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,103][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.0799, 0.1623, 0.0940, 0.3650, 0.1418, 0.1569], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,103][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1345, 0.1675, 0.1243, 0.3525, 0.1479, 0.0734], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,103][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1237, 0.0516, 0.1976, 0.4457, 0.1010, 0.0804], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,104][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0871, 0.2084, 0.1555, 0.2177, 0.0736, 0.2577], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,108][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.1519, 0.1344, 0.1091, 0.3819, 0.1037, 0.0745, 0.0446],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,114][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.0855, 0.1394, 0.1223, 0.2844, 0.1000, 0.1466, 0.1217],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,121][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0345, 0.0746, 0.1177, 0.3070, 0.0575, 0.1825, 0.2262],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,127][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.1184, 0.0856, 0.1784, 0.3127, 0.1113, 0.0954, 0.0983],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,128][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.0609, 0.0760, 0.0944, 0.4461, 0.0471, 0.1261, 0.1493],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,129][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.1236, 0.2420, 0.1750, 0.1459, 0.1005, 0.1014, 0.1116],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,129][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0838, 0.1408, 0.1281, 0.1427, 0.1749, 0.2035, 0.1262],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,130][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0414, 0.1537, 0.1119, 0.1621, 0.1707, 0.2481, 0.1121],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,130][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0662, 0.1378, 0.0644, 0.2124, 0.1635, 0.2714, 0.0842],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,130][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.0758, 0.0741, 0.0831, 0.5309, 0.0600, 0.0911, 0.0850],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,131][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0646, 0.0335, 0.2660, 0.2336, 0.0577, 0.2703, 0.0744],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,131][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0941, 0.1363, 0.1631, 0.1537, 0.0522, 0.1797, 0.2208],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,132][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:59,134][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[98],
        [ 1],
        [14],
        [ 1],
        [ 1],
        [ 1],
        [ 1]], device='cuda:0')
[2024-07-23 21:06:59,137][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[203],
        [  7],
        [ 25],
        [  1],
        [  1],
        [  1],
        [  4]], device='cuda:0')
[2024-07-23 21:06:59,140][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 5065],
        [11744],
        [17012],
        [13714],
        [12593],
        [15752],
        [16889]], device='cuda:0')
[2024-07-23 21:06:59,143][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[2358],
        [5486],
        [5712],
        [5887],
        [4349],
        [3326],
        [3749]], device='cuda:0')
[2024-07-23 21:06:59,147][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[19203],
        [13172],
        [16391],
        [10909],
        [10639],
        [ 9337],
        [13983]], device='cuda:0')
[2024-07-23 21:06:59,150][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[209],
        [152],
        [104],
        [238],
        [267],
        [209],
        [277]], device='cuda:0')
[2024-07-23 21:06:59,153][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[15098],
        [12424],
        [ 9118],
        [ 5349],
        [ 4701],
        [ 4776],
        [ 4140]], device='cuda:0')
[2024-07-23 21:06:59,156][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 687],
        [3343],
        [2783],
        [2896],
        [3859],
        [3149],
        [2766]], device='cuda:0')
[2024-07-23 21:06:59,159][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[3613],
        [1633],
        [1023],
        [1436],
        [1850],
        [1823],
        [1633]], device='cuda:0')
[2024-07-23 21:06:59,161][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[12932],
        [ 5889],
        [ 6105],
        [10425],
        [12916],
        [14712],
        [14550]], device='cuda:0')
[2024-07-23 21:06:59,161][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 980],
        [1426],
        [1976],
        [2014],
        [2630],
        [4275],
        [5405]], device='cuda:0')
[2024-07-23 21:06:59,162][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 9870],
        [ 7248],
        [ 9163],
        [14885],
        [15093],
        [14550],
        [15768]], device='cuda:0')
[2024-07-23 21:06:59,163][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[5984],
        [8885],
        [5637],
        [4075],
        [3746],
        [3801],
        [2772]], device='cuda:0')
[2024-07-23 21:06:59,164][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[22596],
        [23480],
        [ 9892],
        [ 4411],
        [ 4486],
        [ 6389],
        [ 9549]], device='cuda:0')
[2024-07-23 21:06:59,167][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[27],
        [87],
        [77],
        [94],
        [43],
        [51],
        [ 6]], device='cuda:0')
[2024-07-23 21:06:59,170][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[1415],
        [1706],
        [1081],
        [1885],
        [2805],
        [1308],
        [1068]], device='cuda:0')
[2024-07-23 21:06:59,173][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[5139],
        [ 646],
        [ 594],
        [ 741],
        [ 409],
        [ 339],
        [ 325]], device='cuda:0')
[2024-07-23 21:06:59,176][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[200],
        [163],
        [121],
        [125],
        [137],
        [122],
        [251]], device='cuda:0')
[2024-07-23 21:06:59,179][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[114],
        [ 68],
        [608],
        [232],
        [343],
        [316],
        [325]], device='cuda:0')
[2024-07-23 21:06:59,182][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[4467],
        [ 847],
        [1361],
        [3173],
        [4157],
        [3005],
        [2444]], device='cuda:0')
[2024-07-23 21:06:59,185][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[1275],
        [ 637],
        [ 487],
        [ 890],
        [ 690],
        [1501],
        [2006]], device='cuda:0')
[2024-07-23 21:06:59,188][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 67],
        [ 78],
        [259],
        [537],
        [767],
        [430],
        [411]], device='cuda:0')
[2024-07-23 21:06:59,191][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[51],
        [13],
        [20],
        [34],
        [46],
        [45],
        [28]], device='cuda:0')
[2024-07-23 21:06:59,192][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[584],
        [632],
        [265],
        [546],
        [731],
        [499],
        [233]], device='cuda:0')
[2024-07-23 21:06:59,193][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[2116],
        [  92],
        [1734],
        [ 986],
        [ 924],
        [ 562],
        [ 208]], device='cuda:0')
[2024-07-23 21:06:59,193][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[2928],
        [ 853],
        [ 894],
        [2914],
        [2804],
        [1611],
        [2230]], device='cuda:0')
[2024-07-23 21:06:59,194][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[  75],
        [ 649],
        [ 296],
        [ 401],
        [ 718],
        [1673],
        [2297]], device='cuda:0')
[2024-07-23 21:06:59,195][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 7007],
        [14898],
        [ 9229],
        [ 5933],
        [ 4725],
        [ 8172],
        [ 7655]], device='cuda:0')
[2024-07-23 21:06:59,196][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[43162],
        [43705],
        [41674],
        [38944],
        [34298],
        [37584],
        [48818]], device='cuda:0')
[2024-07-23 21:06:59,199][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[11],
        [11],
        [11],
        [11],
        [11],
        [11],
        [11]], device='cuda:0')
[2024-07-23 21:06:59,251][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:59,252][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,253][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,253][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,253][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,254][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,254][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,254][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,254][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,255][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,255][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,255][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,257][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,260][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.2608, 0.7392], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,261][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.5358, 0.4642], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,261][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.1459, 0.8541], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,261][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.2022, 0.7978], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,262][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.5183, 0.4817], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,262][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.4782, 0.5218], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,262][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.2819, 0.7181], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,263][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.4151, 0.5849], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,263][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.3912, 0.6088], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,263][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.3116, 0.6884], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,264][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.3929, 0.6071], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,264][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.3277, 0.6723], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,264][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.1722, 0.5061, 0.3217], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,265][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.1859, 0.3358, 0.4783], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,265][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.1009, 0.7586, 0.1406], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,265][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.1198, 0.6242, 0.2560], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,266][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.1489, 0.2173, 0.6338], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,266][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.2585, 0.3596, 0.3819], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,266][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.1505, 0.4200, 0.4296], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,267][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.2256, 0.3481, 0.4263], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,267][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.2961, 0.4491, 0.2548], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,267][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.2243, 0.4188, 0.3569], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,268][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.1983, 0.3490, 0.4527], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,268][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.2373, 0.5109, 0.2517], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,268][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.1956, 0.4615, 0.2196, 0.1234], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,269][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.0806, 0.0710, 0.3018, 0.5466], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,269][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.0686, 0.3358, 0.1072, 0.4885], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,269][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.1036, 0.3173, 0.1488, 0.4302], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,270][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.1764, 0.2091, 0.3793, 0.2351], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,270][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.1630, 0.2074, 0.2612, 0.3685], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,270][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.1374, 0.1979, 0.3461, 0.3187], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,271][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.1881, 0.2602, 0.2168, 0.3349], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,271][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.1913, 0.3254, 0.2243, 0.2590], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,276][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.1893, 0.3067, 0.2537, 0.2503], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,282][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.1330, 0.2303, 0.2356, 0.4011], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,289][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.1447, 0.3277, 0.1686, 0.3591], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,292][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.1209, 0.2744, 0.2108, 0.0985, 0.2954], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,292][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.0661, 0.0562, 0.1816, 0.6574, 0.0386], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,292][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0759, 0.3929, 0.1021, 0.3306, 0.0985], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,293][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.0736, 0.2318, 0.1847, 0.3776, 0.1322], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,293][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.0996, 0.1327, 0.3341, 0.2902, 0.1434], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,293][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.1132, 0.2270, 0.2522, 0.3358, 0.0717], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,294][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.0651, 0.1734, 0.2081, 0.4174, 0.1361], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,294][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.0817, 0.1792, 0.2286, 0.4366, 0.0739], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,295][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1627, 0.2564, 0.2162, 0.2527, 0.1120], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,298][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.1291, 0.2877, 0.2032, 0.1960, 0.1840], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,305][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1136, 0.1626, 0.2454, 0.2718, 0.2065], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,312][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1012, 0.2260, 0.1701, 0.3009, 0.2019], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,318][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.1171, 0.2371, 0.2968, 0.1012, 0.2185, 0.0292], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,320][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.0498, 0.0587, 0.1154, 0.4535, 0.0363, 0.2864], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,320][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0539, 0.2667, 0.1098, 0.2998, 0.1001, 0.1696], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,320][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.0532, 0.1577, 0.1223, 0.4228, 0.0913, 0.1527], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,321][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.0855, 0.1146, 0.2464, 0.3192, 0.1209, 0.1135], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,321][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.0993, 0.1855, 0.2191, 0.3488, 0.0787, 0.0685], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,321][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.0796, 0.1601, 0.1987, 0.2946, 0.0910, 0.1759], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,322][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.0845, 0.1552, 0.1655, 0.4181, 0.0754, 0.1013], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,322][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.1017, 0.1724, 0.2041, 0.2338, 0.1064, 0.1816], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,322][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.0961, 0.2566, 0.1122, 0.2086, 0.1077, 0.2189], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,326][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.0996, 0.1561, 0.1919, 0.2302, 0.1303, 0.1919], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,332][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0728, 0.1770, 0.1135, 0.2270, 0.1424, 0.2672], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,340][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.1136, 0.2691, 0.1765, 0.0682, 0.2876, 0.0146, 0.0704],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,346][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.0349, 0.0305, 0.1452, 0.2558, 0.0370, 0.4758, 0.0209],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,347][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0392, 0.1961, 0.0768, 0.3350, 0.0646, 0.1485, 0.1397],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,348][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0747, 0.2667, 0.0821, 0.2516, 0.0914, 0.1030, 0.1306],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,348][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.0690, 0.1095, 0.2240, 0.2206, 0.1396, 0.1350, 0.1022],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,349][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.0904, 0.1555, 0.1856, 0.2636, 0.0752, 0.1022, 0.1276],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,349][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0587, 0.1370, 0.1037, 0.2559, 0.0997, 0.1395, 0.2056],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,349][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.0604, 0.1624, 0.1584, 0.3609, 0.0623, 0.0749, 0.1206],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,350][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0957, 0.2131, 0.1368, 0.2016, 0.0866, 0.1525, 0.1137],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,354][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0797, 0.1455, 0.1386, 0.1255, 0.0977, 0.2424, 0.1706],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,360][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0767, 0.1349, 0.1169, 0.2026, 0.1293, 0.1191, 0.2205],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,367][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0662, 0.1506, 0.0820, 0.1343, 0.1336, 0.1751, 0.2583],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,431][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:59,435][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,439][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,443][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,444][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,445][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,445][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,445][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,446][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,446][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,446][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,447][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,447][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,447][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.2608, 0.7392], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,448][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.5358, 0.4642], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,452][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.1459, 0.8541], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,459][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.2022, 0.7978], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,466][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.5183, 0.4817], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,473][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.4782, 0.5218], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,475][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.2819, 0.7181], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,476][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.4151, 0.5849], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,476][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.3912, 0.6088], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,476][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.3116, 0.6884], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,477][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.3929, 0.6071], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,477][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.3277, 0.6723], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,477][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.1722, 0.5061, 0.3217], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,478][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.1859, 0.3358, 0.4783], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,478][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.1009, 0.7586, 0.1406], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,478][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.1198, 0.6242, 0.2560], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,482][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.1489, 0.2173, 0.6338], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,488][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.2585, 0.3596, 0.3819], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,496][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.1505, 0.4200, 0.4296], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,502][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.2256, 0.3481, 0.4263], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,504][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.2961, 0.4491, 0.2548], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,504][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.2243, 0.4188, 0.3569], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,504][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.1983, 0.3490, 0.4527], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,504][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.2373, 0.5109, 0.2517], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,505][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.1956, 0.4615, 0.2196, 0.1234], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,505][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.0806, 0.0710, 0.3018, 0.5466], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,506][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.0686, 0.3358, 0.1072, 0.4885], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,506][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.1036, 0.3173, 0.1488, 0.4302], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,506][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.1764, 0.2091, 0.3793, 0.2351], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,508][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.1630, 0.2074, 0.2612, 0.3685], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,516][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.1374, 0.1979, 0.3461, 0.3187], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,522][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.1881, 0.2602, 0.2168, 0.3349], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,529][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.1913, 0.3254, 0.2243, 0.2590], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,532][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.1893, 0.3067, 0.2537, 0.2503], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,532][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.1330, 0.2303, 0.2356, 0.4011], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,532][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.1447, 0.3277, 0.1686, 0.3591], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,533][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.1209, 0.2744, 0.2108, 0.0985, 0.2954], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,533][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.0661, 0.0562, 0.1816, 0.6574, 0.0386], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,533][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0759, 0.3929, 0.1021, 0.3306, 0.0985], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,534][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.0736, 0.2318, 0.1847, 0.3776, 0.1322], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,534][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.0996, 0.1327, 0.3341, 0.2902, 0.1434], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,534][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.1132, 0.2270, 0.2522, 0.3358, 0.0717], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,538][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.0651, 0.1734, 0.2081, 0.4174, 0.1361], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,545][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.0817, 0.1792, 0.2286, 0.4366, 0.0739], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,552][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1627, 0.2564, 0.2162, 0.2527, 0.1120], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,558][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.1291, 0.2877, 0.2032, 0.1960, 0.1840], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,560][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1136, 0.1626, 0.2454, 0.2718, 0.2065], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,560][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1012, 0.2260, 0.1701, 0.3009, 0.2019], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,561][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.1171, 0.2371, 0.2968, 0.1012, 0.2185, 0.0292], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,561][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.0498, 0.0587, 0.1154, 0.4535, 0.0363, 0.2864], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,562][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0539, 0.2667, 0.1098, 0.2998, 0.1001, 0.1696], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,562][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.0532, 0.1577, 0.1223, 0.4228, 0.0913, 0.1527], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,562][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.0855, 0.1146, 0.2464, 0.3192, 0.1209, 0.1135], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,563][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.0993, 0.1855, 0.2191, 0.3488, 0.0787, 0.0685], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,563][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.0796, 0.1601, 0.1987, 0.2946, 0.0910, 0.1759], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,567][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.0845, 0.1552, 0.1655, 0.4181, 0.0754, 0.1013], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,573][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.1017, 0.1724, 0.2041, 0.2338, 0.1064, 0.1816], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,581][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.0961, 0.2566, 0.1122, 0.2086, 0.1077, 0.2189], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,587][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.0996, 0.1561, 0.1919, 0.2302, 0.1303, 0.1919], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,589][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0728, 0.1770, 0.1135, 0.2270, 0.1424, 0.2672], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,589][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.1136, 0.2691, 0.1765, 0.0682, 0.2876, 0.0146, 0.0704],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,589][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.0349, 0.0305, 0.1452, 0.2558, 0.0370, 0.4758, 0.0209],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,590][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0392, 0.1961, 0.0768, 0.3350, 0.0646, 0.1485, 0.1397],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,590][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0747, 0.2667, 0.0821, 0.2516, 0.0914, 0.1030, 0.1306],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,590][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.0690, 0.1095, 0.2240, 0.2206, 0.1396, 0.1350, 0.1022],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,591][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.0904, 0.1555, 0.1856, 0.2636, 0.0752, 0.1022, 0.1276],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,591][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0587, 0.1370, 0.1037, 0.2559, 0.0997, 0.1395, 0.2056],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,592][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.0604, 0.1624, 0.1584, 0.3609, 0.0623, 0.0749, 0.1206],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,596][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0957, 0.2131, 0.1368, 0.2016, 0.0866, 0.1525, 0.1137],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,604][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.0797, 0.1455, 0.1386, 0.1255, 0.0977, 0.2424, 0.1706],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,610][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0767, 0.1349, 0.1169, 0.2026, 0.1293, 0.1191, 0.2205],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,617][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0662, 0.1506, 0.0820, 0.1343, 0.1336, 0.1751, 0.2583],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,618][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:59,619][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[757],
        [ 20],
        [ 49],
        [  2],
        [  1],
        [  1],
        [ 13]], device='cuda:0')
[2024-07-23 21:06:59,620][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[1169],
        [  24],
        [  68],
        [   2],
        [   1],
        [   2],
        [  39]], device='cuda:0')
[2024-07-23 21:06:59,620][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[  5],
        [160],
        [ 85],
        [ 97],
        [ 44],
        [ 45],
        [ 50]], device='cuda:0')
[2024-07-23 21:06:59,622][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[20189],
        [ 7675],
        [ 5212],
        [28667],
        [31052],
        [32832],
        [27502]], device='cuda:0')
[2024-07-23 21:06:59,625][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[26684],
        [10257],
        [ 9638],
        [ 8891],
        [10344],
        [ 9860],
        [11647]], device='cuda:0')
[2024-07-23 21:06:59,628][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 157],
        [3017],
        [1597],
        [1980],
        [1156],
        [1297],
        [1636]], device='cuda:0')
[2024-07-23 21:06:59,631][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[1344],
        [1182],
        [4138],
        [2108],
        [2133],
        [2021],
        [2089]], device='cuda:0')
[2024-07-23 21:06:59,634][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[7508],
        [6029],
        [5805],
        [4684],
        [4698],
        [4194],
        [3687]], device='cuda:0')
[2024-07-23 21:06:59,637][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 2732],
        [12121],
        [ 8496],
        [ 4600],
        [ 4177],
        [ 3831],
        [ 3431]], device='cuda:0')
[2024-07-23 21:06:59,640][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[12782],
        [14642],
        [12624],
        [12360],
        [12585],
        [14008],
        [14551]], device='cuda:0')
[2024-07-23 21:06:59,644][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[1660],
        [ 630],
        [ 989],
        [ 749],
        [ 857],
        [1127],
        [ 731]], device='cuda:0')
[2024-07-23 21:06:59,647][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[14522],
        [ 9911],
        [10985],
        [13619],
        [12592],
        [13995],
        [14284]], device='cuda:0')
[2024-07-23 21:06:59,650][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[3376],
        [2408],
        [3036],
        [4115],
        [4285],
        [3793],
        [4996]], device='cuda:0')
[2024-07-23 21:06:59,650][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[1734],
        [4318],
        [3554],
        [2924],
        [1821],
        [1197],
        [1412]], device='cuda:0')
[2024-07-23 21:06:59,651][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[30],
        [89],
        [34],
        [46],
        [ 8],
        [16],
        [ 2]], device='cuda:0')
[2024-07-23 21:06:59,652][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[31165],
        [25869],
        [23720],
        [22946],
        [24558],
        [23047],
        [24910]], device='cuda:0')
[2024-07-23 21:06:59,652][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 162],
        [1857],
        [2198],
        [  44],
        [  25],
        [  40],
        [  98]], device='cuda:0')
[2024-07-23 21:06:59,653][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 50],
        [ 77],
        [ 97],
        [289],
        [144],
        [196],
        [189]], device='cuda:0')
[2024-07-23 21:06:59,654][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[8361],
        [2402],
        [7113],
        [4876],
        [6704],
        [5288],
        [5262]], device='cuda:0')
[2024-07-23 21:06:59,657][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[487],
        [430],
        [290],
        [689],
        [748],
        [739],
        [848]], device='cuda:0')
[2024-07-23 21:06:59,661][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[231],
        [112],
        [127],
        [150],
        [105],
        [111],
        [ 99]], device='cuda:0')
[2024-07-23 21:06:59,664][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[  47],
        [  75],
        [1004],
        [1039],
        [ 876],
        [ 537],
        [ 207]], device='cuda:0')
[2024-07-23 21:06:59,667][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[184],
        [ 44],
        [156],
        [235],
        [173],
        [124],
        [ 64]], device='cuda:0')
[2024-07-23 21:06:59,670][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[2431],
        [ 452],
        [ 469],
        [ 100],
        [  91],
        [ 528],
        [ 377]], device='cuda:0')
[2024-07-23 21:06:59,673][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[18],
        [82],
        [20],
        [ 7],
        [ 7],
        [16],
        [18]], device='cuda:0')
[2024-07-23 21:06:59,676][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[  4],
        [159],
        [112],
        [ 32],
        [ 27],
        [ 57],
        [ 44]], device='cuda:0')
[2024-07-23 21:06:59,679][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[5043],
        [3942],
        [2542],
        [1003],
        [2996],
        [2098],
        [2922]], device='cuda:0')
[2024-07-23 21:06:59,682][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[42980],
        [43116],
        [36721],
        [42406],
        [41828],
        [40933],
        [42267]], device='cuda:0')
[2024-07-23 21:06:59,683][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[43373],
        [43766],
        [43670],
        [47546],
        [49147],
        [47428],
        [48592]], device='cuda:0')
[2024-07-23 21:06:59,683][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[38],
        [38],
        [38],
        [38],
        [38],
        [38],
        [38]], device='cuda:0')
[2024-07-23 21:06:59,739][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:59,744][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,745][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,745][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,745][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,746][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,746][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,746][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,746][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,747][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,747][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,747][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,749][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,761][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.3704, 0.6296], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,766][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.8642, 0.1358], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,773][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.2598, 0.7402], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,773][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.7147, 0.2853], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,774][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.0593, 0.9407], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,774][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.1016, 0.8984], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,774][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.3791, 0.6209], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,775][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.6003, 0.3997], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,775][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.4399, 0.5601], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,775][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.5942, 0.4058], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,776][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.4990, 0.5010], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,776][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.1593, 0.8407], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:06:59,780][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.2132, 0.3355, 0.4513], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,788][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.7917, 0.0892, 0.1191], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,794][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.2483, 0.5558, 0.1959], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,801][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.4152, 0.1682, 0.4166], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,801][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.0416, 0.4554, 0.5030], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,801][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.0609, 0.5745, 0.3646], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,802][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.2544, 0.2976, 0.4480], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,802][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.3179, 0.2582, 0.4240], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,802][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.3035, 0.3769, 0.3196], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,803][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.4867, 0.2160, 0.2973], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,803][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.3495, 0.3024, 0.3481], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,803][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.1773, 0.6596, 0.1631], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:06:59,804][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.1636, 0.1658, 0.2824, 0.3882], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,807][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.5418, 0.0822, 0.1278, 0.2481], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,814][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.2141, 0.4174, 0.1789, 0.1897], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,821][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.3234, 0.1782, 0.3248, 0.1737], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,827][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.0287, 0.2266, 0.1838, 0.5609], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,828][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.0800, 0.4225, 0.2941, 0.2035], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,829][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.0973, 0.1296, 0.2175, 0.5557], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,829][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.1747, 0.1543, 0.3256, 0.3453], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,830][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.2187, 0.3060, 0.2944, 0.1809], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,830][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.2744, 0.1656, 0.3104, 0.2496], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,830][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.1832, 0.2422, 0.2646, 0.3100], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,831][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.0899, 0.4503, 0.1089, 0.3510], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:06:59,831][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.0809, 0.1638, 0.1938, 0.2193, 0.3423], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,831][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.4055, 0.0614, 0.1056, 0.2728, 0.1547], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,835][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0863, 0.3139, 0.1019, 0.1841, 0.3139], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,841][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.1856, 0.1538, 0.4102, 0.2027, 0.0476], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,848][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.0155, 0.1890, 0.1953, 0.5275, 0.0726], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,855][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.0385, 0.3520, 0.2981, 0.1901, 0.1214], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,856][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.0862, 0.1022, 0.1730, 0.5041, 0.1345], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,857][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.1578, 0.1272, 0.2356, 0.3568, 0.1227], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,857][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1495, 0.1307, 0.1539, 0.1101, 0.4559], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,857][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.1354, 0.0698, 0.1182, 0.1525, 0.5242], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,858][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1220, 0.1953, 0.3203, 0.2700, 0.0926], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,858][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.0818, 0.3117, 0.1119, 0.3832, 0.1114], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:06:59,858][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.0566, 0.0993, 0.1410, 0.2069, 0.1918, 0.3044], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,859][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.2738, 0.0378, 0.0465, 0.1536, 0.1067, 0.3815], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,859][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0586, 0.2073, 0.0534, 0.1206, 0.1592, 0.4009], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,863][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.1885, 0.1389, 0.3675, 0.1932, 0.0549, 0.0570], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,869][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.0125, 0.1184, 0.1573, 0.3459, 0.0314, 0.3345], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,876][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.0277, 0.2999, 0.2395, 0.1156, 0.0815, 0.2357], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,882][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.0808, 0.0794, 0.1671, 0.3007, 0.1140, 0.2579], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,884][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1249, 0.1226, 0.2192, 0.2713, 0.1078, 0.1542], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,884][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.1122, 0.1442, 0.1013, 0.1004, 0.3328, 0.2091], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,885][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1425, 0.0314, 0.0463, 0.0787, 0.3741, 0.3271], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,885][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1112, 0.2058, 0.2679, 0.2556, 0.0882, 0.0713], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,885][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0541, 0.2052, 0.0675, 0.1962, 0.0498, 0.4272], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:06:59,886][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.0617, 0.1042, 0.1056, 0.1198, 0.1930, 0.1898, 0.2260],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,886][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.2195, 0.0264, 0.0418, 0.1602, 0.0832, 0.2828, 0.1862],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,887][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0713, 0.1740, 0.0512, 0.0846, 0.1270, 0.2995, 0.1924],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,887][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.2166, 0.1263, 0.2854, 0.1714, 0.0621, 0.0656, 0.0725],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,891][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.0156, 0.1651, 0.0904, 0.2722, 0.0486, 0.1792, 0.2290],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,899][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.0308, 0.2242, 0.1514, 0.0894, 0.0671, 0.1045, 0.3326],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,905][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.0439, 0.0573, 0.0718, 0.2507, 0.0612, 0.1882, 0.3268],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,912][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.1012, 0.0926, 0.1236, 0.3037, 0.0718, 0.1606, 0.1465],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,912][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0686, 0.0965, 0.0637, 0.0612, 0.2759, 0.3254, 0.1087],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,912][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0637, 0.0245, 0.0391, 0.0438, 0.2274, 0.2668, 0.3348],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,913][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.1086, 0.1955, 0.1733, 0.2652, 0.0507, 0.0574, 0.1492],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,913][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0314, 0.1214, 0.0273, 0.1386, 0.0364, 0.3584, 0.2866],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:06:59,969][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:59,974][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,979][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,981][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,981][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,981][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,982][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,982][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,982][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,982][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,983][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,984][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,989][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:06:59,995][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.3704, 0.6296], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,002][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.8642, 0.1358], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,008][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.2598, 0.7402], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,008][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.7147, 0.2853], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,008][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.0593, 0.9407], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,009][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.1016, 0.8984], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,009][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.3791, 0.6209], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,009][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.6003, 0.3997], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,009][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.4399, 0.5601], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,010][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.5942, 0.4058], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,010][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.4990, 0.5010], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,010][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.1593, 0.8407], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,011][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.2132, 0.3355, 0.4513], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,015][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.7917, 0.0892, 0.1191], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,021][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.2483, 0.5558, 0.1959], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,028][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.4152, 0.1682, 0.4166], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,034][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.0416, 0.4554, 0.5030], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,035][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.0609, 0.5745, 0.3646], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,036][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.2544, 0.2976, 0.4480], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,036][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.3179, 0.2582, 0.4240], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,036][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.3035, 0.3769, 0.3196], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,037][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.4867, 0.2160, 0.2973], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,037][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.3495, 0.3024, 0.3481], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,037][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.1773, 0.6596, 0.1631], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,038][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.1636, 0.1658, 0.2824, 0.3882], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,038][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.5418, 0.0822, 0.1278, 0.2481], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,042][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.2141, 0.4174, 0.1789, 0.1897], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,048][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.3234, 0.1782, 0.3248, 0.1737], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,055][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.0287, 0.2266, 0.1838, 0.5609], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,061][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.0800, 0.4225, 0.2941, 0.2035], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,063][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.0973, 0.1296, 0.2175, 0.5557], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,063][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.1747, 0.1543, 0.3256, 0.3453], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,063][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.2187, 0.3060, 0.2944, 0.1809], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,064][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.2744, 0.1656, 0.3104, 0.2496], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,064][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.1832, 0.2422, 0.2646, 0.3100], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,064][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.0899, 0.4503, 0.1089, 0.3510], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,065][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.0809, 0.1638, 0.1938, 0.2193, 0.3423], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,065][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.4055, 0.0614, 0.1056, 0.2728, 0.1547], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,065][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0863, 0.3139, 0.1019, 0.1841, 0.3139], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,069][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.1856, 0.1538, 0.4102, 0.2027, 0.0476], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,075][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.0155, 0.1890, 0.1953, 0.5275, 0.0726], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,081][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.0385, 0.3520, 0.2981, 0.1901, 0.1214], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,088][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.0862, 0.1022, 0.1730, 0.5041, 0.1345], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,090][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.1578, 0.1272, 0.2356, 0.3568, 0.1227], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,090][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1495, 0.1307, 0.1539, 0.1101, 0.4559], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,091][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.1354, 0.0698, 0.1182, 0.1525, 0.5242], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,091][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1220, 0.1953, 0.3203, 0.2700, 0.0926], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,091][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.0818, 0.3117, 0.1119, 0.3832, 0.1114], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,092][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.0566, 0.0993, 0.1410, 0.2069, 0.1918, 0.3044], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,092][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.2738, 0.0378, 0.0465, 0.1536, 0.1067, 0.3815], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,093][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0586, 0.2073, 0.0534, 0.1206, 0.1592, 0.4009], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,093][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.1885, 0.1389, 0.3675, 0.1932, 0.0549, 0.0570], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,097][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.0125, 0.1184, 0.1573, 0.3459, 0.0314, 0.3345], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,104][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.0277, 0.2999, 0.2395, 0.1156, 0.0815, 0.2357], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,110][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.0808, 0.0794, 0.1671, 0.3007, 0.1140, 0.2579], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,117][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1249, 0.1226, 0.2192, 0.2713, 0.1078, 0.1542], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,118][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.1122, 0.1442, 0.1013, 0.1004, 0.3328, 0.2091], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,118][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1425, 0.0314, 0.0463, 0.0787, 0.3741, 0.3271], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,118][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1112, 0.2058, 0.2679, 0.2556, 0.0882, 0.0713], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,119][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0541, 0.2052, 0.0675, 0.1962, 0.0498, 0.4272], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,119][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.0617, 0.1042, 0.1056, 0.1198, 0.1930, 0.1898, 0.2260],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,119][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.2195, 0.0264, 0.0418, 0.1602, 0.0832, 0.2828, 0.1862],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,120][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0713, 0.1740, 0.0512, 0.0846, 0.1270, 0.2995, 0.1924],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,120][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.2166, 0.1263, 0.2854, 0.1714, 0.0621, 0.0656, 0.0725],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,124][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.0156, 0.1651, 0.0904, 0.2722, 0.0486, 0.1792, 0.2290],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,132][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.0308, 0.2242, 0.1514, 0.0894, 0.0671, 0.1045, 0.3326],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,138][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.0439, 0.0573, 0.0718, 0.2507, 0.0612, 0.1882, 0.3268],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,144][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.1012, 0.0926, 0.1236, 0.3037, 0.0718, 0.1606, 0.1465],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,145][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0686, 0.0965, 0.0637, 0.0612, 0.2759, 0.3254, 0.1087],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,145][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.0637, 0.0245, 0.0391, 0.0438, 0.2274, 0.2668, 0.3348],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,145][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.1086, 0.1955, 0.1733, 0.2652, 0.0507, 0.0574, 0.1492],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,146][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0314, 0.1214, 0.0273, 0.1386, 0.0364, 0.3584, 0.2866],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,147][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:07:00,149][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[1020],
        [   2],
        [  12],
        [   1],
        [   1],
        [   1],
        [   3]], device='cuda:0')
[2024-07-23 21:07:00,152][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[1668],
        [  15],
        [  45],
        [   2],
        [   1],
        [   1],
        [  48]], device='cuda:0')
[2024-07-23 21:07:00,155][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[5910],
        [7090],
        [5902],
        [6238],
        [4573],
        [3414],
        [3822]], device='cuda:0')
[2024-07-23 21:07:00,158][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[1552],
        [1713],
        [1779],
        [ 883],
        [ 815],
        [1561],
        [1369]], device='cuda:0')
[2024-07-23 21:07:00,161][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[4957],
        [2696],
        [3023],
        [3036],
        [3786],
        [4322],
        [4153]], device='cuda:0')
[2024-07-23 21:07:00,164][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[14872],
        [12512],
        [10183],
        [ 8922],
        [ 7976],
        [ 8462],
        [ 8920]], device='cuda:0')
[2024-07-23 21:07:00,167][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[  921],
        [12372],
        [10079],
        [ 8523],
        [ 7944],
        [ 5925],
        [ 5635]], device='cuda:0')
[2024-07-23 21:07:00,170][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[7497],
        [2936],
        [2249],
        [1906],
        [1918],
        [1614],
        [1756]], device='cuda:0')
[2024-07-23 21:07:00,173][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[4677],
        [4016],
        [2759],
        [3195],
        [3265],
        [3370],
        [3903]], device='cuda:0')
[2024-07-23 21:07:00,176][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[3907],
        [3623],
        [3466],
        [3924],
        [4196],
        [4497],
        [4146]], device='cuda:0')
[2024-07-23 21:07:00,176][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[3614],
        [5195],
        [5665],
        [5828],
        [4529],
        [4721],
        [4482]], device='cuda:0')
[2024-07-23 21:07:00,177][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[3701],
        [4731],
        [3595],
        [3397],
        [3462],
        [3873],
        [4084]], device='cuda:0')
[2024-07-23 21:07:00,178][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[5181],
        [2738],
        [3968],
        [4567],
        [5001],
        [4662],
        [3904]], device='cuda:0')
[2024-07-23 21:07:00,178][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[15386],
        [ 6384],
        [ 6829],
        [10054],
        [11012],
        [ 6614],
        [13905]], device='cuda:0')
[2024-07-23 21:07:00,179][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[34],
        [46],
        [37],
        [31],
        [11],
        [36],
        [ 8]], device='cuda:0')
[2024-07-23 21:07:00,180][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[36],
        [ 3],
        [ 2],
        [ 2],
        [ 4],
        [23],
        [ 9]], device='cuda:0')
[2024-07-23 21:07:00,183][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[223],
        [280],
        [272],
        [103],
        [ 43],
        [162],
        [ 53]], device='cuda:0')
[2024-07-23 21:07:00,186][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 708],
        [1770],
        [1327],
        [2015],
        [1543],
        [ 198],
        [ 240]], device='cuda:0')
[2024-07-23 21:07:00,189][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 4],
        [ 7],
        [19],
        [26],
        [37],
        [30],
        [24]], device='cuda:0')
[2024-07-23 21:07:00,192][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 341],
        [2731],
        [1843],
        [ 279],
        [ 245],
        [ 250],
        [ 220]], device='cuda:0')
[2024-07-23 21:07:00,195][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 82],
        [ 51],
        [108],
        [ 79],
        [116],
        [332],
        [134]], device='cuda:0')
[2024-07-23 21:07:00,198][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 763],
        [6741],
        [2585],
        [1321],
        [1237],
        [1250],
        [ 729]], device='cuda:0')
[2024-07-23 21:07:00,201][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 7],
        [ 6],
        [42],
        [39],
        [11],
        [ 7],
        [ 4]], device='cuda:0')
[2024-07-23 21:07:00,204][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[4187],
        [ 412],
        [1014],
        [ 535],
        [1211],
        [1917],
        [2319]], device='cuda:0')
[2024-07-23 21:07:00,207][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 522],
        [6872],
        [8419],
        [5166],
        [5565],
        [4556],
        [3653]], device='cuda:0')
[2024-07-23 21:07:00,208][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[176],
        [343],
        [ 64],
        [ 71],
        [ 62],
        [ 76],
        [103]], device='cuda:0')
[2024-07-23 21:07:00,209][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[133],
        [818],
        [704],
        [486],
        [540],
        [552],
        [210]], device='cuda:0')
[2024-07-23 21:07:00,209][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[45138],
        [44570],
        [43267],
        [48171],
        [47920],
        [46923],
        [48574]], device='cuda:0')
[2024-07-23 21:07:00,210][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[46972],
        [32479],
        [34390],
        [38291],
        [43490],
        [41352],
        [46995]], device='cuda:0')
[2024-07-23 21:07:00,211][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[25],
        [25],
        [25],
        [25],
        [25],
        [25],
        [25]], device='cuda:0')
[2024-07-23 21:07:00,259][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:07:00,264][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,268][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,272][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,273][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,273][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,273][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,273][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,274][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,274][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,274][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,275][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,275][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,275][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.7873, 0.2127], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,278][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.9514, 0.0486], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,281][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.3214, 0.6786], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,286][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.1592, 0.8408], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,293][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.5270, 0.4730], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,299][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.7184, 0.2816], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,301][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [lf] are: tensor([0.5828, 0.4172], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,301][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.2078, 0.7922], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,302][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [lf] are: tensor([0.3207, 0.6793], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,302][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.7627, 0.2373], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,302][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.5071, 0.4929], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,303][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.5563, 0.4437], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,303][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.5555, 0.1137, 0.3308], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,303][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.9568, 0.0373, 0.0059], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,304][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.1755, 0.2824, 0.5421], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,306][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.1211, 0.5489, 0.3300], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,313][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.3856, 0.3476, 0.2667], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,319][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.5970, 0.1823, 0.2207], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,326][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [red] are: tensor([0.4993, 0.3186, 0.1821], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,331][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.2202, 0.4449, 0.3349], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,331][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [red] are: tensor([0.2191, 0.3422, 0.4387], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,332][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.4878, 0.1312, 0.3809], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,332][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.2982, 0.2300, 0.4718], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,332][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.4109, 0.3678, 0.2213], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,333][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.4545, 0.1318, 0.2796, 0.1340], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,333][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.7235, 0.0317, 0.0071, 0.2377], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,333][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.0860, 0.1880, 0.3734, 0.3526], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,334][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.1573, 0.3729, 0.3367, 0.1331], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,335][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.2890, 0.3202, 0.2612, 0.1296], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,341][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.3982, 0.1957, 0.2110, 0.1950], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,348][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([0.3126, 0.1639, 0.1188, 0.4047], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,354][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.1380, 0.3094, 0.1838, 0.3687], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,358][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([0.1158, 0.3570, 0.2241, 0.3031], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,359][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.2557, 0.0925, 0.2596, 0.3922], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,359][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.2441, 0.1336, 0.3517, 0.2706], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,359][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.3656, 0.2935, 0.1965, 0.1443], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,360][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.3206, 0.0683, 0.2206, 0.1469, 0.2437], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,360][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.4655, 0.0311, 0.0079, 0.3798, 0.1157], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,360][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.0784, 0.1436, 0.3057, 0.2740, 0.1982], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,361][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.1168, 0.2106, 0.2103, 0.1031, 0.3592], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,361][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.1852, 0.1871, 0.2319, 0.1602, 0.2356], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,365][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.2585, 0.2671, 0.2405, 0.1750, 0.0589], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,371][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([0.1749, 0.1242, 0.1592, 0.3881, 0.1535], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,378][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.2028, 0.2348, 0.2350, 0.1941, 0.1332], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,384][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([0.1589, 0.3309, 0.1697, 0.1802, 0.1603], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,386][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.2553, 0.0752, 0.2684, 0.3383, 0.0629], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,386][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.1448, 0.1629, 0.3521, 0.2904, 0.0497], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,386][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.1580, 0.2313, 0.1929, 0.2250, 0.1929], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,387][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.2287, 0.0562, 0.1957, 0.1305, 0.1961, 0.1927], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,387][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.4685, 0.0295, 0.0063, 0.2413, 0.1010, 0.1534], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,388][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.0528, 0.1133, 0.1751, 0.1667, 0.0908, 0.4012], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,388][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.0609, 0.2327, 0.0973, 0.0624, 0.3330, 0.2139], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,388][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.1974, 0.2067, 0.1506, 0.1255, 0.1889, 0.1310], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,389][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.2661, 0.1700, 0.2168, 0.2038, 0.0493, 0.0940], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,395][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([0.1552, 0.0728, 0.0799, 0.3209, 0.1139, 0.2573], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,401][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1642, 0.2043, 0.1565, 0.2169, 0.0855, 0.1725], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,408][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([0.0945, 0.2375, 0.1210, 0.1000, 0.0979, 0.3490], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,413][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1730, 0.0726, 0.1807, 0.3453, 0.0498, 0.1785], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,413][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1568, 0.1285, 0.2780, 0.3105, 0.0566, 0.0696], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,414][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.1274, 0.1451, 0.1152, 0.1568, 0.1383, 0.3172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,414][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.1973, 0.0644, 0.1132, 0.0734, 0.1611, 0.2156, 0.1750],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,414][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.3687, 0.0154, 0.0033, 0.2220, 0.0753, 0.1542, 0.1610],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,415][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.0377, 0.0572, 0.0856, 0.0873, 0.0724, 0.2824, 0.3774],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,415][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.0424, 0.1506, 0.0951, 0.0546, 0.1952, 0.1706, 0.2914],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,415][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.1296, 0.1431, 0.0898, 0.0516, 0.1180, 0.2293, 0.2386],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,416][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.2876, 0.1386, 0.1393, 0.2042, 0.0365, 0.0817, 0.1120],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,419][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ on] are: tensor([0.1093, 0.0848, 0.0585, 0.2158, 0.0619, 0.2155, 0.2542],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,420][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.1397, 0.2350, 0.1481, 0.1864, 0.0533, 0.0883, 0.1491],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,420][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ on] are: tensor([0.0820, 0.1735, 0.1156, 0.1070, 0.0800, 0.2415, 0.2004],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,426][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.2307, 0.0677, 0.1604, 0.2753, 0.0533, 0.1749, 0.0377],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,432][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.1245, 0.1247, 0.2487, 0.2384, 0.0417, 0.0563, 0.1657],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,439][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.1729, 0.1680, 0.0580, 0.0426, 0.1263, 0.1910, 0.2413],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,487][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:07:00,489][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,489][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,489][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,490][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,490][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,490][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,490][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,491][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,491][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,491][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,492][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,492][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,492][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.7873, 0.2127], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,493][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.9514, 0.0486], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,496][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.3214, 0.6786], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,501][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.1592, 0.8408], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,508][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.5270, 0.4730], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,511][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.7184, 0.2816], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,511][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([0.5828, 0.4172], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,511][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.2078, 0.7922], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,512][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([0.3207, 0.6793], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,512][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.7627, 0.2373], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,512][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.5071, 0.4929], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,513][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.5563, 0.4437], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,513][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.5555, 0.1137, 0.3308], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,513][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.9568, 0.0373, 0.0059], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,514][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.1755, 0.2824, 0.5421], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,520][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.1211, 0.5489, 0.3300], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,526][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.3856, 0.3476, 0.2667], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,533][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.5970, 0.1823, 0.2207], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,538][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([0.4993, 0.3186, 0.1821], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,538][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.2202, 0.4449, 0.3349], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,538][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([0.2191, 0.3422, 0.4387], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,539][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.4878, 0.1312, 0.3809], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,539][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.2982, 0.2300, 0.4718], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,539][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.4109, 0.3678, 0.2213], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,540][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.4545, 0.1318, 0.2796, 0.1340], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,540][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.7235, 0.0317, 0.0071, 0.2377], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,541][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.0860, 0.1880, 0.3734, 0.3526], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,542][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.1573, 0.3729, 0.3367, 0.1331], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,548][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.2890, 0.3202, 0.2612, 0.1296], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,555][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.3982, 0.1957, 0.2110, 0.1950], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,561][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([0.3126, 0.1639, 0.1188, 0.4047], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,565][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.1380, 0.3094, 0.1838, 0.3687], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,566][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([0.1158, 0.3570, 0.2241, 0.3031], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,566][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.2557, 0.0925, 0.2596, 0.3922], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,566][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.2441, 0.1336, 0.3517, 0.2706], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,567][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.3656, 0.2935, 0.1965, 0.1443], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,567][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.3206, 0.0683, 0.2206, 0.1469, 0.2437], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,568][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.4655, 0.0311, 0.0079, 0.3798, 0.1157], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,568][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.0784, 0.1436, 0.3057, 0.2740, 0.1982], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,568][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.1168, 0.2106, 0.2103, 0.1031, 0.3592], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,572][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.1852, 0.1871, 0.2319, 0.1602, 0.2356], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,578][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.2585, 0.2671, 0.2405, 0.1750, 0.0589], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,585][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([0.1749, 0.1242, 0.1592, 0.3881, 0.1535], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,591][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.2028, 0.2348, 0.2350, 0.1941, 0.1332], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,593][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([0.1589, 0.3309, 0.1697, 0.1802, 0.1603], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,593][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.2553, 0.0752, 0.2684, 0.3383, 0.0629], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,594][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.1448, 0.1629, 0.3521, 0.2904, 0.0497], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,594][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.1580, 0.2313, 0.1929, 0.2250, 0.1929], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,595][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.2287, 0.0562, 0.1957, 0.1305, 0.1961, 0.1927], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,595][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.4685, 0.0295, 0.0063, 0.2413, 0.1010, 0.1534], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,595][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.0528, 0.1133, 0.1751, 0.1667, 0.0908, 0.4012], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,596][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.0609, 0.2327, 0.0973, 0.0624, 0.3330, 0.2139], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,596][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.1974, 0.2067, 0.1506, 0.1255, 0.1889, 0.1310], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,600][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.2661, 0.1700, 0.2168, 0.2038, 0.0493, 0.0940], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,608][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([0.1552, 0.0728, 0.0799, 0.3209, 0.1139, 0.2573], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,614][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1642, 0.2043, 0.1565, 0.2169, 0.0855, 0.1725], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,620][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([0.0945, 0.2375, 0.1210, 0.1000, 0.0979, 0.3490], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,621][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.1730, 0.0726, 0.1807, 0.3453, 0.0498, 0.1785], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,621][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.1568, 0.1285, 0.2780, 0.3105, 0.0566, 0.0696], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,622][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.1274, 0.1451, 0.1152, 0.1568, 0.1383, 0.3172], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,622][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.1973, 0.0644, 0.1132, 0.0734, 0.1611, 0.2156, 0.1750],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,622][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.3687, 0.0154, 0.0033, 0.2220, 0.0753, 0.1542, 0.1610],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,623][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.0377, 0.0572, 0.0856, 0.0873, 0.0724, 0.2824, 0.3774],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,623][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.0424, 0.1506, 0.0951, 0.0546, 0.1952, 0.1706, 0.2914],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,623][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.1296, 0.1431, 0.0898, 0.0516, 0.1180, 0.2293, 0.2386],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,627][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.2876, 0.1386, 0.1393, 0.2042, 0.0365, 0.0817, 0.1120],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,633][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([0.1093, 0.0848, 0.0585, 0.2158, 0.0619, 0.2155, 0.2542],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,640][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.1397, 0.2350, 0.1481, 0.1864, 0.0533, 0.0883, 0.1491],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,647][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([0.0820, 0.1735, 0.1156, 0.1070, 0.0800, 0.2415, 0.2004],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,648][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.2307, 0.0677, 0.1604, 0.2753, 0.0533, 0.1749, 0.0377],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,649][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.1245, 0.1247, 0.2487, 0.2384, 0.0417, 0.0563, 0.1657],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,649][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.1729, 0.1680, 0.0580, 0.0426, 0.1263, 0.1910, 0.2413],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,650][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:07:00,651][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[161],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:07:00,652][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[499],
        [  3],
        [  7],
        [  1],
        [  1],
        [  1],
        [  6]], device='cuda:0')
[2024-07-23 21:07:00,655][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[3389],
        [3436],
        [3537],
        [3687],
        [3587],
        [3713],
        [3701]], device='cuda:0')
[2024-07-23 21:07:00,658][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[4774],
        [4822],
        [4844],
        [4969],
        [5055],
        [5191],
        [5439]], device='cuda:0')
[2024-07-23 21:07:00,661][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 8718],
        [11455],
        [13173],
        [11049],
        [10205],
        [ 9167],
        [ 9072]], device='cuda:0')
[2024-07-23 21:07:00,664][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[1709],
        [1452],
        [1369],
        [1525],
        [1697],
        [1516],
        [1495]], device='cuda:0')
[2024-07-23 21:07:00,667][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 74],
        [310],
        [259],
        [477],
        [294],
        [394],
        [455]], device='cuda:0')
[2024-07-23 21:07:00,671][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[19091],
        [13128],
        [10233],
        [ 9696],
        [ 7803],
        [ 8920],
        [10707]], device='cuda:0')
[2024-07-23 21:07:00,674][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[4972],
        [4783],
        [4731],
        [4530],
        [4041],
        [3748],
        [4284]], device='cuda:0')
[2024-07-23 21:07:00,677][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[1135],
        [2422],
        [2116],
        [2923],
        [2230],
        [2665],
        [2392]], device='cuda:0')
[2024-07-23 21:07:00,679][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[11920],
        [ 5316],
        [ 4092],
        [ 1726],
        [ 2720],
        [ 1107],
        [  838]], device='cuda:0')
[2024-07-23 21:07:00,680][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 5558],
        [ 7940],
        [ 4623],
        [ 9255],
        [ 8844],
        [10657],
        [10629]], device='cuda:0')
[2024-07-23 21:07:00,681][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 8],
        [14],
        [16],
        [10],
        [14],
        [ 9],
        [ 8]], device='cuda:0')
[2024-07-23 21:07:00,681][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[1247],
        [1221],
        [ 896],
        [ 939],
        [ 999],
        [1122],
        [1158]], device='cuda:0')
[2024-07-23 21:07:00,682][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[10],
        [ 9],
        [ 9],
        [ 8],
        [ 8],
        [10],
        [ 8]], device='cuda:0')
[2024-07-23 21:07:00,683][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 26],
        [ 29],
        [318],
        [196],
        [121],
        [ 47],
        [ 36]], device='cuda:0')
[2024-07-23 21:07:00,684][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[81],
        [69],
        [74],
        [32],
        [16],
        [18],
        [11]], device='cuda:0')
[2024-07-23 21:07:00,687][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[14],
        [37],
        [23],
        [69],
        [36],
        [ 9],
        [10]], device='cuda:0')
[2024-07-23 21:07:00,690][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[  47],
        [ 268],
        [1004],
        [ 827],
        [ 847],
        [ 629],
        [ 984]], device='cuda:0')
[2024-07-23 21:07:00,693][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[17],
        [36],
        [26],
        [40],
        [30],
        [34],
        [34]], device='cuda:0')
[2024-07-23 21:07:00,696][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 55],
        [ 49],
        [102],
        [106],
        [116],
        [ 94],
        [ 91]], device='cuda:0')
[2024-07-23 21:07:00,699][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 5],
        [ 6],
        [ 3],
        [ 5],
        [14],
        [19],
        [26]], device='cuda:0')
[2024-07-23 21:07:00,702][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[39],
        [85],
        [29],
        [20],
        [20],
        [18],
        [16]], device='cuda:0')
[2024-07-23 21:07:00,705][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[  92],
        [  48],
        [ 594],
        [1861],
        [ 563],
        [ 488],
        [ 403]], device='cuda:0')
[2024-07-23 21:07:00,708][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[589],
        [392],
        [192],
        [138],
        [165],
        [102],
        [108]], device='cuda:0')
[2024-07-23 21:07:00,711][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 88],
        [ 57],
        [ 68],
        [125],
        [116],
        [115],
        [ 98]], device='cuda:0')
[2024-07-23 21:07:00,712][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[ 54],
        [ 95],
        [134],
        [239],
        [419],
        [182],
        [246]], device='cuda:0')
[2024-07-23 21:07:00,712][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[2440],
        [6442],
        [5345],
        [3532],
        [3618],
        [6743],
        [7756]], device='cuda:0')
[2024-07-23 21:07:00,713][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[13418],
        [14552],
        [17502],
        [20687],
        [14482],
        [15598],
        [12444]], device='cuda:0')
[2024-07-23 21:07:00,714][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[24],
        [24],
        [24],
        [24],
        [24],
        [24],
        [24]], device='cuda:0')
[2024-07-23 21:07:00,765][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:07:00,765][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,766][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,766][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,766][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,766][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,767][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,767][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,767][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,767][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,770][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,770][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,770][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,771][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [lf] are: tensor([0.1818, 0.8182], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,771][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [lf] are: tensor([0.9893, 0.0107], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,771][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [lf] are: tensor([0.9944, 0.0056], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,772][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [lf] are: tensor([0.9784, 0.0216], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,772][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [lf] are: tensor([0.9802, 0.0198], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,772][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [lf] are: tensor([0.3125, 0.6875], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,773][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [lf] are: tensor([9.9996e-01, 4.1606e-05], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,773][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [lf] are: tensor([0.2492, 0.7508], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,773][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [lf] are: tensor([1.0000e+00, 4.7188e-13], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,774][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [lf] are: tensor([0.9323, 0.0677], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,774][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [lf] are: tensor([0.0231, 0.9769], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,774][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [lf] are: tensor([0.6941, 0.3059], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,775][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [red] are: tensor([0.0737, 0.7402, 0.1860], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,775][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [red] are: tensor([0.9421, 0.0205, 0.0374], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,775][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [red] are: tensor([0.8650, 0.0073, 0.1277], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,775][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [red] are: tensor([0.9278, 0.0248, 0.0473], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,776][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [red] are: tensor([0.9772, 0.0130, 0.0099], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,776][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [red] are: tensor([0.2537, 0.4330, 0.3133], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,778][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [red] are: tensor([9.9974e-01, 6.5391e-05, 1.9368e-04], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,784][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [red] are: tensor([0.0705, 0.4714, 0.4581], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,790][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [red] are: tensor([1.0000e+00, 5.4208e-14, 8.6398e-17], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,792][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [red] are: tensor([0.9107, 0.0732, 0.0161], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,792][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [red] are: tensor([0.0220, 0.8332, 0.1449], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,793][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [red] are: tensor([0.2107, 0.1894, 0.5998], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,793][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Hitchcock] are: tensor([0.0484, 0.3143, 0.0648, 0.5726], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,793][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Hitchcock] are: tensor([0.9015, 0.0198, 0.0313, 0.0473], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,794][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Hitchcock] are: tensor([0.7386, 0.0047, 0.2526, 0.0041], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,794][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Hitchcock] are: tensor([0.5480, 0.0556, 0.0835, 0.3129], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,794][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Hitchcock] are: tensor([0.9437, 0.0287, 0.0120, 0.0156], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,795][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Hitchcock] are: tensor([0.1306, 0.3394, 0.2840, 0.2460], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,796][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Hitchcock] are: tensor([9.9916e-01, 7.8032e-05, 6.1907e-04, 1.4125e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,802][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Hitchcock] are: tensor([0.0533, 0.3946, 0.5004, 0.0516], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,807][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Hitchcock] are: tensor([1.0000e+00, 5.0988e-14, 1.1380e-16, 3.5147e-18], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,814][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Hitchcock] are: tensor([0.7479, 0.0369, 0.0237, 0.1915], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,819][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Hitchcock] are: tensor([0.0308, 0.5297, 0.2169, 0.2226], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,820][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Hitchcock] are: tensor([0.0161, 0.0209, 0.1461, 0.8169], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:00,820][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Presents] are: tensor([0.0587, 0.3393, 0.0690, 0.5104, 0.0225], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,820][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Presents] are: tensor([0.8054, 0.0206, 0.0336, 0.0581, 0.0824], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,821][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Presents] are: tensor([0.7589, 0.0067, 0.2152, 0.0157, 0.0036], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,821][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Presents] are: tensor([0.5998, 0.0283, 0.0798, 0.2583, 0.0338], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,821][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Presents] are: tensor([0.9084, 0.0168, 0.0080, 0.0050, 0.0617], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,822][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Presents] are: tensor([0.0926, 0.2397, 0.1474, 0.2127, 0.3077], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,822][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Presents] are: tensor([9.9341e-01, 1.7670e-04, 6.3139e-04, 3.5725e-04, 5.4272e-03],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,826][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Presents] are: tensor([0.1467, 0.3498, 0.4353, 0.0421, 0.0261], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,831][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Presents] are: tensor([1.0000e+00, 2.4592e-17, 3.2114e-21, 1.4301e-23, 6.1937e-30],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,837][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Presents] are: tensor([0.0568, 0.0118, 0.0057, 0.0917, 0.8340], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,844][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Presents] are: tensor([0.0496, 0.6157, 0.1764, 0.1494, 0.0089], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,847][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Presents] are: tensor([0.0402, 0.0217, 0.2194, 0.7060, 0.0126], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:00,847][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ debuted] are: tensor([0.0397, 0.4185, 0.0886, 0.4214, 0.0157, 0.0161], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,847][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ debuted] are: tensor([0.7325, 0.0394, 0.0482, 0.0973, 0.0791, 0.0036], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,848][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ debuted] are: tensor([0.8053, 0.0080, 0.1537, 0.0286, 0.0035, 0.0010], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,848][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ debuted] are: tensor([0.4752, 0.0232, 0.0368, 0.3649, 0.0197, 0.0803], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,848][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ debuted] are: tensor([0.9109, 0.0185, 0.0116, 0.0075, 0.0473, 0.0043], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,849][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ debuted] are: tensor([0.0585, 0.0916, 0.0344, 0.1035, 0.0865, 0.6255], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,849][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ debuted] are: tensor([9.9261e-01, 2.9094e-04, 1.1983e-03, 6.8388e-04, 5.1392e-03, 7.2997e-05],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,849][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ debuted] are: tensor([0.1621, 0.4065, 0.3431, 0.0469, 0.0231, 0.0183], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,853][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ debuted] are: tensor([1.0000e+00, 1.2966e-14, 1.0360e-17, 2.1025e-19, 1.3284e-24, 5.8541e-19],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,859][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ debuted] are: tensor([0.1120, 0.0153, 0.0038, 0.0532, 0.8081, 0.0076], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,866][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ debuted] are: tensor([0.1139, 0.5776, 0.1284, 0.1292, 0.0045, 0.0465], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,872][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ debuted] are: tensor([0.0530, 0.0214, 0.1493, 0.7559, 0.0195, 0.0010], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:00,874][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ on] are: tensor([0.0381, 0.2184, 0.0851, 0.3438, 0.0065, 0.0054, 0.3027],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,874][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ on] are: tensor([0.7660, 0.0296, 0.0328, 0.0691, 0.0911, 0.0032, 0.0082],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,875][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ on] are: tensor([0.6993, 0.0139, 0.2686, 0.0116, 0.0044, 0.0014, 0.0008],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,875][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ on] are: tensor([0.5396, 0.0097, 0.0210, 0.2406, 0.0192, 0.1682, 0.0017],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,875][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ on] are: tensor([0.8870, 0.0180, 0.0085, 0.0060, 0.0702, 0.0037, 0.0066],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,876][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ on] are: tensor([0.0234, 0.0827, 0.0390, 0.0564, 0.0892, 0.6839, 0.0254],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,876][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ on] are: tensor([9.9362e-01, 1.7925e-04, 8.3999e-04, 3.3775e-04, 4.9387e-03, 4.1216e-05,
        3.9244e-05], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,876][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ on] are: tensor([0.1533, 0.3043, 0.4064, 0.0460, 0.0331, 0.0122, 0.0447],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,878][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ on] are: tensor([1.0000e+00, 9.0754e-13, 2.6085e-15, 1.6654e-16, 1.3399e-20, 2.3403e-16,
        1.3327e-09], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,884][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ on] are: tensor([0.0776, 0.0057, 0.0024, 0.0346, 0.8465, 0.0043, 0.0290],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,891][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ on] are: tensor([0.0334, 0.4424, 0.1167, 0.1144, 0.0017, 0.0114, 0.2800],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,896][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ on] are: tensor([0.0918, 0.0467, 0.1465, 0.6961, 0.0156, 0.0008, 0.0024],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:00,949][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:07:00,953][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,953][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,953][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,954][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,954][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,954][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,954][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,955][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,955][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,955][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,956][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,956][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [A] are: tensor([1.], device='cuda:0') for source tokens [A]
[2024-07-23 21:07:00,956][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [lf] are: tensor([0.1818, 0.8182], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,957][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [lf] are: tensor([0.9893, 0.0107], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,960][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [lf] are: tensor([0.9944, 0.0056], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,967][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [lf] are: tensor([0.9784, 0.0216], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,972][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [lf] are: tensor([0.9802, 0.0198], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,973][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [lf] are: tensor([0.3125, 0.6875], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,973][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [lf] are: tensor([9.9996e-01, 4.1606e-05], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,973][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [lf] are: tensor([0.2492, 0.7508], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,974][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [lf] are: tensor([1.0000e+00, 1.3427e-13], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,974][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [lf] are: tensor([0.8292, 0.1708], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,974][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [lf] are: tensor([0.0248, 0.9752], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,975][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [lf] are: tensor([0.6941, 0.3059], device='cuda:0') for source tokens [Alf]
[2024-07-23 21:07:00,975][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [red] are: tensor([0.0737, 0.7402, 0.1860], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,975][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [red] are: tensor([0.9421, 0.0205, 0.0374], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,979][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [red] are: tensor([0.8650, 0.0073, 0.1277], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,986][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [red] are: tensor([0.9278, 0.0248, 0.0473], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,993][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [red] are: tensor([0.9772, 0.0130, 0.0099], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:00,999][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [red] are: tensor([0.2537, 0.4330, 0.3133], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,001][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [red] are: tensor([9.9974e-01, 6.5391e-05, 1.9368e-04], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,001][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [red] are: tensor([0.0705, 0.4714, 0.4581], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,002][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [red] are: tensor([1.0000e+00, 2.5679e-12, 3.5999e-14], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,002][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [red] are: tensor([0.8841, 0.1117, 0.0042], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,002][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [red] are: tensor([0.0239, 0.8321, 0.1439], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,003][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [red] are: tensor([0.2107, 0.1894, 0.5998], device='cuda:0') for source tokens [Alfred]
[2024-07-23 21:07:01,003][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Hitchcock] are: tensor([0.0484, 0.3143, 0.0648, 0.5726], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,003][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Hitchcock] are: tensor([0.9015, 0.0198, 0.0313, 0.0473], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,004][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Hitchcock] are: tensor([0.7386, 0.0047, 0.2526, 0.0041], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,004][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Hitchcock] are: tensor([0.5480, 0.0556, 0.0835, 0.3129], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,011][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Hitchcock] are: tensor([0.9437, 0.0287, 0.0120, 0.0156], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,017][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Hitchcock] are: tensor([0.1306, 0.3394, 0.2840, 0.2460], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,022][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Hitchcock] are: tensor([9.9916e-01, 7.8032e-05, 6.1907e-04, 1.4125e-04], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,028][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Hitchcock] are: tensor([0.0533, 0.3946, 0.5004, 0.0516], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,030][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Hitchcock] are: tensor([1.0000e+00, 5.6074e-11, 3.1997e-12, 1.6283e-07], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,030][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Hitchcock] are: tensor([0.6663, 0.0558, 0.0055, 0.2725], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,030][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Hitchcock] are: tensor([0.0307, 0.5443, 0.1963, 0.2287], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,031][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Hitchcock] are: tensor([0.0161, 0.0209, 0.1461, 0.8169], device='cuda:0') for source tokens [Alfred Hitchcock]
[2024-07-23 21:07:01,031][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Presents] are: tensor([0.0587, 0.3393, 0.0690, 0.5104, 0.0225], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,031][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Presents] are: tensor([0.8054, 0.0206, 0.0336, 0.0581, 0.0824], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,032][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Presents] are: tensor([0.7589, 0.0067, 0.2152, 0.0157, 0.0036], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,032][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Presents] are: tensor([0.5998, 0.0283, 0.0798, 0.2583, 0.0338], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,032][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Presents] are: tensor([0.9084, 0.0168, 0.0080, 0.0050, 0.0617], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,036][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Presents] are: tensor([0.0926, 0.2397, 0.1474, 0.2127, 0.3077], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,040][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Presents] are: tensor([9.9341e-01, 1.7670e-04, 6.3139e-04, 3.5725e-04, 5.4272e-03],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,047][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Presents] are: tensor([0.1467, 0.3498, 0.4353, 0.0421, 0.0261], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,051][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Presents] are: tensor([1.0000e+00, 1.9174e-11, 7.4463e-13, 1.2850e-07, 1.3667e-06],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,058][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Presents] are: tensor([0.2105, 0.0503, 0.0054, 0.2806, 0.4532], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,058][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Presents] are: tensor([0.0513, 0.6053, 0.1698, 0.1630, 0.0105], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,059][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Presents] are: tensor([0.0402, 0.0217, 0.2194, 0.7060, 0.0126], device='cuda:0') for source tokens [Alfred Hitchcock Presents]
[2024-07-23 21:07:01,059][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ debuted] are: tensor([0.0397, 0.4185, 0.0886, 0.4214, 0.0157, 0.0161], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,059][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ debuted] are: tensor([0.7325, 0.0394, 0.0482, 0.0973, 0.0791, 0.0036], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,060][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ debuted] are: tensor([0.8053, 0.0080, 0.1537, 0.0286, 0.0035, 0.0010], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,060][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ debuted] are: tensor([0.4752, 0.0232, 0.0368, 0.3649, 0.0197, 0.0803], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,061][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ debuted] are: tensor([0.9109, 0.0185, 0.0116, 0.0075, 0.0473, 0.0043], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,061][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ debuted] are: tensor([0.0585, 0.0916, 0.0344, 0.1035, 0.0865, 0.6255], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,063][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ debuted] are: tensor([9.9261e-01, 2.9094e-04, 1.1983e-03, 6.8388e-04, 5.1392e-03, 7.2997e-05],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,070][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ debuted] are: tensor([0.1621, 0.4065, 0.3431, 0.0469, 0.0231, 0.0183], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,074][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ debuted] are: tensor([9.9934e-01, 5.6977e-10, 2.3581e-11, 1.2150e-06, 6.9412e-06, 6.5677e-04],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,081][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ debuted] are: tensor([0.3054, 0.0564, 0.0039, 0.2451, 0.3722, 0.0170], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,086][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ debuted] are: tensor([0.0959, 0.5891, 0.1278, 0.1430, 0.0052, 0.0391], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,087][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ debuted] are: tensor([0.0530, 0.0214, 0.1493, 0.7559, 0.0195, 0.0010], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted]
[2024-07-23 21:07:01,087][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ on] are: tensor([0.0381, 0.2184, 0.0851, 0.3438, 0.0065, 0.0054, 0.3027],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,087][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ on] are: tensor([0.7660, 0.0296, 0.0328, 0.0691, 0.0911, 0.0032, 0.0082],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,088][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ on] are: tensor([0.6993, 0.0139, 0.2686, 0.0116, 0.0044, 0.0014, 0.0008],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,088][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ on] are: tensor([0.5396, 0.0097, 0.0210, 0.2406, 0.0192, 0.1682, 0.0017],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,089][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ on] are: tensor([0.8870, 0.0180, 0.0085, 0.0060, 0.0702, 0.0037, 0.0066],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,089][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ on] are: tensor([0.0234, 0.0827, 0.0390, 0.0564, 0.0892, 0.6839, 0.0254],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,089][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ on] are: tensor([9.9362e-01, 1.7925e-04, 8.3999e-04, 3.3775e-04, 4.9387e-03, 4.1216e-05,
        3.9244e-05], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,094][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ on] are: tensor([0.1533, 0.3043, 0.4064, 0.0460, 0.0331, 0.0122, 0.0447],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,099][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ on] are: tensor([9.9969e-01, 2.5038e-11, 8.4293e-13, 1.2082e-07, 1.6246e-06, 3.0577e-04,
        1.4577e-09], device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,105][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ on] are: tensor([0.2476, 0.0401, 0.0019, 0.1809, 0.4683, 0.0176, 0.0436],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,112][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ on] are: tensor([0.0307, 0.4612, 0.1133, 0.1262, 0.0022, 0.0109, 0.2555],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,115][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ on] are: tensor([0.0918, 0.0467, 0.1465, 0.6961, 0.0156, 0.0008, 0.0024],
       device='cuda:0') for source tokens [Alfred Hitchcock Presents debuted on]
[2024-07-23 21:07:01,116][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:07:01,117][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[324],
        [  2],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:07:01,118][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[1438],
        [   3],
        [  16],
        [   2],
        [   2],
        [   3],
        [  39]], device='cuda:0')
[2024-07-23 21:07:01,118][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[2],
        [6],
        [6],
        [6],
        [6],
        [6],
        [5]], device='cuda:0')
[2024-07-23 21:07:01,120][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[2413],
        [2418],
        [2559],
        [2589],
        [2809],
        [2912],
        [2886]], device='cuda:0')
[2024-07-23 21:07:01,123][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[7209],
        [7236],
        [7627],
        [7905],
        [7849],
        [7708],
        [7954]], device='cuda:0')
[2024-07-23 21:07:01,126][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[2117],
        [2087],
        [2056],
        [2407],
        [2266],
        [2351],
        [1936]], device='cuda:0')
[2024-07-23 21:07:01,129][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[826],
        [884],
        [871],
        [944],
        [907],
        [907],
        [938]], device='cuda:0')
[2024-07-23 21:07:01,132][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[4640],
        [1839],
        [2590],
        [2026],
        [2527],
        [5087],
        [5370]], device='cuda:0')
[2024-07-23 21:07:01,135][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[4494],
        [4494],
        [4494],
        [4493],
        [4497],
        [4494],
        [4497]], device='cuda:0')
[2024-07-23 21:07:01,138][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[13894],
        [11992],
        [ 5587],
        [ 4748],
        [ 5484],
        [ 6473],
        [ 5296]], device='cuda:0')
[2024-07-23 21:07:01,142][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[1],
        [1],
        [1],
        [1],
        [1],
        [1],
        [1]], device='cuda:0')
[2024-07-23 21:07:01,145][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[5815],
        [5919],
        [5966],
        [5120],
        [4529],
        [4667],
        [4661]], device='cuda:0')
[2024-07-23 21:07:01,147][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[2340],
        [6046],
        [6061],
        [6316],
        [6135],
        [5676],
        [5657]], device='cuda:0')
[2024-07-23 21:07:01,148][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[2339],
        [3106],
        [ 796],
        [1871],
        [1602],
        [1921],
        [2028]], device='cuda:0')
[2024-07-23 21:07:01,149][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[13],
        [ 7],
        [16],
        [ 7],
        [ 9],
        [ 8],
        [ 1]], device='cuda:0')
[2024-07-23 21:07:01,150][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[8],
        [7],
        [8],
        [4],
        [4],
        [5],
        [4]], device='cuda:0')
[2024-07-23 21:07:01,150][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[29],
        [28],
        [28],
        [30],
        [28],
        [28],
        [27]], device='cuda:0')
[2024-07-23 21:07:01,151][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[15],
        [15],
        [15],
        [14],
        [14],
        [15],
        [14]], device='cuda:0')
[2024-07-23 21:07:01,153][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[11],
        [11],
        [ 9],
        [ 7],
        [ 7],
        [ 7],
        [ 7]], device='cuda:0')
[2024-07-23 21:07:01,156][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[2],
        [2],
        [2],
        [2],
        [2],
        [2],
        [2]], device='cuda:0')
[2024-07-23 21:07:01,159][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[15],
        [30],
        [12],
        [ 9],
        [11],
        [ 7],
        [ 7]], device='cuda:0')
[2024-07-23 21:07:01,162][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[17],
        [17],
        [17],
        [17],
        [17],
        [17],
        [17]], device='cuda:0')
[2024-07-23 21:07:01,166][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[25],
        [ 8],
        [13],
        [14],
        [15],
        [14],
        [17]], device='cuda:0')
[2024-07-23 21:07:01,169][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[148],
        [148],
        [148],
        [148],
        [148],
        [148],
        [148]], device='cuda:0')
[2024-07-23 21:07:01,172][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[18],
        [17],
        [17],
        [22],
        [17],
        [18],
        [17]], device='cuda:0')
[2024-07-23 21:07:01,175][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[12],
        [10],
        [10],
        [14],
        [13],
        [13],
        [16]], device='cuda:0')
[2024-07-23 21:07:01,178][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[19],
        [28],
        [37],
        [42],
        [43],
        [43],
        [43]], device='cuda:0')
[2024-07-23 21:07:01,180][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[15862],
        [19792],
        [16107],
        [18050],
        [16436],
        [17060],
        [17671]], device='cuda:0')
[2024-07-23 21:07:01,181][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[16907],
        [24319],
        [27569],
        [28316],
        [31229],
        [30492],
        [30689]], device='cuda:0')
[2024-07-23 21:07:01,182][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[38],
        [38],
        [38],
        [38],
        [38],
        [38],
        [38]], device='cuda:0')
